{% extends "base.html" %}

{% block title %}Transformer Architecture Deep Dive - {{ site_title }}{% endblock %}

{% block extra_css %}
<link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
{% endblock %}

{% block content %}
<main style="padding-top: 100px;">
    <section class="azbn-section">
        <div class="azbn-container">
            <h1 class="tutorial-main-title">Transformer Architecture Deep Dive</h1>
            <p class="tutorial-subtitle">
                Master the Transformer architecture that revolutionized NLP. 10 comprehensive chapters covering attention mechanisms, self-attention, multi-head attention, positional encoding, encoder-decoder architecture, and implementation details with extensive formulas, code examples, and visual explanations.
            </p>
            
            <div class="azbn-grid" style="grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 2rem;">
                <div class="azbn-card">
                    <h3>Chapter 1: The Attention Mechanism</h3>
                    <p><strong>Foundation of Modern NLP</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Why attention? Limitations of RNNs/LSTMs</li>
                        <li>Attention in sequence-to-sequence models</li>
                        <li>Query, Key, Value (QKV) concept</li>
                        <li>Attention scores and weights</li>
                        <li>Mathematical formulation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Attention</span>
                        <span class="azbn-tag">Foundation</span>
                        <span class="azbn-tag">Theory</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter1" class="azbn-btn" style="text-decoration: none;">Start Chapter 1 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 2: Self-Attention Mechanism</h3>
                    <p><strong>Attention is All You Need</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Self-attention vs attention</li>
                        <li>Computing attention within a sequence</li>
                        <li>Scaled dot-product attention formula</li>
                        <li>Understanding attention weights</li>
                        <li>Self-attention implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Self-Attention</span>
                        <span class="azbn-tag">QKV</span>
                        <span class="azbn-tag">Implementation</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter2" class="azbn-btn" style="text-decoration: none;">Start Chapter 2 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 3: Multi-Head Attention</h3>
                    <p><strong>Learning Multiple Relationships</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Why multiple attention heads?</li>
                        <li>Parallel attention computation</li>
                        <li>Head specialization (syntax, semantics, etc.)</li>
                        <li>Concatenation and linear projection</li>
                        <li>Multi-head attention implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Multi-Head</span>
                        <span class="azbn-tag">Parallel</span>
                        <span class="azbn-tag">Specialization</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter3" class="azbn-btn" style="text-decoration: none;">Start Chapter 3 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 4: Positional Encoding</h3>
                    <p><strong>Adding Order Information</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Why positional encoding is needed</li>
                        <li>Sinusoidal positional encoding</li>
                        <li>Learned positional embeddings</li>
                        <li>Position encoding formulas</li>
                        <li>Relative vs absolute positions</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Position</span>
                        <span class="azbn-tag">Encoding</span>
                        <span class="azbn-tag">Sinusoidal</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter4" class="azbn-btn" style="text-decoration: none;">Start Chapter 4 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 5: Feed-Forward Networks</h3>
                    <p><strong>Processing After Attention</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>FFN architecture in transformers</li>
                        <li>Two linear transformations</li>
                        <li>ReLU activation</li>
                        <li>Dimension expansion (4x rule)</li>
                        <li>FFN implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">FFN</span>
                        <span class="azbn-tag">Linear</span>
                        <span class="azbn-tag">ReLU</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter5" class="azbn-btn" style="text-decoration: none;">Start Chapter 5 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 6: Residual Connections & Layer Normalization</h3>
                    <p><strong>Stabilizing Deep Networks</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Residual (skip) connections</li>
                        <li>Why residuals help training</li>
                        <li>Layer normalization vs batch normalization</li>
                        <li>Pre-norm vs post-norm</li>
                        <li>Add & Norm implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Residual</span>
                        <span class="azbn-tag">Normalization</span>
                        <span class="azbn-tag">Stability</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter6" class="azbn-btn" style="text-decoration: none;">Start Chapter 6 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 7: Encoder Architecture</h3>
                    <p><strong>Understanding the Encoder Stack</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Encoder layer components</li>
                        <li>Stacking encoder layers</li>
                        <li>Information flow through layers</li>
                        <li>What each layer learns</li>
                        <li>Complete encoder implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Encoder</span>
                        <span class="azbn-tag">Stack</span>
                        <span class="azbn-tag">Layers</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter7" class="azbn-btn" style="text-decoration: none;">Start Chapter 7 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 8: Decoder Architecture</h3>
                    <p><strong>Generating Sequences</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Decoder layer components</li>
                        <li>Masked self-attention</li>
                        <li>Encoder-decoder attention</li>
                        <li>Causal masking explained</li>
                        <li>Complete decoder implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Decoder</span>
                        <span class="azbn-tag">Masking</span>
                        <span class="azbn-tag">Generation</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter8" class="azbn-btn" style="text-decoration: none;">Start Chapter 8 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 9: Complete Transformer Architecture</h3>
                    <p><strong>Putting It All Together</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Full encoder-decoder transformer</li>
                        <li>Input/output embeddings</li>
                        <li>End-to-end forward pass</li>
                        <li>Training the transformer</li>
                        <li>Complete PyTorch implementation</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Complete</span>
                        <span class="azbn-tag">Architecture</span>
                        <span class="azbn-tag">Implementation</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter9" class="azbn-btn" style="text-decoration: none;">Start Chapter 9 →</a>
                    </div>
                </div>

                <div class="azbn-card">
                    <h3>Chapter 10: Transformer Variants & Optimizations</h3>
                    <p><strong>Beyond the Original</strong></p>
                    <ul style="margin: 1rem 0;">
                        <li>Encoder-only models (BERT)</li>
                        <li>Decoder-only models (GPT)</li>
                        <li>Efficient transformers (Linformer, Performer)</li>
                        <li>Sparse attention patterns</li>
                        <li>Recent architectural improvements</li>
                    </ul>
                    <div class="azbn-skills">
                        <span class="azbn-tag">Variants</span>
                        <span class="azbn-tag">BERT</span>
                        <span class="azbn-tag">GPT</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="/tutorials/transformers/chapter10" class="azbn-btn" style="text-decoration: none;">Start Chapter 10 →</a>
                    </div>
                </div>
            </div>
        </div>
    </section>
</main>
{% endblock %}

