<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: Positional Encoding - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWAT2dVgYnHwpIK/NS" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 4: Positional Encoding</h1>
                <p class="chapter-subtitle">Adding Order Information</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="40"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn active">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn ">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn ">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand positional encoding fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Positional Encoding</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Position Matters</h3>
                            <p><strong>Transformers have no inherent sense of word order - self-attention treats all positions equally.</strong> Positional encoding adds information about the position of each token in the sequence, allowing the model to understand order-dependent relationships.</p>
                            
                            <p><strong>Think of positional encoding like page numbers in a book:</strong></p>
                            <ul>
                                <li><strong>Without page numbers:</strong> You know what's written, but not where it appears</li>
                                <li><strong>With page numbers:</strong> You know both content and position</li>
                                <li><strong>In transformers:</strong> Positional encoding tells the model "this word is at position 3"</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why Order Matters</h4>
                            <p><strong>Example sentences with same words, different order:</strong></p>
                            <ul>
                                <li>"Dog bites man" vs "Man bites dog" - completely different meanings!</li>
                                <li>"The cat sat on the mat" vs "The mat sat on the cat" - nonsensical without order</li>
                            </ul>
                            
                            <p><strong>Problem:</strong> Self-attention without positional encoding would treat these as identical!</p>
                            <p><strong>Solution:</strong> Add positional encoding to preserve order information</p>
                        </div>

                        <div class="explanation-box">
                            <h4>Two Types of Positional Encoding</h4>
                            
                            <div class="example-box">
                                <h5>1. Sinusoidal (Fixed) Positional Encoding</h5>
                                <p><strong>Used in original Transformer paper:</strong></p>
                                <ul>
                                    <li>Mathematical functions (sine and cosine) generate position encodings</li>
                                    <li>No learnable parameters</li>
                                    <li>Can extrapolate to longer sequences than seen during training</li>
                                    <li>Like a mathematical pattern that encodes position</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. Learned Positional Embeddings</h5>
                                <p><strong>Used in BERT and many modern models:</strong></p>
                                <ul>
                                    <li>Learnable parameters (like word embeddings)</li>
                                    <li>Model learns optimal position representations</li>
                                    <li>Fixed maximum sequence length</li>
                                    <li>Like learning a lookup table for positions</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>üîë Fundamental Ideas</h3>
                            <p>Detailed explanation of key concepts will be provided here with analogies, step-by-step breakdowns, and visual explanations.</p>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Sinusoidal Positional Encoding</h4>
                            
                            <div class="formula-display">
                                <strong>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</strong><br>
                                <strong>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Notation:</h5>
                                <ul>
                                    <li><strong>pos:</strong> Position in the sequence (0, 1, 2, ...)</li>
                                    <li><strong>i:</strong> Dimension index (0, 1, 2, ..., d_model/2 - 1)</li>
                                    <li><strong>d_model:</strong> Embedding dimension (e.g., 512)</li>
                                    <li><strong>2i:</strong> Even dimensions use sine</li>
                                    <li><strong>2i+1:</strong> Odd dimensions use cosine</li>
                                </ul>
                                
                                <h5>Why Sine and Cosine?</h5>
                                <ul>
                                    <li>Creates unique patterns for each position</li>
                                    <li>Relative positions can be computed: PE(pos+k) can be derived from PE(pos)</li>
                                    <li>Allows model to understand relative distances</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h4>Final Embedding</h4>
                            
                            <div class="formula-display">
                                <strong>Embedding = Token_Embedding + Positional_Encoding</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>How It Works:</h5>
                                <ul>
                                    <li>Token embedding: Semantic meaning of the word</li>
                                    <li>Positional encoding: Position information</li>
                                    <li>Addition: Combines both types of information</li>
                                    <li>Result: Each token has both semantic and positional information</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Step-by-Step Examples</h4>
                            <p>Comprehensive examples with detailed explanations will be provided here.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Sinusoidal Positional Encoding Implementation</h4>
                            <pre><code class="language-python">import numpy as np
import math

def get_positional_encoding(seq_len, d_model):
    """
    Generate sinusoidal positional encoding
    
    Parameters:
    seq_len: Maximum sequence length
    d_model: Embedding dimension
    
    Returns:
    Positional encoding matrix (seq_len, d_model)
    """
    pe = np.zeros((seq_len, d_model))
    
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            # Even dimensions: sine
            pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))
            
            # Odd dimensions: cosine
            if i + 1 < d_model:
                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))
    
    return pe

# Example: Generate positional encoding for sequence length 10, dimension 512
seq_len = 10
d_model = 512
positional_encoding = get_positional_encoding(seq_len, d_model)

print(f"Positional encoding shape: {positional_encoding.shape}")  # (10, 512)
print(f"First position encoding (first 10 dims): {positional_encoding[0, :10]}")
print(f"Second position encoding (first 10 dims): {positional_encoding[1, :10]}")

# Visualize: Each position has a unique encoding pattern
# Position 0: [0.0, 1.0, 0.0, 1.0, ...]
# Position 1: [0.841, 0.540, 0.002, 0.999, ...]
# Position 2: [0.909, -0.416, 0.004, 0.999, ...]</code></pre>
                        </div>

                        <div class="code-box">
                            <h4>Learned Positional Embeddings (BERT-style)</h4>
                            <pre><code class="language-python">import numpy as np

class LearnedPositionalEmbedding:
    """Learnable positional embeddings"""
    
    def __init__(self, max_seq_len, d_model):
        self.max_seq_len = max_seq_len
        self.d_model = d_model
        
        # Initialize positional embeddings (learnable parameters)
        self.position_embeddings = np.random.randn(max_seq_len, d_model) * 0.02
    
    def get_embeddings(self, seq_len):
        """Get positional embeddings for sequence"""
        if seq_len > self.max_seq_len:
            raise ValueError(f"Sequence length {seq_len} exceeds max {self.max_seq_len}")
        
        return self.position_embeddings[:seq_len, :]
    
    def forward(self, token_embeddings, positions=None):
        """
        Add positional encoding to token embeddings
        
        Parameters:
        token_embeddings: (batch, seq_len, d_model)
        positions: Optional position indices
        """
        batch_size, seq_len, d_model = token_embeddings.shape
        
        if positions is None:
            # Use sequential positions
            pos_emb = self.get_embeddings(seq_len)
        else:
            # Use provided positions
            pos_emb = self.position_embeddings[positions, :]
        
        # Add positional encoding
        return token_embeddings + pos_emb

# Example usage
max_seq_len, d_model = 512, 768
pos_embedding = LearnedPositionalEmbedding(max_seq_len, d_model)

# Token embeddings (batch=2, seq_len=10, d_model=768)
token_emb = np.random.randn(2, 10, d_model)

# Add positional encoding
output = pos_embedding.forward(token_emb)
print(f"Output shape: {output.shape}")  # (2, 10, 768)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Where This Is Used</h3>
                            <p>Real-world applications and use cases will be detailed here.</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: Why do transformers need positional encoding?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Attention mechanisms are permutation-invariant and don't naturally understand word order, so positional encoding adds information about token positions in the sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To make computation faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To reduce memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It's not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the formula for sinusoidal positional encoding?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})\) and \(PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})\) where pos is position and i is dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(PE = pos\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(PE = sin(pos)\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(PE = random\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: How do you add positional encoding to token embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Element-wise addition: final_embedding = token_embedding + positional_encoding, where both have the same dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Concatenate them</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Multiply them</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Use only positional encoding</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Why use sinusoidal encoding instead of learned positional embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Sinusoidal encoding can extrapolate to longer sequences than seen during training, while learned embeddings are fixed to training sequence length. However, learned embeddings often work better in practice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Sinusoidal is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Learned is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They're the same</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What happens if you don't use positional encoding in a transformer?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The model treats "cat sat mat" and "mat sat cat" as identical, losing all word order information, which is crucial for understanding language</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Nothing changes</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It works better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It becomes faster</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: How does positional encoding help with relative positions?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The sinusoidal patterns create unique encodings for each position, and the model can learn to compute relative positions from these encodings through attention mechanisms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It doesn't help</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only absolute positions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random positions</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is the difference between absolute and relative positional encoding?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Absolute encoding adds position information directly to embeddings, while relative encoding modifies attention scores to encode distances between positions. Relative encoding can be more flexible</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Absolute is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Relative is always better</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: How would you implement positional encoding from scratch?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) For each position pos and dimension i, compute sin and cos values using the formula, create a matrix of shape (max_len, d_model), then add this to token embeddings during forward pass</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just use random values</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use only position numbers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No implementation needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: Why do different frequency components in sinusoidal encoding matter?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Different frequencies (via the 10000^{2i/d} term) create unique patterns for each position. Lower frequencies capture coarse position, higher frequencies capture fine-grained position differences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They don't matter</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only one frequency needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random frequencies</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What happens to positional encoding in different transformer architectures?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) BERT uses learned positional embeddings, GPT uses learned, original Transformer used sinusoidal. Some newer models use relative positional encoding or rotary position encoding (RoPE)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All use the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) None use it</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only sinusoidal</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: How does positional encoding scale to very long sequences?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Sinusoidal encoding can theoretically handle any length, but in practice models are trained on fixed max lengths. For longer sequences, you may need to extend learned embeddings or use relative encoding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It doesn't scale</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always works perfectly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only for short sequences</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: How would you debug issues related to positional encoding?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Verify encoding values are in reasonable range, check that encoding is actually added to embeddings, test if model performance degrades when positions are shuffled, visualize positional encoding patterns, ensure encoding dimension matches embedding dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just ignore it</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Remove it</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Use random values</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter3" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 3</a>
                <a href="/tutorials/transformers/chapter5" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 5 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlG8jLC0KXLSyHiQtD6lqG3t3a3H4RbQT6GhhDYFyK4aQo5hk6g/AVC/gw" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>