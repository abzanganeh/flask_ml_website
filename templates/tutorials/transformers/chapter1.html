<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: The Attention Mechanism - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 1: The Attention Mechanism</h1>
                <p class="chapter-subtitle">Foundation of Modern NLP - Understanding how attention revolutionized sequence modeling</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="10"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn active">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="problem">The Problem</button>
                    <button class="section-nav-btn azbn-btn" data-section="attention">Attention Concept</button>
                    <button class="section-nav-btn azbn-btn" data-section="qkv">QKV Framework</button>
                    <button class="section-nav-btn azbn-btn" data-section="formula">Attention Formula</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand why attention was needed in sequence-to-sequence models</li>
                        <li>Master the Query, Key, Value (QKV) framework</li>
                        <li>Learn the mathematical formulation of attention</li>
                        <li>Understand how attention solves the information bottleneck</li>
                        <li>Implement attention mechanism from scratch</li>
                        <li>Recognize the limitations of RNNs/LSTMs that attention addresses</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>What is Attention?</h2>
                        
                        <div class="explanation-box">
                            <h3>The Revolutionary Idea</h3>
                            <p><strong>Attention is a mechanism that allows models to focus on relevant parts of the input when making predictions.</strong> It was introduced to solve a critical limitation in sequence-to-sequence models: the information bottleneck.</p>
                            
                            <p><strong>Think of attention like reading comprehension:</strong></p>
                            <ul>
                                <li><strong>Without attention:</strong> Like trying to summarize a book after reading it once - you remember only the main points, lose details</li>
                                <li><strong>With attention:</strong> Like having the book open while writing - you can look back at any part when needed</li>
                                <li><strong>Key insight:</strong> When generating each word, the model can "attend to" (look at) any part of the input sequence</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Example: Translation</h4>
                            <p><strong>Translating: "The cat sat on the mat" ‚Üí "Le chat s'est assis sur le tapis"</strong></p>
                            
                            <div class="explanation-box">
                                <h5>Without Attention (Old RNN Approach):</h5>
                                <ul>
                                    <li>Encoder processes entire sentence into one fixed-size vector</li>
                                    <li>Decoder must generate translation from this single vector</li>
                                    <li><strong>Problem:</strong> Long sentences get compressed into same-size vector ‚Üí information loss</li>
                                    <li>Like trying to fit a 1000-page book summary into one sentence!</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>With Attention (Modern Approach):</h5>
                                <ul>
                                    <li>Encoder keeps all hidden states (one per input word)</li>
                                    <li>When generating "chat", decoder attends to "cat" in input</li>
                                    <li>When generating "tapis", decoder attends to "mat" in input</li>
                                    <li><strong>Result:</strong> Each output word can access any input word directly!</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="problem" class="content-section">
                        <h2>The Problem Attention Solves</h2>
                        
                        <div class="explanation-box">
                            <h3>‚ö†Ô∏è The Information Bottleneck</h3>
                            <p><strong>Sequence-to-sequence models (like RNNs) had a fundamental limitation:</strong></p>
                            
                            <div class="example-box">
                                <h4>The Bottleneck Problem</h4>
                                <p><strong>Old architecture (RNN encoder-decoder):</strong></p>
                                <ul>
                                    <li><strong>Encoder:</strong> Processes input sequence word by word</li>
                                    <li><strong>Final hidden state:</strong> Single vector representing entire input</li>
                                    <li><strong>Decoder:</strong> Must generate output from this single vector</li>
                                </ul>
                                
                                <p><strong>Why this is a problem:</strong></p>
                                <ul>
                                    <li>Short sentence: "Hello" ‚Üí Easy to compress into one vector</li>
                                    <li>Long sentence: "The quick brown fox jumps over the lazy dog and then runs through the forest..." ‚Üí Same-size vector!</li>
                                    <li><strong>Result:</strong> Information loss for long sequences</li>
                                    <li>Like trying to summarize War and Peace in one sentence</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="attention" class="content-section">
                        <h2>The Attention Concept</h2>
                        
                        <div class="explanation-box">
                            <h3>üîç How Attention Works</h3>
                            <p><strong>Attention allows the decoder to look at ALL encoder hidden states, not just the final one:</strong></p>
                            
                            <div class="formula-box">
                                <h4>Attention Mechanism</h4>
                                <p><strong>For each decoder step:</strong></p>
                                <ol>
                                    <li>Compute attention scores for all encoder positions</li>
                                    <li>Convert scores to attention weights (probabilities)</li>
                                    <li>Create weighted combination of encoder hidden states</li>
                                    <li>Use this combination (context vector) for decoding</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div id="qkv" class="content-section">
                        <h2>Query, Key, Value (QKV) Framework</h2>
                        
                        <div class="explanation-box">
                            <h3>üîë The Three Components</h3>
                            <p><strong>Attention uses three vectors: Query, Key, and Value</strong></p>
                            
                            <div class="example-box">
                                <h4>üìö Library Analogy</h4>
                                <p><strong>Think of attention like searching a library:</strong></p>
                                <ul>
                                    <li><strong>Query (Q):</strong> Your question - "I need information about cats"</li>
                                    <li><strong>Key (K):</strong> Book titles/index - Each book has a title describing its content</li>
                                    <li><strong>Value (V):</strong> Book content - The actual information in each book</li>
                                </ul>
                                
                                <p><strong>The process:</strong></p>
                                <ol>
                                    <li>Compare your Query to all Keys (book titles)</li>
                                    <li>Find which Keys match your Query best</li>
                                    <li>Retrieve the Values (content) from matching books</li>
                                    <li>Combine the relevant Values to answer your question</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div id="formula" class="content-section">
                        <h2>Attention Formula</h2>
                        
                        <div class="formula-box">
                            <h4>Scaled Dot-Product Attention</h4>
                            
                            <div class="formula-display">
                                <strong>Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) √ó V</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Step-by-Step Breakdown:</h5>
                                <ol>
                                    <li><strong>QK^T:</strong> Compute similarity between queries and keys (dot product)</li>
                                    <li><strong>/ ‚àöd_k:</strong> Scale by square root of key dimension (prevents large values)</li>
                                    <li><strong>softmax(...):</strong> Convert scores to probabilities (attention weights)</li>
                                    <li><strong>√ó V:</strong> Weighted sum of values based on attention weights</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Attention Implementation</h4>
                            <pre><code class="language-python">import numpy as np

def attention(Q, K, V):
    """
    Scaled dot-product attention
    
    Parameters:
    Q: Query matrix (seq_len_q, d_k)
    K: Key matrix (seq_len_k, d_k)
    V: Value matrix (seq_len_v, d_v)
    """
    d_k = K.shape[-1]
    
    # Compute attention scores
    scores = np.dot(Q, K.T) / np.sqrt(d_k)
    
    # Apply softmax
    attention_weights = softmax(scores, axis=-1)
    
    # Weighted sum of values
    output = np.dot(attention_weights, V)
    
    return output, attention_weights

def softmax(x, axis=-1):
    """Softmax function"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# Example usage
Q = np.random.randn(5, 64)  # 5 queries, 64 dimensions
K = np.random.randn(10, 64)  # 10 keys, 64 dimensions
V = np.random.randn(10, 64)  # 10 values, 64 dimensions

output, weights = attention(Q, K, V)
print(f"Output shape: {output.shape}")  # (5, 64)
print(f"Attention weights shape: {weights.shape}")  # (5, 10)</code></pre>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What problem does attention solve in sequence-to-sequence models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The information bottleneck where all input is compressed into one vector</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Slow computation speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Memory limitations</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Overfitting</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What does the Query (Q) represent in attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) What we're looking for</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The input data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) The output data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The weights</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: Explain the attention mechanism in your own words.</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Attention lets a model focus on relevant parts of the input when producing each output. It computes how much each input position should influence the current output by comparing queries with keys and using those scores to weight the values</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It processes all inputs equally</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It only looks at the first input</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It ignores inputs</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is the mathematical formula for attention scores?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\) where d_k is the dimension of keys</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(Attention = Q + K\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(Attention = Q \times K\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(Attention = V\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Why do we divide by sqrt(d_k) in the attention formula?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) To prevent the dot products from growing too large, which would push softmax into regions with extremely small gradients, making training unstable</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To make computation faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To reduce memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It's not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is the difference between Key (K) and Value (V) in attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Keys are used to compute attention scores (how relevant each position is), while values contain the actual information that gets weighted and summed to produce the output</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Keys are outputs, values are inputs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: How does attention solve the information bottleneck in sequence-to-sequence models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Instead of compressing all input into a single fixed-size vector, attention allows the decoder to directly access all encoder states, dynamically focusing on relevant parts for each output step</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) By using larger vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) By processing faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) By using less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What happens after computing attention scores?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Scores are passed through softmax to get attention weights (probabilities), then these weights are used to compute a weighted sum of the values</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Scores are used directly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Scores are discarded</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only maximum score is used</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: Why is attention better than fixed-size encoding for long sequences?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Fixed encoding loses information as sequence length increases, while attention maintains access to all positions and can focus on the most relevant parts regardless of sequence length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It uses less memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: How would you implement attention from scratch?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Create Q, K, V matrices from input, compute scores as Q times K transpose, scale by sqrt(d_k), apply softmax, multiply by V to get weighted sum. Each step uses matrix operations</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just add Q and K</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use only V</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random operations</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the computational complexity of attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) O(n¬≤) where n is sequence length, because we compute attention scores between every pair of positions (each query attends to all keys)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) O(n)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) O(1)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) O(log n)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: How does attention differ from RNN-based sequence modeling?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) RNNs process sequentially and struggle with long dependencies, while attention processes all positions in parallel and can directly model relationships between any positions regardless of distance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) RNNs are always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Attention is sequential</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter2" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 2: Self-Attention ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
            }
        });
    </script>
</body>
</html>
