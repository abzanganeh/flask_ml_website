<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Residual Connections & Layer Normalization - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWAT2dVgYnHwpIK/NS" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 6: Residual Connections & Layer Normalization</h1>
                <p class="chapter-subtitle">Stabilizing Deep Networks</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="60"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn active">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn ">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn ">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand residual connections & layer normalization fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Residual Connections & Layer Normalization</h2>
                        
                        <div class="explanation-box">
                            <h3>Why These Components Are Critical</h3>
                            <p><strong>Residual connections and layer normalization are essential for training deep transformer networks.</strong> They solve two critical problems: vanishing gradients and training instability.</p>
                            
                            <p><strong>Think of them like safety features in a car:</strong></p>
                            <ul>
                                <li><strong>Residual connections:</strong> Like a backup system - ensures information can always flow through, even if a layer doesn't learn well</li>
                                <li><strong>Layer normalization:</strong> Like cruise control - keeps the network stable and prevents it from going too fast or too slow</li>
                                <li><strong>Together:</strong> They enable training of very deep networks (50+ layers) that would otherwise be impossible</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h4>‚ö†Ô∏è The Problem Without These Components</h4>
                            
                            <div class="example-box">
                                <h5>Problem 1: Vanishing Gradients</h5>
                                <p><strong>In deep networks without residuals:</strong></p>
                                <ul>
                                    <li>Gradients get smaller as they flow backward through layers</li>
                                    <li>Early layers receive almost no gradient signal</li>
                                    <li>Result: Early layers don't learn, only later layers learn</li>
                                    <li>Like a message getting distorted as it passes through many people</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>Problem 2: Training Instability</h5>
                                <p><strong>Without layer normalization:</strong></p>
                                <ul>
                                    <li>Activations can become very large or very small</li>
                                    <li>Gradients can explode or vanish</li>
                                    <li>Training becomes unstable and may not converge</li>
                                    <li>Like a car without cruise control - speed fluctuates wildly</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üìö Historical Context</h4>
                            <p><strong>These innovations came from different sources:</strong></p>
                            <ul>
                                <li><strong>Residual Connections:</strong> Introduced in ResNet (2015) for image recognition</li>
                                <li><strong>Layer Normalization:</strong> Introduced in 2016, specifically for sequence models</li>
                                <li><strong>Transformer (2017):</strong> Combined both, enabling deep language models</li>
                                <li><strong>Result:</strong> Enabled training of models with 100+ layers!</li>
                            </ul>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>üîë Residual Connections (Skip Connections)</h3>
                            <p><strong>Residual connections add the input directly to the output of a layer:</strong></p>
                            
                            <div class="example-box">
                                <h4>How It Works</h4>
                                <ul>
                                    <li><strong>Without residual:</strong> output = Layer(input)</li>
                                    <li><strong>With residual:</strong> output = input + Layer(input)</li>
                                    <li><strong>Key insight:</strong> The layer learns the "residual" (difference) rather than the full transformation</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>Why This Helps</h4>
                                <ul>
                                    <li><strong>Identity mapping:</strong> If Layer(input) = 0, output = input (information preserved!)</li>
                                    <li><strong>Gradient flow:</strong> Gradients can flow directly through the addition</li>
                                    <li><strong>Easier learning:</strong> Layer can learn small refinements rather than complete transformations</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Layer Normalization</h3>
                            <p><strong>Layer normalization normalizes activations across features for each sample:</strong></p>
                            
                            <div class="example-box">
                                <h4>What It Does</h4>
                                <ul>
                                    <li>Normalizes each token's features independently</li>
                                    <li>Mean = 0, Variance = 1 for each token</li>
                                    <li>Stabilizes training by keeping activations in a reasonable range</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>Layer Norm vs Batch Norm</h4>
                                <ul>
                                    <li><strong>Batch Norm:</strong> Normalizes across batch dimension (problematic for sequences of different lengths)</li>
                                    <li><strong>Layer Norm:</strong> Normalizes across feature dimension (works for any sequence length)</li>
                                    <li><strong>Why Layer Norm for transformers:</strong> Sequences have variable lengths, batch norm doesn't work well</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Pre-Norm vs Post-Norm</h3>
                            <p><strong>Two common architectures:</strong></p>
                            
                            <div class="example-box">
                                <h4>Post-Norm (Original Transformer)</h4>
                                <p><strong>Structure:</strong> Sublayer ‚Üí LayerNorm ‚Üí Residual</p>
                                <ul>
                                    <li>x ‚Üí Attention ‚Üí LayerNorm ‚Üí x + attention_output</li>
                                    <li>Used in original Transformer paper</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>Pre-Norm (Modern Standard)</h4>
                                <p><strong>Structure:</strong> LayerNorm ‚Üí Sublayer ‚Üí Residual</p>
                                <ul>
                                    <li>x ‚Üí LayerNorm ‚Üí Attention ‚Üí x + attention_output</li>
                                    <li>More stable, easier to train</li>
                                    <li>Used in modern models (GPT, BERT variants)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Residual Connection Formula</h4>
                            
                            <div class="formula-display">
                                <strong>output = x + Sublayer(x)</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Breaking Down:</h5>
                                <ul>
                                    <li><strong>x:</strong> Input to the layer</li>
                                    <li><strong>Sublayer(x):</strong> Output of attention or FFN</li>
                                    <li><strong>x + Sublayer(x):</strong> Residual connection (element-wise addition)</li>
                                    <li><strong>Key:</strong> If Sublayer(x) = 0, output = x (identity mapping preserved)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h4>Layer Normalization Formula</h4>
                            
                            <div class="formula-display">
                                <strong>LayerNorm(x) = Œ≥ ‚äô ((x - Œº) / ‚àö(œÉ¬≤ + Œµ)) + Œ≤</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Step-by-Step:</h5>
                                <ol>
                                    <li><strong>Œº = mean(x):</strong> Compute mean across features</li>
                                    <li><strong>œÉ¬≤ = var(x):</strong> Compute variance across features</li>
                                    <li><strong>(x - Œº) / ‚àö(œÉ¬≤ + Œµ):</strong> Normalize (mean=0, std=1)</li>
                                    <li><strong>Œ≥ ‚äô ... + Œ≤:</strong> Scale and shift (learnable parameters)</li>
                                </ol>
                                
                                <h5>Notation:</h5>
                                <ul>
                                    <li><strong>Œº:</strong> Mean vector (computed per token)</li>
                                    <li><strong>œÉ¬≤:</strong> Variance vector (computed per token)</li>
                                    <li><strong>Œµ:</strong> Small constant (e.g., 1e-5) to prevent division by zero</li>
                                    <li><strong>Œ≥:</strong> Learnable scale parameter</li>
                                    <li><strong>Œ≤:</strong> Learnable shift parameter</li>
                                    <li><strong>‚äô:</strong> Element-wise multiplication</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h4>Complete Transformer Sublayer (Pre-Norm)</h4>
                            
                            <div class="formula-display">
                                <strong>output = x + Sublayer(LayerNorm(x))</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>For Attention Sublayer:</h5>
                                <ul>
                                    <li>x ‚Üí LayerNorm(x) ‚Üí MultiHeadAttention(...) ‚Üí x + attention_output</li>
                                </ul>
                                
                                <h5>For FFN Sublayer:</h5>
                                <ul>
                                    <li>x ‚Üí LayerNorm(x) ‚Üí FFN(...) ‚Üí x + ffn_output</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Residual Connection in Action</h4>
                            <p><strong>Let's see how residual connections preserve information:</strong></p>
                            
                            <div class="explanation-box">
                                <h5>Scenario: Deep Network (10 layers)</h5>
                                <p><strong>Without Residual:</strong></p>
                                <ul>
                                    <li>Input: x = [1.0, 2.0, 3.0]</li>
                                    <li>After Layer 1: [0.9, 1.8, 2.7] (slight change)</li>
                                    <li>After Layer 2: [0.8, 1.6, 2.4] (more change)</li>
                                    <li>... (information degrades through layers)</li>
                                    <li>After Layer 10: [0.1, 0.2, 0.3] (most information lost!)</li>
                                </ul>
                                
                                <p><strong>With Residual:</strong></p>
                                <ul>
                                    <li>Input: x = [1.0, 2.0, 3.0]</li>
                                    <li>After Layer 1: x + Layer1(x) = [1.0, 2.0, 3.0] + [-0.1, -0.2, -0.3] = [0.9, 1.8, 2.7]</li>
                                    <li>After Layer 2: previous + Layer2(...) = [0.9, 1.8, 2.7] + [-0.1, -0.2, -0.3] = [0.8, 1.6, 2.4]</li>
                                    <li>... (but original x is always accessible through the residual path!)</li>
                                    <li>Even if layers learn nothing, output ‚âà input (information preserved)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>Example: Layer Normalization</h4>
                            <p><strong>Normalizing a token's features:</strong></p>
                            
                            <div class="explanation-box">
                                <h5>Input Token Features</h5>
                                <p><strong>Before normalization:</strong> x = [10.0, -5.0, 2.0, 8.0, -3.0]</p>
                                
                                <p><strong>Step 1: Compute mean</strong></p>
                                <ul>
                                    <li>Œº = (10.0 + (-5.0) + 2.0 + 8.0 + (-3.0)) / 5 = 2.4</li>
                                </ul>
                                
                                <p><strong>Step 2: Compute variance</strong></p>
                                <ul>
                                    <li>œÉ¬≤ = ((10.0-2.4)¬≤ + (-5.0-2.4)¬≤ + (2.0-2.4)¬≤ + (8.0-2.4)¬≤ + (-3.0-2.4)¬≤) / 5</li>
                                    <li>œÉ¬≤ = (57.76 + 54.76 + 0.16 + 31.36 + 29.16) / 5 = 34.64</li>
                                    <li>œÉ = ‚àö34.64 ‚âà 5.89</li>
                                </ul>
                                
                                <p><strong>Step 3: Normalize</strong></p>
                                <ul>
                                    <li>normalized = (x - Œº) / œÉ</li>
                                    <li>normalized = [(10.0-2.4)/5.89, (-5.0-2.4)/5.89, (2.0-2.4)/5.89, (8.0-2.4)/5.89, (-3.0-2.4)/5.89]</li>
                                    <li>normalized ‚âà [1.29, -1.26, -0.07, 0.95, -0.92]</li>
                                    <li>Mean ‚âà 0, Std ‚âà 1 ‚úì</li>
                                </ul>
                                
                                <p><strong>Step 4: Scale and shift (if learned)</strong></p>
                                <ul>
                                    <li>If Œ≥ = [1.0, 1.0, 1.0, 1.0, 1.0] and Œ≤ = [0.0, 0.0, 0.0, 0.0, 0.0]</li>
                                    <li>Output = normalized (no change)</li>
                                    <li>But Œ≥ and Œ≤ are learned, so they can adjust the distribution</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Layer Normalization Implementation</h4>
                            <pre><code class="language-python">import numpy as np

class LayerNormalization:
    """Layer Normalization for Transformers"""
    
    def __init__(self, d_model, eps=1e-5):
        """
        Initialize Layer Normalization
        
        Parameters:
        d_model: Model dimension
        eps: Small constant for numerical stability
        """
        self.d_model = d_model
        self.eps = eps
        
        # Learnable parameters
        self.gamma = np.ones(d_model)  # Scale parameter
        self.beta = np.zeros(d_model)  # Shift parameter
    
    def forward(self, x):
        """
        Forward pass through layer normalization
        
        Parameters:
        x: Input (batch, seq_len, d_model) or (seq_len, d_model)
        
        Returns:
        Normalized output (same shape as input)
        """
        # Handle different input shapes
        if len(x.shape) == 2:
            x = x.reshape(1, *x.shape)
            squeeze_output = True
        else:
            squeeze_output = False
        
        batch_size, seq_len, d_model = x.shape
        
        # Compute mean and variance across features (last dimension)
        # Shape: (batch, seq_len, 1)
        mean = np.mean(x, axis=-1, keepdims=True)
        variance = np.var(x, axis=-1, keepdims=True)
        
        # Normalize
        x_normalized = (x - mean) / np.sqrt(variance + self.eps)
        
        # Scale and shift
        output = self.gamma * x_normalized + self.beta
        
        if squeeze_output:
            output = output.squeeze(0)
        
        return output

# Example usage
d_model = 512
layer_norm = LayerNormalization(d_model)

# Input: (batch=2, seq_len=10, d_model=512)
x = np.random.randn(2, 10, 512) * 5  # Large values

# Normalize
output = layer_norm.forward(x)
print(f"Input mean: {np.mean(x):.2f}, std: {np.std(x):.2f}")
print(f"Output mean: {np.mean(output):.2f}, std: {np.std(output):.2f}")
# Output should have mean ‚âà 0, std ‚âà 1</code></pre>
                        </div>

                        <div class="code-box">
                            <h4>Residual Connection Implementation</h4>
                            <pre><code class="language-python">import numpy as np

def residual_connection(x, sublayer_output):
    """
    Apply residual connection
    
    Parameters:
    x: Input (batch, seq_len, d_model)
    sublayer_output: Output from attention or FFN (batch, seq_len, d_model)
    
    Returns:
    x + sublayer_output (element-wise addition)
    """
    return x + sublayer_output

# Complete transformer sublayer with pre-norm
def transformer_sublayer(x, sublayer_fn, layer_norm):
    """
    Transformer sublayer with pre-norm and residual
    
    Parameters:
    x: Input (batch, seq_len, d_model)
    sublayer_fn: Function for attention or FFN
    layer_norm: LayerNormalization instance
    
    Returns:
    Output with residual connection
    """
    # Pre-norm: normalize before sublayer
    x_norm = layer_norm.forward(x)
    
    # Apply sublayer (attention or FFN)
    sublayer_output = sublayer_fn(x_norm)
    
    # Residual connection
    output = residual_connection(x, sublayer_output)
    
    return output

# Example: Complete attention sublayer
def attention_sublayer(x, attention_fn, layer_norm):
    """Attention sublayer with pre-norm and residual"""
    x_norm = layer_norm.forward(x)
    attention_out = attention_fn(x_norm)
    return x + attention_out

# Example: Complete FFN sublayer
def ffn_sublayer(x, ffn_fn, layer_norm):
    """FFN sublayer with pre-norm and residual"""
    x_norm = layer_norm.forward(x)
    ffn_out = ffn_fn(x_norm)
    return x + ffn_out</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Critical for All Deep Transformers</h3>
                            <p><strong>Residual connections and layer normalization are used in virtually every transformer model:</strong></p>
                            
                            <div class="example-box">
                                <h4>1. BERT (Bidirectional Encoder)</h4>
                                <ul>
                                    <li>12-24 layers, all use residuals and layer norm</li>
                                    <li>Enables training deep bidirectional encoders</li>
                                    <li>Without these, BERT couldn't train effectively</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>2. GPT Models</h4>
                                <ul>
                                    <li>GPT-3: 96 layers! Impossible without residuals</li>
                                    <li>Layer norm stabilizes training across all layers</li>
                                    <li>Enables autoregressive generation at scale</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>3. Vision Transformers (ViT)</h4>
                                <ul>
                                    <li>Apply transformers to images</li>
                                    <li>Residuals and layer norm critical for deep vision models</li>
                                    <li>Enable state-of-the-art image classification</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Impact on Training</h3>
                            <p><strong>These components enable:</strong></p>
                            <ul>
                                <li><strong>Deeper networks:</strong> 100+ layers vs 10-20 without residuals</li>
                                <li><strong>Faster convergence:</strong> Layer norm stabilizes gradients</li>
                                <li><strong>Better performance:</strong> Deeper = more capacity = better results</li>
                                <li><strong>Stable training:</strong> Prevents gradient explosion/vanishing</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main purpose of residual connections?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) To reduce parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) To enable gradient flow and preserve information through deep networks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To add non-linearity</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) To normalize activations</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What does layer normalization normalize across?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Batch dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Feature dimension (for each token independently)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Sequence dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) All dimensions</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: In pre-norm architecture, where is layer normalization applied?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Before the sublayer (attention or FFN)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) After the sublayer</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Both before and after</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not used in pre-norm</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter5" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 5</a>
                <a href="/tutorials/transformers/chapter7" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 7 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlG8jLC0KXLSyHiQtD6lqG3t3a3H4RbQT6GhhDYFyK4aQo5hk6g/AVC/gw" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>