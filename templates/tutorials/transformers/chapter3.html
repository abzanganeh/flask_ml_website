<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Multi-Head Attention - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWAT2dVgYnHwpIK/NS" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 3: Multi-Head Attention</h1>
                <p class="chapter-subtitle">Learning Multiple Relationships</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="30"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn active">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn ">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn ">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand multi-head attention fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Multi-Head Attention</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Multiple Heads?</h3>
                            <p><strong>Multi-head attention allows the model to attend to information from different representation subspaces simultaneously.</strong> Instead of one attention mechanism, we use multiple parallel attention "heads", each learning different types of relationships.</p>
                            
                            <p><strong>Think of multi-head attention like having multiple experts:</strong></p>
                            <ul>
                                <li><strong>Single-head attention:</strong> Like one person trying to understand everything - they might miss some relationships</li>
                                <li><strong>Multi-head attention:</strong> Like a team of specialists - one focuses on syntax, another on semantics, another on long-range dependencies</li>
                                <li><strong>Result:</strong> Each head learns different patterns, then we combine their insights</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Analogy: The Research Team</h4>
                            <p><strong>Imagine analyzing a complex document:</strong></p>
                            <ul>
                                <li><strong>Head 1 (Syntax Expert):</strong> Focuses on grammatical relationships - "What is the subject? What is the verb?"</li>
                                <li><strong>Head 2 (Semantic Expert):</strong> Focuses on meaning - "What concepts are related? What is the topic?"</li>
                                <li><strong>Head 3 (Reference Expert):</strong> Focuses on references - "What does 'it' refer to? What does 'this' mean?"</li>
                                <li><strong>Head 4 (Temporal Expert):</strong> Focuses on time relationships - "What happened first? What's the sequence?"</li>
                            </ul>
                            <p><strong>Multi-head attention:</strong> All experts work in parallel, then we combine their findings for a complete understanding!</p>
                        </div>

                        <div class="explanation-box">
                            <h4>How Multi-Head Attention Works</h4>
                            <p><strong>The process:</strong></p>
                            <ol>
                                <li><strong>Split:</strong> Divide the embedding dimension into multiple heads</li>
                                <li><strong>Parallel Attention:</strong> Each head computes attention independently</li>
                                <li><strong>Specialization:</strong> Each head learns different relationships</li>
                                <li><strong>Concatenate:</strong> Combine all head outputs</li>
                                <li><strong>Project:</strong> Linear transformation to final dimension</li>
                            </ol>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>üîë Head Specialization</h3>
                            <p><strong>Different attention heads learn to focus on different aspects of the input:</strong></p>
                            
                            <div class="example-box">
                                <h4>Example: Sentence Analysis</h4>
                                <p><strong>Sentence:</strong> "The cat, which was very fluffy, sat on the mat."</p>
                                
                                <ul>
                                    <li><strong>Head 1 (Subject-Verb):</strong> High attention from "sat" to "cat" (subject-verb relationship)</li>
                                    <li><strong>Head 2 (Modifier):</strong> High attention from "fluffy" to "cat" (adjective-noun relationship)</li>
                                    <li><strong>Head 3 (Relative Clause):</strong> High attention from "which" to "cat" (relative pronoun reference)</li>
                                    <li><strong>Head 4 (Preposition):</strong> High attention from "on" to "mat" (preposition-object relationship)</li>
                                </ul>
                                
                                <p><strong>Key insight:</strong> Each head specializes in a different linguistic relationship!</p>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Dimension Splitting</h3>
                            <p><strong>How dimensions are divided:</strong></p>
                            <ul>
                                <li>If embedding dimension = 512 and num_heads = 8</li>
                                <li>Each head gets: 512 / 8 = 64 dimensions</li>
                                <li>Each head operates in its own 64-dimensional subspace</li>
                                <li>This allows parallel computation and specialization</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Multi-Head Attention Formula</h4>
                            
                            <div class="formula-display">
                                \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O\]
                                \[\text{where } \text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\]
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Notation:</h5>
                                <ul>
                                    <li><strong>h:</strong> Number of attention heads</li>
                                    <li><strong>QW^Q·µ¢:</strong> Query projection for head i</li>
                                    <li><strong>KW^K·µ¢:</strong> Key projection for head i</li>
                                    <li><strong>VW^V·µ¢:</strong> Value projection for head i</li>
                                    <li><strong>Concat:</strong> Concatenate all head outputs</li>
                                    <li><strong>W^O:</strong> Output projection matrix</li>
                                </ul>
                                
                                <h5>Dimension Breakdown:</h5>
                                <ul>
                                    <li>Input: (batch, seq_len, d_model)</li>
                                    <li>Each head: (batch, seq_len, d_model/h)</li>
                                    <li>After concat: (batch, seq_len, d_model)</li>
                                    <li>After W^O: (batch, seq_len, d_model)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Multi-Head Attention on "The cat sat on the mat"</h4>
                            <p><strong>Input:</strong> Sequence of 6 tokens with embedding dimension 512, using 8 heads</p>
                            
                            <p><strong>Step 1: Create Q, K, V for each head</strong></p>
                            <ul>
                                <li>Head 1: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ (each 64-dim, from 512/8)</li>
                                <li>Head 2: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ (64-dim)</li>
                                <li>... Head 8: Q‚Çà, K‚Çà, V‚Çà (64-dim)</li>
                            </ul>
                            
                            <p><strong>Step 2: Compute attention for each head independently</strong></p>
                            <ul>
                                <li>Head 1 might focus on subject-verb: "sat" ‚Üí "cat"</li>
                                <li>Head 2 might focus on preposition: "on" ‚Üí "mat"</li>
                                <li>Head 3 might focus on articles: "the" ‚Üí "cat", "the" ‚Üí "mat"</li>
                                <li>Each head produces its own attention output (6√ó64)</li>
                            </ul>
                            
                            <p><strong>Step 3: Concatenate all heads</strong></p>
                            <ul>
                                <li>Concat[head‚ÇÅ, head‚ÇÇ, ..., head‚Çà] ‚Üí (6√ó512)</li>
                                <li>All 8 heads' outputs combined</li>
                            </ul>
                            
                            <p><strong>Step 4: Apply output projection</strong></p>
                            <ul>
                                <li>Multiply by W^O (512√ó512) ‚Üí Final output (6√ó512)</li>
                                <li>This combines information from all heads</li>
                            </ul>
                            
                            <p><strong>Result:</strong> Each token now has a representation that incorporates information from all tokens, with different heads capturing different relationships.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Why Multiple Heads Help</h4>
                            <p><strong>Single-head attention limitation:</strong></p>
                            <p>With one head, the model must learn all relationships in one attention pattern. This is like asking one person to be an expert in grammar, syntax, semantics, and discourse all at once.</p>
                            
                            <p><strong>Multi-head advantage:</strong></p>
                            <p>With 8 heads, each head can specialize:</p>
                            <ul>
                                <li>Head 1: Subject-verb relationships</li>
                                <li>Head 2: Adjective-noun relationships</li>
                                <li>Head 3: Preposition-object relationships</li>
                                <li>Head 4: Pronoun-antecedent relationships</li>
                                <li>Head 5: Long-range dependencies</li>
                                <li>Head 6: Negation scope</li>
                                <li>Head 7: Temporal relationships</li>
                                <li>Head 8: Causal relationships</li>
                            </ul>
                            
                            <p>This specialization allows the model to capture more nuanced relationships than a single head could.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Multi-Head Attention Implementation</h4>
                            <pre><code class="language-python">import numpy as np

def multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):
    """
    Multi-head attention implementation
    
    Parameters:
    X: Input (batch, seq_len, d_model)
    W_Q: Query weight matrices for each head (num_heads, d_model, d_k)
    W_K: Key weight matrices for each head (num_heads, d_model, d_k)
    W_V: Value weight matrices for each head (num_heads, d_model, d_v)
    W_O: Output projection (d_model, d_model)
    num_heads: Number of attention heads
    """
    batch_size, seq_len, d_model = X.shape
    d_k = d_model // num_heads
    
    # List to store outputs from each head
    head_outputs = []
    
    # Process each head
    for h in range(num_heads):
        # Project to Q, K, V for this head
        Q_h = np.dot(X, W_Q[h])  # (batch, seq_len, d_k)
        K_h = np.dot(X, W_K[h])  # (batch, seq_len, d_k)
        V_h = np.dot(X, W_V[h])  # (batch, seq_len, d_v)
        
        # Compute attention for this head
        scores = np.dot(Q_h, K_h.transpose(0, 2, 1))  # (batch, seq_len, seq_len)
        scores = scores / np.sqrt(d_k)
        
        # Softmax
        attention_weights = softmax(scores, axis=-1)
        
        # Weighted sum
        head_output = np.dot(attention_weights, V_h)  # (batch, seq_len, d_v)
        head_outputs.append(head_output)
    
    # Concatenate all heads
    multi_head_output = np.concatenate(head_outputs, axis=-1)  # (batch, seq_len, d_model)
    
    # Final output projection
    output = np.dot(multi_head_output, W_O)  # (batch, seq_len, d_model)
    
    return output

def softmax(x, axis=-1):
    """Softmax function"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# Example usage
batch_size, seq_len, d_model, num_heads = 2, 10, 512, 8
X = np.random.randn(batch_size, seq_len, d_model)
d_k = d_model // num_heads

# Initialize weight matrices for each head
W_Q = np.random.randn(num_heads, d_model, d_k) * 0.1
W_K = np.random.randn(num_heads, d_model, d_k) * 0.1
W_V = np.random.randn(num_heads, d_model, d_k) * 0.1
W_O = np.random.randn(d_model, d_model) * 0.1

output = multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads)
print(f"Output shape: {output.shape}")  # (2, 10, 512)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Multi-Head Attention in Modern NLP</h3>
                            <p><strong>Multi-head attention is the core mechanism in:</strong></p>
                            <ul>
                                <li><strong>BERT:</strong> Uses 12 heads to understand bidirectional context, with different heads capturing different linguistic patterns</li>
                                <li><strong>GPT:</strong> Uses 12-96 heads depending on model size, with heads specializing in different aspects of language generation</li>
                                <li><strong>Translation models:</strong> Encoder heads focus on source language relationships, decoder heads on target language and cross-lingual alignment</li>
                                <li><strong>Question answering:</strong> Different heads attend to question tokens, context tokens, and relationships between them</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Why Multi-Head is Essential</h3>
                            <p><strong>Single-head attention cannot capture the complexity of language:</strong></p>
                            <ul>
                                <li>Language has multiple simultaneous relationships (syntax, semantics, discourse)</li>
                                <li>Different relationships require different attention patterns</li>
                                <li>Multi-head allows parallel processing of different relationship types</li>
                                <li>Empirically, models with more heads perform better (up to a point)</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Head Specialization in Practice</h3>
                            <p><strong>Research shows heads naturally specialize:</strong></p>
                            <ul>
                                <li>Some heads focus on local patterns (adjacent words)</li>
                                <li>Others focus on long-range dependencies (distant relationships)</li>
                                <li>Some heads capture syntactic structure</li>
                                <li>Others capture semantic relationships</li>
                                <li>This specialization emerges during training, not by design</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is multi-head attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Running multiple attention mechanisms in parallel with different learned projections</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Using multiple layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Processing multiple sequences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Using larger matrices</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Why use multiple attention heads instead of one?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Different heads can learn to focus on different types of relationships (syntax, semantics, long-range dependencies), allowing the model to capture diverse patterns simultaneously</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It uses less memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No reason</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: How do you combine outputs from multiple attention heads?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Concatenate all head outputs, then apply a linear projection to get the final output with the desired dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Average them</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use only the first head</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Multiply them</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is the formula for multi-head attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W_O\) where each head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(MultiHead = Q + K + V\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(MultiHead = Q \times K\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(MultiHead = Attention\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: How do you split dimensions for multiple heads?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) If embedding dimension is d_model and you have h heads, each head gets d_model/h dimensions. For example, 768 dimensions with 12 heads gives 64 dimensions per head</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Each head gets full dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random split</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only first head gets dimensions</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What types of relationships can different attention heads learn?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Some heads focus on syntactic relationships (subject-verb), others on semantic (word meaning), positional (distance), or task-specific patterns, creating a rich representation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All heads learn the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only positional</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only semantic</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: How does the number of heads affect model capacity?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) More heads increase capacity by allowing the model to learn diverse attention patterns, but too many heads can lead to redundancy. Typical values are 8-16 heads</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) More heads always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Fewer heads always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Number doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is the computational cost of multi-head attention compared to single-head?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Similar cost because dimensions are split across heads. With h heads, each head processes d/h dimensions, so total computation is roughly the same as single head with full dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) h times more expensive</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) h times less expensive</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: How would you visualize what different attention heads learn?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Create separate attention heatmaps for each head, showing which positions each head focuses on. Compare patterns across heads to see if they specialize in different relationships</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only visualize one head</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Average all heads</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Can't visualize</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What happens if you use too many attention heads?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Heads may become redundant, learning similar patterns, wasting parameters, and potentially hurting performance. There's a sweet spot based on model size and task</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always improves performance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No effect</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Makes it faster</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: How do you implement multi-head attention from scratch?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Split Q, K, V into h heads (reshape to add head dimension), compute attention for each head independently, concatenate all head outputs, apply output projection matrix W_O to combine heads</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just use single head</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Stack heads sequentially</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random operations</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Why is multi-head attention a key innovation in transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It allows the model to attend to information from different representation subspaces simultaneously, capturing multiple types of relationships in parallel, which is crucial for understanding complex language patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's just faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It uses less memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No special reason</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 2</a>
                <a href="/tutorials/transformers/chapter4" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 4 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlG8jLC0KXLSyHiQtD6lqG3t3a3H4RbQT6GhhDYFyK4aQo5hk6g/AVC/gw" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>