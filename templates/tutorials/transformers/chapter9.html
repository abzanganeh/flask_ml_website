<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 9: Complete Transformer Architecture - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 9: Complete Transformer Architecture</h1>
                <p class="chapter-subtitle">Putting It All Together</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="90"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn active">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn ">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand complete transformer architecture fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Complete Transformer Architecture</h2>
                        
                        <div class="explanation-box">
                            <h3>The Complete Picture</h3>
                            <p><strong>The Transformer combines all components we've learned: embeddings, positional encoding, multi-head attention, FFN, residuals, and layer normalization into a powerful architecture.</strong></p>
                            
                            <div style="background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%); border: 2px solid #0ea5e9; border-radius: 12px; padding: 2rem; margin: 2rem 0;">
                                <h4 style="color: #0c4a6e; margin-bottom: 1.5rem; text-align: center;">üèóÔ∏è Complete Transformer Architecture Diagram</h4>
                                
                                <div style="background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
                                    <!-- Input Layer -->
                                    <div style="text-align: center; margin-bottom: 1.5rem;">
                                        <div style="background: #dbeafe; border: 2px solid #3b82f6; border-radius: 8px; padding: 1rem; display: inline-block; min-width: 200px;">
                                            <p style="font-weight: 700; color: #1e40af; margin: 0;">Input Tokens</p>
                                            <p style="font-size: 0.9rem; color: #1e40af; margin: 0.3rem 0 0 0;">["The", "cat", "sat"]</p>
                                        </div>
                                    </div>
                                    
                                    <div style="text-align: center; margin: 1rem 0;">
                                        <div style="font-size: 1.5rem; color: #0ea5e9; font-weight: bold;">‚Üì</div>
                                    </div>
                                    
                                    <!-- Embeddings -->
                                    <div style="text-align: center; margin-bottom: 1.5rem;">
                                        <div style="background: #fef3c7; border: 2px solid #f59e0b; border-radius: 8px; padding: 1rem; display: inline-block; min-width: 200px;">
                                            <p style="font-weight: 700; color: #92400e; margin: 0;">Token Embeddings</p>
                                            <p style="font-size: 0.85rem; color: #92400e; margin: 0.3rem 0 0 0;">+ Positional Encoding</p>
                                        </div>
                                    </div>
                                    
                                    <div style="text-align: center; margin: 1rem 0;">
                                        <div style="font-size: 1.5rem; color: #0ea5e9; font-weight: bold;">‚Üì</div>
                                    </div>
                                    
                                    <!-- Encoder Stack -->
                                    <div style="background: #dcfce7; border: 2px solid #22c55e; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem;">
                                        <p style="font-weight: 700; color: #166534; margin: 0 0 1rem 0; text-align: center;">Encoder Stack (N layers)</p>
                                        
                                        <div style="background: white; border-radius: 6px; padding: 1rem; margin-bottom: 0.5rem;">
                                            <p style="font-weight: 600; color: #14532d; margin: 0 0 0.5rem 0; font-size: 0.9rem;">Encoder Layer 1:</p>
                                            <div style="display: flex; justify-content: space-around; gap: 0.5rem; flex-wrap: wrap;">
                                                <div style="flex: 1; min-width: 120px; background: #f0f9ff; padding: 0.5rem; border-radius: 4px; text-align: center;">
                                                    <p style="font-size: 0.8rem; color: #0c4a6e; margin: 0; font-weight: 600;">Multi-Head<br>Attention</p>
                                                </div>
                                                <div style="flex: 1; min-width: 120px; background: #fef3c7; padding: 0.5rem; border-radius: 4px; text-align: center;">
                                                    <p style="font-size: 0.8rem; color: #92400e; margin: 0; font-weight: 600;">Feed-Forward<br>Network</p>
                                                </div>
                                            </div>
                                        </div>
                                        
                                        <div style="text-align: center; margin: 0.5rem 0;">
                                            <p style="font-size: 0.8rem; color: #166534; margin: 0;">... (N-2 more layers) ...</p>
                                        </div>
                                        
                                        <div style="background: white; border-radius: 6px; padding: 1rem;">
                                            <p style="font-weight: 600; color: #14532d; margin: 0 0 0.5rem 0; font-size: 0.9rem;">Encoder Layer N:</p>
                                            <div style="display: flex; justify-content: space-around; gap: 0.5rem; flex-wrap: wrap;">
                                                <div style="flex: 1; min-width: 120px; background: #f0f9ff; padding: 0.5rem; border-radius: 4px; text-align: center;">
                                                    <p style="font-size: 0.8rem; color: #0c4a6e; margin: 0; font-weight: 600;">Multi-Head<br>Attention</p>
                                                </div>
                                                <div style="flex: 1; min-width: 120px; background: #fef3c7; padding: 0.5rem; border-radius: 4px; text-align: center;">
                                                    <p style="font-size: 0.8rem; color: #92400e; margin: 0; font-weight: 600;">Feed-Forward<br>Network</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <div style="text-align: center; margin: 1rem 0;">
                                        <div style="font-size: 1.5rem; color: #0ea5e9; font-weight: bold;">‚Üì</div>
                                    </div>
                                    
                                    <!-- Output -->
                                    <div style="text-align: center;">
                                        <div style="background: #dcfce7; border: 2px solid #22c55e; border-radius: 8px; padding: 1rem; display: inline-block; min-width: 200px;">
                                            <p style="font-weight: 700; color: #166534; margin: 0;">Output Representations</p>
                                            <p style="font-size: 0.9rem; color: #166534; margin: 0.3rem 0 0 0;">Rich, contextualized</p>
                                        </div>
                                    </div>
                                    
                                    <div style="background: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin-top: 1.5rem; border-radius: 4px;">
                                        <p style="margin: 0; color: #0c4a6e; font-size: 0.9rem;"><strong>üí° Key Components:</strong> Each encoder layer has Multi-Head Attention + FFN, both with residual connections and layer normalization. The stack processes input through N layers, building increasingly rich representations!</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h4>Information Flow Through Transformer</h4>
                            <p><strong>Complete data flow:</strong></p>
                            <ol>
                                <li><strong>Input:</strong> Tokenized text ‚Üí ["The", "cat", "sat"]</li>
                                <li><strong>Embeddings:</strong> Each token ‚Üí dense vector (512 dimensions)</li>
                                <li><strong>Positional Encoding:</strong> Add position information</li>
                                <li><strong>Encoder Layer 1:</strong> Multi-head attention + FFN ‚Üí Refined representations</li>
                                <li><strong>Encoder Layer 2-N:</strong> Further refinement through each layer</li>
                                <li><strong>Output:</strong> Rich, contextualized representations ready for tasks</li>
                            </ol>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Complete Transformer Architecture</h3>
                            <p><strong>The full transformer combines all components:</strong></p>
                            <ul>
                                <li><strong>Input Processing:</strong> Tokenization ‚Üí Embeddings ‚Üí Positional Encoding</li>
                                <li><strong>Encoder Stack:</strong> Multiple encoder layers (self-attention + FFN)</li>
                                <li><strong>Decoder Stack:</strong> Multiple decoder layers (masked self-attention + cross-attention + FFN)</li>
                                <li><strong>Output Generation:</strong> Linear projection ‚Üí Softmax ‚Üí Token prediction</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Information Flow Through Layers</h3>
                            <p><strong>Early layers:</strong> Capture local patterns, syntax, word-level relationships</p>
                            <p><strong>Middle layers:</strong> Build phrase-level understanding, semantic relationships</p>
                            <p><strong>Deep layers:</strong> Develop high-level abstractions, task-specific features</p>
                            <p>Each layer refines and abstracts the representation from the previous layer.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Training Challenges</h3>
                            <p><strong>Key challenges in training transformers:</strong></p>
                            <ul>
                                <li><strong>Memory:</strong> Large models require significant GPU memory</li>
                                <li><strong>Compute:</strong> Training takes weeks to months on many GPUs</li>
                                <li><strong>Data:</strong> Need massive, high-quality datasets</li>
                                <li><strong>Hyperparameters:</strong> Learning rate, warmup, batch size all critical</li>
                                <li><strong>Stability:</strong> Deep networks prone to gradient issues</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Complete Transformer Forward Pass</h4>
                            <div class="formula-display">
                                \[\text{Transformer}(X) = \text{Decoder}(\text{Encoder}(X_{\text{enc}}), X_{\text{dec}})\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Encoder Stack:</h5>
                                <ul>
                                    <li>\(X_{\text{enc}} = \text{Embed}(X_{\text{enc}}) + PE\)</li>
                                    <li>\(H = \text{EncoderLayer}_N(\ldots\text{EncoderLayer}_1(X_{\text{enc}}))\)</li>
                                </ul>
                                
                                <h5>Decoder Stack:</h5>
                                <ul>
                                    <li>\(X_{\text{dec}} = \text{Embed}(X_{\text{dec}}) + PE\)</li>
                                    <li>\(O = \text{DecoderLayer}_N(\ldots\text{DecoderLayer}_1(X_{\text{dec}}, H))\)</li>
                                </ul>
                                
                                <h5>Output:</h5>
                                <ul>
                                    <li>\(\text{Output} = \text{Softmax}(O \cdot W_{\text{out}})\)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Training Loss</h4>
                            <div class="formula-display">
                                \[L = -\sum_{i=1}^{N} \log P(y_i | x_i, \theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(N\): Number of training examples</li>
                                    <li>\(y_i\): Target token at position i</li>
                                    <li>\(x_i\): Input context up to position i</li>
                                    <li>\(\theta\): Model parameters</li>
                                    <li>\(P(y_i | x_i, \theta)\): Model's predicted probability</li>
                                </ul>
                                
                                <p>This is the standard cross-entropy loss for language modeling.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Learning Rate Schedule</h4>
                            <div class="formula-display">
                                \[\text{lr}(t) = \begin{cases} 
                                \text{lr}_{\text{max}} \times \frac{t}{T_{\text{warmup}}} & \text{if } t < T_{\text{warmup}} \\
                                \text{lr}_{\text{max}} & \text{if } T_{\text{warmup}} \leq t < T_{\text{decay}} \\
                                \text{lr}_{\text{max}} \times \frac{T_{\text{total}} - t}{T_{\text{total}} - T_{\text{decay}}} & \text{if } t \geq T_{\text{decay}}
                                \end{cases}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Phases:</h5>
                                <ul>
                                    <li><strong>Warmup:</strong> Gradually increase from 0 to max learning rate</li>
                                    <li><strong>Constant:</strong> Maintain max learning rate</li>
                                    <li><strong>Decay:</strong> Gradually decrease learning rate</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Complete Transformer Processing</h4>
                            <p><strong>Task:</strong> Translate "Hello world" to French</p>
                            
                            <p><strong>Step 1: Input Processing</strong></p>
                            <ul>
                                <li>Encoder input: "Hello world" ‚Üí Token IDs ‚Üí Embeddings ‚Üí + Positional Encoding</li>
                                <li>Decoder input: [START] ‚Üí Token ID ‚Üí Embedding ‚Üí + Positional Encoding</li>
                            </ul>
                            
                            <p><strong>Step 2: Encoder Processing</strong></p>
                            <ul>
                                <li>Layer 1: Self-attention captures relationships between "Hello" and "world"</li>
                                <li>Layer 2-6: Further refinement of representations</li>
                                <li>Output: Rich contextualized representation of input</li>
                            </ul>
                            
                            <p><strong>Step 3: Decoder Processing</strong></p>
                            <ul>
                                <li>Masked self-attention: [START] attends to itself</li>
                                <li>Cross-attention: [START] attends to encoder output</li>
                                <li>FFN: Processes the combined information</li>
                                <li>Output: Probability distribution over French vocabulary</li>
                            </ul>
                            
                            <p><strong>Step 4: Generation</strong></p>
                            <ul>
                                <li>Sample "Bonjour" (highest probability)</li>
                                <li>Add to decoder input: [START, "Bonjour"]</li>
                                <li>Repeat until [END] token or max length</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Training Setup</h4>
                            <p><strong>Typical configuration for large transformer:</strong></p>
                            <ul>
                                <li><strong>Model size:</strong> 12 layers, 768 dimensions, 12 heads</li>
                                <li><strong>Batch size:</strong> 256 (with gradient accumulation)</li>
                                <li><strong>Learning rate:</strong> 1e-4 with warmup to 1e-3</li>
                                <li><strong>Optimizer:</strong> AdamW with weight decay 0.01</li>
                                <li><strong>Training time:</strong> 1-2 weeks on 8 GPUs</li>
                                <li><strong>Data:</strong> Millions of sentence pairs</li>
                            </ul>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Complete Transformer Training Loop</h4>
                            <pre><code class="language-python">import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR

def train_transformer(model, train_loader, num_epochs, warmup_steps, total_steps):
    """
    Training loop for transformer model
    """
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    
    # Learning rate schedule with warmup
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps  # Warmup
        else:
            return (total_steps - step) / (total_steps - warmup_steps)  # Decay
    
    scheduler = LambdaLR(optimizer, lr_lambda)
    criterion = nn.CrossEntropyLoss(ignore_index=-1)
    
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_idx, (src, tgt) in enumerate(train_loader):
            # Forward pass
            output = model(src, tgt[:, :-1])  # Exclude last token
            target = tgt[:, 1:]  # Shift by one for next token prediction
            
            # Compute loss
            loss = criterion(output.reshape(-1, output.size(-1)), target.reshape(-1))
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        print(f'Epoch {epoch} average loss: {total_loss / len(train_loader):.4f}')

# Example usage
# model = TransformerModel(vocab_size=50000, d_model=512, nhead=8, num_layers=6)
# train_transformer(model, train_loader, num_epochs=10, warmup_steps=4000, total_steps=100000)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Complete Transformer Applications</h3>
                            <p><strong>Encoder-decoder transformers are used for:</strong></p>
                            <ul>
                                <li><strong>Machine Translation:</strong> Google Translate, DeepL use transformer architectures</li>
                                <li><strong>Text Summarization:</strong> Generating concise summaries from long documents</li>
                                <li><strong>Question Answering:</strong> Systems that answer questions based on context</li>
                                <li><strong>Dialogue Systems:</strong> Conversational AI that maintains context</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Training Infrastructure</h3>
                            <p><strong>Large-scale training requires:</strong></p>
                            <ul>
                                <li><strong>Distributed Training:</strong> Multiple GPUs/TPUs working together</li>
                                <li><strong>Data Pipeline:</strong> Efficient data loading and preprocessing</li>
                                <li><strong>Monitoring:</strong> Track loss, learning rate, gradient norms</li>
                                <li><strong>Checkpointing:</strong> Save model state regularly</li>
                                <li><strong>Mixed Precision:</strong> Use float16 for speed</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Production Deployment</h3>
                            <p><strong>Deploying trained transformers:</strong></p>
                            <ul>
                                <li><strong>Model Optimization:</strong> Quantization, pruning for efficiency</li>
                                <li><strong>Inference Optimization:</strong> Batch processing, caching</li>
                                <li><strong>Scalability:</strong> Handle multiple concurrent requests</li>
                                <li><strong>Monitoring:</strong> Track latency, throughput, accuracy</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are the key challenges in training transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Large model size requiring significant memory, long training times, need for large datasets, careful hyperparameter tuning, and managing computational costs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No challenges</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only small datasets needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Very fast training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the typical learning rate schedule for transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Warmup phase (gradually increase), then constant or decay. Warmup helps stabilize training early on, then learning rate may decay to fine-tune convergence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always constant</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always decreasing</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random schedule</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: How do you handle memory constraints when training large transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use gradient checkpointing (recompute activations), mixed precision training, smaller batch sizes, model parallelism, or gradient accumulation to simulate larger batches</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just use larger batches</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Remove layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No solutions</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is gradient accumulation and why use it?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Gradient accumulation computes gradients over multiple small batches before updating weights, allowing you to simulate larger batch sizes when memory is limited, improving training stability</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It reduces gradients</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It's not useful</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only for small models</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What optimizer is commonly used for training transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Adam or AdamW (Adam with weight decay) are most common, as they adapt learning rates per parameter and work well for large models. Some use SGD with momentum for fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only SGD</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only RMSprop</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random optimizer</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is mixed precision training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Using both float16 and float32 precision - forward pass and gradients in float16 for speed and memory savings, but maintaining float32 master weights for numerical stability</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Using only float16</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Using only float32</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random precision</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: How do you choose batch size for transformer training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance between memory constraints and training stability. Larger batches provide more stable gradients but require more memory. Use gradient accumulation if needed. Typical sizes range from 16 to 512 depending on model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use largest possible</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use smallest</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is the purpose of learning rate warmup?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Warmup gradually increases learning rate from small value to target, preventing large gradient updates early in training that could destabilize the model, especially important for large models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To decrease learning rate</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Not needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only for small models</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: How do you monitor transformer training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Track loss curves (training and validation), learning rate, gradient norms, perplexity for language models, and task-specific metrics. Watch for overfitting, underfitting, or training instability</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only loss</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Don't monitor</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only at end</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is gradient clipping and when do you use it?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Gradient clipping caps gradient magnitude to prevent exploding gradients. Use it when gradients become very large, especially in deep networks or when using large learning rates</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To increase gradients</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Not needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only for small models</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: How do you handle long sequences in transformer training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use gradient checkpointing, chunk sequences, use sparse attention patterns, or train with shorter sequences and fine-tune on longer ones. O(n¬≤) attention complexity makes very long sequences expensive</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use full length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always truncate</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No solutions</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the typical training setup for large language models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Train on massive text corpora, use large batch sizes with gradient accumulation, AdamW optimizer with warmup and decay, mixed precision, distributed training across many GPUs, train for many epochs or until convergence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Small dataset, single GPU</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No warmup needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Train for one epoch</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter8" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 8</a>
                <a href="/tutorials/transformers/chapter10" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 10 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>