<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 7: Encoder Architecture - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 7: Encoder Architecture</h1>
                <p class="chapter-subtitle">Understanding the Encoder Stack</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="70"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn active">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn ">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn ">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand encoder architecture fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Encoder Architecture</h2>
                        
                        <div class="explanation-box">
                            <h3>What is the Encoder?</h3>
                            <p><strong>The encoder is a stack of identical layers that process input sequences to create rich, contextualized representations.</strong> Each encoder layer refines the representations, building increasingly abstract and task-relevant features.</p>
                            
                            <p><strong>Think of the encoder like a team of editors:</strong></p>
                            <ul>
                                <li><strong>Layer 1:</strong> Like a copy editor - fixes basic grammar and spelling</li>
                                <li><strong>Layer 6:</strong> Like a content editor - understands meaning and context</li>
                                <li><strong>Layer 12:</strong> Like a senior editor - understands deep relationships and nuances</li>
                                <li><strong>Result:</strong> Each layer adds more understanding, building up to a complete representation</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h4>Encoder Layer Components</h4>
                            <p><strong>Each encoder layer consists of two main sublayers:</strong></p>
                            
                            <div class="example-box">
                                <h5>1. Multi-Head Self-Attention</h5>
                                <ul>
                                    <li>Allows each position to attend to all positions</li>
                                    <li>Learns relationships between words</li>
                                    <li>Creates contextualized representations</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. Feed-Forward Network</h5>
                                <ul>
                                    <li>Processes information at each position</li>
                                    <li>Adds non-linearity and capacity</li>
                                    <li>Transforms the attention output</li>
                                </ul>
                            </div>
                            
                            <p><strong>Both sublayers use:</strong> Residual connections + Layer normalization</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Information Flow Through Layers</h4>
                            <p><strong>How representations evolve:</strong></p>
                            <ul>
                                <li><strong>Layer 1:</strong> Detects local patterns (adjacent words, simple syntax)</li>
                                <li><strong>Layer 3:</strong> Understands phrases and basic semantics</li>
                                <li><strong>Layer 6:</strong> Captures sentence-level meaning and relationships</li>
                                <li><strong>Layer 12:</strong> Understands complex semantics, long-range dependencies, task-specific features</li>
                            </ul>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>üîë Complete Encoder Layer Structure</h3>
                            <p><strong>Each encoder layer follows this structure (pre-norm):</strong></p>
                            
                            <div class="example-box">
                                <h4>Step-by-Step Flow</h4>
                                <ol>
                                    <li><strong>Input:</strong> x (from previous layer or embeddings)</li>
                                    <li><strong>Layer Norm:</strong> Normalize x</li>
                                    <li><strong>Multi-Head Attention:</strong> Apply self-attention</li>
                                    <li><strong>Residual:</strong> x + attention_output</li>
                                    <li><strong>Layer Norm:</strong> Normalize again</li>
                                    <li><strong>FFN:</strong> Apply feed-forward network</li>
                                    <li><strong>Residual:</strong> previous + ffn_output</li>
                                    <li><strong>Output:</strong> Refined representation</li>
                                </ol>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Stacking Layers</h3>
                            <p><strong>Multiple encoder layers are stacked:</strong></p>
                            <ul>
                                <li>Output of layer N becomes input to layer N+1</li>
                                <li>Each layer refines and builds upon previous representations</li>
                                <li>Like a pipeline: raw input ‚Üí refined ‚Üí more refined ‚Üí final representation</li>
                            </ul>
                            
                            <div class="example-box">
                                <h4>Common Configurations</h4>
                                <ul>
                                    <li><strong>BERT-base:</strong> 12 encoder layers</li>
                                    <li><strong>BERT-large:</strong> 24 encoder layers</li>
                                    <li><strong>GPT-2:</strong> 12-48 encoder layers (decoder-only, but similar structure)</li>
                                    <li><strong>Original Transformer:</strong> 6 encoder layers</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Bidirectional Processing</h3>
                            <p><strong>Encoder processes sequences bidirectionally:</strong></p>
                            <ul>
                                <li>Each position can attend to ALL positions (left and right)</li>
                                <li>Unlike decoders which are causal (only left-to-right)</li>
                                <li>Enables understanding full context</li>
                                <li>Perfect for tasks like classification, NER, Q&A</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Single Encoder Layer</h4>
                            
                            <div class="formula-display">
                                \[\text{EncoderLayer}(x) = x + \text{FFN}(\text{LayerNorm}(x + \text{MultiHeadAttention}(\text{LayerNorm}(x))))\]
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Breaking Down:</h5>
                                <ol>
                                    <li><strong>LayerNorm(x):</strong> Normalize input</li>
                                    <li><strong>MultiHeadAttention(...):</strong> Self-attention</li>
                                    <li><strong>x + ...:</strong> Residual connection</li>
                                    <li><strong>LayerNorm(...):</strong> Normalize again</li>
                                    <li><strong>FFN(...):</strong> Feed-forward network</li>
                                    <li><strong>x + ...:</strong> Final residual connection</li>
                                </ol>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h4>Complete Encoder Stack</h4>
                            
                            <div class="formula-display">
                                \[\text{Encoder}(x) = \text{EncoderLayer}_N(\ldots\text{EncoderLayer}_2(\text{EncoderLayer}_1(x)))\]
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>How Layers Stack:</h5>
                                <ul>
                                    <li>Input embeddings ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí ... ‚Üí Layer N</li>
                                    <li>Each layer's output becomes next layer's input</li>
                                    <li>Final output: Rich, contextualized representations</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Processing "The cat sat on the mat"</h4>
                            <p><strong>Let's trace through a 3-layer encoder:</strong></p>
                            
                            <div class="explanation-box">
                                <h5>Input (After Embeddings + Positional Encoding)</h5>
                                <ul>
                                    <li>Each word: [embedding + position] (512 dimensions)</li>
                                    <li>"The": [0.1, 0.2, ..., 0.5]</li>
                                    <li>"cat": [0.3, 0.4, ..., 0.7]</li>
                                    <li>... (all 6 words)</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>After Layer 1 (Local Patterns)</h5>
                                <ul>
                                    <li>Attention learns: "sat" attends to "cat" (subject-verb)</li>
                                    <li>Attention learns: "on" attends to "sat" (preposition-verb)</li>
                                    <li>FFN processes these local relationships</li>
                                    <li>Output: Basic syntactic patterns detected</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>After Layer 3 (Phrase Understanding)</h5>
                                <ul>
                                    <li>Attention learns: "The cat" as a noun phrase</li>
                                    <li>Attention learns: "sat on the mat" as a verb phrase</li>
                                    <li>FFN processes phrase-level semantics</li>
                                    <li>Output: Phrase structures understood</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>After Layer 6 (Sentence Meaning)</h5>
                                <ul>
                                    <li>Attention learns: Complete sentence structure</li>
                                    <li>Attention learns: Semantic relationships (cat ‚Üí animal, mat ‚Üí object)</li>
                                    <li>FFN processes sentence-level meaning</li>
                                    <li>Output: Rich semantic representation ready for tasks</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Complete Encoder Implementation</h4>
                            <pre><code class="language-python">import numpy as np

class EncoderLayer:
    """Single encoder layer"""
    
    def __init__(self, d_model, num_heads, d_ff):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        
        # Multi-head attention (simplified - would need full implementation)
        # self.attention = MultiHeadAttention(d_model, num_heads)
        
        # Feed-forward network
        # self.ffn = FeedForwardNetwork(d_model, d_ff)
        
        # Layer normalizations
        self.layer_norm1 = LayerNormalization(d_model)
        self.layer_norm2 = LayerNormalization(d_model)
    
    def forward(self, x, attention_fn, ffn_fn):
        """
        Forward pass through encoder layer
        
        Parameters:
        x: Input (batch, seq_len, d_model)
        attention_fn: Function for multi-head attention
        ffn_fn: Function for feed-forward network
        """
        # Sublayer 1: Multi-head self-attention with residual
        x_norm1 = self.layer_norm1.forward(x)
        attention_output = attention_fn(x_norm1)
        x = x + attention_output  # Residual connection
        
        # Sublayer 2: FFN with residual
        x_norm2 = self.layer_norm2.forward(x)
        ffn_output = ffn_fn(x_norm2)
        x = x + ffn_output  # Residual connection
        
        return x

class Encoder:
    """Complete encoder stack"""
    
    def __init__(self, num_layers, d_model, num_heads, d_ff):
        """
        Initialize encoder
        
        Parameters:
        num_layers: Number of encoder layers
        d_model: Model dimension
        num_heads: Number of attention heads
        d_ff: Feed-forward dimension
        """
        self.num_layers = num_layers
        self.layers = [
            EncoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]
    
    def forward(self, x, attention_fn, ffn_fn):
        """
        Forward pass through encoder stack
        
        Parameters:
        x: Input embeddings (batch, seq_len, d_model)
        attention_fn: Function for attention
        ffn_fn: Function for FFN
        """
        # Process through each layer sequentially
        for layer in self.layers:
            x = layer.forward(x, attention_fn, ffn_fn)
        
        return x

# Example usage
num_layers, d_model, num_heads, d_ff = 12, 512, 8, 2048
encoder = Encoder(num_layers, d_model, num_heads, d_ff)

# Input: (batch=2, seq_len=10, d_model=512)
x = np.random.randn(2, 10, 512)

# Forward pass (would need actual attention_fn and ffn_fn)
# output = encoder.forward(x, attention_fn, ffn_fn)
print(f"Encoder with {num_layers} layers created")
print(f"Input shape: {x.shape}")</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Encoder-Only Models</h3>
                            <p><strong>Encoder architecture is used in many important models:</strong></p>
                            
                            <div class="example-box">
                                <h4>1. BERT (Bidirectional Encoder Representations)</h4>
                                <ul>
                                    <li>12-24 encoder layers</li>
                                    <li>Bidirectional processing (sees full context)</li>
                                    <li>Used for: Classification, NER, Q&A, sentence similarity</li>
                                    <li>Revolutionary for understanding tasks</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>2. RoBERTa (Robust BERT)</h4>
                                <ul>
                                    <li>Improved training of BERT</li>
                                    <li>Same encoder architecture</li>
                                    <li>Better performance through better training</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>3. ALBERT (A Lite BERT)</h4>
                                <ul>
                                    <li>Parameter sharing across layers</li>
                                    <li>More efficient than BERT</li>
                                    <li>Same encoder structure, shared weights</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>What Each Layer Learns</h3>
                            <p><strong>Research shows layers specialize:</strong></p>
                            <ul>
                                <li><strong>Early layers (1-3):</strong> Syntax, POS tags, local patterns</li>
                                <li><strong>Middle layers (4-8):</strong> Semantics, phrase structure, relationships</li>
                                <li><strong>Deep layers (9-12):</strong> Task-specific features, complex reasoning</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: How many sublayers does each encoder layer have?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) 1</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) 2 (Multi-head attention and FFN)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) 3</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) 4</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the key difference between encoder and decoder?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Encoder is bidirectional, decoder is causal (left-to-right only)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Encoder has more layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Encoder doesn't use attention</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They are the same</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What happens to representations as they go through more encoder layers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) They become more abstract and task-specific</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They become simpler</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They stay the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They become random</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: How does the encoder process input bidirectionally?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Encoder uses self-attention that allows each token to attend to all other tokens in both directions simultaneously, creating representations that incorporate context from the entire sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only left to right</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only right to left</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Randomly</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What are the main components of an encoder layer?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Multi-head self-attention, feed-forward network, residual connections, and layer normalization, typically in pre-norm or post-norm configuration</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only attention</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only FFN</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random components</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: How do multiple encoder layers create deeper understanding?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Each layer refines representations from the previous layer, with early layers capturing local patterns and syntax, while deeper layers capture semantic relationships and high-level abstractions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're all the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only first layer matters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only last layer matters</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is the difference between encoder-only and encoder-decoder architectures?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Encoder-only (like BERT) processes input for understanding tasks, while encoder-decoder (like T5) uses encoder for input and decoder for generation, suitable for seq2seq tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Encoder-only generates</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Encoder-decoder only encodes</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: How would you implement an encoder stack from scratch?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Stack N encoder layers, each with self-attention and FFN. Add positional encoding to input, pass through layers with residual connections and layer norm. Output contextualized representations</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just one layer</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No attention needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random layers</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What happens to information as it flows through encoder layers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Information gets progressively refined and abstracted. Early layers focus on word-level and local patterns, middle layers capture phrase-level relationships, deeper layers capture document-level semantics and complex relationships</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It stays the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It gets simpler</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It's lost</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Why do encoder models like BERT use [CLS] token?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The [CLS] token's final representation aggregates information from the entire sequence, making it useful as a sentence-level representation for classification tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's not used</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It's just padding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No reason</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: How does encoder architecture scale with sequence length?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Self-attention has O(n¬≤) complexity, so computation grows quadratically with sequence length. This limits practical sequence lengths, leading to techniques like sparse attention or chunking for longer sequences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Linear scaling</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Constant time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No scaling issues</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What tasks are encoder-only models best suited for?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Understanding tasks like classification, named entity recognition, question answering, sentiment analysis, where you need to process and understand input rather than generate new text</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only translation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) All tasks equally</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter6" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 6</a>
                <a href="/tutorials/transformers/chapter8" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 8 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
