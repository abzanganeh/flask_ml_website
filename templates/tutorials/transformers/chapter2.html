<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Self-Attention Mechanism - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWAT2dVgYnHwpIK/NS" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 2: Self-Attention Mechanism</h1>
                <p class="chapter-subtitle">Attention is All You Need - How self-attention enables parallel processing</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="20"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn active">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="difference">Self vs Regular</button>
                    <button class="section-nav-btn azbn-btn" data-section="computation">Computation</button>
                    <button class="section-nav-btn azbn-btn" data-section="weights">Attention Weights</button>
                    <button class="section-nav-btn azbn-btn" data-section="parallel">Parallel Processing</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the difference between attention and self-attention</li>
                        <li>Master how self-attention computes relationships within a sequence</li>
                        <li>Learn the scaled dot-product attention formula in detail</li>
                        <li>Understand how attention weights reveal relationships</li>
                        <li>Recognize why self-attention enables parallel processing</li>
                        <li>Implement self-attention from scratch</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>What is Self-Attention?</h2>
                        
                        <div class="explanation-box">
                            <h3>Attention Within a Sequence</h3>
                            <p><strong>Self-attention is attention applied to the same sequence - each position attends to all positions in the same sequence, including itself.</strong></p>
                            
                            <p><strong>Key difference from regular attention:</strong></p>
                            <ul>
                                <li><strong>Regular Attention:</strong> Query from one sequence, Keys/Values from another sequence (encoder-decoder)</li>
                                <li><strong>Self-Attention:</strong> Query, Key, and Value all come from the same sequence</li>
                            </ul>
                            
                            <p><strong>Think of self-attention like a group discussion:</strong></p>
                            <ul>
                                <li>Each person (word) can look at and consider what everyone else (all words) is saying</li>
                                <li>Each person forms their understanding based on the full context</li>
                                <li>This happens simultaneously for everyone - parallel processing!</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Example: Understanding "it" in Context</h4>
                            <p><strong>Sentence:</strong> "The cat sat on the mat because it was tired."</p>
                            
                            <div class="explanation-box">
                                <h5>Self-Attention Process:</h5>
                                <ul>
                                    <li>When processing "it", self-attention looks at ALL words in the sentence</li>
                                    <li>It computes: "How relevant is each word for understanding 'it'?"</li>
                                    <li>High attention weight to "cat" ‚Üí "it" refers to "cat"</li>
                                    <li>Lower attention weights to other words</li>
                                    <li><strong>Result:</strong> "it" gets a representation that includes information about "cat"</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="difference" class="content-section">
                        <h2>Self-Attention vs Regular Attention</h2>
                        
                        <div class="explanation-box">
                            <h3>üîç The Key Distinction</h3>
                            
                            <div class="example-box">
                                <h4>Regular Attention (Encoder-Decoder)</h4>
                                <ul>
                                    <li><strong>Source:</strong> Encoder hidden states (e.g., French sentence)</li>
                                    <li><strong>Target:</strong> Decoder hidden state (e.g., English word being generated)</li>
                                    <li><strong>Query:</strong> From decoder (what am I looking for?)</li>
                                    <li><strong>Keys/Values:</strong> From encoder (what information is available?)</li>
                                    <li><strong>Use case:</strong> Translation, summarization</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h4>Self-Attention (Within Sequence)</h4>
                                <ul>
                                    <li><strong>Source:</strong> Same sequence (e.g., English sentence)</li>
                                    <li><strong>Target:</strong> Same sequence positions</li>
                                    <li><strong>Query, Key, Value:</strong> All from the same sequence</li>
                                    <li><strong>Use case:</strong> Understanding relationships within text, encoding context</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="computation" class="content-section">
                        <h2>How Self-Attention Computes</h2>
                        
                        <div class="formula-box">
                            <h4>Self-Attention Formula</h4>
                            
                            <div class="formula-display">
                                \[\text{SelfAttention}(X) = \text{Attention}(Q, K, V)\]
                                \[\text{where } Q = XW_Q, \quad K = XW_K, \quad V = XW_V\]
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Step-by-Step Process:</h5>
                                <ol>
                                    <li><strong>Input:</strong> Sequence X (n √ó d_model)</li>
                                    <li><strong>Linear Projections:</strong> Create Q, K, V from X using learned weight matrices</li>
                                    <li><strong>Attention Scores:</strong> Compute QK^T (similarity between all positions)</li>
                                    <li><strong>Scale:</strong> Divide by ‚àöd_k to prevent large values</li>
                                    <li><strong>Softmax:</strong> Convert to probabilities (attention weights)</li>
                                    <li><strong>Weighted Sum:</strong> Multiply attention weights by V</li>
                                    <li><strong>Output:</strong> New representation for each position</li>
                                </ol>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>Detailed Example: "The cat sat" with Visual Attention Flow</h4>
                            <p><strong>Input sequence (3 words):</strong></p>
                            
                            <div style="background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%); border: 2px solid #0ea5e9; border-radius: 12px; padding: 2rem; margin: 1.5rem 0;">
                                <h4 style="color: #0c4a6e; margin-bottom: 1.5rem; text-align: center;">üîÑ Self-Attention Flow Diagram</h4>
                                
                                <div style="background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); margin-bottom: 1.5rem;">
                                    <p style="font-weight: 600; color: #0c4a6e; margin-bottom: 1rem; text-align: center;">Step 1: Create Q, K, V from Input</p>
                                    <div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 1rem;">
                                        <div style="text-align: center; flex: 1; min-width: 150px;">
                                            <div style="background: #dbeafe; border: 2px solid #3b82f6; border-radius: 8px; padding: 0.8rem; margin-bottom: 0.5rem;">
                                                <p style="font-weight: 700; color: #1e40af; margin: 0; font-size: 0.9rem;">Input X</p>
                                            </div>
                                            <p style="font-size: 0.85rem; color: #1e40af; margin: 0;">["The", "cat", "sat"]</p>
                                        </div>
                                        <div style="font-size: 1.5rem; color: #0ea5e9;">‚Üí</div>
                                        <div style="text-align: center; flex: 1; min-width: 150px;">
                                            <div style="background: #fef3c7; border: 2px solid #f59e0b; border-radius: 8px; padding: 0.8rem; margin-bottom: 0.5rem;">
                                                <p style="font-weight: 700; color: #92400e; margin: 0; font-size: 0.9rem;">Q, K, V</p>
                                            </div>
                                            <p style="font-size: 0.85rem; color: #92400e; margin: 0;">3 queries, 3 keys, 3 values</p>
                                        </div>
                                    </div>
                                </div>
                                
                                <div style="background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); margin-bottom: 1.5rem;">
                                    <p style="font-weight: 600; color: #0c4a6e; margin-bottom: 1rem; text-align: center;">Step 2: Attention Flow for "cat" (position 1)</p>
                                    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin-bottom: 1rem;">
                                        <div style="text-align: center; background: #f3f4f6; padding: 1rem; border-radius: 8px;">
                                            <p style="font-weight: 600; margin: 0 0 0.5rem 0; color: #374151;">"The"</p>
                                            <div style="background: #fee2e2; height: 15px; border-radius: 4px; margin: 0 auto; width: 20%;"></div>
                                            <p style="font-size: 0.75rem; color: #991b1b; margin: 0.3rem 0 0 0;">Score: 0.1</p>
                                        </div>
                                        <div style="text-align: center; background: #dcfce7; padding: 1rem; border-radius: 8px; border: 2px solid #22c55e;">
                                            <p style="font-weight: 700; margin: 0 0 0.5rem 0; color: #166534;">"cat"</p>
                                            <div style="background: #22c55e; height: 60px; border-radius: 4px; margin: 0 auto; width: 80%;"></div>
                                            <p style="font-size: 0.75rem; color: #166534; margin: 0.3rem 0 0 0; font-weight: 700;">Score: 0.9</p>
                                        </div>
                                        <div style="text-align: center; background: #f3f4f6; padding: 1rem; border-radius: 8px;">
                                            <p style="font-weight: 600; margin: 0 0 0.5rem 0; color: #374151;">"sat"</p>
                                            <div style="background: #fee2e2; height: 25px; border-radius: 4px; margin: 0 auto; width: 30%;"></div>
                                            <p style="font-size: 0.75rem; color: #991b1b; margin: 0.3rem 0 0 0;">Score: 0.3</p>
                                        </div>
                                    </div>
                                    <div style="background: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 0.8rem; border-radius: 4px;">
                                        <p style="margin: 0; color: #0c4a6e; font-size: 0.9rem;"><strong>Query("cat")</strong> compares to all Keys ‚Üí High similarity with Key("cat") ‚Üí Strong attention!</p>
                                    </div>
                                </div>
                                
                                <div style="background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
                                    <p style="font-weight: 600; color: #0c4a6e; margin-bottom: 1rem; text-align: center;">Step 3 & 4: Softmax ‚Üí Weighted Sum</p>
                                    <div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 1rem;">
                                        <div style="text-align: center; flex: 1; min-width: 100px;">
                                            <p style="font-size: 0.85rem; color: #374151; margin-bottom: 0.5rem;">0.15 √ó V("The")</p>
                                            <div style="background: #fee2e2; height: 15px; border-radius: 4px; margin: 0 auto; width: 20%;"></div>
                                        </div>
                                        <div style="text-align: center; flex: 1; min-width: 100px;">
                                            <p style="font-size: 0.85rem; color: #166534; margin-bottom: 0.5rem; font-weight: 600;">0.70 √ó V("cat")</p>
                                            <div style="background: #22c55e; height: 50px; border-radius: 4px; margin: 0 auto; width: 70%; border: 2px solid #16a34a;"></div>
                                        </div>
                                        <div style="text-align: center; flex: 1; min-width: 100px;">
                                            <p style="font-size: 0.85rem; color: #374151; margin-bottom: 0.5rem;">0.15 √ó V("sat")</p>
                                            <div style="background: #fee2e2; height: 15px; border-radius: 4px; margin: 0 auto; width: 20%;"></div>
                                        </div>
                                        <div style="font-size: 1.5rem; color: #0ea5e9; font-weight: bold;">=</div>
                                        <div style="text-align: center; flex: 1; min-width: 120px; background: #dcfce7; padding: 1rem; border-radius: 8px; border: 2px solid #22c55e;">
                                            <p style="font-weight: 700; color: #166534; margin: 0;">Output("cat")</p>
                                            <p style="font-size: 0.85rem; color: #166534; margin: 0.3rem 0 0 0;">Contextualized!</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="explanation-box" style="margin-top: 1.5rem;">
                                <h5>Step-by-Step Breakdown:</h5>
                                <ol>
                                    <li><strong>Create Q, K, V:</strong> X = [embedding("The"), embedding("cat"), embedding("sat")] ‚Üí Q, K, V (3 queries, 3 keys, 3 values)</li>
                                    <li><strong>Compute Attention Scores:</strong> For "cat" (position 1), Query("cat") compares to all Keys ‚Üí Scores: [0.1, 0.9, 0.3]</li>
                                    <li><strong>Apply Softmax:</strong> Attention weights: [0.15, 0.70, 0.15] - "cat" attends most to itself, some to neighbors</li>
                                    <li><strong>Weighted Sum:</strong> Output("cat") = 0.15√óV("The") + 0.70√óV("cat") + 0.15√óV("sat") - New representation combines information from all positions</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div id="weights" class="content-section">
                        <h2>Understanding Attention Weights</h2>
                        
                        <div class="explanation-box">
                            <h3>What Attention Weights Reveal</h3>
                            <p><strong>Attention weights show which positions are most relevant for understanding each position.</strong></p>
                            
                            <div class="example-box">
                                <h4>Example: Pronoun Resolution with Visual Attention Map</h4>
                                <p><strong>Sentence:</strong> "The cat sat on the mat because it was tired."</p>
                                
                                <div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border: 2px solid #22c55e; border-radius: 12px; padding: 2rem; margin: 1.5rem 0;">
                                    <h4 style="color: #14532d; margin-bottom: 1rem; text-align: center;">üéØ Attention Weight Visualization</h4>
                                    
                                    <div style="background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
                                        <p style="font-weight: 600; color: #14532d; margin-bottom: 1rem; text-align: center;">Attention weights for word "it" (position 6):</p>
                                        
                                        <div style="display: flex; justify-content: space-between; align-items: center; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 1rem;">
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">The</p>
                                                <div style="background: #fee2e2; height: 20px; border-radius: 4px; margin-top: 0.3rem; width: 20%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.05</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">cat</p>
                                                <div style="background: #dcfce7; height: 60px; border-radius: 4px; margin-top: 0.3rem; width: 70%; margin: 0.3rem auto 0 auto; border: 2px solid #22c55e;"></div>
                                                <p style="font-size: 0.75rem; color: #166534; margin: 0.2rem 0 0 0; font-weight: 700;">0.70</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">sat</p>
                                                <div style="background: #fee2e2; height: 15px; border-radius: 4px; margin-top: 0.3rem; width: 15%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.03</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">on</p>
                                                <div style="background: #fee2e2; height: 10px; border-radius: 4px; margin-top: 0.3rem; width: 10%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.02</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">the</p>
                                                <div style="background: #fee2e2; height: 10px; border-radius: 4px; margin-top: 0.3rem; width: 10%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.02</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">mat</p>
                                                <div style="background: #fee2e2; height: 10px; border-radius: 4px; margin-top: 0.3rem; width: 10%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.02</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #dbeafe; border: 2px solid #3b82f6; border-radius: 6px;">
                                                <p style="font-weight: 700; margin: 0; font-size: 0.9rem; color: #1e40af;">it</p>
                                                <div style="background: #3b82f6; height: 30px; border-radius: 4px; margin-top: 0.3rem; width: 100%;"></div>
                                                <p style="font-size: 0.75rem; color: #1e40af; margin: 0.2rem 0 0 0; font-weight: 600;">(self)</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">was</p>
                                                <div style="background: #fee2e2; height: 15px; border-radius: 4px; margin-top: 0.3rem; width: 15%; margin: 0.3rem auto 0 auto;"></div>
                                                <p style="font-size: 0.75rem; color: #991b1b; margin: 0.2rem 0 0 0;">0.03</p>
                                            </div>
                                            <div style="flex: 1; min-width: 60px; text-align: center; padding: 0.5rem; background: #f3f4f6; border-radius: 6px;">
                                                <p style="font-weight: 600; margin: 0; font-size: 0.9rem;">tired</p>
                                                <div style="background: #fef3c7; height: 25px; border-radius: 4px; margin-top: 0.3rem; width: 25%; margin: 0.3rem auto 0 auto; border: 1px solid #f59e0b;"></div>
                                                <p style="font-size: 0.75rem; color: #92400e; margin: 0.2rem 0 0 0; font-weight: 600;">0.20</p>
                                            </div>
                                        </div>
                                        
                                        <div style="background: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin-top: 1rem; border-radius: 4px;">
                                            <p style="margin: 0; color: #0c4a6e;"><strong>üí° Interpretation:</strong> "it" attends most strongly to "cat" (0.70) - the model correctly identifies the referent! It also attends to "tired" (0.20) as a related concept. The bar heights represent attention weights.</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div id="parallel" class="content-section">
                        <h2>Parallel Processing Advantage</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Self-Attention is Fast</h3>
                            <p><strong>Self-attention can process all positions simultaneously, unlike RNNs which must process sequentially.</strong></p>
                            
                            <div class="example-box">
                                <h4>RNN vs Self-Attention</h4>
                                <p><strong>RNN (Sequential):</strong></p>
                                <ul>
                                    <li>Process word 1 ‚Üí wait ‚Üí process word 2 ‚Üí wait ‚Üí process word 3</li>
                                    <li>Time complexity: O(n) sequential steps</li>
                                    <li>Cannot parallelize across sequence</li>
                                </ul>
                                
                                <p><strong>Self-Attention (Parallel):</strong></p>
                                <ul>
                                    <li>Process all words simultaneously</li>
                                    <li>Time complexity: O(1) parallel steps (though O(n¬≤) operations)</li>
                                    <li>Can use GPU parallelism effectively</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Self-Attention Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Complete Self-Attention Implementation</h4>
                            <pre><code class="language-python">import numpy as np

def self_attention(X, W_Q, W_K, W_V, d_k):
    """
    Self-attention mechanism
    
    Parameters:
    X: Input sequence (n, d_model)
    W_Q: Query weight matrix (d_model, d_k)
    W_K: Key weight matrix (d_model, d_k)
    W_V: Value weight matrix (d_model, d_v)
    d_k: Dimension of keys/queries
    """
    n = X.shape[0]
    
    # Step 1: Create Q, K, V from input
    Q = np.dot(X, W_Q)  # (n, d_k)
    K = np.dot(X, W_K)  # (n, d_k)
    V = np.dot(X, W_V)  # (n, d_v)
    
    # Step 2: Compute attention scores
    scores = np.dot(Q, K.T)  # (n, n)
    
    # Step 3: Scale
    scores = scores / np.sqrt(d_k)
    
    # Step 4: Apply softmax
    attention_weights = softmax(scores, axis=-1)  # (n, n)
    
    # Step 5: Weighted sum of values
    output = np.dot(attention_weights, V)  # (n, d_v)
    
    return output, attention_weights

def softmax(x, axis=-1):
    """Softmax function"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# Example usage
n, d_model, d_k, d_v = 10, 512, 64, 64
X = np.random.randn(n, d_model)
W_Q = np.random.randn(d_model, d_k) * 0.1
W_K = np.random.randn(d_model, d_k) * 0.1
W_V = np.random.randn(d_model, d_v) * 0.1

output, weights = self_attention(X, W_Q, W_K, W_V, d_k)
print(f"Output shape: {output.shape}")  # (10, 64)
print(f"Attention weights shape: {weights.shape}")  # (10, 10)</code></pre>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the key difference between self-attention and regular attention?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Self-attention uses Q, K, V from the same sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Self-attention is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Self-attention uses fewer parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Self-attention doesn't use softmax</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Why can self-attention process all positions in parallel?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Each position can attend to all positions independently</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It uses less memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It doesn't need gradients</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It uses smaller matrices</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 1</a>
                <a href="/tutorials/transformers/chapter3" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 3: Multi-Head Attention ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlG8jLC0KXLSyHiQtD6lqG3t3a3H4RbQT6GhhDYFyK4aQo5hk6g/AVC/gw" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>
