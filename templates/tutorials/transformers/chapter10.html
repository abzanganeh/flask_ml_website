<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 10: Transformer Variants & Optimizations - Transformer Architecture Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/transformers/transformers.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWAT2dVgYnHwpIK/NS" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/transformers" class="course-link">
                    <span>Transformer Architecture Deep Dive</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 10: Transformer Variants & Optimizations</h1>
                <p class="chapter-subtitle">Beyond the Original</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="100"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/transformers/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/transformers/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/transformers/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/transformers/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/transformers/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/transformers/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/transformers/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/transformers/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                    <a href="/tutorials/transformers/chapter9" class="chapter-nav-btn ">Chapter 9</a>
                    <a href="/tutorials/transformers/chapter10" class="chapter-nav-btn active">Chapter 10</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand transformer variants & optimizations fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Transformer Variants & Optimizations</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Beyond the Original</strong></p>
                            <p>This chapter provides comprehensive coverage of transformer variants & optimizations, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding transformer variants & optimizations is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Transformer Variants</h3>
                            <p><strong>Many architectures extend the original transformer:</strong></p>
                            <ul>
                                <li><strong>BERT:</strong> Bidirectional encoder for understanding tasks</li>
                                <li><strong>GPT:</strong> Autoregressive decoder for generation</li>
                                <li><strong>T5:</strong> Encoder-decoder for text-to-text tasks</li>
                                <li><strong>RoBERTa:</strong> Optimized BERT training</li>
                                <li><strong>ALBERT:</strong> Parameter-sharing for efficiency</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Optimization Techniques</h3>
                            <p><strong>Key optimizations improve efficiency:</strong></p>
                            <ul>
                                <li><strong>Sparse Attention:</strong> Reduce computation by attending to fewer positions</li>
                                <li><strong>Linear Attention:</strong> Approximate attention with linear complexity</li>
                                <li><strong>Quantization:</strong> Use lower precision to reduce memory</li>
                                <li><strong>Pruning:</strong> Remove less important weights</li>
                                <li><strong>Knowledge Distillation:</strong> Train smaller models from larger ones</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Scaling Strategies</h3>
                            <p><strong>Methods to scale transformers:</strong></p>
                            <ul>
                                <li><strong>Model Parallelism:</strong> Distribute model across devices</li>
                                <li><strong>Pipeline Parallelism:</strong> Split layers across GPUs</li>
                                <li><strong>Mixed Precision:</strong> Use float16 for speed, float32 for stability</li>
                                <li><strong>Gradient Checkpointing:</strong> Trade compute for memory</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Efficient Attention Variants</h4>
                            <div class="formula-display">
                                <strong>Sparse Attention:</strong> Only compute attention for selected positions<br>
                                <strong>Linear Attention:</strong> \[O(n)\] complexity instead of \[O(n^2)\]<br>
                                <strong>Flash Attention:</strong> Memory-efficient attention computation
                            </div>
                            <div class="formula-explanation">
                                <p>Various optimizations reduce the quadratic complexity of standard attention, making transformers more efficient for long sequences.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Model Parallelism</h4>
                            <div class="formula-display">
                                <strong>Pipeline Parallelism:</strong> Split layers across devices<br>
                                <strong>Tensor Parallelism:</strong> Split matrix operations<br>
                                <strong>Data Parallelism:</strong> Replicate model, split data
                            </div>
                            <div class="formula-explanation">
                                <p>Parallelization strategies enable training and inference of very large models across multiple GPUs/TPUs.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Quantization</h4>
                            <div class="formula-display">
                                <strong>Quantized Weight:</strong> \[W_q = \text{round}(W \times s)\]<br>
                                <strong>Dequantization:</strong> \[W \approx W_q / s\]<br>
                                Where \(s\) is the quantization scale factor
                            </div>
                            <div class="formula-explanation">
                                <p>Quantization reduces model size and memory by using lower precision (e.g., int8 instead of float32) while maintaining reasonable accuracy.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Knowledge Distillation</h4>
                            <div class="formula-display">
                                <strong>Distillation Loss:</strong> \[L = \alpha L_{\text{task}} + (1-\alpha) L_{\text{distill}}\]<br>
                                <strong>Distillation Component:</strong> \[L_{\text{distill}} = \text{KL}(P_{\text{student}} || P_{\text{teacher}})\]<br>
                                Where \(P\) are probability distributions from teacher and student models
                            </div>
                            <div class="formula-explanation">
                                <p>Knowledge distillation trains smaller student models to mimic larger teacher models, transferring knowledge while reducing size.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Sparse Attention Pattern</h4>
                            <p><strong>Instead of full attention (all-to-all), sparse attention might use:</strong></p>
                            <ul>
                                <li>Local attention: Each token attends to nearby tokens (window of size w)</li>
                                <li>Strided attention: Attend to every k-th token</li>
                                <li>Global attention: Some tokens attend to all positions</li>
                            </ul>
                            <p>This reduces computation from O(n¬≤) to O(n√ów) for local attention.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Model Variants Comparison</h4>
                            <p><strong>BERT vs GPT vs T5:</strong></p>
                            <ul>
                                <li><strong>BERT:</strong> "The cat sat" ‚Üí [CLS] token for classification</li>
                                <li><strong>GPT:</strong> "The cat sat" ‚Üí predicts "on" (next token)</li>
                                <li><strong>T5:</strong> "translate: The cat sat" ‚Üí "Le chat s'est assis"</li>
                            </ul>
                            <p>Each architecture is optimized for different task types.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Quantization Impact</h4>
                            <p><strong>Original model:</strong> 1 billion parameters √ó 4 bytes (float32) = 4GB</p>
                            <p><strong>Quantized (int8):</strong> 1 billion parameters √ó 1 byte = 1GB</p>
                            <p>4√ó reduction in memory with minimal accuracy loss when done carefully.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Sparse Attention Implementation</h4>
                            <pre><code class="language-python">import torch
import torch.nn as nn

def sparse_attention(Q, K, V, window_size=3):
    """
    Sparse attention with local window
    
    Args:
        Q, K, V: Query, Key, Value tensors (batch, seq_len, d_model)
        window_size: Number of nearby tokens to attend to
    """
    seq_len = Q.size(1)
    scores = torch.matmul(Q, K.transpose(-2, -1))
    
    # Create sparse mask (only attend to nearby tokens)
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2 + 1)
        mask[i, start:end] = 1
    
    # Apply mask
    scores = scores.masked_fill(mask == 0, float('-inf'))
    scores = scores / (Q.size(-1) ** 0.5)
    attn_weights = torch.softmax(scores, dim=-1)
    
    output = torch.matmul(attn_weights, V)
    return output</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Quantization Example</h4>
                            <pre><code class="language-python">import numpy as np

def quantize_weights(weights, bits=8):
    """
    Quantize weights to int8
    """
    # Calculate scale factor
    max_val = np.abs(weights).max()
    scale = (2 ** (bits - 1) - 1) / max_val
    
    # Quantize
    quantized = np.round(weights * scale).astype(np.int8)
    
    return quantized, scale

def dequantize_weights(quantized, scale):
    """
    Dequantize back to float
    """
    return quantized.astype(np.float32) / scale

# Example
weights = np.random.randn(100, 100).astype(np.float32)
quantized, scale = quantize_weights(weights)
reconstructed = dequantize_weights(quantized, scale)
print(f"Original size: {weights.nbytes} bytes")
print(f"Quantized size: {quantized.nbytes} bytes")
print(f"Compression: {weights.nbytes / quantized.nbytes:.1f}x")</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>BERT Applications</h3>
                            <p><strong>Understanding tasks:</strong></p>
                            <ul>
                                <li>Sentiment analysis in customer reviews</li>
                                <li>Named entity recognition in documents</li>
                                <li>Question answering systems</li>
                                <li>Text classification and tagging</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>GPT Applications</h3>
                            <p><strong>Generation tasks:</strong></p>
                            <ul>
                                <li>Chatbots and conversational AI</li>
                                <li>Code generation and completion</li>
                                <li>Content creation and writing assistance</li>
                                <li>Text summarization</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>T5 Applications</h3>
                            <p><strong>Text-to-text tasks:</strong></p>
                            <ul>
                                <li>Machine translation</li>
                                <li>Text summarization</li>
                                <li>Question answering</li>
                                <li>Text classification (formatted as generation)</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Optimization Benefits</h3>
                            <p><strong>Efficiency improvements enable:</strong></p>
                            <ul>
                                <li>Faster inference for real-time applications</li>
                                <li>Deployment on edge devices</li>
                                <li>Reduced computational costs</li>
                                <li>Handling longer sequences</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are the main applications of transformer models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Natural language understanding (BERT), text generation (GPT), translation, summarization, question answering, chatbots, code generation, and many NLP tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Limited applications</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is BERT and how does it work?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) BERT is a bidirectional encoder-only transformer pre-trained on masked language modeling and next sentence prediction, then fine-tuned for downstream tasks like classification and QA</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's a decoder model</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It only generates</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It's not a transformer</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is GPT and how does it differ from BERT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) GPT is a decoder-only transformer pre-trained on next token prediction, generating text autoregressively. BERT is encoder-only for understanding, GPT is for generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) GPT is encoder-only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) BERT generates text</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: How do you fine-tune a pre-trained transformer?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Start with pre-trained weights, add task-specific head if needed, train on your task data with lower learning rate than pre-training, often freezing early layers and only fine-tuning later layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Train from scratch</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Don't use pre-trained weights</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Same as pre-training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is transfer learning in transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Using knowledge learned from large-scale pre-training on general tasks and applying it to specific downstream tasks, allowing models to perform well with less task-specific data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Training from scratch</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only using task data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No pre-training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What tasks can encoder models like BERT handle?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Classification, named entity recognition, question answering, sentiment analysis, text similarity - tasks requiring understanding and processing input rather than generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only translation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Limited tasks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What tasks can decoder models like GPT handle?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Text generation, completion, story writing, code generation, chatbots, creative writing - tasks requiring generating new text from context</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only understanding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Limited tasks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is T5 and how does it work?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) T5 is an encoder-decoder transformer that frames all NLP tasks as text-to-text problems, using the same architecture for translation, summarization, QA, and classification by converting tasks to text generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's encoder-only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It's decoder-only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not a transformer</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: How do you choose between BERT, GPT, and T5 for a task?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) BERT for understanding/classification tasks, GPT for generation tasks, T5 for seq2seq tasks like translation/summarization. Consider your task type, data availability, and computational resources</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use BERT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use GPT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They're interchangeable</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is prompt engineering and why is it important?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Prompt engineering is crafting input prompts to guide model behavior, especially for large language models. Good prompts can significantly improve performance without fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Not important</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only for small models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random prompts work</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: How do modern transformers scale to billions of parameters?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Through model parallelism, pipeline parallelism, efficient attention mechanisms, mixture-of-experts architectures, and distributed training across many GPUs/TPUs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Single GPU training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No special techniques</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Can't scale</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What are some limitations of transformer models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) O(n¬≤) attention complexity limits sequence length, high computational and memory requirements, need for large datasets, potential for generating biased or incorrect information, and difficulty with very long-range dependencies</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No limitations</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only computational</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Perfect for all tasks</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/transformers" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/transformers/chapter9" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 9</a>
                
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlG8jLC0KXLSyHiQtD6lqG3t3a3H4RbQT6GhhDYFyK4aQo5hk6g/AVC/gw" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/transformers/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>