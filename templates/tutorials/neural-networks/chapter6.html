<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Recurrent Neural Networks (RNNs) - Neural Networks Fundamentals</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/neural-networks/neural-networks.css') }}">
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/neural-networks" class="course-link">
                    <span>Neural Networks Fundamentals</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 6: Recurrent Neural Networks (RNNs)</h1>
                <p class="chapter-subtitle">Networks with memory for sequential data processing</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="75"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/neural-networks/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/neural-networks/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/neural-networks/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/neural-networks/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/neural-networks/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/neural-networks/chapter6" class="chapter-nav-btn active">Chapter 6</a>
                    <a href="/tutorials/neural-networks/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/neural-networks/chapter8" class="chapter-nav-btn">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="recurrence">Recurrence</button>
                    <button class="section-nav-btn azbn-btn" data-section="unfolding">Unfolding</button>
                    <button class="section-nav-btn azbn-btn" data-section="backprop">Backprop Through Time</button>
                    <button class="section-nav-btn azbn-btn" data-section="vanishing">Vanishing Gradients</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand why RNNs are needed for sequences</li>
                        <li>Master the recurrence relation in RNNs</li>
                        <li>Learn unrolling/unfolding RNNs through time</li>
                        <li>Understand Backpropagation Through Time (BPTT)</li>
                        <li>Recognize the vanishing gradient problem</li>
                        <li>Implement a simple RNN from scratch</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>What are Recurrent Neural Networks?</h2>
                        
                        <div class="explanation-box">
                            <h3>üîÑ Networks with Memory</h3>
                            <p><strong>Recurrent Neural Networks (RNNs) are designed to process sequential data by maintaining a hidden state that captures information about previous inputs.</strong> Unlike feedforward networks, RNNs have connections that form cycles, allowing information to persist.</p>
                            
                            <p><strong>Key Characteristics:</strong></p>
                            <ul>
                                <li><strong>Sequential Processing:</strong> Process one element at a time</li>
                                <li><strong>Hidden State:</strong> Maintains memory of previous inputs</li>
                                <li><strong>Shared Weights:</strong> Same parameters used at each time step</li>
                                <li><strong>Variable Length:</strong> Can handle sequences of any length</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why RNNs for Sequences?</h4>
                            <p><strong>Problem with Feedforward Networks:</strong></p>
                            <ul>
                                <li>Fixed input size (can't handle variable-length sequences)</li>
                                <li>No memory of previous inputs</li>
                                <li>Can't understand context or order</li>
                            </ul>
                            
                            <p><strong>RNN Solution:</strong></p>
                            <ul>
                                <li>Process sequences element by element</li>
                                <li>Hidden state remembers previous context</li>
                                <li>Context influences current prediction</li>
                                <li>Perfect for text, speech, time series</li>
                            </ul>
                        </div>
                    </div>

                    <div id="recurrence" class="content-section">
                        <h2>The Recurrence Relation</h2>
                        
                        <div class="formula-box">
                            <h4>RNN Forward Pass</h4>
                            <p><strong>At each time step t:</strong></p>
                            
                            <div class="formula-display">
                                <strong>h_t = tanh(W_hh √ó h_{t-1} + W_xh √ó x_t + b_h)</strong><br>
                                <strong>y_t = W_hy √ó h_t + b_y</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Notation:</h5>
                                <ul>
                                    <li><strong>h_t:</strong> Hidden state at time t (memory)</li>
                                    <li><strong>x_t:</strong> Input at time t</li>
                                    <li><strong>y_t:</strong> Output at time t</li>
                                    <li><strong>W_hh:</strong> Hidden-to-hidden weights</li>
                                    <li><strong>W_xh:</strong> Input-to-hidden weights</li>
                                    <li><strong>W_hy:</strong> Hidden-to-output weights</li>
                                    <li><strong>b_h, b_y:</strong> Bias terms</li>
                                </ul>
                            </div>
                        </div>

                        <div class="code-box">
                            <h4>üíª Simple RNN Implementation</h4>
                            <pre><code>import numpy as np

class SimpleRNN:
    """Simple Recurrent Neural Network"""
    
    def __init__(self, input_size, hidden_size, output_size):
        # Weight matrices
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.1
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.1
        self.W_hy = np.random.randn(output_size, hidden_size) * 0.1
        
        # Biases
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((output_size, 1))
        
        # Hidden state (initialized to zero)
        self.h = np.zeros((hidden_size, 1))
    
    def tanh(self, x):
        return np.tanh(x)
    
    def forward_step(self, x):
        """Process one time step"""
        # Update hidden state
        self.h = self.tanh(np.dot(self.W_xh, x) + 
                          np.dot(self.W_hh, self.h) + 
                          self.b_h)
        
        # Compute output
        y = np.dot(self.W_hy, self.h) + self.b_y
        
        return y, self.h
    
    def forward(self, sequence):
        """Process entire sequence"""
        outputs = []
        self.h = np.zeros((self.W_hh.shape[0], 1))  # Reset state
        
        for x in sequence:
            y, h = self.forward_step(x)
            outputs.append(y)
        
        return outputs

# Example: Process sequence
rnn = SimpleRNN(input_size=3, hidden_size=4, output_size=2)
sequence = [np.array([[1], [2], [3]]), 
            np.array([[4], [5], [6]]),
            np.array([[7], [8], [9]])]

outputs = rnn.forward(sequence)
print(f"Processed {len(outputs)} time steps")</code></pre>
                        </div>
                    </div>

                    <div id="unfolding" class="content-section">
                        <h2>Unfolding RNNs Through Time</h2>
                        
                        <div class="explanation-box">
                            <h3>üìñ Visualizing Recurrence</h3>
                            <p><strong>Unfolding (or unrolling) an RNN means visualizing it as a feedforward network where each time step becomes a separate layer.</strong> This helps understand how information flows through time.</p>
                        </div>

                        <div class="example-box">
                            <h4>üî¢ Unfolding Example</h4>
                            <p><strong>For a sequence of length 3:</strong></p>
                            <ul>
                                <li><strong>t=0:</strong> h‚ÇÄ = tanh(W_xh √ó x‚ÇÄ + W_hh √ó h_{-1} + b_h)</li>
                                <li><strong>t=1:</strong> h‚ÇÅ = tanh(W_xh √ó x‚ÇÅ + W_hh √ó h‚ÇÄ + b_h)</li>
                                <li><strong>t=2:</strong> h‚ÇÇ = tanh(W_xh √ó x‚ÇÇ + W_hh √ó h‚ÇÅ + b_h)</li>
                            </ul>
                            
                            <p><strong>Unfolded view:</strong></p>
                            <pre style="background: #f5f5f5; padding: 1rem; border-radius: 5px;">
x‚ÇÄ ‚Üí [RNN] ‚Üí h‚ÇÄ ‚Üí [RNN] ‚Üí h‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÇ
      ‚Üë            ‚Üë            ‚Üë
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      (shared weights W_hh)</pre>
                        </div>
                    </div>

                    <div id="backprop" class="content-section">
                        <h2>Backpropagation Through Time (BPTT)</h2>
                        
                        <div class="explanation-box">
                            <h3>üîÑ Training RNNs</h3>
                            <p><strong>Backpropagation Through Time (BPTT) is the algorithm used to train RNNs.</strong> It's essentially backpropagation applied to the unfolded network, computing gradients across all time steps.</p>
                        </div>

                        <div class="formula-box">
                            <h4>BPTT Gradient Computation</h4>
                            <p><strong>Gradient flows backward through time:</strong></p>
                            
                            <div class="formula-display">
                                <strong>‚àÇL/‚àÇh_t = ‚àÇL/‚àÇy_t √ó W_hy + ‚àÇL/‚àÇh_{t+1} √ó W_hh</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Key Insight:</h5>
                                <ul>
                                    <li>Gradient at time t depends on gradient at time t+1</li>
                                    <li>Must compute gradients backward through entire sequence</li>
                                    <li>Gradients are multiplied by W_hh at each step</li>
                                    <li>This can cause vanishing or exploding gradients</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="vanishing" class="content-section">
                        <h2>The Vanishing Gradient Problem</h2>
                        
                        <div class="explanation-box">
                            <h3>‚ö†Ô∏è RNN's Main Limitation</h3>
                            <p><strong>The vanishing gradient problem occurs when gradients become exponentially small as they propagate backward through time.</strong> This makes it difficult for RNNs to learn long-term dependencies.</p>
                        </div>

                        <div class="formula-box">
                            <h4>Why Gradients Vanish</h4>
                            <p><strong>Gradient at time t-k:</strong></p>
                            
                            <div class="formula-display">
                                <strong>‚àÇL/‚àÇh_{t-k} = ‚àÇL/‚àÇh_t √ó (W_hh)^k √ó tanh'(z_{t-k}) √ó ... √ó tanh'(z_t)</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Problem:</h5>
                                <ul>
                                    <li>Gradient is multiplied by W_hh k times</li>
                                    <li>If |W_hh| < 1, gradient shrinks exponentially</li>
                                    <li>tanh'(z) ‚â§ 1, further reducing gradient</li>
                                    <li>Result: Early time steps receive almost no gradient</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Impact</h4>
                            <p><strong>Example: Language Modeling</strong></p>
                            <p>Sentence: "The cat, which was very fluffy, sat on the mat."</p>
                            <p>To predict "sat", the network needs to remember "cat" from much earlier. With vanishing gradients, the network can't learn this long-term dependency!</p>
                            
                            <p><strong>Solution:</strong> LSTMs and GRUs (covered in Chapter 7) solve this problem.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Complete RNN Implementation</h2>
                        
                        <div class="code-box">
                            <h4>üíª Full RNN with BPTT</h4>
                            <pre><code>import numpy as np

class RNN:
    """Recurrent Neural Network with Backpropagation Through Time"""
    
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize weights
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01
        
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((output_size, 1))
    
    def forward(self, x_sequence):
        """Forward pass through sequence"""
        seq_len = len(x_sequence)
        h_states = [np.zeros((self.hidden_size, 1))]
        y_outputs = []
        
        for t in range(seq_len):
            # Compute hidden state
            h_t = np.tanh(np.dot(self.W_xh, x_sequence[t]) + 
                         np.dot(self.W_hh, h_states[-1]) + 
                         self.b_h)
            h_states.append(h_t)
            
            # Compute output
            y_t = np.dot(self.W_hy, h_t) + self.b_y
            y_outputs.append(y_t)
        
        return y_outputs, h_states
    
    def backward(self, x_sequence, y_outputs, h_states, targets, learning_rate=0.01):
        """Backpropagation Through Time"""
        seq_len = len(x_sequence)
        
        # Initialize gradients
        dW_xh = np.zeros_like(self.W_xh)
        dW_hh = np.zeros_like(self.W_hh)
        dW_hy = np.zeros_like(self.W_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)
        
        dh_next = np.zeros((self.hidden_size, 1))
        
        # Backward through time
        for t in reversed(range(seq_len)):
            # Output layer gradient
            dy = y_outputs[t] - targets[t]
            dW_hy += np.dot(dy, h_states[t+1].T)
            db_y += dy
            
            # Hidden layer gradient
            dh = np.dot(self.W_hy.T, dy) + dh_next
            dh_raw = (1 - h_states[t+1]**2) * dh  # tanh derivative
            
            dW_xh += np.dot(dh_raw, x_sequence[t].T)
            dW_hh += np.dot(dh_raw, h_states[t].T)
            db_h += dh_raw
            
            dh_next = np.dot(self.W_hh.T, dh_raw)
        
        # Update weights
        self.W_xh -= learning_rate * dW_xh
        self.W_hh -= learning_rate * dW_hh
        self.W_hy -= learning_rate * dW_hy
        self.b_h -= learning_rate * db_h
        self.b_y -= learning_rate * db_y

# Usage
rnn = RNN(input_size=10, hidden_size=20, output_size=5)
# Training loop would go here</code></pre>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main advantage of RNNs over feedforward networks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) They are faster to train</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) They can process sequences and maintain memory of previous inputs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They require fewer parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They always give better accuracy</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the vanishing gradient problem in RNNs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Gradients become exponentially small when propagating backward through time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) RNNs forget previous inputs too quickly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) RNNs require too much memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) RNNs can't process sequences</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What does BPTT stand for?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Backpropagation Through Training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Backpropagation Through Time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Backward Propagation Through Time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Best Performance Through Training</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/neural-networks" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/neural-networks/chapter5" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 5</a>
                <a href="/tutorials/neural-networks/chapter7" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 7: LSTMs ‚Üí</a>
            </div>
        </div>
    </footer>
    
    <script src="{{ url_for('static', filename='js/tutorials/neural-networks/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
    </script>
</body>
</html>
