<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Recurrent Neural Networks (RNNs) - Neural Networks Fundamentals</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/neural-networks/neural-networks.css') }}">
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/neural-networks" class="course-link">
                    <span>Neural Networks Fundamentals</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 6: Recurrent Neural Networks (RNNs)</h1>
                <p class="chapter-subtitle">Networks with memory for sequential data processing</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="75"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/neural-networks/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/neural-networks/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/neural-networks/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/neural-networks/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/neural-networks/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/neural-networks/chapter6" class="chapter-nav-btn active">Chapter 6</a>
                    <a href="/tutorials/neural-networks/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/neural-networks/chapter8" class="chapter-nav-btn">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="recurrence">Recurrence</button>
                    <button class="section-nav-btn azbn-btn" data-section="unfolding">Unfolding</button>
                    <button class="section-nav-btn azbn-btn" data-section="backprop">Backprop Through Time</button>
                    <button class="section-nav-btn azbn-btn" data-section="vanishing">Vanishing Gradients</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand why RNNs are needed for sequences</li>
                        <li>Master the recurrence relation in RNNs</li>
                        <li>Learn unrolling/unfolding RNNs through time</li>
                        <li>Understand Backpropagation Through Time (BPTT)</li>
                        <li>Recognize the vanishing gradient problem</li>
                        <li>Implement a simple RNN from scratch</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>What are Recurrent Neural Networks?</h2>
                        
                        <div class="explanation-box">
                            <h3>üîÑ Networks with Memory</h3>
                            <p><strong>Recurrent Neural Networks (RNNs) are designed to process sequential data by maintaining a hidden state that captures information about previous inputs.</strong> Unlike feedforward networks, RNNs have connections that form cycles, allowing information to persist.</p>
                            
                            <p><strong>Think of RNNs like reading a book:</strong></p>
                            <ul>
                                <li><strong>Feedforward Network:</strong> Like reading random sentences from different pages - no context, no understanding of the story</li>
                                <li><strong>RNN:</strong> Like reading page by page, remembering what happened before - you understand the plot because you remember previous chapters</li>
                                <li><strong>Hidden State:</strong> Like your memory of the story so far - it influences how you understand the current page</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h4>üéØ The Fundamental Problem with Feedforward Networks</h4>
                            <p><strong>Why can't we just use regular neural networks for sequences?</strong></p>
                            
                            <div class="example-box">
                                <h5>Problem 1: Fixed Input Size</h5>
                                <p><strong>Feedforward networks require fixed-size inputs:</strong></p>
                                <ul>
                                    <li>If your network expects 10 inputs, you MUST provide exactly 10</li>
                                    <li>But sequences vary in length: "Hello" (5 characters) vs "Hello, how are you?" (19 characters)</li>
                                    <li><strong>Solution needed:</strong> Process one element at a time, regardless of sequence length</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>Problem 2: No Memory</h5>
                                <p><strong>Feedforward networks have no memory of previous inputs:</strong></p>
                                <p><strong>Example sentence:</strong> "The cat sat on the mat because it was tired."</p>
                                <ul>
                                    <li>To understand "it", you need to remember "cat" from earlier</li>
                                    <li>Feedforward network: "What is 'it'?" ‚Üí No idea, no memory of "cat"</li>
                                    <li>RNN: "What is 'it'?" ‚Üí Remembers "cat" from hidden state ‚Üí "it" refers to "cat"</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>Problem 3: Order Matters</h5>
                                <p><strong>In sequences, order is critical:</strong></p>
                                <ul>
                                    <li>"Dog bites man" vs "Man bites dog" - completely different meanings!</li>
                                    <li>Feedforward network: Treats input as unordered set ‚Üí can't distinguish</li>
                                    <li>RNN: Processes in order ‚Üí understands the difference</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h4>üîÑ The RNN Solution: Three Key Innovations</h4>
                            
                            <div class="explanation-box">
                                <h5>1. Sequential Processing</h5>
                                <p><strong>RNNs process sequences one element at a time:</strong></p>
                                <ul>
                                    <li>Input: "The cat sat"</li>
                                    <li>Step 1: Process "The" ‚Üí update hidden state</li>
                                    <li>Step 2: Process "cat" ‚Üí update hidden state (remembers "The")</li>
                                    <li>Step 3: Process "sat" ‚Üí update hidden state (remembers "The cat")</li>
                                    <li><strong>Key:</strong> Each step builds on previous context</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>2. Hidden State (Memory)</h5>
                                <p><strong>The hidden state is the RNN's memory:</strong></p>
                                <ul>
                                    <li>It's a vector that encodes information from all previous inputs</li>
                                    <li>Like a summary of "what has happened so far"</li>
                                    <li>Updated at each time step to incorporate new information</li>
                                    <li><strong>Analogy:</strong> Like a running summary of a conversation - you remember key points, not every word</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>3. Shared Weights</h5>
                                <p><strong>The same weights are used at every time step:</strong></p>
                                <ul>
                                    <li>Same transformation applied to each input</li>
                                    <li>Like using the same "reading comprehension" skill for every word</li>
                                    <li><strong>Benefit:</strong> Much fewer parameters than having separate weights for each position</li>
                                    <li><strong>Analogy:</strong> Like using the same grammar rules for every sentence, regardless of position</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Analogy: The RNN as a Conversation</h4>
                            <p><strong>Imagine having a conversation:</strong></p>
                            <ul>
                                <li><strong>Feedforward Network:</strong> Like talking to someone with amnesia - they respond to each sentence independently, forgetting everything you said before</li>
                                <li><strong>RNN:</strong> Like a normal conversation - the person remembers what you discussed earlier and uses that context to understand your current statement</li>
                            </ul>
                            
                            <p><strong>Example conversation:</strong></p>
                            <ul>
                                <li>You: "I have a pet."</li>
                                <li>RNN hidden state: [remembers: "pet mentioned"]</li>
                                <li>You: "It's very fluffy."</li>
                                <li>RNN: Uses hidden state ‚Üí "It" must refer to the pet ‚Üí understands correctly</li>
                                <li>You: "It likes to sleep."</li>
                                <li>RNN: Still remembers the pet ‚Üí continues to understand "it"</li>
                            </ul>
                        </div>
                    </div>

                    <div id="recurrence" class="content-section">
                        <h2>The Recurrence Relation</h2>
                        
                        <div class="explanation-box">
                            <h3>üîÑ The Heart of RNNs</h3>
                            <p><strong>The recurrence relation is what makes RNNs special - it connects the current state to previous states, creating memory.</strong></p>
                            
                            <p><strong>Think of it like updating your understanding:</strong></p>
                            <ul>
                                <li><strong>Previous understanding (h_{t-1}):</strong> What you knew before</li>
                                <li><strong>New information (x_t):</strong> What you're learning now</li>
                                <li><strong>Updated understanding (h_t):</strong> Combines old and new information</li>
                                <li><strong>Key insight:</strong> Your current understanding depends on both what you knew before AND what you're learning now</li>
                            </ul>
                        </div>

                        <div class="formula-box">
                            <h4>RNN Forward Pass - The Complete Formula</h4>
                            <p><strong>At each time step t, the RNN performs two operations:</strong></p>
                            
                            <div class="formula-display">
                                <strong>Step 1: Update Hidden State</strong><br>
                                <strong>h_t = tanh(W_hh √ó h_{t-1} + W_xh √ó x_t + b_h)</strong><br><br>
                                <strong>Step 2: Compute Output</strong><br>
                                <strong>y_t = W_hy √ó h_t + b_y</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Breaking Down Step 1: Hidden State Update</h5>
                                <p><strong>h_t = tanh(W_hh √ó h_{t-1} + W_xh √ó x_t + b_h)</strong></p>
                                
                                <p><strong>This formula has three parts:</strong></p>
                                <ul>
                                    <li><strong>W_hh √ó h_{t-1}:</strong> "What to remember from the past"
                                        <ul>
                                            <li>Transforms previous hidden state</li>
                                            <li>Decides which parts of memory to keep</li>
                                            <li>Like filtering your memory: "What's still relevant?"</li>
                                        </ul>
                                    </li>
                                    <li><strong>W_xh √ó x_t:</strong> "What to learn from the current input"
                                        <ul>
                                            <li>Transforms current input</li>
                                            <li>Extracts relevant information</li>
                                            <li>Like taking notes: "What's important in this new information?"</li>
                                        </ul>
                                    </li>
                                    <li><strong>+ b_h:</strong> Bias term (allows shifting the activation)</li>
                                    <li><strong>tanh(...):</strong> Activation function (squashes values to [-1, 1])</li>
                                </ul>
                                
                                <p><strong>In plain English:</strong> "My new understanding = combine (filtered memory from before + processed new information), then normalize it"</p>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Breaking Down Step 2: Output Computation</h5>
                                <p><strong>y_t = W_hy √ó h_t + b_y</strong></p>
                                
                                <p><strong>This is simpler:</strong></p>
                                <ul>
                                    <li>Take the updated hidden state</li>
                                    <li>Transform it to produce the desired output</li>
                                    <li>Like translating your understanding into an answer</li>
                                </ul>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Notation Reference:</h5>
                                <ul>
                                    <li><strong>h_t:</strong> Hidden state at time t (the memory/understanding)</li>
                                    <li><strong>h_{t-1}:</strong> Hidden state at previous time step (previous memory)</li>
                                    <li><strong>x_t:</strong> Input at time t (new information)</li>
                                    <li><strong>y_t:</strong> Output at time t (prediction/response)</li>
                                    <li><strong>W_hh:</strong> Hidden-to-hidden weight matrix (how to update memory)</li>
                                    <li><strong>W_xh:</strong> Input-to-hidden weight matrix (how to process new input)</li>
                                    <li><strong>W_hy:</strong> Hidden-to-output weight matrix (how to generate output)</li>
                                    <li><strong>b_h, b_y:</strong> Bias terms (allow shifting activations)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üî¢ Step-by-Step Example: Processing "The cat sat"</h4>
                            <p><strong>Let's trace through how an RNN processes this sentence:</strong></p>
                            
                            <div class="explanation-box">
                                <h5>Time Step 1: Processing "The"</h5>
                                <ul>
                                    <li><strong>Input x‚ÇÅ:</strong> "The" (encoded as a vector, e.g., [0.2, 0.1, 0.8, ...])</li>
                                    <li><strong>Previous hidden state h‚ÇÄ:</strong> [0, 0, 0, ...] (initialized to zeros - no memory yet)</li>
                                    <li><strong>Compute h‚ÇÅ:</strong>
                                        <ul>
                                            <li>W_hh √ó h‚ÇÄ = [0, 0, 0, ...] (no previous memory)</li>
                                            <li>W_xh √ó x‚ÇÅ = [0.1, 0.3, 0.2, ...] (processed "The")</li>
                                            <li>h‚ÇÅ = tanh([0, 0, 0, ...] + [0.1, 0.3, 0.2, ...] + b_h)</li>
                                            <li>h‚ÇÅ = [0.1, 0.3, 0.2, ...] (memory now contains information about "The")</li>
                                        </ul>
                                    </li>
                                    <li><strong>Output y‚ÇÅ:</strong> W_hy √ó h‚ÇÅ + b_y (might predict next word or classify)</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>Time Step 2: Processing "cat"</h5>
                                <ul>
                                    <li><strong>Input x‚ÇÇ:</strong> "cat" (encoded as [0.5, 0.7, 0.1, ...])</li>
                                    <li><strong>Previous hidden state h‚ÇÅ:</strong> [0.1, 0.3, 0.2, ...] (remembers "The")</li>
                                    <li><strong>Compute h‚ÇÇ:</strong>
                                        <ul>
                                            <li>W_hh √ó h‚ÇÅ = [0.05, 0.15, 0.1, ...] (filtered memory of "The")</li>
                                            <li>W_xh √ó x‚ÇÇ = [0.3, 0.4, 0.2, ...] (processed "cat")</li>
                                            <li>h‚ÇÇ = tanh([0.05, 0.15, 0.1, ...] + [0.3, 0.4, 0.2, ...] + b_h)</li>
                                            <li>h‚ÇÇ = [0.35, 0.55, 0.3, ...] (memory now contains "The cat")</li>
                                        </ul>
                                    </li>
                                    <li><strong>Key insight:</strong> h‚ÇÇ combines information from both "The" and "cat"!</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>Time Step 3: Processing "sat"</h5>
                                <ul>
                                    <li><strong>Input x‚ÇÉ:</strong> "sat" (encoded as [0.2, 0.3, 0.6, ...])</li>
                                    <li><strong>Previous hidden state h‚ÇÇ:</strong> [0.35, 0.55, 0.3, ...] (remembers "The cat")</li>
                                    <li><strong>Compute h‚ÇÉ:</strong>
                                        <ul>
                                            <li>W_hh √ó h‚ÇÇ = [0.18, 0.28, 0.15, ...] (filtered memory of "The cat")</li>
                                            <li>W_xh √ó x‚ÇÉ = [0.1, 0.15, 0.3, ...] (processed "sat")</li>
                                            <li>h‚ÇÉ = tanh([0.18, 0.28, 0.15, ...] + [0.1, 0.15, 0.3, ...] + b_h)</li>
                                            <li>h‚ÇÉ = [0.28, 0.43, 0.45, ...] (memory now contains "The cat sat")</li>
                                        </ul>
                                    </li>
                                    <li><strong>Result:</strong> The hidden state has accumulated information from the entire sequence!</li>
                                </ul>
                            </div>
                            
                            <div class="explanation-box">
                                <h5>üéØ Key Takeaways</h5>
                                <ul>
                                    <li><strong>Information accumulates:</strong> Each time step adds to the memory</li>
                                    <li><strong>Context builds:</strong> Later predictions can use information from much earlier</li>
                                    <li><strong>Same weights:</strong> W_hh, W_xh, W_hy are used at every time step</li>
                                    <li><strong>Variable length:</strong> Can process sequences of any length using the same process</li>
                                </ul>
                            </div>
                        </div>

                        <div class="code-box">
                            <h4>üíª Simple RNN Implementation</h4>
                            <pre><code>import numpy as np

class SimpleRNN:
    """Simple Recurrent Neural Network"""
    
    def __init__(self, input_size, hidden_size, output_size):
        # Weight matrices
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.1
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.1
        self.W_hy = np.random.randn(output_size, hidden_size) * 0.1
        
        # Biases
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((output_size, 1))
        
        # Hidden state (initialized to zero)
        self.h = np.zeros((hidden_size, 1))
    
    def tanh(self, x):
        return np.tanh(x)
    
    def forward_step(self, x):
        """Process one time step"""
        # Update hidden state
        self.h = self.tanh(np.dot(self.W_xh, x) + 
                          np.dot(self.W_hh, self.h) + 
                          self.b_h)
        
        # Compute output
        y = np.dot(self.W_hy, self.h) + self.b_y
        
        return y, self.h
    
    def forward(self, sequence):
        """Process entire sequence"""
        outputs = []
        self.h = np.zeros((self.W_hh.shape[0], 1))  # Reset state
        
        for x in sequence:
            y, h = self.forward_step(x)
            outputs.append(y)
        
        return outputs

# Example: Process sequence
rnn = SimpleRNN(input_size=3, hidden_size=4, output_size=2)
sequence = [np.array([[1], [2], [3]]), 
            np.array([[4], [5], [6]]),
            np.array([[7], [8], [9]])]

outputs = rnn.forward(sequence)
print(f"Processed {len(outputs)} time steps")</code></pre>
                        </div>
                    </div>

                    <div id="unfolding" class="content-section">
                        <h2>Unfolding RNNs Through Time</h2>
                        
                        <div class="explanation-box">
                            <h3>üìñ Visualizing Recurrence</h3>
                            <p><strong>Unfolding (or unrolling) an RNN means visualizing it as a feedforward network where each time step becomes a separate layer.</strong> This helps understand how information flows through time.</p>
                        </div>

                        <div class="example-box">
                            <h4>üî¢ Unfolding Example</h4>
                            <p><strong>For a sequence of length 3:</strong></p>
                            <ul>
                                <li><strong>t=0:</strong> h‚ÇÄ = tanh(W_xh √ó x‚ÇÄ + W_hh √ó h_{-1} + b_h)</li>
                                <li><strong>t=1:</strong> h‚ÇÅ = tanh(W_xh √ó x‚ÇÅ + W_hh √ó h‚ÇÄ + b_h)</li>
                                <li><strong>t=2:</strong> h‚ÇÇ = tanh(W_xh √ó x‚ÇÇ + W_hh √ó h‚ÇÅ + b_h)</li>
                            </ul>
                            
                            <p><strong>Unfolded view:</strong></p>
                            <pre style="background: #f5f5f5; padding: 1rem; border-radius: 5px;">
x‚ÇÄ ‚Üí [RNN] ‚Üí h‚ÇÄ ‚Üí [RNN] ‚Üí h‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÇ
      ‚Üë            ‚Üë            ‚Üë
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      (shared weights W_hh)</pre>
                        </div>
                    </div>

                    <div id="backprop" class="content-section">
                        <h2>Backpropagation Through Time (BPTT)</h2>
                        
                        <div class="explanation-box">
                            <h3>üîÑ Training RNNs</h3>
                            <p><strong>Backpropagation Through Time (BPTT) is the algorithm used to train RNNs.</strong> It's essentially backpropagation applied to the unfolded network, computing gradients across all time steps.</p>
                        </div>

                        <div class="formula-box">
                            <h4>BPTT Gradient Computation</h4>
                            <p><strong>Gradient flows backward through time:</strong></p>
                            
                            <div class="formula-display">
                                <strong>‚àÇL/‚àÇh_t = ‚àÇL/‚àÇy_t √ó W_hy + ‚àÇL/‚àÇh_{t+1} √ó W_hh</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Key Insight:</h5>
                                <ul>
                                    <li>Gradient at time t depends on gradient at time t+1</li>
                                    <li>Must compute gradients backward through entire sequence</li>
                                    <li>Gradients are multiplied by W_hh at each step</li>
                                    <li>This can cause vanishing or exploding gradients</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="vanishing" class="content-section">
                        <h2>The Vanishing Gradient Problem</h2>
                        
                        <div class="explanation-box">
                            <h3>‚ö†Ô∏è RNN's Main Limitation</h3>
                            <p><strong>The vanishing gradient problem occurs when gradients become exponentially small as they propagate backward through time.</strong> This makes it difficult for RNNs to learn long-term dependencies.</p>
                        </div>

                        <div class="formula-box">
                            <h4>Why Gradients Vanish</h4>
                            <p><strong>Gradient at time t-k:</strong></p>
                            
                            <div class="formula-display">
                                <strong>‚àÇL/‚àÇh_{t-k} = ‚àÇL/‚àÇh_t √ó (W_hh)^k √ó tanh'(z_{t-k}) √ó ... √ó tanh'(z_t)</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Problem:</h5>
                                <ul>
                                    <li>Gradient is multiplied by W_hh k times</li>
                                    <li>If |W_hh| < 1, gradient shrinks exponentially</li>
                                    <li>tanh'(z) ‚â§ 1, further reducing gradient</li>
                                    <li>Result: Early time steps receive almost no gradient</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Impact</h4>
                            <p><strong>Example: Language Modeling</strong></p>
                            <p>Sentence: "The cat, which was very fluffy, sat on the mat."</p>
                            <p>To predict "sat", the network needs to remember "cat" from much earlier. With vanishing gradients, the network can't learn this long-term dependency!</p>
                            
                            <p><strong>Solution:</strong> LSTMs and GRUs (covered in Chapter 7) solve this problem.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Complete RNN Implementation</h2>
                        
                        <div class="code-box">
                            <h4>üíª Full RNN with BPTT</h4>
                            <pre><code>import numpy as np

class RNN:
    """Recurrent Neural Network with Backpropagation Through Time"""
    
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize weights
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01
        
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((output_size, 1))
    
    def forward(self, x_sequence):
        """Forward pass through sequence"""
        seq_len = len(x_sequence)
        h_states = [np.zeros((self.hidden_size, 1))]
        y_outputs = []
        
        for t in range(seq_len):
            # Compute hidden state
            h_t = np.tanh(np.dot(self.W_xh, x_sequence[t]) + 
                         np.dot(self.W_hh, h_states[-1]) + 
                         self.b_h)
            h_states.append(h_t)
            
            # Compute output
            y_t = np.dot(self.W_hy, h_t) + self.b_y
            y_outputs.append(y_t)
        
        return y_outputs, h_states
    
    def backward(self, x_sequence, y_outputs, h_states, targets, learning_rate=0.01):
        """Backpropagation Through Time"""
        seq_len = len(x_sequence)
        
        # Initialize gradients
        dW_xh = np.zeros_like(self.W_xh)
        dW_hh = np.zeros_like(self.W_hh)
        dW_hy = np.zeros_like(self.W_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)
        
        dh_next = np.zeros((self.hidden_size, 1))
        
        # Backward through time
        for t in reversed(range(seq_len)):
            # Output layer gradient
            dy = y_outputs[t] - targets[t]
            dW_hy += np.dot(dy, h_states[t+1].T)
            db_y += dy
            
            # Hidden layer gradient
            dh = np.dot(self.W_hy.T, dy) + dh_next
            dh_raw = (1 - h_states[t+1]**2) * dh  # tanh derivative
            
            dW_xh += np.dot(dh_raw, x_sequence[t].T)
            dW_hh += np.dot(dh_raw, h_states[t].T)
            db_h += dh_raw
            
            dh_next = np.dot(self.W_hh.T, dh_raw)
        
        # Update weights
        self.W_xh -= learning_rate * dW_xh
        self.W_hh -= learning_rate * dW_hh
        self.W_hy -= learning_rate * dW_hy
        self.b_h -= learning_rate * db_h
        self.b_y -= learning_rate * db_y

# Usage
rnn = RNN(input_size=10, hidden_size=20, output_size=5)
# Training loop would go here</code></pre>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main advantage of RNNs over feedforward networks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) They are faster to train</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) They can process sequences and maintain memory of previous inputs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They require fewer parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They always give better accuracy</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the vanishing gradient problem in RNNs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Gradients become exponentially small when propagating backward through time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) RNNs forget previous inputs too quickly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) RNNs require too much memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) RNNs can't process sequences</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What does BPTT stand for?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Backpropagation Through Training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Backpropagation Through Time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Backward Propagation Through Time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Best Performance Through Training</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/neural-networks" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/neural-networks/chapter5" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 5</a>
                <a href="/tutorials/neural-networks/chapter7" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 7: LSTMs ‚Üí</a>
            </div>
        </div>
    </footer>
    
    <script src="{{ url_for('static', filename='js/tutorials/neural-networks/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
    </script>
</body>
</html>
