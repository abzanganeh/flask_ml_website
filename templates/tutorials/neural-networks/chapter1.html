<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Introduction to Neural Networks - Neural Networks Fundamentals</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/neural-networks/neural-networks.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/neural-networks" class="course-link">
                    <span>Neural Networks Fundamentals</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 1: Introduction to Neural Networks</h1>
                <p class="chapter-subtitle">From Biological Neurons to Artificial Networks - Understanding the foundation of deep learning</p>
                
                <!-- Chapter Progress Bar (1/8) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="12.5"></div>
                </div>
                
                <!-- Chapter Navigation (All 8 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/neural-networks/chapter1" class="chapter-nav-btn active">Chapter 1</a>
                    <a href="/tutorials/neural-networks/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/neural-networks/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/neural-networks/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/neural-networks/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/neural-networks/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/neural-networks/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/neural-networks/chapter8" class="chapter-nav-btn">Chapter 8</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="biological">Biological Inspiration</button>
                    <button class="section-nav-btn azbn-btn" data-section="perceptron">Perceptron</button>
                    <button class="section-nav-btn azbn-btn" data-section="mlp">Multi-Layer Perceptron</button>
                    <button class="section-nav-btn azbn-btn" data-section="architecture">Architecture Basics</button>
                    <button class="section-nav-btn azbn-btn" data-section="universal">Universal Approximation</button>
                    <button class="section-nav-btn azbn-btn" data-section="example">Code Example</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the biological inspiration behind neural networks</li>
                        <li>Master the perceptron model and its limitations</li>
                        <li>Learn the architecture of multi-layer perceptrons (MLPs)</li>
                        <li>Understand the universal approximation theorem</li>
                        <li>Implement a simple neural network from scratch</li>
                        <li>Recognize when to use neural networks vs other ML methods</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">
                    <!-- Biological Inspiration Section -->
                    <div id="biological" class="content-section active">
                        <h2>Biological Inspiration: The Human Brain</h2>
                        
                        <div class="explanation-box">
                            <h3>üß† The Biological Neuron</h3>
                            <p><strong>Neural networks are inspired by how the human brain works.</strong> Your brain contains approximately 86 billion neurons, each connected to thousands of other neurons through structures called synapses. When a neuron receives enough input signals, it "fires" and sends signals to connected neurons.</p>
                            
                            <p><strong>Key Components of a Biological Neuron:</strong></p>
                            <ul>
                                <li><strong>Dendrites:</strong> Receive input signals from other neurons</li>
                                <li><strong>Cell Body (Soma):</strong> Processes the incoming signals</li>
                                <li><strong>Axon:</strong> Transmits output signals to other neurons</li>
                                <li><strong>Synapses:</strong> Connections between neurons that can strengthen or weaken</li>
                            </ul>
                        </div>

                        <h3>From Biology to Mathematics</h3>
                        <p>Artificial neural networks mimic this biological process using mathematical operations:</p>
                        
                        <div class="formula-box">
                            <h4>Biological Process ‚Üí Mathematical Model</h4>
                            <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                <thead>
                                    <tr style="background-color: #f0f0f0;">
                                        <th style="padding: 0.75rem; border: 1px solid #ddd; text-align: left;">Biological Component</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd; text-align: left;">Mathematical Equivalent</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Input signals (dendrites)</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Input features x‚ÇÅ, x‚ÇÇ, ..., x‚Çô</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Synaptic strength</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Weights w‚ÇÅ, w‚ÇÇ, ..., w‚Çô</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Neuron activation threshold</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Bias term b</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Neuron firing</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Activation function f(¬∑)</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Output signal (axon)</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Output y = f(Œ£w·µ¢x·µ¢ + b)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Analogy</h4>
                            <p><strong>Think of a neuron like a voting committee:</strong></p>
                            <ul>
                                <li>Each committee member (input feature) has a different influence (weight)</li>
                                <li>Some members' votes count more than others (higher weights)</li>
                                <li>The committee needs a minimum number of "yes" votes to make a decision (threshold/bias)</li>
                                <li>Once the threshold is reached, the committee makes a decision (activation)</li>
                            </ul>
                            <p><strong>Example:</strong> Deciding if you should go to a movie:</p>
                            <ul>
                                <li>Input 1: "Is it a good movie?" (weight: 0.8 - very important)</li>
                                <li>Input 2: "Do I have time?" (weight: 0.6 - important)</li>
                                <li>Input 3: "Is it expensive?" (weight: 0.3 - less important)</li>
                                <li>Bias: -0.5 (you need enough positive signals to overcome laziness)</li>
                                <li>If weighted sum > threshold ‚Üí Go to movie!</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Perceptron Section -->
                    <div id="perceptron" class="content-section">
                        <h2>The Perceptron: The Simplest Neural Network</h2>
                        
                        <div class="explanation-box">
                            <h3>What is a Perceptron?</h3>
                            <p><strong>The perceptron is the simplest form of a neural network.</strong> Invented by Frank Rosenblatt in 1957, it's a single-layer neural network that can learn to classify linearly separable data.</p>
                            
                            <p><strong>Key Characteristics:</strong></p>
                            <ul>
                                <li>Takes multiple inputs (features)</li>
                                <li>Applies weights to each input</li>
                                <li>Sums the weighted inputs</li>
                                <li>Applies an activation function (typically step function)</li>
                                <li>Produces a binary output (0 or 1)</li>
                            </ul>
                        </div>

                        <h3>Mathematical Formulation</h3>
                        
                        <div class="formula-box">
                            <h4>Perceptron Formula</h4>
                            <p>Given inputs <strong>x = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]</strong> and weights <strong>w = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]</strong>, the perceptron computes:</p>
                            
                            <div class="formula-display">
                                \[z = \sum_{i=1}^{n} w_i x_i + b\]
                            </div>
                            
                            <p>Where <strong>b</strong> is the bias term. Then the output is:</p>
                            
                            <div class="formula-display">
                                \[y = f(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}\]
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Formula Breakdown:</h5>
                                <ul>
                                    <li><strong>z</strong>: The weighted sum (also called the "net input" or "pre-activation")</li>
                                    <li><strong>w·µ¢</strong>: Weight for the i-th input feature</li>
                                    <li><strong>x·µ¢</strong>: The i-th input feature value</li>
                                    <li><strong>b</strong>: Bias term (allows shifting the decision boundary)</li>
                                    <li><strong>f(¬∑)</strong>: Step function (also called Heaviside function)</li>
                                    <li><strong>y</strong>: Binary output (0 or 1)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h4>Vectorized Form</h4>
                            <p>Using linear algebra, we can write this more compactly:</p>
                            
                            <div class="formula-display">
                                <strong>z = w·µÄx + b</strong>
                            </div>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>w·µÄ</strong>: Transpose of weight vector (row vector)</li>
                                <li><strong>x</strong>: Input vector (column vector)</li>
                                <li><strong>w·µÄx</strong>: Dot product (sum of element-wise multiplication)</li>
                            </ul>
                            
                            <div class="formula-explanation">
                                <h5>Why Vectorization Matters:</h5>
                                <p>Vectorized operations are:</p>
                                <ul>
                                    <li><strong>Faster:</strong> Can use optimized linear algebra libraries</li>
                                    <li><strong>Cleaner:</strong> Less code, easier to read</li>
                                    <li><strong>Parallelizable:</strong> Modern CPUs/GPUs can process vectors efficiently</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>Concrete Example: AND Gate</h4>
                            <p><strong>Let's build a perceptron that implements an AND logic gate:</strong></p>
                            
                            <p><strong>Truth Table:</strong></p>
                            <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                <thead>
                                    <tr style="background-color: #f0f0f0;">
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">x‚ÇÅ</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">x‚ÇÇ</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">Output</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                    </tr>
                                </tbody>
                            </table>
                            
                            <p><strong>Solution:</strong> We need weights w‚ÇÅ = 1, w‚ÇÇ = 1, and bias b = -1.5</p>
                            
                            <p><strong>Verification:</strong></p>
                            <ul>
                                <li>x‚ÇÅ=0, x‚ÇÇ=0: z = 1√ó0 + 1√ó0 - 1.5 = -1.5 ‚Üí y = 0 ‚úì</li>
                                <li>x‚ÇÅ=0, x‚ÇÇ=1: z = 1√ó0 + 1√ó1 - 1.5 = -0.5 ‚Üí y = 0 ‚úì</li>
                                <li>x‚ÇÅ=1, x‚ÇÇ=0: z = 1√ó1 + 1√ó0 - 1.5 = -0.5 ‚Üí y = 0 ‚úì</li>
                                <li>x‚ÇÅ=1, x‚ÇÇ=1: z = 1√ó1 + 1√ó1 - 1.5 = 0.5 ‚Üí y = 1 ‚úì</li>
                            </ul>
                        </div>

                        <div class="code-box">
                            <h4>Python Implementation</h4>
                            <pre><code class="language-python">import numpy as np

class Perceptron:
    """Simple Perceptron Implementation"""
    
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
    
    def fit(self, X, y):
        """
        Train the perceptron
        
        Parameters:
        X: Input features (n_samples, n_features)
        y: Target labels (n_samples,)
        """
        n_samples, n_features = X.shape
        
        # Initialize weights and bias
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # Training loop
        for _ in range(self.n_iterations):
            for idx, x_i in enumerate(X):
                # Compute linear output
                linear_output = np.dot(x_i, self.weights) + self.bias
                
                # Apply step function
                y_predicted = self.activation(linear_output)
                
                # Update rule (Perceptron Learning Rule)
                update = self.learning_rate * (y[idx] - y_predicted)
                self.weights += update * x_i
                self.bias += update
    
    def activation(self, x):
        """Step activation function"""
        return np.where(x >= 0, 1, 0)
    
    def predict(self, X):
        """Make predictions"""
        linear_output = np.dot(X, self.weights) + self.bias
        return self.activation(linear_output)

# Example usage
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # AND gate

perceptron = Perceptron()
perceptron.fit(X, y)

# Test
predictions = perceptron.predict(X)
print("Predictions:", predictions)
print("Weights:", perceptron.weights)
print("Bias:", perceptron.bias)</code></pre>
                            
                            <div class="code-explanation">
                                <h5>Code Explanation:</h5>
                                <ul>
                                    <li><strong>__init__:</strong> Initializes learning rate and number of iterations</li>
                                    <li><strong>fit:</strong> Trains the perceptron using the Perceptron Learning Rule</li>
                                    <li><strong>activation:</strong> Step function that outputs 1 if input ‚â• 0, else 0</li>
                                    <li><strong>predict:</strong> Makes predictions on new data</li>
                                    <li><strong>Update Rule:</strong> w ‚Üê w + Œ∑(y - ≈∑)x, where Œ∑ is learning rate</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h4>‚ö†Ô∏è Limitations of Perceptron</h4>
                            <p><strong>The perceptron has a critical limitation:</strong> It can only learn linearly separable patterns. This was famously demonstrated by Marvin Minsky and Seymour Papert in 1969 with the XOR problem.</p>
                            
                            <p><strong>XOR Problem:</strong> The XOR (exclusive OR) function cannot be learned by a single perceptron because it's not linearly separable:</p>
                            
                            <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                <thead>
                                    <tr style="background-color: #f0f0f0;">
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">x‚ÇÅ</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">x‚ÇÇ</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd;">XOR Output</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">1</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">0</td>
                                    </tr>
                                </tbody>
                            </table>
                            
                            <p><strong>Why it fails:</strong> You cannot draw a single straight line to separate the 0s from the 1s. This limitation led to the development of multi-layer perceptrons (MLPs).</p>
                        </div>
                    </div>

                    <!-- Multi-Layer Perceptron Section -->
                    <div id="mlp" class="content-section">
                        <h2>Multi-Layer Perceptron (MLP): Solving Complex Problems</h2>
                        
                        <div class="explanation-box">
                            <h3>üèóÔ∏è What is an MLP?</h3>
                            <p><strong>A Multi-Layer Perceptron (MLP) is a feedforward neural network with one or more hidden layers.</strong> Unlike the single-layer perceptron, MLPs can learn non-linear patterns and solve complex problems like the XOR problem.</p>
                            
                            <p><strong>Key Components:</strong></p>
                            <ul>
                                <li><strong>Input Layer:</strong> Receives the input features</li>
                                <li><strong>Hidden Layer(s):</strong> One or more layers between input and output</li>
                                <li><strong>Output Layer:</strong> Produces the final predictions</li>
                                <li><strong>Fully Connected:</strong> Every neuron in one layer connects to every neuron in the next</li>
                            </ul>
                        </div>

                        <h3>MLP Architecture</h3>
                        
                        <div class="formula-box">
                            <h4>Forward Propagation in MLP</h4>
                            <p>For an MLP with L layers, the forward propagation is computed as follows:</p>
                            
                            <p><strong>For each layer l = 1, 2, ..., L:</strong></p>
                            
                            <div class="formula-display">
                                <strong>z‚ÅΩÀ°‚Åæ = W‚ÅΩÀ°‚Åæa‚ÅΩÀ°‚Åª¬π‚Åæ + b‚ÅΩÀ°‚Åæ</strong><br>
                                <strong>a‚ÅΩÀ°‚Åæ = f‚ÅΩÀ°‚Åæ(z‚ÅΩÀ°‚Åæ)</strong>
                            </div>
                            
                            <div class="formula-explanation">
                                <h5>Notation:</h5>
                                <ul>
                                    <li><strong>z‚ÅΩÀ°‚Åæ</strong>: Pre-activation (weighted sum) at layer l</li>
                                    <li><strong>a‚ÅΩÀ°‚Åæ</strong>: Activation (output) at layer l</li>
                                    <li><strong>W‚ÅΩÀ°‚Åæ</strong>: Weight matrix for layer l</li>
                                    <li><strong>b‚ÅΩÀ°‚Åæ</strong>: Bias vector for layer l</li>
                                    <li><strong>f‚ÅΩÀ°‚Åæ</strong>: Activation function for layer l</li>
                                    <li><strong>a‚ÅΩ‚Å∞‚Åæ</strong>: Input features x</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>Example: 2-Layer MLP for XOR</h4>
                            <p><strong>Architecture:</strong> 2 inputs ‚Üí 2 hidden neurons ‚Üí 1 output</p>
                            
                            <p><strong>Layer 1 (Hidden):</strong></p>
                            <ul>
                                <li>h‚ÇÅ = f(w‚ÇÅ‚ÇÅx‚ÇÅ + w‚ÇÅ‚ÇÇx‚ÇÇ + b‚ÇÅ)</li>
                                <li>h‚ÇÇ = f(w‚ÇÇ‚ÇÅx‚ÇÅ + w‚ÇÇ‚ÇÇx‚ÇÇ + b‚ÇÇ)</li>
                            </ul>
                            
                            <p><strong>Layer 2 (Output):</strong></p>
                            <ul>
                                <li>y = f(w‚ÇÉ‚ÇÅh‚ÇÅ + w‚ÇÉ‚ÇÇh‚ÇÇ + b‚ÇÉ)</li>
                            </ul>
                            
                            <p><strong>With appropriate weights, this MLP can solve XOR!</strong> The hidden layer creates non-linear combinations of inputs that make the problem linearly separable in the output layer.</p>
                        </div>

                        <div class="code-box">
                            <h4>MLP Implementation</h4>
                            <pre><code class="language-python">import numpy as np

class MLP:
    """Multi-Layer Perceptron Implementation"""
    
    def __init__(self, layers, activation='relu'):
        """
        Initialize MLP
        
        Parameters:
        layers: List of layer sizes, e.g., [2, 4, 1] for 2 inputs, 4 hidden, 1 output
        activation: Activation function ('relu', 'sigmoid', 'tanh')
        """
        self.layers = layers
        self.activation = activation
        self.weights = []
        self.biases = []
        
        # Initialize weights and biases
        for i in range(len(layers) - 1):
            # Xavier initialization
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def _activate(self, x):
        """Apply activation function"""
        if self.activation == 'relu':
            return np.maximum(0, x)
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
        elif self.activation == 'tanh':
            return np.tanh(x)
        return x
    
    def forward(self, X):
        """Forward propagation"""
        a = X
        activations = [a]
        
        for w, b in zip(self.weights, self.biases):
            z = np.dot(a, w) + b
            a = self._activate(z)
            activations.append(a)
        
        return activations
    
    def predict(self, X):
        """Make predictions"""
        activations = self.forward(X)
        return activations[-1]

# Example: XOR problem
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])  # XOR

mlp = MLP([2, 4, 1], activation='sigmoid')
# Note: This is a simplified version. Full training requires backpropagation (covered in Chapter 4)</code></pre>
                        </div>
                    </div>

                    <!-- Architecture Basics Section -->
                    <div id="architecture" class="content-section">
                        <h2>Neural Network Architecture Basics</h2>
                        
                        <div class="explanation-box">
                            <h3>üèóÔ∏è Understanding Network Structure</h3>
                            <p><strong>Neural network architecture refers to the overall design and organization of the network.</strong> This includes the number of layers, number of neurons per layer, how layers are connected, and the types of operations performed.</p>
                            
                            <p><strong>Key Architectural Components:</strong></p>
                            <ul>
                                <li><strong>Depth:</strong> Number of layers (shallow vs deep networks)</li>
                                <li><strong>Width:</strong> Number of neurons per layer</li>
                                <li><strong>Connections:</strong> How neurons connect (fully connected, sparse, etc.)</li>
                                <li><strong>Activation Functions:</strong> Non-linear transformations at each layer</li>
                            </ul>
                        </div>

                        <h3>Layer Types</h3>
                        
                        <div class="example-box">
                            <h4>Common Layer Types</h4>
                            <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                <thead>
                                    <tr style="background-color: #f0f0f0;">
                                        <th style="padding: 0.75rem; border: 1px solid #ddd; text-align: left;">Layer Type</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd; text-align: left;">Purpose</th>
                                        <th style="padding: 0.75rem; border: 1px solid #ddd; text-align: left;">Example Use</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;"><strong>Dense/Fully Connected</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Every neuron connects to all neurons in next layer</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Standard MLPs, classification</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;"><strong>Convolutional</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Sparse connections, shared weights</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Image processing, CNNs</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;"><strong>Recurrent</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Connections form cycles, maintain state</td>
                                        <td style="padding: 0.75rem; border: 1px solid #ddd;">Sequences, RNNs, LSTMs</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <!-- Universal Approximation Section -->
                    <div id="universal" class="content-section">
                        <h2>Universal Approximation Theorem</h2>
                        
                        <div class="explanation-box">
                            <h3>The Power of Neural Networks</h3>
                            <p><strong>The Universal Approximation Theorem is a fundamental result that explains why neural networks are so powerful.</strong> It states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy, given appropriate activation functions and weights.</p>
                        </div>

                        <div class="formula-box">
                            <h4>Mathematical Statement</h4>
                            <p><strong>For any continuous function f: [0,1]‚Åø ‚Üí ‚Ñù and any Œµ > 0,</strong> there exists a feedforward neural network with:</p>
                            
                            <div class="formula-display">
                                <strong>‚Ä¢ One hidden layer</strong><br>
                                <strong>‚Ä¢ Sufficiently many neurons</strong><br>
                                <strong>‚Ä¢ Appropriate activation function (e.g., sigmoid, ReLU)</strong>
                            </div>
                            
                            <p>Such that the network approximates f with error less than Œµ.</p>
                            
                            <div class="formula-explanation">
                                <h5>What This Means:</h5>
                                <ul>
                                    <li><strong>Any function:</strong> No matter how complex, a neural network can learn it</li>
                                    <li><strong>Arbitrary accuracy:</strong> Can get as close as you want (given enough neurons)</li>
                                    <li><strong>Single hidden layer:</strong> Even shallow networks are powerful</li>
                                    <li><strong>Practical limitation:</strong> Theorem doesn't tell us how to find the weights!</li>
                                </ul>
                            </div>
                        </div>

                        <div class="example-box">
                            <h4>üìö Real-World Implication</h4>
                            <p><strong>This theorem explains why neural networks work so well:</strong></p>
                            <ul>
                                <li>They can learn any pattern (given enough capacity)</li>
                                <li>No need to manually design features - the network learns them</li>
                                <li>Deep networks (multiple layers) are even more powerful</li>
                                <li>This is why deep learning has been so successful</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Code Example Section -->
                    <div id="example" class="content-section">
                        <h2>Complete Code Example</h2>
                        
                        <div class="code-box">
                            <h4>Simple Neural Network from Scratch</h4>
                            <pre><code class="language-python">import numpy as np

class SimpleNeuralNetwork:
    """A simple feedforward neural network implementation"""
    
    def __init__(self, input_size, hidden_size, output_size):
        # Initialize weights randomly
        self.W1 = np.random.randn(input_size, hidden_size) * 0.1
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.1
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, x):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def forward(self, X):
        """Forward propagation"""
        # Layer 1
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        
        # Layer 2 (output)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        
        return self.a2
    
    def predict(self, X):
        """Make predictions"""
        return self.forward(X)

# Example usage
# Create network: 2 inputs ‚Üí 3 hidden ‚Üí 1 output
network = SimpleNeuralNetwork(input_size=2, hidden_size=3, output_size=1)

# Test input
X = np.array([[0.5, 0.8]])
output = network.predict(X)

print(f"Input: {X}")
print(f"Output: {output}")
print(f"Prediction: {'Positive' if output > 0.5 else 'Negative'}")</code></pre>
                            
                            <div class="code-explanation">
                                <h5>Code Breakdown:</h5>
                                <ul>
                                    <li><strong>__init__:</strong> Initializes weights and biases for 2-layer network</li>
                                    <li><strong>sigmoid:</strong> Activation function (clips to prevent overflow)</li>
                                    <li><strong>forward:</strong> Computes output through both layers</li>
                                    <li><strong>predict:</strong> Wrapper for making predictions</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main limitation of a single-layer perceptron?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It can only learn linearly separable patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It's too slow</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It requires too much memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It can't handle numerical data</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What does the Universal Approximation Theorem tell us?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Neural networks always find the best solution</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) A neural network can approximate any continuous function with sufficient neurons</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Neural networks are faster than other methods</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Neural networks don't need training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is the purpose of activation functions in neural networks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) To make computation faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) To introduce non-linearity and enable learning complex patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To reduce memory usage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) To prevent overfitting</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "Explain the difference between a perceptron and a multi-layer perceptron (MLP)."</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A perceptron is a single-layer network that can only learn linearly separable patterns, while an MLP has multiple layers with activation functions that can learn non-linear, complex patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are the same thing</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Perceptron is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) MLP uses less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is the mathematical representation of a neuron's output?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(y = f(\sum_{i=1}^{n} w_i x_i + b)\) where f is the activation function, w_i are weights, x_i are inputs, and b is bias</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(y = \sum w_i x_i\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(y = w \times x\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(y = x + b\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: Interview question: "How would you initialize weights in a neural network and why?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use small random values (e.g., Xavier/Glorot or He initialization) to break symmetry, prevent vanishing/exploding gradients, and ensure different neurons learn different features</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Initialize all weights to zero</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Initialize all weights to one</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Use large random values</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What does the Universal Approximation Theorem guarantee?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of R^n, given appropriate activation functions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Neural networks always converge to the global optimum</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Neural networks are always better than other methods</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Neural networks don't need training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "What happens if you don't use an activation function in a neural network?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The network becomes a linear model, regardless of depth, because the composition of linear transformations is still linear, losing the ability to learn non-linear patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The network becomes faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) The network uses less memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The network becomes more accurate</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the role of bias in a neural network?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Bias allows the activation function to shift, enabling the network to fit data that doesn't pass through the origin and learn more flexible decision boundaries</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Bias makes computation faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Bias prevents overfitting</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Bias is optional and not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How would you choose the number of neurons in a hidden layer?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Start with a rule of thumb (e.g., between input and output size, or 2/3 of input size), then use validation set to tune, balancing model capacity (too few = underfitting, too many = overfitting)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use the same as input size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use as many as possible</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Use exactly 10 neurons</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is forward propagation in a neural network?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The process of passing input data through the network layers, computing weighted sums and applying activation functions, to produce an output prediction</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The process of updating weights</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) The process of calculating loss</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The process of initializing weights</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "What are the key components of a neural network and how do they work together?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Input layer receives data, hidden layers transform data through weighted connections and activation functions, output layer produces predictions. Weights store learned patterns, biases shift activations, activation functions introduce non-linearity. Forward pass computes predictions, backpropagation updates weights based on error</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just weights and inputs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only activation functions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Just the output layer</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <!-- Footer Navigation -->
    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/neural-networks" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/neural-networks/chapter2" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 2: Feedforward Networks ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/neural-networks/shared-tutorial.js') }}"></script>
    <script>
        // Function to scroll to section navigation
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        // Quiz function
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
        
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
