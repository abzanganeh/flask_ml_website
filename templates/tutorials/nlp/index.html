{% extends "base.html" %}

{% block title %}Complete Interactive NLP Course - {{ site_title }}{% endblock %}

{% block extra_css %}
<link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/nlp/nlp.css') }}">
{% endblock %}

{% block content %}
<div class="container">
    <div class="back-to-tutorials">
        <a href="{{ url_for('tutorials') }}">
            <i class="fas fa-arrow-left"></i> Back to Tutorials
        </a>
    </div>
</div>

<div class="nlp-course-container">
    <!-- Header -->
    <div class="nlp-header">
        <h1>Complete Interactive NLP Course</h1>
        <p>Master Natural Language Processing from fundamentals to advanced Transformers</p>
        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>
    </div>
    
    <!-- Navigation -->
    <div id="course-navigation" class="course-nav">
        <button class="nav-btn active" onclick="showSection('intro')">Introduction</button>
        <button class="nav-btn" onclick="showSection('workflow')">Complete Workflow</button>
        <button class="nav-btn" onclick="showSection('text-repr')">Text Representation</button>
        <button class="nav-btn" onclick="showSection('embeddings')">Word Embeddings</button>
        <button class="nav-btn" onclick="showSection('sentiment')">Sentiment Analysis</button>
        <button class="nav-btn" onclick="showSection('seq2seq')">Seq2Seq Models</button>
        <button class="nav-btn" onclick="showSection('transformers')">Transformers</button>
        <button class="nav-btn" onclick="showSection('attention')">Self-Attention</button>
        <button class="nav-btn" onclick="showSection('applications')">Applications</button>
    </div>

    <!-- Introduction Section -->
    <div id="intro" class="nlp-section active">
        <h2>Welcome to the NLP Course!</h2>
        <h2>Introduction to Natural Language Processing</h2>
        
        <p>Natural Language Processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language. It involves reading, deciphering, understanding, and making sense of human languages.</p>

        <p>In this course, you will learn the fundamentals of NLP, from basic text representation techniques to advanced transformer models like BERT and GPT. Each section includes interactive demos, quizzes, and practical applications.</p>

        <h3>Key Topics Covered</h3>
        <ul>
            <li>Text Representation Techniques</li>
            <li>Word Embeddings</li>
            <li>Sentiment Analysis</li>
            <li>Seq2Seq Models</li>
            <li>Transformers and Self-Attention</li>
            <li>Applications in Real-World Scenarios</li>
        </ul>

        <h3>Who This Course is For</h3>
        <p>This course is designed for anyone interested in learning about NLP, from beginners to advanced practitioners. No prior experience with machine learning is required, but familiarity with Python is recommended.</p>

        <div class="interactive-demo">
            <h4>Try NLP in Action!</h4>
            <p>Enter some text to see basic NLP preprocessing:</p>
            <input type="text" class="demo-input" id="nlpInput" placeholder="Enter your text here..." value="Hello! This is a GREAT example of NLP preprocessing.">
            <button class="demo-btn" onclick="preprocessText()">Process Text</button>
            <div class="demo-output" id="nlpOutput"></div>
        </div>

        <h3>Key Applications of NLP</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>Communication</h4>
                <ul>
                    <li>Spam Filters (Gmail)</li>
                    <li>Email Classification</li>
                    <li>Chatbots & Virtual Assistants</li>
                    <li>Language Translation</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Business Intelligence</h4>
                <ul>
                    <li>Sentiment Analysis</li>
                    <li>Market Research</li>
                    <li>Algorithmic Trading</li>
                    <li>Document Summarization</li>
                </ul>
            </div>
        </div>

        <div class="quiz-container">
            <div class="quiz-question">Quick Quiz: Which of these is NOT a typical NLP application?</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">A) Email spam detection</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">B) Language translation</div>
            <div class="quiz-option" onclick="checkAnswer(this, true)">C) Image object detection</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">D) Sentiment analysis</div>
        </div>
    </div>

    <!-- Workflow Section -->
    <div id="workflow" class="nlp-section">
        <h2>Complete Workflow: Classical NLP to Transformers</h2>

        <div class="explanation-box">
            <h4>The Big Picture</h4>
            <p>Modern NLP has evolved through two broad eras. Classical pipelines relied on heavy preprocessing and feature engineering, while transformers learn rich representations directly from raw text. Understanding both perspectives clarifies why the transformer paradigm is so powerful.</p>

            <div class="pros-cons">
                <div class="pros">
                    <h4>Era 1 &mdash; Classical (pre-2017)</h4>
                    <ul>
                        <li>Extensive text cleaning and normalization</li>
                        <li>Tokenization, stemming, lemmatization, POS tagging</li>
                        <li>Feature engineering (BoW, TF, TF-IDF, n-grams)</li>
                        <li>Statistical / traditional ML models (Naive Bayes, SVM)</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>Era 2 &mdash; Transformers (2017+)</h4>
                    <ul>
                        <li>Minimal preprocessing beyond basic normalization</li>
                        <li>Subword tokenization feeds trainable embeddings</li>
                        <li>Self-attention learns context on the fly</li>
                        <li>Large pre-trained models fine-tuned per task</li>
                    </ul>
                </div>
            </div>
        </div>

        <h3>Classical Text Processing Pipeline</h3>
        <div class="architecture-diagram">
            <div class="layer">Text Cleaning</div>
            <span class="arrow">→</span>
            <div class="layer">Tokenization</div>
            <span class="arrow">→</span>
            <div class="layer">Linguistic Features (POS/NER)</div>
            <span class="arrow">→</span>
            <div class="layer">Vectorization (BoW / TF-IDF)</div>
            <span class="arrow">→</span>
            <div class="layer">Classical ML Model</div>
        </div>

        <div class="explanation-box">
            <h4>Step-by-Step</h4>
            <ol>
                <li><strong>Text Cleaning &amp; Normalization:</strong> Lowercasing, punctuation removal, handling contractions.</li>
                <li><strong>Tokenization:</strong> Split into words, subwords, or characters.</li>
                <li><strong>Advanced Linguistics:</strong> Stemming, lemmatization, POS tagging, NER, dependency parsing.</li>
                <li><strong>Feature Engineering:</strong> BoW, TF, TF-IDF, n-grams capture frequency and limited context.</li>
                <li><strong>Statistical Modeling:</strong> Train algorithms like Naive Bayes, SVM, logistic regression.</li>
            </ol>
            <p>Strengths: transparent, lightweight, works on small datasets. Limitations: sparse features, no deep context, heavy manual engineering.</p>
        </div>

        <h3>Modern Transformer Workflow</h3>
        <div class="architecture-diagram">
            <div class="layer">Minimal Cleanup</div>
            <span class="arrow">→</span>
            <div class="layer">Subword Tokenizer</div>
            <span class="arrow">→</span>
            <div class="layer">Embedding + Positional Encoding</div>
            <span class="arrow">→</span>
            <div class="layer">Transformer Stack</div>
            <span class="arrow">→</span>
            <div class="layer">Task Head &amp; Predictions</div>
        </div>

        <div class="explanation-box">
            <h4>What Changes with Transformers?</h4>
            <ul>
                <li><strong>Contextual Embeddings:</strong> Each token gains meaning from its surroundings (bank → financial vs. river).</li>
                <li><strong>Self-Attention Layers:</strong> Learn relationships such as subject-verb, coreference, syntax, and semantics in parallel.</li>
                <li><strong>Feed-Forward Blocks &amp; Residuals:</strong> Provide depth, non-linearity, and stable training.</li>
                <li><strong>Task Heads:</strong> Add a classifier, decoder, or generation head depending on the downstream use case.</li>
            </ul>
        </div>

        <h3>Bridging the Two Eras</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>Keep from Classical</h4>
                <ul>
                    <li>Basic normalization and quality checks</li>
                    <li>Domain dictionaries for evaluation and interpretability</li>
                    <li>Lightweight baselines for quick prototypes</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Superseded by Transformers</h4>
                <ul>
                    <li>Manual feature engineering for semantics</li>
                    <li>Separate POS/NER pipelines for deep models</li>
                    <li>Fixed embeddings with one vector per word</li>
                </ul>
            </div>
        </div>

        <h3>Learning Path to Master Transformers</h3>
        <div class="explanation-box">
            <ol>
                <li><strong>Foundations:</strong> Practice with BoW and TF-IDF to see how text becomes vectors.</li>
                <li><strong>Neural Basics:</strong> Grasp forward/backward passes, matrix operations, activation functions.</li>
                <li><strong>Static Embeddings:</strong> Understand Word2Vec/GloVe, then their limitations.</li>
                <li><strong>Attention Mechanism:</strong> Compute Q/K/V, scaling, softmax weighting.</li>
                <li><strong>Full Transformer Stack:</strong> Position encodings, encoder/decoder roles, masked attention.</li>
                <li><strong>Hands-on Fine-Tuning:</strong> Use Hugging Face to adapt BERT/GPT-style models for real tasks.</li>
            </ol>
        </div>

        <div class="example-box">
            <strong>Quick Reference:</strong><br>
            Small labeled dataset → start with TF-IDF + SVM baseline.<br>
            Need deep context or multilingual support → fine-tune a transformer.<br>
            Interpretability critical → compare classical features with transformer outputs.
        </div>
    </div>

    <!-- Text Representation Section -->
    <div id="text-repr" class="nlp-section">
        <h2>Text Representation Techniques</h2>
        
        <h3>1. Bag of Words (BoW)</h3>
        
        <div class="explanation-box">
            <h4>What is Bag of Words?</h4>
            <p>Bag of Words (BoW) is one of the simplest and most fundamental techniques for converting text into numerical representations that machine learning algorithms can process. The name "Bag of Words" comes from the fact that it treats text as an unordered collection (or "bag") of words, completely ignoring grammar, word order, and context.</p>
            
            <h5>How Does BoW Work?</h5>
            <p>The process involves three main steps:</p>
            <ol>
                <li><strong>Vocabulary Creation:</strong> Collect all unique words from all documents in your corpus to create a vocabulary.</li>
                <li><strong>Word Counting:</strong> For each document, count how many times each word from the vocabulary appears.</li>
                <li><strong>Vector Representation:</strong> Create a vector where each dimension represents a word from the vocabulary, and the value is the count (or presence) of that word in the document.</li>
            </ol>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                <p>For a document <code>d</code> and vocabulary <code>V = {w₁, w₂, ..., wₙ}</code>, the BoW vector is:</p>
                <p><strong>BoW(d) = [count(w₁, d), count(w₂, d), ..., count(wₙ, d)]</strong></p>
                <p>Where <code>count(wᵢ, d)</code> is the number of times word <code>wᵢ</code> appears in document <code>d</code>.</p>
                
                <p><strong>Binary BoW (Presence/Absence):</strong></p>
                <p>BoW(d) = [1 if w₁ ∈ d else 0, 1 if w₂ ∈ d else 0, ..., 1 if wₙ ∈ d else 0]</p>
            </div>
            
            <div class="usage-section">
                <h5>Common Use Cases</h5>
                <ul>
                    <li><strong>Text Classification:</strong> Spam detection, sentiment analysis, topic classification</li>
                    <li><strong>Information Retrieval:</strong> Search engines, document similarity measurement</li>
                    <li><strong>Feature Extraction:</strong> As a baseline method before applying more advanced techniques</li>
                    <li><strong>Document Clustering:</strong> Grouping similar documents together</li>
                </ul>
                
                <h5>When to Use BoW</h5>
                <ul>
                    <li>When you have a small to medium-sized vocabulary</li>
                    <li>When word order is not critical for your task</li>
                    <li>As a baseline for text classification tasks</li>
                    <li>When computational efficiency is important</li>
                    <li>For simple document similarity tasks</li>
                </ul>
            </div>
            
            <h5>Example</h5>
            <p>Consider these two documents:</p>
            <ul>
                <li>Document 1: "I love machine learning"</li>
                <li>Document 2: "Machine learning is powerful"</li>
            </ul>
            <p><strong>Vocabulary:</strong> {"I", "love", "machine", "learning", "is", "powerful"}</p>
            <p><strong>BoW for Document 1:</strong> [1, 1, 1, 1, 0, 0]</p>
            <p><strong>BoW for Document 2:</strong> [0, 0, 1, 1, 1, 1]</p>
            <p>Notice how both documents share the words "machine" and "learning", which creates a similarity connection between them!</p>
        </div>

        <div class="interactive-demo">
            <h4>Bag of Words Demo</h4>
            <p>Try creating your own Bag of Words vectors. Enter multiple sentences separated by | and see how they're converted into numerical vectors.</p>
            <input type="text" class="demo-input" id="bowInput" placeholder="Enter sentences separated by | (e.g., I love NLP | NLP is amazing)" value="I love machine learning | Machine learning is powerful">
            <button class="demo-btn" onclick="demonstrateBOW()">Create BoW</button>
            <div class="demo-output" id="bowOutput"></div>
        </div>

        <div class="pros-cons">
            <div class="pros">
                <h4>Advantages</h4>
                <ul>
                    <li>Simple and easy to implement</li>
                    <li>Works well for text classification</li>
                    <li>Computationally efficient</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Disadvantages</h4>
                <ul>
                    <li>High dimensionality</li>
                    <li>Sparse features</li>
                    <li>Treats synonyms differently</li>
                    <li>Ignores word order</li>
                </ul>
            </div>
        </div>

        <h3>2. TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
        
        <div class="explanation-box">
            <h4>What is TF-IDF?</h4>
            <p>TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It's one of the most popular weighting schemes in text mining and information retrieval. Unlike Bag of Words, which treats all words equally, TF-IDF gives higher weights to words that are frequent in a document but rare across the entire corpus.</p>
            
            <h5>Why TF-IDF?</h5>
            <p>The intuition behind TF-IDF is twofold:</p>
            <ul>
                <li><strong>Term Frequency (TF):</strong> Words that appear more frequently in a document are likely more important to that document.</li>
                <li><strong>Inverse Document Frequency (IDF):</strong> Words that appear in many documents are less distinctive and should be weighted lower. Common words like "the", "is", "a" appear in almost every document, so they get low IDF scores.</li>
            </ul>
            <p>By combining these two factors, TF-IDF identifies words that are distinctive to a particular document while filtering out common words that appear everywhere.</p>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>TF-IDF Formula:</strong></p>
                <p><strong>TF-IDF(t, d, D) = TF(t, d) × IDF(t, D)</strong></p>
                
                <p>Where:</p>
                <ul>
                    <li><code>t</code> = term (word)</li>
                    <li><code>d</code> = document</li>
                    <li><code>D</code> = collection of documents (corpus)</li>
                </ul>
                
                <h5>Term Frequency (TF)</h5>
                <p>There are several ways to calculate TF:</p>
                <p><strong>Raw Count:</strong> TF(t, d) = count(t, d)</p>
                <p><strong>Normalized (Most Common):</strong></p>
                <p>TF(t, d) = count(t, d) / total_words_in_d</p>
                <p><strong>Log Scale:</strong> TF(t, d) = log(1 + count(t, d))</p>
                <p><strong>Double Normalization:</strong> TF(t, d) = 0.5 + 0.5 × (count(t, d) / max_count_in_d)</p>
                
                <h5>Inverse Document Frequency (IDF)</h5>
                <p><strong>Standard IDF:</strong></p>
                <p>IDF(t, D) = log(N / |{d ∈ D : t ∈ d}|)</p>
                <p>Where:</p>
                <ul>
                    <li><code>N</code> = total number of documents in corpus D</li>
                    <li><code>|{d ∈ D : t ∈ d}|</code> = number of documents containing term t</li>
                </ul>
                
                <p><strong>Smoothed IDF (to avoid division by zero):</strong></p>
                <p>IDF(t, D) = log(1 + N / (1 + |{d ∈ D : t ∈ d}|))</p>
                
                <p><strong>IDF with Add-One Smoothing:</strong></p>
                <p>IDF(t, D) = log(N / (1 + |{d ∈ D : t ∈ d}|))</p>
            </div>
            
            <div class="usage-section">
                <h5>Common Use Cases</h5>
                <ul>
                    <li><strong>Information Retrieval:</strong> Search engines use TF-IDF to rank documents by relevance to search queries</li>
                    <li><strong>Text Classification:</strong> Feature extraction for machine learning models (Naive Bayes, SVM, etc.)</li>
                    <li><strong>Document Similarity:</strong> Computing cosine similarity between TF-IDF vectors to find similar documents</li>
                    <li><strong>Keyword Extraction:</strong> Identifying the most important words in a document</li>
                    <li><strong>Content Recommendation:</strong> Recommending similar articles or products based on content</li>
                    <li><strong>Topic Modeling:</strong> As a preprocessing step for algorithms like LDA (Latent Dirichlet Allocation)</li>
                </ul>
                
                <h5>When to Use TF-IDF</h5>
                <ul>
                    <li>When you need to identify distinctive words in documents</li>
                    <li>For search and information retrieval tasks</li>
                    <li>When building text classification models</li>
                    <li>For document similarity and clustering tasks</li>
                    <li>When you want to filter out common stop words automatically</li>
                    <li>As an improvement over simple word counts (BoW)</li>
                </ul>
                
                <h5>Advantages over BoW</h5>
                <ul>
                    <li>Automatically downweights common words</li>
                    <li>Better captures document-specific important terms</li>
                    <li>More effective for information retrieval</li>
                    <li>Produces more meaningful feature vectors for ML models</li>
                </ul>
            </div>
            
            <h5>Example Walkthrough</h5>
            <p>Consider a corpus with 3 documents:</p>
            <ul>
                <li>Doc 1: "The cat sat on the mat"</li>
                <li>Doc 2: "The dog ran in the park"</li>
                <li>Doc 3: "Cats and dogs are pets"</li>
            </ul>
            <p>For the word "cat" in Doc 1:</p>
            <ul>
                <li><strong>TF:</strong> count("cat", Doc1) / total_words = 1 / 6 = 0.167</li>
                <li><strong>IDF:</strong> log(3 / 2) = log(1.5) = 0.405 (since "cat" appears in 2 documents)</li>
                <li><strong>TF-IDF:</strong> 0.167 × 0.405 = 0.068</li>
            </ul>
            <p>For the word "the" in Doc 1:</p>
            <ul>
                <li><strong>TF:</strong> 2 / 6 = 0.333 (appears twice)</li>
                <li><strong>IDF:</strong> log(3 / 3) = log(1) = 0 (appears in all documents)</li>
                <li><strong>TF-IDF:</strong> 0.333 × 0 = 0 (correctly weighted as unimportant!)</li>
            </ul>
            <p>This shows how TF-IDF correctly identifies "cat" as more important than "the" for document classification!</p>
        </div>

        <div class="interactive-demo">
            <h4>TF-IDF Demo</h4>
            <p>Enter multiple documents separated by | and see how TF-IDF calculates the importance of each word. Notice how common words get lower scores and distinctive words get higher scores!</p>
            <input type="text" class="demo-input" id="tfidfInput" placeholder="Enter documents separated by |" value="The cat sat on the mat | The dog ran in the park | Cats and dogs are pets">
            <button class="demo-btn" onclick="demonstrateTFIDF()">Calculate TF-IDF</button>
            <div class="demo-output" id="tfidfOutput"></div>
        </div>
    </div>

    <!-- Word Embeddings Section -->
    <div id="embeddings" class="nlp-section">
        <h2>Word Embeddings</h2>
        
        <div class="explanation-box">
            <h4>What are Word Embeddings?</h4>
            <p>Word embeddings are dense, low-dimensional vector representations of words that capture semantic and syntactic relationships. Unlike sparse representations like BoW and TF-IDF, embeddings are dense vectors (typically 100-300 dimensions) where semantically similar words are positioned close to each other in the vector space.</p>
            
            <h5>Why Word Embeddings?</h5>
            <p>The key advantages of word embeddings include:</p>
            <ul>
                <li><strong>Semantic Similarity:</strong> Similar words have similar vectors (e.g., "king" and "queen" are close)</li>
                <li><strong>Context Awareness:</strong> Words with similar contexts have similar embeddings</li>
                <li><strong>Dense Representation:</strong> Much smaller than sparse BoW vectors (300 dimensions vs. thousands)</li>
                <li><strong>Transfer Learning:</strong> Pre-trained embeddings can be used across different tasks</li>
                <li><strong>Mathematical Operations:</strong> Can perform analogical reasoning (king - man + woman ≈ queen)</li>
            </ul>
            
            <div class="usage-section">
                <h5>Common Use Cases</h5>
                <ul>
                    <li><strong>Feature Extraction:</strong> Initial word representations for neural networks</li>
                    <li><strong>Semantic Search:</strong> Finding similar words or documents</li>
                    <li><strong>Recommendation Systems:</strong> Understanding user preferences from text</li>
                    <li><strong>Machine Translation:</strong> Cross-lingual word representations</li>
                    <li><strong>Question Answering:</strong> Understanding query semantics</li>
                    <li><strong>Text Classification:</strong> Input features for classifiers</li>
                </ul>
            </div>
        </div>

        <div class="example-box">
            <strong>Famous Example:</strong><br>
            king - man + woman = queen<br>
            This demonstrates how embeddings capture semantic relationships!
        </div>

        <h3>1. Word2Vec</h3>
        
        <div class="explanation-box">
            <h4>What is Word2Vec?</h4>
            <p>Word2Vec is a neural network-based technique introduced by Google in 2013 that learns word embeddings by predicting words in context. It uses a shallow neural network (typically 2-3 layers) to learn word representations from large text corpora. The key insight is that words appearing in similar contexts should have similar meanings.</p>
            
            <h5>How Word2Vec Works</h5>
            <p>Word2Vec uses the distributional hypothesis: "You shall know a word by the company it keeps." It learns embeddings by training a neural network to predict:</p>
            <ul>
                <li><strong>CBOW:</strong> Predict the target word from surrounding context words</li>
                <li><strong>Skip-gram:</strong> Predict context words from a target word</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Skip-gram Objective:</strong></p>
                <p>Maximize: <strong>P(w<sub>t-c</sub>, ..., w<sub>t+c</sub> | w<sub>t</sub>)</strong></p>
                <p>Where <code>w<sub>t</sub></code> is the target word and <code>w<sub>t-c</sub>, ..., w<sub>t+c</sub></code> are context words.</p>
                
                <p><strong>CBOW Objective:</strong></p>
                <p>Maximize: <strong>P(w<sub>t</sub> | w<sub>t-c</sub>, ..., w<sub>t-1</sub>, w<sub>t+1</sub>, ..., w<sub>t+c</sub>)</strong></p>
                
                <p><strong>Word Similarity (Cosine Similarity):</strong></p>
                <p>similarity(w₁, w₂) = <strong>(w₁ · w₂) / (||w₁|| × ||w₂||)</strong></p>
                <p>Where · is dot product and ||w|| is the vector norm.</p>
                
                <p><strong>Negative Sampling (Efficient Training):</strong></p>
                <p>Instead of updating all vocabulary weights, sample negative examples:</p>
                <p>P(w<sub>neg</sub>) ∝ <strong>(freq(w<sub>neg</sub>))<sup>3/4</sup></strong></p>
                <p>Where <code>freq(w)</code> is the frequency of word <code>w</code> in the corpus.</p>
            </div>
            
            <div class="usage-section">
                <h5>When to Use Word2Vec</h5>
                <ul>
                    <li>When you need semantic word representations</li>
                    <li>For tasks requiring word similarity calculations</li>
                    <li>When working with large text corpora</li>
                    <li>As a baseline for more advanced embedding methods</li>
                    <li>For downstream NLP tasks (classification, clustering, etc.)</li>
                </ul>
            </div>
        </div>
        
        <p>Word2Vec uses neural networks to learn word associations from a large corpus of text.</p>

        <div class="architecture-diagram">
            <div class="layer">Input Layer</div>
            <span class="arrow">→</span>
            <div class="layer">Hidden Layer (Embeddings)</div>
            <span class="arrow">→</span>
            <div class="layer">Output Layer</div>
        </div>

        <div class="interactive-demo">
            <h4>Word Similarity Demo</h4>
            <p>Enter two words to see their conceptual similarity:</p>
            <input type="text" class="demo-input" id="word1" placeholder="First word" value="king">
            <input type="text" class="demo-input" id="word2" placeholder="Second word" value="queen">
            <button class="demo-btn" onclick="calculateWordSimilarity()">Calculate Similarity</button>
            <div class="demo-output" id="similarityOutput"></div>
        </div>

        <h3>Word2Vec Variants</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>CBOW (Continuous Bag of Words)</h4>
                <ul>
                    <li>Predicts target word from context</li>
                    <li>Faster training</li>
                    <li>Better for frequent words</li>
                    <li>Good for large datasets</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Skip-gram</h4>
                <ul>
                    <li>Predicts context from target word</li>
                    <li>Better for rare words</li>
                    <li>Higher accuracy</li>
                    <li>Good for small datasets</li>
                </ul>
            </div>
        </div>

        <h3>2. GloVe (Global Vectors)</h3>
        
        <div class="explanation-box">
            <h4>What is GloVe?</h4>
            <p>GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm developed by Stanford in 2014 that combines the advantages of global matrix factorization methods (like LSA) with local context window methods (like Word2Vec). It learns word embeddings by leveraging global word-word co-occurrence statistics from a corpus.</p>
            
            <h5>How GloVe Works</h5>
            <p>GloVe constructs a co-occurrence matrix from the entire corpus, then learns embeddings that preserve the ratios of co-occurrence probabilities. The key insight is that the ratio of co-occurrence probabilities encodes meaningful semantic relationships.</p>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Co-occurrence Matrix:</strong></p>
                <p>X<sub>ij</sub> = number of times word <code>j</code> appears in the context of word <code>i</code></p>
                
                <p><strong>Objective Function:</strong></p>
                <p>Minimize: <strong>J = Σ<sub>i,j=1</sub><sup>V</sup> f(X<sub>ij</sub>)(w<sub>i</sub>·w̃<sub>j</sub> + b<sub>i</sub> + b̃<sub>j</sub> - log X<sub>ij</sub>)²</strong></p>
                <p>Where:</p>
                <ul>
                    <li><code>w<sub>i</sub></code> = word embedding vector for word i</li>
                    <li><code>w̃<sub>j</sub></code> = context embedding vector for word j</li>
                    <li><code>b<sub>i</sub>, b̃<sub>j</sub></code> = bias terms</li>
                    <li><code>f(X<sub>ij</sub>)</code> = weighting function</li>
                </ul>
                
                <p><strong>Weighting Function:</strong></p>
                <p>f(x) = <strong>(x/x<sub>max</sub>)<sup>α</sup></strong> if x < x<sub>max</sub>, else 1</p>
                <p>Typical values: α = 0.75, x<sub>max</sub> = 100</p>
                
                <p><strong>Final Embedding:</strong></p>
                <p>w<sub>i</sub><sup>final</sup> = <strong>(w<sub>i</sub> + w̃<sub>i</sub>) / 2</strong></p>
            </div>
            
            <div class="usage-section">
                <h5>Advantages of GloVe</h5>
                <ul>
                    <li>Captures global statistics efficiently</li>
                    <li>Better performance on word analogy tasks</li>
                    <li>Faster training than Word2Vec on large corpora</li>
                    <li>Produces high-quality embeddings</li>
                </ul>
                
                <h5>When to Use GloVe</h5>
                <ul>
                    <li>When you have access to large corpora</li>
                    <li>For tasks requiring word analogy reasoning</li>
                    <li>When you need global word relationships</li>
                    <li>As an alternative to Word2Vec for pre-trained embeddings</li>
                </ul>
            </div>
        </div>
        
        <p>GloVe generates word vectors based on co-occurrence statistics in a large corpus.</p>

        <div class="interactive-demo">
            <h4>Co-occurrence Matrix Demo</h4>
            <input type="text" class="demo-input" id="gloveInput" placeholder="Enter a sentence" value="The quick brown fox jumps over the lazy dog">
            <button class="demo-btn" onclick="demonstrateCooccurrence()">Generate Co-occurrence</button>
            <div class="demo-output" id="gloveOutput"></div>
        </div>

        <h3>3. FastText</h3>
        
        <div class="explanation-box">
            <h4>What is FastText?</h4>
            <p>FastText is an extension of Word2Vec developed by Facebook AI Research in 2016. Unlike Word2Vec, which treats each word as an atomic unit, FastText represents words as bags of character n-grams. This allows it to handle out-of-vocabulary (OOV) words and morphologically rich languages effectively.</p>
            
            <h5>How FastText Works</h5>
            <p>FastText breaks words into character n-grams (substrings) and represents each word as the sum of its n-gram vectors. For example, "hello" with n=3 becomes: "&lt;he", "hel", "ell", "llo", "lo&gt;". This approach allows the model to:</p>
            <ul>
                <li>Handle rare words by sharing representations with similar words</li>
                <li>Handle OOV words by composing their n-grams</li>
                <li>Better understand morphologically rich languages</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Word Representation:</strong></p>
                <p>w = <strong>Σ<sub>g∈G<sub>w</sub></sub> z<sub>g</sub></strong></p>
                <p>Where:</p>
                <ul>
                    <li><code>G<sub>w</sub></code> = set of n-grams in word w</li>
                    <li><code>z<sub>g</sub></code> = vector representation of n-gram g</li>
                </ul>
                
                <p><strong>N-gram Generation:</strong></p>
                <p>For word "hello" with n=3:</p>
                <p>G<sub>hello</sub> = {"&lt;he", "hel", "ell", "llo", "lo&gt;"}</p>
                <p>Note: &lt; and &gt; are special boundary characters</p>
                
                <p><strong>Skip-gram with N-grams:</strong></p>
                <p>Same objective as Word2Vec, but uses word representation w instead of word vector</p>
            </div>
            
            <div class="usage-section">
                <h5>Advantages of FastText</h5>
                <ul>
                    <li>Handles OOV words effectively</li>
                    <li>Better for morphologically rich languages</li>
                    <li>Can represent rare words better</li>
                    <li>Faster training than Word2Vec</li>
                    <li>Better performance on small datasets</li>
                </ul>
                
                <h5>When to Use FastText</h5>
                <ul>
                    <li>When dealing with morphologically rich languages</li>
                    <li>For tasks with many rare or unseen words</li>
                    <li>When working with social media text (misspellings, slang)</li>
                    <li>For multilingual applications</li>
                    <li>When you need word-level and subword-level features</li>
                </ul>
            </div>
        </div>
        
        <p>FastText extends Word2Vec by using subword representations (character n-grams), making it excellent for handling out-of-vocabulary words.</p>

        <div class="example-box">
            <strong>FastText Advantage:</strong><br>
            Even if "unhappiness" wasn't in training data, FastText can understand it through subwords:<br>
            "un-", "-happy-", "-ness", "unhappy", "happiness", etc.
        </div>
    </div>

    <!-- Sentiment Analysis Section -->
    <div id="sentiment" class="nlp-section">
        <h2>Sentiment Analysis</h2>
        
        <div class="explanation-box">
            <h4>What is Sentiment Analysis?</h4>
            <p>Sentiment analysis (also known as opinion mining) is a natural language processing technique that identifies and extracts subjective information from text, determining the emotional tone, attitude, or opinion expressed. It classifies text as positive, negative, or neutral, and can also detect specific emotions like joy, anger, sadness, etc.</p>
            
            <h5>Why Sentiment Analysis Matters</h5>
            <p>Sentiment analysis is crucial because:</p>
            <ul>
                <li><strong>Business Intelligence:</strong> Companies monitor customer opinions about products and services</li>
                <li><strong>Social Media Monitoring:</strong> Track public opinion and brand reputation</li>
                <li><strong>Market Research:</strong> Understand consumer preferences and trends</li>
                <li><strong>Customer Service:</strong> Prioritize negative feedback for immediate attention</li>
                <li><strong>Political Analysis:</strong> Gauge public opinion on policies and candidates</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Binary Classification:</strong></p>
                <p>P(sentiment | text) = <strong>softmax(W · f(text) + b)</strong></p>
                <p>Where <code>f(text)</code> is the feature representation (BoW, TF-IDF, embeddings)</p>
                
                <p><strong>Multi-class Sentiment:</strong></p>
                <p>P(s<sub>i</sub> | text) = <strong>exp(W<sub>i</sub> · f(text) + b<sub>i</sub>) / Σ<sub>j</sub> exp(W<sub>j</sub> · f(text) + b<sub>j</sub>)</strong></p>
                <p>Where <code>s<sub>i</sub></code> represents sentiment class i (positive, negative, neutral)</p>
                
                <p><strong>Sentiment Score (Continuous):</strong></p>
                <p>score(text) = <strong>Σ<sub>w∈text</sub> sentiment_weight(w) × tfidf(w, text)</strong></p>
                <p>Normalized to range [-1, 1] where -1 = negative, 0 = neutral, 1 = positive</p>
                
                <p><strong>Attention-based Sentiment:</strong></p>
                <p>sentiment = <strong>Σ<sub>i</sub> α<sub>i</sub> · h<sub>i</sub></strong></p>
                <p>Where <code>α<sub>i</sub></code> is attention weight and <code>h<sub>i</sub></code> is hidden state</p>
            </div>
            
            <div class="usage-section">
                <h5>Common Approaches</h5>
                <ul>
                    <li><strong>Lexicon-based:</strong> Uses sentiment dictionaries (e.g., VADER, TextBlob)</li>
                    <li><strong>Machine Learning:</strong> Naive Bayes, SVM, Logistic Regression with features</li>
                    <li><strong>Deep Learning:</strong> LSTM, CNN, BERT for sequence understanding</li>
                    <li><strong>Hybrid:</strong> Combines lexicon and ML approaches</li>
                </ul>
                
                <h5>When to Use Sentiment Analysis</h5>
                <ul>
                    <li>Customer feedback analysis</li>
                    <li>Social media monitoring</li>
                    <li>Product review analysis</li>
                    <li>Brand reputation management</li>
                    <li>Market research and trend analysis</li>
                    <li>Political opinion tracking</li>
                </ul>
            </div>
        </div>
        
        <p>Sentiment analysis determines the emotional tone behind words, helping understand opinions, attitudes, and emotions expressed in text.</p>

        <div class="interactive-demo">
            <h4>Live Sentiment Analysis</h4>
            <input type="text" class="demo-input" id="sentimentInput" placeholder="Enter text to analyze sentiment..." value="I love this amazing product! It works perfectly.">
            <button class="demo-btn" onclick="analyzeSentiment()">Analyze Sentiment</button>
            <div class="demo-output" id="sentimentOutput"></div>
        </div>

        <h3>Sentiment Analysis Workflow</h3>
        <div class="architecture-diagram">
            <div class="layer">Data Collection</div>
            <span class="arrow">→</span>
            <div class="layer">Preprocessing</div>
            <span class="arrow">→</span>
            <div class="layer">Feature Extraction</div>
            <span class="arrow">→</span>
            <div class="layer">Model Training</div>
            <span class="arrow">→</span>
            <div class="layer">Evaluation</div>
        </div>

        <h3>Applications</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>Business Applications</h4>
                <ul>
                    <li>Brand reputation monitoring</li>
                    <li>Product review analysis</li>
                    <li>Customer feedback processing</li>
                    <li>Market research</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Social & Political</h4>
                <ul>
                    <li>Social media monitoring</li>
                    <li>Political opinion tracking</li>
                    <li>Public sentiment analysis</li>
                    <li>Crisis management</li>
                </ul>
            </div>
        </div>

        <h3>Challenges in Sentiment Analysis</h3>
        <ul>
            <li><strong>Sarcasm Detection:</strong> "Great job!" might be sarcastic</li>
            <li><strong>Context Dependency:</strong> Same word, different sentiments</li>
            <li><strong>Imbalanced Datasets:</strong> More positive than negative examples</li>
            <li><strong>Domain Specificity:</strong> Movie reviews vs. product reviews</li>
        </ul>

        <div class="quiz-container">
            <div class="quiz-question">Quiz: Which is the biggest challenge in sentiment analysis?</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">A) Processing speed</div>
            <div class="quiz-option" onclick="checkAnswer(this, true)">B) Understanding context and sarcasm</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">C) Memory requirements</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">D) Data storage</div>
        </div>
    </div>

    <!-- Seq2Seq Section -->
    <div id="seq2seq" class="nlp-section">
        <h2>Sequence-to-Sequence Models</h2>
        
        <div class="explanation-box">
            <h4>What are Seq2Seq Models?</h4>
            <p>Sequence-to-Sequence (Seq2Seq) models are neural network architectures designed to map variable-length input sequences to variable-length output sequences. They revolutionized NLP tasks like machine translation, text summarization, and conversational AI by learning to generate sequences rather than just classify them.</p>
            
            <h5>Why Seq2Seq Models?</h5>
            <p>Seq2Seq models are essential because:</p>
            <ul>
                <li><strong>Variable Length:</strong> Handle inputs and outputs of different lengths</li>
                <li><strong>Context Preservation:</strong> Encode entire input sequence into a context vector</li>
                <li><strong>Generation:</strong> Generate new sequences token by token</li>
                <li><strong>Flexibility:</strong> Applicable to many sequence generation tasks</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Encoder:</strong></p>
                <p>h<sub>t</sub> = <strong>RNN(x<sub>t</sub>, h<sub>t-1</sub>)</strong></p>
                <p>c = <strong>h<sub>T</sub></strong> (context vector = final hidden state)</p>
                
                <p><strong>Decoder:</strong></p>
                <p>s<sub>t</sub> = <strong>RNN(y<sub>t-1</sub>, s<sub>t-1</sub>, c)</strong></p>
                <p>P(y<sub>t</sub> | y<sub>&lt;t</sub>, c) = <strong>softmax(W · s<sub>t</sub> + b)</strong></p>
                <p>Where:</p>
                <ul>
                    <li><code>h<sub>t</sub></code> = encoder hidden state at time t</li>
                    <li><code>s<sub>t</sub></code> = decoder hidden state at time t</li>
                    <li><code>c</code> = context vector</li>
                    <li><code>x<sub>t</sub></code> = input token at time t</li>
                    <li><code>y<sub>t</sub></code> = output token at time t</code></li>
                </ul>
                
                <p><strong>Attention Mechanism (Extended):</strong></p>
                <p>α<sub>t,i</sub> = <strong>exp(score(s<sub>t</sub>, h<sub>i</sub>)) / Σ<sub>j</sub> exp(score(s<sub>t</sub>, h<sub>j</sub>))</strong></p>
                <p>c<sub>t</sub> = <strong>Σ<sub>i</sub> α<sub>t,i</sub> · h<sub>i</sub></strong></p>
                <p>Where <code>c<sub>t</sub></code> is the context vector at decoding step t</p>
            </div>
            
            <div class="usage-section">
                <h5>Common Applications</h5>
                <ul>
                    <li><strong>Machine Translation:</strong> English → French, etc.</li>
                    <li><strong>Text Summarization:</strong> Long article → short summary</li>
                    <li><strong>Chatbots:</strong> User query → response</li>
                    <li><strong>Question Answering:</strong> Context + question → answer</li>
                    <li><strong>Image Captioning:</strong> Image → text description</li>
                    <li><strong>Code Generation:</strong> Natural language → code</li>
                </ul>
                
                <h5>When to Use Seq2Seq</h5>
                <ul>
                    <li>When input and output are both sequences</li>
                    <li>For generation tasks (translation, summarization)</li>
                    <li>When sequence length varies</li>
                    <li>For conversational AI applications</li>
                    <li>Before transformers were available (now often replaced by them)</li>
                </ul>
            </div>
        </div>
        
        <p>Seq2Seq models are specialized neural network architectures designed to handle sequences as both input and output. They're perfect for tasks like translation, summarization, and chatbots.</p>

        <div class="architecture-diagram">
            <h4>Seq2Seq Architecture</h4>
            <div class="layer">Encoder</div>
            <span class="arrow">→</span>
            <div class="layer">Context Vector</div>
            <span class="arrow">→</span>
            <div class="layer">Decoder</div>
        </div>

        <div class="interactive-demo">
            <h4>Translation Demo (Conceptual)</h4>
            <input type="text" class="demo-input" id="translateInput" placeholder="Enter English text..." value="Hello how are you today">
            <select class="demo-input" id="targetLang">
                <option value="spanish">Spanish</option>
                <option value="french">French</option>
                <option value="german">German</option>
            </select>
            <button class="demo-btn" onclick="demonstrateTranslation()">Translate</button>
            <div class="demo-output" id="translateOutput"></div>
        </div>

        <h3>Key Components</h3>
        
        <div class="example-box">
            <h4>Encoder</h4>
            <p>Processes each token in the input sequence and creates a fixed-length context vector that encapsulates the meaning of the entire input sequence.</p>
        </div>

        <div class="example-box">
            <h4>Context Vector</h4>
            <p>The final internal state of the encoder - a dense representation that captures the essence of the input sequence.</p>
        </div>

        <div class="example-box">
            <h4>Decoder</h4>
            <p>Reads the context vector and generates the target sequence token by token, using the context and previously generated tokens.</p>
        </div>

        <h3>Types of Seq2Seq Models</h3>
        <ul>
            <li><strong>Many-to-One:</strong> Sentiment analysis (sequence → single label)</li>
            <li><strong>One-to-Many:</strong> Image captioning (image → sequence of words)</li>
            <li><strong>Many-to-Many:</strong> Machine translation (sequence → sequence)</li>
            <li><strong>Synchronized:</strong> Video classification (frame by frame)</li>
        </ul>

        <h3>Limitations</h3>
        <div class="pros-cons">
            <div class="cons">
                <h4>RNN/LSTM Based Seq2Seq Issues</h4>
                <ul>
                    <li>Vanishing gradient problems</li>
                    <li>Sequential processing (no parallelization)</li>
                    <li>Information bottleneck in context vector</li>
                    <li>Difficulty with long sequences</li>
                </ul>
            </div>
            <div class="pros">
                <h4>Solutions</h4>
                <ul>
                    <li>Attention mechanisms</li>
                    <li>Transformer architecture</li>
                    <li>Better initialization techniques</li>
                    <li>Advanced optimization methods</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Transformers Section -->
    <div id="transformers" class="nlp-section">
        <h2>Transformers: The Revolution</h2>
        
        <div class="explanation-box">
            <h4>What are Transformers?</h4>
            <p>Transformers are a revolutionary neural network architecture introduced in 2017 by the paper "Attention is All You Need". They replaced RNNs and LSTMs by using self-attention mechanisms to process entire sequences in parallel, achieving state-of-the-art performance on virtually all NLP tasks.</p>
            
            <h5>Why Transformers Changed Everything</h5>
            <p>Transformers revolutionized NLP because:</p>
            <ul>
                <li><strong>Parallelization:</strong> Process all positions simultaneously (not sequential like RNNs)</li>
                <li><strong>Long-range Dependencies:</strong> Direct connections between all positions via attention</li>
                <li><strong>Scalability:</strong> Easy to scale to billions of parameters</li>
                <li><strong>Transfer Learning:</strong> Pre-trained models (BERT, GPT) work across many tasks</li>
                <li><strong>State-of-the-art:</strong> Best performance on translation, summarization, QA, etc.</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Multi-Head Attention:</strong></p>
                <p>MultiHead(Q, K, V) = <strong>Concat(head₁, ..., head<sub>h</sub>)W<sup>O</sup></strong></p>
                <p>head<sub>i</sub> = <strong>Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)</strong></p>
                
                <p><strong>Position Encoding:</strong></p>
                <p>PE<sub>(pos, 2i)</sub> = <strong>sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong></p>
                <p>PE<sub>(pos, 2i+1)</sub> = <strong>cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong></p>
                
                <p><strong>Feed-Forward Network:</strong></p>
                <p>FFN(x) = <strong>max(0, xW₁ + b₁)W₂ + b₂</strong></p>
                
                <p><strong>Layer Normalization:</strong></p>
                <p>LayerNorm(x) = <strong>γ · (x - μ) / √(σ² + ε) + β</strong></p>
                <p>Where μ and σ² are mean and variance, γ and β are learnable parameters</p>
            </div>
            
            <div class="usage-section">
                <h5>Key Components</h5>
                <ul>
                    <li><strong>Self-Attention:</strong> Allows each position to attend to all positions</li>
                    <li><strong>Multi-Head Attention:</strong> Multiple attention heads capture different relationships</li>
                    <li><strong>Position Encoding:</strong> Adds positional information to embeddings</li>
                    <li><strong>Feed-Forward Networks:</strong> Non-linear transformations</li>
                    <li><strong>Residual Connections:</strong> Helps with gradient flow</li>
                    <li><strong>Layer Normalization:</strong> Stabilizes training</li>
                </ul>
                
                <h5>When to Use Transformers</h5>
                <ul>
                    <li>For any NLP task (translation, summarization, QA, etc.)</li>
                    <li>When you need state-of-the-art performance</li>
                    <li>For transfer learning (use pre-trained models)</li>
                    <li>When working with long sequences</li>
                    <li>For tasks requiring understanding of context</li>
                </ul>
            </div>
        </div>
        
        <p>Transformers revolutionized NLP by introducing the "Attention is All You Need" paradigm, eliminating the need for recurrent connections while achieving superior performance.</p>

        <div class="example-box">
            <h4>Key Innovation: Self-Attention</h4>
            <p>Instead of processing sequences step-by-step, Transformers look at all positions simultaneously and learn which parts are most relevant to each other.</p>
        </div>

        <div class="interactive-demo">
            <h4>Transformer Components Explorer</h4>
            <select class="demo-input" id="transformerComponent">
                <option value="overview">Architecture Overview</option>
                <option value="encoder">Encoder Stack</option>
                <option value="decoder">Decoder Stack</option>
                <option value="attention">Multi-Head Attention</option>
            </select>
            <button class="demo-btn" onclick="exploreTransformer()">Explore Component</button>
            <div class="demo-output" id="transformerOutput"></div>
        </div>

        <h3>Transformer Architecture</h3>
        <div class="architecture-diagram">
            <div style="display: flex; justify-content: space-between; width: 100%;">
                <div style="text-align: center;">
                    <h4>Encoder</h4>
                    <div class="layer">Multi-Head Attention</div>
                    <div class="layer">Add & Norm</div>
                    <div class="layer">Feed Forward</div>
                    <div class="layer">Add & Norm</div>
                    <p>×6 layers</p>
                </div>
                <div style="text-align: center;">
                    <h4>Decoder</h4>
                    <div class="layer">Masked Multi-Head Attention</div>
                    <div class="layer">Add & Norm</div>
                    <div class="layer">Multi-Head Attention</div>
                    <div class="layer">Add & Norm</div>
                    <div class="layer">Feed Forward</div>
                    <div class="layer">Add & Norm</div>
                    <p>×6 layers</p>
                </div>
            </div>
        </div>

        <h3>Why Transformers?</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>Advantages</h4>
                <ul>
                    <li><strong>Parallelization:</strong> Process entire sequences simultaneously</li>
                    <li><strong>Long-term Dependencies:</strong> Better at capturing relationships</li>
                    <li><strong>Scalability:</strong> Easy to scale to larger datasets</li>
                    <li><strong>Transfer Learning:</strong> Pre-trained models work across tasks</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Limitations</h4>
                <ul>
                    <li><strong>Computational Cost:</strong> Quadratic complexity with sequence length</li>
                    <li><strong>Data Hungry:</strong> Requires large amounts of training data</li>
                    <li><strong>Memory Requirements:</strong> High memory usage</li>
                    <li><strong>Overfitting:</strong> Prone to overfitting on small datasets</li>
                </ul>
            </div>
        </div>

        <h3>Famous Transformer Models</h3>
        <ul>
            <li><strong>BERT:</strong> Bidirectional Encoder Representations from Transformers</li>
            <li><strong>GPT:</strong> Generative Pre-trained Transformer</li>
            <li><strong>T5:</strong> Text-to-Text Transfer Transformer</li>
            <li><strong>RoBERTa:</strong> Robustly Optimized BERT Pretraining Approach</li>
        </ul>
    </div>

    <!-- Self-Attention Section -->
    <div id="attention" class="nlp-section">
        <h2>Self-Attention Mechanism</h2>
        
        <div class="explanation-box">
            <h4>What is Self-Attention?</h4>
            <p>Self-attention (also called intra-attention) is a mechanism that allows each position in a sequence to attend to all positions in the same sequence, including itself. It computes a weighted sum of all positions, where the weights are learned based on how relevant each position is to the current position.</p>
            
            <h5>Why Self-Attention is Revolutionary</h5>
            <p>Self-attention is transformative because:</p>
            <ul>
                <li><strong>Direct Connections:</strong> Directly connects all positions, avoiding information bottleneck</li>
                <li><strong>Parallel Computation:</strong> All attention scores can be computed in parallel</li>
                <li><strong>Interpretability:</strong> Attention weights show which parts are important</li>
                <li><strong>Long-range Dependencies:</strong> Easily captures relationships between distant positions</li>
                <li><strong>Flexibility:</strong> Dynamically focuses on relevant parts of the sequence</li>
            </ul>
            
            <div class="formula-section">
                <h5>Mathematical Formulation</h5>
                
                <p><strong>Self-Attention Formula:</strong></p>
                <p>Attention(Q, K, V) = <strong>softmax(QK<sup>T</sup> / √d<sub>k</sub>)V</strong></p>
                <p>Where:</p>
                <ul>
                    <li><code>Q</code> = Query matrix (n × d<sub>k</sub>)</li>
                    <li><code>K</code> = Key matrix (n × d<sub>k</sub>)</li>
                    <li><code>V</code> = Value matrix (n × d<sub>v</sub>)</li>
                    <li><code>d<sub>k</sub></code> = dimension of queries/keys</li>
                    <li><code>√d<sub>k</sub></code> = scaling factor (prevents small gradients)</li>
                </ul>
                
                <p><strong>Query, Key, Value Generation:</strong></p>
                <p>Q = <strong>XW<sup>Q</sup></strong>, K = <strong>XW<sup>K</sup></strong>, V = <strong>XW<sup>V</sup></strong></p>
                <p>Where <code>X</code> is the input embedding matrix</p>
                
                <p><strong>Attention Scores:</strong></p>
                <p>scores = <strong>QK<sup>T</sup> / √d<sub>k</sub></strong></p>
                <p>attention_weights = <strong>softmax(scores)</strong></p>
                <p>output = <strong>attention_weights × V</strong></p>
                
                <p><strong>Multi-Head Attention:</strong></p>
                <p>head<sub>i</sub> = <strong>Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)</strong></p>
                <p>MultiHead = <strong>Concat(head₁, ..., head<sub>h</sub>)W<sup>O</sup></strong></p>
            </div>
            
            <div class="usage-section">
                <h5>How Self-Attention Works (Step by Step)</h5>
                <ol>
                    <li><strong>Create Q, K, V:</strong> Transform input into Query, Key, Value matrices</li>
                    <li><strong>Compute Scores:</strong> Calculate similarity between queries and keys</li>
                    <li><strong>Scale:</strong> Divide scores by √d<sub>k</sub> to prevent extreme values</li>
                    <li><strong>Softmax:</strong> Convert scores to attention weights (probabilities)</li>
                    <li><strong>Weighted Sum:</strong> Multiply attention weights with values</li>
                    <li><strong>Output:</strong> Result is the weighted combination of all positions</li>
                </ol>
                
                <h5>When to Use Self-Attention</h5>
                <ul>
                    <li>In Transformer architectures</li>
                    <li>When you need to model long-range dependencies</li>
                    <li>For tasks requiring understanding of relationships between all positions</li>
                    <li>When you want interpretability (attention weights)</li>
                    <li>For parallel processing of sequences</li>
                </ul>
            </div>
        </div>
        
        <p>Self-attention is the core innovation of Transformers. It allows each position in a sequence to attend to all positions in the same sequence to compute a representation.</p>

        <div class="interactive-demo">
            <h4>Attention Visualization</h4>
            <input type="text" class="demo-input" id="attentionInput" placeholder="Enter a sentence to visualize attention..." value="The cat sat on the mat">
            <button class="demo-btn" onclick="visualizeAttention()">Visualize Attention</button>
            <div class="demo-output" id="attentionOutput"></div>
        </div>

        <h3>How Self-Attention Works</h3>
        
        <div class="example-box">
            <h4>Key Components</h4>
            <ul>
                <li><strong>Query (Q):</strong> What information are we looking for?</li>
                <li><strong>Key (K):</strong> What information does each position offer?</li>
                <li><strong>Value (V):</strong> The actual information to be retrieved</li>
            </ul>
        </div>

        <div class="formula-box">
Attention(Q, K, V) = softmax(QK^T / √d_k)V

Where:
- Q, K, V are matrices of queries, keys, and values
- d_k is the dimension of the key vectors
- √d_k is used for scaling to prevent extremely small gradients
        </div>

        <div class="interactive-demo">
            <h4>Step-by-Step Attention Calculation</h4>
            <button class="demo-btn" onclick="demonstrateAttentionSteps()">Show Attention Steps</button>
            <div class="demo-output" id="attentionStepsOutput"></div>
        </div>

        <h3>Multi-Head Attention</h3>
        <p>Instead of performing a single attention function, multi-head attention runs multiple attention "heads" in parallel, each focusing on different types of relationships.</p>

        <div class="architecture-diagram">
            <div style="display: flex; justify-content: space-around; flex-wrap: wrap;">
                <div class="layer">Head 1</div>
                <div class="layer">Head 2</div>
                <div class="layer">Head 3</div>
                <div class="layer">...</div>
                <div class="layer">Head 8</div>
            </div>
            <div style="text-align: center; margin: 20px 0;">
                <span class="arrow">↓</span>
            </div>
            <div class="layer">Concatenate & Linear</div>
        </div>

        <div class="interactive-demo">
            <h4>Multi-Head Attention Demo</h4>
            <input type="number" class="demo-input" id="numHeads" min="1" max="8" value="4" placeholder="Number of attention heads">
            <button class="demo-btn" onclick="demonstrateMultiHead()">Simulate Multi-Head</button>
            <div class="demo-output" id="multiHeadOutput"></div>
        </div>

        <div class="quiz-container">
            <div class="quiz-question">Quiz: What is the main advantage of multi-head attention?</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">A) Faster computation</div>
            <div class="quiz-option" onclick="checkAnswer(this, true)">B) Captures different types of relationships simultaneously</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">C) Uses less memory</div>
            <div class="quiz-option" onclick="checkAnswer(this, false)">D) Simpler to implement</div>
        </div>
    </div>

    <!-- Applications Section -->
    <div id="applications" class="nlp-section">
        <h2>Modern NLP Applications</h2>
        
        <div class="explanation-box">
            <h4>Real-World NLP Applications</h4>
            <p>Natural Language Processing has transformed countless industries and applications. Modern NLP technologies power everything from search engines to virtual assistants, enabling machines to understand, interpret, and generate human language at unprecedented levels.</p>
            
            <h5>Why NLP Applications Matter</h5>
            <p>NLP applications are revolutionizing how we interact with technology because:</p>
            <ul>
                <li><strong>Automation:</strong> Automate repetitive text-based tasks</li>
                <li><strong>Accessibility:</strong> Make technology accessible through natural language</li>
                <li><strong>Insights:</strong> Extract valuable insights from unstructured text data</li>
                <li><strong>Efficiency:</strong> Process and analyze massive amounts of text quickly</li>
                <li><strong>Personalization:</strong> Provide personalized experiences through language understanding</li>
            </ul>
            
            <div class="formula-section">
                <h5>Key NLP Tasks and Their Applications</h5>
                
                <p><strong>Text Classification:</strong></p>
                <p>P(class | text) = <strong>model(text)</strong></p>
                <p>Applications: Spam detection, sentiment analysis, topic classification</p>
                
                <p><strong>Named Entity Recognition:</strong></p>
                <p>P(entities | text) = <strong>sequence_model(text)</strong></p>
                <p>Applications: Information extraction, knowledge graphs, document indexing</p>
                
                <p><strong>Text Summarization:</strong></p>
                <p>summary = <strong>argmax<sub>s</sub> P(s | text)</strong></p>
                <p>Applications: News summarization, document summarization, meeting notes</p>
                
                <p><strong>Question Answering:</strong></p>
                <p>answer = <strong>argmax<sub>a</sub> P(a | context, question)</strong></p>
                <p>Applications: Chatbots, search engines, virtual assistants</p>
                
                <p><strong>Machine Translation:</strong></p>
                <p>translation = <strong>argmax<sub>t</sub> P(t | source_text)</strong></p>
                <p>Applications: Real-time translation, multilingual support, localization</p>
            </div>
            
            <div class="usage-section">
                <h5>Major Application Categories</h5>
                <ul>
                    <li><strong>Information Retrieval:</strong> Search engines, document retrieval, recommendation systems</li>
                    <li><strong>Text Generation:</strong> Content creation, chatbots, code generation, creative writing</li>
                    <li><strong>Text Analysis:</strong> Sentiment analysis, topic modeling, text classification</li>
                    <li><strong>Language Understanding:</strong> Question answering, reading comprehension, reasoning</li>
                    <li><strong>Language Translation:</strong> Machine translation, multilingual communication</li>
                    <li><strong>Speech Processing:</strong> Speech-to-text, text-to-speech, voice assistants</li>
                </ul>
                
                <h5>Industry Impact</h5>
                <ul>
                    <li><strong>Healthcare:</strong> Clinical documentation, drug discovery, patient care</li>
                    <li><strong>Finance:</strong> Fraud detection, risk assessment, algorithmic trading</li>
                    <li><strong>E-commerce:</strong> Product recommendations, review analysis, search</li>
                    <li><strong>Education:</strong> Automated grading, personalized learning, tutoring</li>
                    <li><strong>Customer Service:</strong> Chatbots, email routing, support automation</li>
                    <li><strong>Media:</strong> Content generation, fact-checking, news summarization</li>
                </ul>
            </div>
        </div>
        
        <p>Modern NLP has enabled countless applications that we use daily. Let's explore some cutting-edge applications and try them out!</p>

        <div class="interactive-demo">
            <h4>Text Summarization</h4>
            <textarea class="demo-input" id="summaryInput" rows="5" placeholder="Enter a long text to summarize...">Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.</textarea>
            <button class="demo-btn" onclick="summarizeText()">Summarize</button>
            <div class="demo-output" id="summaryOutput"></div>
        </div>

        <div class="interactive-demo">
            <h4>Named Entity Recognition (NER)</h4>
            <input type="text" class="demo-input" id="nerInput" placeholder="Enter text with names, places, organizations..." value="Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.">
            <button class="demo-btn" onclick="performNER()">Extract Entities</button>
            <div class="demo-output" id="nerOutput"></div>
        </div>

        <div class="interactive-demo">
            <h4>Question Answering</h4>
            <textarea class="demo-input" id="qaContext" rows="3" placeholder="Enter context...">The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing. Like recurrent neural networks, transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization.</textarea>
            <input type="text" class="demo-input" id="qaQuestion" placeholder="Ask a question about the context..." value="When was the Transformer model introduced?">
            <button class="demo-btn" onclick="answerQuestion()">Answer Question</button>
            <div class="demo-output" id="qaOutput"></div>
        </div>

        <h3>Industry Applications</h3>
        <div class="pros-cons">
            <div class="pros">
                <h4>Healthcare</h4>
                <ul>
                    <li>Medical record analysis</li>
                    <li>Drug discovery assistance</li>
                    <li>Clinical decision support</li>
                    <li>Patient interaction chatbots</li>
                </ul>
            </div>
            <div class="cons">
                <h4>Finance</h4>
                <ul>
                    <li>Fraud detection</li>
                    <li>Risk assessment</li>
                    <li>Algorithmic trading</li>
                    <li>Customer service automation</li>
                </ul>
            </div>
        </div>

        <div class="pros-cons">
            <div class="pros">
                <h4>Education</h4>
                <ul>
                    <li>Automated essay scoring</li>
                    <li>Personalized learning</li>
                    <li>Language learning apps</li>
                    <li>Research assistance</li>
                </ul>
            </div>
            <div class="cons">
                <h4>E-commerce</h4>
                <ul>
                    <li>Product recommendations</li>
                    <li>Review analysis</li>
                    <li>Customer support</li>
                    <li>Search optimization</li>
                </ul>
            </div>
        </div>

        <h3>Future of NLP</h3>
        <ul>
            <li><strong>Multimodal Models:</strong> Combining text, images, and audio</li>
            <li><strong>Few-shot Learning:</strong> Learning from minimal examples</li>
            <li><strong>Efficient Models:</strong> Smaller, faster models for mobile devices</li>
            <li><strong>Ethical AI:</strong> Reducing bias and improving fairness</li>
            <li><strong>Specialized Models:</strong> Domain-specific fine-tuned models</li>
        </ul>

        <div class="example-box">
            <h4>Congratulations!</h4>
            <p>You've completed the comprehensive NLP course! You now understand the fundamental concepts from basic text representation to advanced Transformer architectures. Keep practicing and exploring to master these powerful techniques!</p>
        </div>
    </div>
</div>
{% endblock %}

{% block extra_js %}
<script>
let currentSection = 'intro';
const sections = ['intro', 'workflow', 'text-repr', 'embeddings', 'sentiment', 'seq2seq', 'transformers', 'attention', 'applications'];

function showSection(sectionId) {
    // Hide all sections
    document.querySelectorAll('.nlp-section').forEach(section => {
        section.classList.remove('active');
    });
    
    // Show target section
    document.getElementById(sectionId).classList.add('active');
    
    // Update navigation
    document.querySelectorAll('.nav-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    event.target.classList.add('active');
    
    // Update progress
    const progress = (sections.indexOf(sectionId) + 1) / sections.length * 100;
    document.getElementById('progressFill').style.width = progress + '%';
    
    currentSection = sectionId;
}

function preprocessText() {
    const input = document.getElementById('nlpInput').value;
    const output = document.getElementById('nlpOutput');
    
    const steps = [
        `Original: "${input}"`,
        `Lowercased: "${input.toLowerCase()}"`,
        `Tokens: [${input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).map(w => `"${w}"`).join(', ')}]`,
        `Filtered (no stopwords): [${input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).filter(w => !['the', 'is', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'].includes(w)).map(w => `"${w}"`).join(', ')}]`
    ];
    
    output.innerHTML = steps.join('<br>');
}

function demonstrateBOW() {
    const input = document.getElementById('bowInput').value;
    const output = document.getElementById('bowOutput');
    
    const documents = input.split('|').map(doc => doc.trim());
    const allWords = new Set();
    
    documents.forEach(doc => {
        const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
        words.forEach(word => allWords.add(word));
    });
    
    const vocabulary = Array.from(allWords).sort();
    
    let result = '<strong>Vocabulary:</strong> [' + vocabulary.join(', ') + ']<br><br>';
    
    documents.forEach((doc, i) => {
        const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
        const vector = vocabulary.map(word => words.includes(word) ? 1 : 0);
        result += `<strong>Document ${i + 1}:</strong> "${doc}"<br>`;
        result += `<strong>BoW Vector:</strong> [${vector.join(', ')}]<br><br>`;
    });
    
    output.innerHTML = result;
}

function demonstrateTFIDF() {
    const input = document.getElementById('tfidfInput').value;
    const output = document.getElementById('tfidfOutput');
    
    const documents = input.split('|').map(doc => doc.trim());
    const allWords = new Set();
    
    documents.forEach(doc => {
        const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
        words.forEach(word => allWords.add(word));
    });
    
    const vocabulary = Array.from(allWords).sort();
    
    let result = '<strong>TF-IDF Calculation:</strong><br><br>';
    
    documents.forEach((doc, i) => {
        const words = doc.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
        const wordCount = {};
        words.forEach(word => wordCount[word] = (wordCount[word] || 0) + 1);
        
        result += `<strong>Document ${i + 1}:</strong> "${doc}"<br>`;
        
        vocabulary.forEach(word => {
            const tf = (wordCount[word] || 0) / words.length;
            const docsWithWord = documents.filter(d => 
                d.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/).includes(word)
            ).length;
            const idf = Math.log(documents.length / docsWithWord);
            const tfidf = tf * idf;
            
            if (tfidf > 0) {
                result += `&nbsp;&nbsp;${word}: TF=${tf.toFixed(3)}, IDF=${idf.toFixed(3)}, TF-IDF=${tfidf.toFixed(3)}<br>`;
            }
        });
        result += '<br>';
    });
    
    output.innerHTML = result;
}

function calculateWordSimilarity() {
    const word1 = document.getElementById('word1').value.toLowerCase();
    const word2 = document.getElementById('word2').value.toLowerCase();
    const output = document.getElementById('similarityOutput');
    
    const similarities = {
        'king,queen': 0.85, 'queen,king': 0.85,
        'man,woman': 0.75, 'woman,man': 0.75,
        'dog,cat': 0.70, 'cat,dog': 0.70,
        'happy,joy': 0.80, 'joy,happy': 0.80,
        'computer,laptop': 0.85, 'laptop,computer': 0.85,
        'car,vehicle': 0.80, 'vehicle,car': 0.80
    };
    
    const key = `${word1},${word2}`;
    const similarity = similarities[key] || (Math.random() * 0.5 + 0.2);
    
    output.innerHTML = `
        <strong>Similarity Analysis:</strong><br>
        Word 1: "${word1}"<br>
        Word 2: "${word2}"<br>
        <strong>Cosine Similarity: ${similarity.toFixed(3)}</strong><br>
        <em>Note: This is a simplified demonstration. Real embeddings require trained models.</em>
    `;
}

function demonstrateCooccurrence() {
    const input = document.getElementById('gloveInput').value;
    const output = document.getElementById('gloveOutput');
    
    const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
    const windowSize = 2;
    const cooccurrence = {};
    
    words.forEach(word => {
        cooccurrence[word] = {};
        words.forEach(otherWord => {
            cooccurrence[word][otherWord] = 0;
        });
    });
    
    for (let i = 0; i < words.length; i++) {
        for (let j = Math.max(0, i - windowSize); j <= Math.min(words.length - 1, i + windowSize); j++) {
            if (i !== j) {
                cooccurrence[words[i]][words[j]]++;
            }
        }
    }
    
    let result = '<strong>Co-occurrence Matrix (window size: 2):</strong><br><br>';
    result += '<table border="1" style="border-collapse: collapse; margin: 10px 0;"><tr><th></th>';
    
    words.forEach(word => {
        result += `<th>${word}</th>`;
    });
    result += '</tr>';
    
    words.forEach(word1 => {
        result += `<tr><th>${word1}</th>`;
        words.forEach(word2 => {
            result += `<td style="padding: 5px; text-align: center;">${cooccurrence[word1][word2]}</td>`;
        });
        result += '</tr>';
    });
    result += '</table>';
    
    output.innerHTML = result;
}

function analyzeSentiment() {
    const input = document.getElementById('sentimentInput').value;
    const output = document.getElementById('sentimentOutput');
    
    const positiveWords = ['love', 'amazing', 'great', 'excellent', 'wonderful', 'fantastic', 'good', 'perfect', 'awesome', 'brilliant'];
    const negativeWords = ['hate', 'terrible', 'awful', 'bad', 'horrible', 'disgusting', 'worst', 'disappointing', 'poor', 'useless'];
    
    const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
    
    let positiveScore = 0;
    let negativeScore = 0;
    
    words.forEach(word => {
        if (positiveWords.includes(word)) positiveScore++;
        if (negativeWords.includes(word)) negativeScore++;
    });
    
    let sentiment, confidence;
    if (positiveScore > negativeScore) {
        sentiment = 'Positive';
        confidence = ((positiveScore / (positiveScore + negativeScore + 1)) * 100).toFixed(1);
    } else if (negativeScore > positiveScore) {
        sentiment = 'Negative';
        confidence = ((negativeScore / (positiveScore + negativeScore + 1)) * 100).toFixed(1);
    } else {
        sentiment = 'Neutral';
        confidence = '50.0';
    }
    
    output.innerHTML = `
        <strong>Sentiment:</strong> ${sentiment}<br>
        <strong>Confidence:</strong> ${confidence}%<br>
        <strong>Analysis:</strong><br>
        • Positive words found: ${positiveScore}<br>
        • Negative words found: ${negativeScore}<br>
        <em>Note: This is a simplified demonstration using rule-based analysis.</em>
    `;
}

function demonstrateTranslation() {
    const input = document.getElementById('translateInput').value;
    const targetLang = document.getElementById('targetLang').value;
    const output = document.getElementById('translateOutput');
    
    const translations = {
        'spanish': {
            'hello': 'hola', 'how': 'cómo', 'are': 'estás', 'you': 'tú', 'today': 'hoy'
        },
        'french': {
            'hello': 'bonjour', 'how': 'comment', 'are': 'êtes', 'you': 'vous', 'today': 'aujourd\'hui'
        },
        'german': {
            'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie', 'today': 'heute'
        }
    };
    
    const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
    const translatedWords = words.map(word => 
        translations[targetLang][word] || word
    );
    
    output.innerHTML = `
        <strong>Original (English):</strong> ${input}<br>
        <strong>Translated (${targetLang.charAt(0).toUpperCase() + targetLang.slice(1)}):</strong> ${translatedWords.join(' ')}<br>
        <em>Note: This is a simplified word-by-word translation for demonstration.</em>
    `;
}

function exploreTransformer() {
    const component = document.getElementById('transformerComponent').value;
    const output = document.getElementById('transformerOutput');
    
    const explanations = {
        'overview': `
            <strong>Transformer Architecture Overview:</strong><br><br>
            The Transformer consists of:<br>
            • <strong>Encoder Stack:</strong> 6 identical layers processing input<br>
            • <strong>Decoder Stack:</strong> 6 identical layers generating output<br>
            • <strong>Input/Output Embeddings:</strong> Convert tokens to vectors<br>
            • <strong>Positional Encoding:</strong> Add position information<br><br>
            <em>Key Innovation: No recurrence, only attention!</em>
        `,
        'encoder': `
            <strong>Encoder Stack Details:</strong><br><br>
            Each encoder layer contains:<br>
            1. <strong>Multi-Head Self-Attention:</strong> Looks at other positions in input<br>
            2. <strong>Add & Norm:</strong> Residual connection + layer normalization<br>
            3. <strong>Feed-Forward Network:</strong> Two linear transformations with ReLU<br>
            4. <strong>Add & Norm:</strong> Another residual connection + normalization<br><br>
            <em>Output: Rich contextual representations of input sequence</em>
        `,
        'decoder': `
            <strong>Decoder Stack Details:</strong><br><br>
            Each decoder layer contains:<br>
            1. <strong>Masked Multi-Head Attention:</strong> Prevents looking at future tokens<br>
            2. <strong>Add & Norm:</strong> Residual connection + normalization<br>
            3. <strong>Encoder-Decoder Attention:</strong> Attends to encoder output<br>
            4. <strong>Add & Norm:</strong> Another residual connection<br>
            5. <strong>Feed-Forward Network:</strong> Final processing<br>
            6. <strong>Add & Norm:</strong> Final residual connection<br><br>
            <em>Output: Generated sequence (e.g., translation)</em>
        `,
        'attention': `
            <strong>Multi-Head Attention:</strong><br><br>
            Instead of single attention:<br>
            • <strong>8 parallel attention heads</strong> (typically)<br>
            • Each head learns different relationships<br>
            • Results are concatenated and linearly transformed<br><br>
            Benefits:<br>
            • Head 1: Syntactic relationships<br>
            • Head 2: Semantic relationships<br>
            • Head 3: Long-distance dependencies<br>
            • ... and so on<br><br>
            <em>Like having multiple experts examining the same data!</em>
        `
    };
    
    output.innerHTML = explanations[component];
}

function visualizeAttention() {
    const input = document.getElementById('attentionInput').value;
    const output = document.getElementById('attentionOutput');
    
    const words = input.split(/\s+/);
    if (words.length === 0) return;
    
    let visualization = '<div class="attention-visualization">';
    
    words.forEach((word, i) => {
        const weights = words.map((_, j) => {
            const distance = Math.abs(i - j);
            return Math.max(0.1, 1 - distance * 0.3 + Math.random() * 0.2);
        });
        
        const sum = weights.reduce((a, b) => a + b, 0);
        const normalizedWeights = weights.map(w => w / sum);
        
        const maxWeight = Math.max(...normalizedWeights);
        const intensity = Math.floor((normalizedWeights[i] / maxWeight) * 255);
        const color = `rgb(${255-intensity}, ${255-intensity}, 255)`;
        
        visualization += `<div class="attention-word" style="background-color: ${color}; border: 2px solid #667eea;">${word}</div>`;
    });
    
    visualization += '</div>';
    visualization += '<p><em>Color intensity represents attention weight (darker = higher attention)</em></p>';
    
    output.innerHTML = visualization;
}

function demonstrateAttentionSteps() {
    const output = document.getElementById('attentionStepsOutput');
    
    output.innerHTML = `
        <strong>Step-by-Step Attention Calculation:</strong><br><br>
        
        <strong>Step 1: Create Q, K, V matrices</strong><br>
        • Query (Q) = Input × W_Q<br>
        • Key (K) = Input × W_K<br>
        • Value (V) = Input × W_V<br><br>
        
        <strong>Step 2: Calculate attention scores</strong><br>
        • Scores = Q × K^T<br>
        • Example: [0.8, 0.2, 0.1, 0.9]<br><br>
        
        <strong>Step 3: Scale by √d_k</strong><br>
        • Scaled = Scores / √64 = Scores / 8<br>
        • Prevents extremely small gradients<br><br>
        
        <strong>Step 4: Apply softmax</strong><br>
        • Weights = softmax([0.1, 0.025, 0.0125, 0.1125])<br>
        • Weights = [0.28, 0.23, 0.22, 0.27]<br><br>
        
        <strong>Step 5: Weighted sum of values</strong><br>
        • Output = Weights × V<br>
        • Final contextualized representation!<br><br>
        
        <em>This happens for every position in parallel!</em>
    `;
}

function demonstrateMultiHead() {
    const numHeads = document.getElementById('numHeads').value;
    const output = document.getElementById('multiHeadOutput');
    
    let result = `<strong>Multi-Head Attention with ${numHeads} heads:</strong><br><br>`;
    
    for (let i = 1; i <= numHeads; i++) {
        const focus = ['syntactic relations', 'semantic meaning', 'long-distance deps', 'local context', 'coreference', 'temporal relations', 'causal relations', 'thematic roles'][i-1] || 'specialized patterns';
        result += `<strong>Head ${i}:</strong> Focuses on ${focus}<br>`;
    }
    
    result += `<br><strong>Process:</strong><br>`;
    result += `1. Each head computes its own Q, K, V matrices<br>`;
    result += `2. Each head produces attention output independently<br>`;
    result += `3. All head outputs are concatenated<br>`;
    result += `4. Final linear transformation combines information<br><br>`;
    result += `<em>Result: Rich, multi-faceted understanding of relationships!</em>`;
    
    output.innerHTML = result;
}

function summarizeText() {
    const input = document.getElementById('summaryInput').value;
    const output = document.getElementById('summaryOutput');
    
    const sentences = input.split(/[.!?]+/).filter(s => s.trim().length > 0);
    
    if (sentences.length <= 2) {
        output.innerHTML = '<em>Text is already concise!</em>';
        return;
    }
    
    const words = input.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
    const wordFreq = {};
    words.forEach(word => {
        if (word.length > 3) {
            wordFreq[word] = (wordFreq[word] || 0) + 1;
        }
    });
    
    const sentenceScores = sentences.map(sentence => {
        const sentWords = sentence.toLowerCase().replace(/[^\w\s]/g, '').split(/\s+/);
        const score = sentWords.reduce((sum, word) => sum + (wordFreq[word] || 0), 0);
        return { sentence: sentence.trim() + '.', score: score / sentWords.length };
    });
    
    const topSentences = sentenceScores
        .sort((a, b) => b.score - a.score)
        .slice(0, 2)
        .map(item => item.sentence);
    
    output.innerHTML = `
        <strong>Summary:</strong><br>
        ${topSentences.join(' ')}<br><br>
        <em>Note: This is a simplified extractive summarization for demonstration.</em>
    `;
}

function performNER() {
    const input = document.getElementById('nerInput').value;
    const output = document.getElementById('nerOutput');
    
    const patterns = [
        { regex: /\b[A-Z][a-z]+ Inc\.\b/g, type: 'ORGANIZATION', color: '#ff6b6b' },
        { regex: /\b[A-Z][a-z]+ [A-Z][a-z]+\b/g, type: 'PERSON', color: '#4ecdc4' },
        { regex: /\b[A-Z][a-z]+(?:, [A-Z][a-z]+)?\b/g, type: 'LOCATION', color: '#45b7d1' },
        { regex: /\b\d{4}\b/g, type: 'DATE', color: '#96ceb4' }
    ];
    
    let processedText = input;
    const entities = [];
    
    patterns.forEach(pattern => {
        const matches = input.match(pattern.regex);
        if (matches) {
            matches.forEach(match => {
                entities.push({ text: match, type: pattern.type, color: pattern.color });
                processedText = processedText.replace(match, 
                    `<span style="background-color: ${pattern.color}; padding: 2px 4px; border-radius: 3px; color: white; font-weight: bold;">${match} (${pattern.type})</span>`
                );
            });
        }
    });
    
    let result = `<strong>Named Entities Found:</strong><br><br>`;
    result += processedText + '<br><br>';
    
    if (entities.length > 0) {
        result += '<strong>Entity List:</strong><br>';
        entities.forEach(entity => {
            result += `• <span style="color: ${entity.color}; font-weight: bold;">${entity.text}</span> - ${entity.type}<br>`;
        });
    } else {
        result += '<em>No named entities detected with current patterns.</em>';
    }
    
    output.innerHTML = result;
}

function answerQuestion() {
    const context = document.getElementById('qaContext').value;
    const question = document.getElementById('qaQuestion').value;
    const output = document.getElementById('qaOutput');
    
    const questionLower = question.toLowerCase();
    
    let answer = "I couldn't find a specific answer in the context.";
    
    if (questionLower.includes('when') && questionLower.includes('introduced')) {
        const yearMatch = context.match(/\b(19|20)\d{2}\b/);
        if (yearMatch) {
            answer = `The Transformer model was introduced in ${yearMatch[0]}.`;
        }
    } else if (questionLower.includes('what') && questionLower.includes('transformer')) {
        const sentences = context.split(/[.!?]+/);
        const relevantSentence = sentences.find(s => s.toLowerCase().includes('transformer'));
        if (relevantSentence) {
            answer = relevantSentence.trim() + '.';
        }
    }
    
    output.innerHTML = `
        <strong>Question:</strong> ${question}<br>
        <strong>Answer:</strong> ${answer}<br><br>
        <em>Note: This is a simplified keyword-based QA system for demonstration.</em>
    `;
}

function checkAnswer(element, isCorrect) {
    if (!element.dataset.originalText) {
        element.dataset.originalText = element.textContent.trim();
    }
    
    element.parentNode.querySelectorAll('.quiz-option').forEach(option => {
        option.classList.remove('correct', 'incorrect');
        if (option.dataset.originalText) {
            option.textContent = option.dataset.originalText;
        }
    });
    
    if (isCorrect) {
        element.classList.add('correct');
        element.textContent = element.dataset.originalText + ' ✅ Correct!';
    } else {
        element.classList.add('incorrect');
        element.textContent = element.dataset.originalText + ' ❌ Incorrect';
        
        const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
            .find(option => {
                const onclickStr = option.getAttribute('onclick') || '';
                return onclickStr.includes('true');
            });
        if (correctOption) {
            correctOption.classList.add('correct');
            correctOption.textContent = correctOption.dataset.originalText + ' ✅ Correct Answer';
        }
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.quiz-option').forEach(option => {
        option.dataset.originalText = option.textContent.trim();
    });
    
    const demoInputs = document.querySelectorAll('.demo-input');
    demoInputs.forEach(input => {
        input.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                const button = this.parentNode.querySelector('.demo-btn');
                if (button) button.click();
            }
        });
    });
});
</script>

{% block footer_navigation %}
    <button class="footer-nav-btn" onclick="scrollToCourseNav()" aria-label="Go to course navigation">
        <i class="fas fa-list"></i> Course Navigation
    </button>
    <a href="{{ url_for('tutorials') }}" class="footer-nav-btn">
        <i class="fas fa-book"></i> All Tutorials
    </a>
    <a href="{{ url_for('projects') }}" class="footer-nav-btn">
        <i class="fas fa-code"></i> View Projects
    </a>
    <a href="{{ url_for('contact') }}" class="footer-nav-btn">
        <i class="fas fa-envelope"></i> Get in Touch
    </a>
{% endblock %}

<script>
function scrollToCourseNav() {
    const courseNav = document.getElementById('course-navigation');
    if (courseNav) {
        courseNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }
}
</script>
{% endblock %}