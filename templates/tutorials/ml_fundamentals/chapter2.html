<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Regression Analysis - Ali Zanganeh</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/ml_fundamentals_ch2.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
        <a id="top"></a>
    <header class="azbn-header">
        <div class="azbn-container">
            <h1><a href="../../" style="text-decoration: none; color: #4f46e5;">Ali Zanganeh</a></h1>
            <nav>
                <a href="../../#home">Home</a>
                <a href="../">Tutorials</a>
                <a href="./chapter1.html">‚Üê Chapter 1</a>
                <a href="./chapter3.html">Chapter 3 ‚Üí</a>
            </nav>
        </div>
    </header>

    <main style="padding-top: 100px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <h1>Chapter 2: Regression Analysis Mastery</h1>
                <p class="azbn-subtitle">From linear relationships to complex polynomial modeling with mathematical foundations</p>
                
                <div class="learning-objectives-card">
                    <h2> Learning Objectives</h2>
                    <ul>
                        <li>Master linear regression theory and mathematical foundations</li>
                        <li>Understand polynomial regression and feature engineering</li>
                        <li>Learn multiple regression with feature importance analysis</li>
                        <li>Evaluate models using proper metrics (MSE, MAE, R¬≤)</li>
                        <li>Recognize and prevent overfitting in regression models</li>
                        <li>Apply regularization techniques (Ridge, Lasso)</li>
                    </ul>
                </div>

                <h2> What is Regression?</h2>
                <div class="azbn-card">
                    <h3>Core Concept and Mathematical Foundation</h3>
                    <p><strong>Regression Analysis</strong> is a supervised learning technique used to predict continuous numerical values by modeling the relationship between input features and target variables.</p>
                    
                    <div class="formula-box">
                        <h4 class="formula-title">üéØ The Fundamental Equation:</h4>
                        <div class="formula-display">
                            <strong>y = f(X) + Œµ</strong>
                        </div>
                        <div class="formula-explanation">
                            <ul>
                                <li><strong>y:</strong> Target variable (what we want to predict)</li>
                                <li><strong>f(X):</strong> The function we want to learn</li>
                                <li><strong>X:</strong> Input features (independent variables)</li>
                                <li><strong>Œµ:</strong> Error term (noise and unmeasured factors)</li>
                            </ul>
                        </div>
                    </div>

                    <h4> Real-World Regression Examples:</h4>
                    <div class="azbn-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1rem 0;">
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5> Real Estate Pricing</h5>
                            <p><strong>Predict:</strong> House price</p>
                            <p><strong>Features:</strong> Size, location, bedrooms, age</p>
                            <p><strong>Why Linear:</strong> Generally, larger houses cost more</p>
                        </div>
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5> Stock Market Analysis</h5>
                            <p><strong>Predict:</strong> Stock price movement</p>
                            <p><strong>Features:</strong> Trading volume, market indicators</p>
                            <p><strong>Challenge:</strong> Non-linear, highly volatile</p>
                        </div>
                        <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px;">
                            <h5> Weather Forecasting</h5>
                            <p><strong>Predict:</strong> Tomorrow's temperature</p>
                            <p><strong>Features:</strong> Today's weather, pressure, humidity</p>
                            <p><strong>Complexity:</strong> Seasonal patterns, non-linear trends</p>
                        </div>
                    </div>
                </div>

                <h2> Linear Regression: The Foundation</h2>
                <div class="azbn-card">
                    <h3>Mathematical Deep Dive</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4> Simple Linear Regression Formula:</h4>
                        <div style="text-align: center; font-size: 1.3rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ</strong>
                        </div>
                        <ul>
                            <li><strong>Œ≤‚ÇÄ (Beta Zero):</strong> Y-intercept - value when x = 0</li>
                            <li><strong>Œ≤‚ÇÅ (Beta One):</strong> Slope - change in y per unit change in x</li>
                            <li><strong>x:</strong> Independent variable (feature)</li>
                            <li><strong>y:</strong> Dependent variable (target)</li>
                            <li><strong>Œµ:</strong> Random error term</li>
                        </ul>
                    </div>

                    <h4> Key Assumptions of Linear Regression:</h4>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3;">
                            <h5>1Ô∏è‚É£ Linearity</h5>
                            <p>The relationship between X and y is linear</p>
                            <p><em>Check: Scatter plots, residual plots</em></p>
                        </div>
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px; border-left: 4px solid #9c27b0;">
                            <h5>2Ô∏è‚É£ Independence</h5>
                            <p>Observations are independent of each other</p>
                            <p><em>Important for time series and spatial data</em></p>
                        </div>
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px; border-left: 4px solid #4caf50;">
                            <h5>3Ô∏è‚É£ Homoscedasticity</h5>
                            <p>Constant variance of residuals</p>
                            <p><em>Check: Residuals vs fitted values plot</em></p>
                        </div>
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px; border-left: 4px solid #ff9800;">
                            <h5>4Ô∏è‚É£ Normality</h5>
                            <p>Residuals are normally distributed</p>
                            <p><em>Check: Q-Q plots, Shapiro-Wilk test</em></p>
                        </div>
                    </div>

                    <h4> How Linear Regression Works - The Math Behind the Magic:</h4>
                    <div style="background: #fff3e0; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5> Ordinary Least Squares (OLS) Method:</h5>
                        <p>Linear regression finds the best line by minimizing the sum of squared residuals:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Objective Function:</strong></p>
                            <div style="text-align: center; font-size: 1.1rem; margin: 0.5rem 0;">
                                <strong>Minimize: Œ£(y·µ¢ - ≈∑·µ¢)¬≤</strong>
                            </div>
                            <p style="font-size: 0.9rem; text-align: center; margin-top: 0.5rem;">
                                Where ≈∑·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅx·µ¢ (predicted value)
                            </p>
                        </div>

                        <h5> The Solution (for simple linear regression):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Slope (Œ≤‚ÇÅ):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                Œ≤‚ÇÅ = Œ£((x·µ¢ - xÃÑ)(y·µ¢ - »≥)) / Œ£((x·µ¢ - xÃÑ)¬≤)
                            </div>
                            <p><strong>Intercept (Œ≤‚ÇÄ):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
                            </div>
                        </div>

                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong> Intuition:</strong> The slope tells us the correlation scaled by the ratio of standard deviations. The intercept ensures the line passes through the point (xÃÑ, »≥).
                        </div>
                    </div>
                </div>

                <h2> Multiple Linear Regression</h2>
                <div class="azbn-card">
                    <h3>Extending to Multiple Features</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4> Multiple Regression Formula:</h4>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çöx‚Çö + Œµ</strong>
                        </div>
                        <p>Or in matrix form: <strong>y = XŒ≤ + Œµ</strong></p>
                        <ul>
                            <li><strong>p:</strong> Number of features</li>
                            <li><strong>Œ≤‚±º:</strong> Coefficient for feature x‚±º</li>
                            <li><strong>X:</strong> Design matrix (n √ó p matrix)</li>
                            <li><strong>Œ≤:</strong> Parameter vector</li>
                        </ul>
                    </div>

                    <h4> Feature Importance and Interpretation:</h4>
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5> Coefficient Interpretation:</h5>
                        <ul>
                            <li><strong>Magnitude:</strong> Larger |Œ≤‚±º| means more influence on prediction</li>
                            <li><strong>Sign:</strong> Positive Œ≤ increases y, negative Œ≤ decreases y</li>
                            <li><strong>Units:</strong> Œ≤‚±º represents change in y per unit change in x‚±º</li>
                        </ul>
                        
                        <div style="background: #fff3e0; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong>‚ö†Ô∏è Important Caveat:</strong> Coefficients represent the effect of changing one feature while holding all others constant. In practice, features are often correlated!
                        </div>
                    </div>

                    <h4> Multicollinearity: When Features are Too Similar</h4>
                    <div style="background: #ffebee; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5>‚ùå Problems with Highly Correlated Features:</h5>
                        <ul>
                            <li>Unstable coefficient estimates</li>
                            <li>Difficult to interpret individual feature importance</li>
                            <li>High variance in predictions</li>
                            <li>Numerical instability in matrix inversion</li>
                        </ul>

                        <h5> Detection Methods:</h5>
                        <ul>
                            <li><strong>Correlation Matrix:</strong> Look for correlations > 0.8</li>
                            <li><strong>Variance Inflation Factor (VIF):</strong> VIF > 10 indicates problems</li>
                            <li><strong>Condition Number:</strong> > 30 suggests multicollinearity</li>
                        </ul>

                        <h5> Solutions:</h5>
                        <ul>
                            <li>Remove highly correlated features</li>
                            <li>Use Principal Component Analysis (PCA)</li>
                            <li>Apply regularization (Ridge, Lasso)</li>
                            <li>Collect more data if possible</li>
                        </ul>
                    </div>
                </div>

                <h2>Ô∏è Polynomial Regression: Capturing Non-Linear Relationships</h2>
                <div class="azbn-card">
                    <h3>Beyond Straight Lines</h3>
                    
                    <div style="background: #f3e5f5; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4>üîÑ Polynomial Transformation:</h4>
                        <p>Polynomial regression extends linear regression by adding polynomial features:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Degree 2 (Quadratic):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œµ
                            </div>
                            <p><strong>Degree 3 (Cubic):</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + Œµ
                            </div>
                            <p><strong>General Form:</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + ... + Œ≤‚Çêx·µà + Œµ
                            </div>
                        </div>

                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong> Key Insight:</strong> Polynomial regression is still linear in the parameters Œ≤! We just transform the features.
                        </div>
                    </div>

                    <h4>Ô∏è The Bias-Variance Tradeoff</h4>
                    <div style="background: #fff8e1; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem;">
                            <div style="background: #ffebee; padding: 1rem; border-radius: 6px;">
                                <h5> Underfitting (High Bias)</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li>Model too simple</li>
                                    <li>Cannot capture underlying pattern</li>
                                    <li>Poor performance on both training and test data</li>
                                    <li><strong>Solution:</strong> Increase model complexity</li>
                                </ul>
                            </div>
                            <div style="background: #e8f5e8; padding: 1rem; border-radius: 6px;">
                                <h5> Overfitting (High Variance)</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li>Model too complex</li>
                                    <li>Memorizes training data noise</li>
                                    <li>Good training, poor test performance</li>
                                    <li><strong>Solution:</strong> Reduce complexity or add data</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong> Sweet Spot:</strong> Find the optimal degree that minimizes total error = bias¬≤ + variance + noise
                        </div>
                    </div>

                    <h4> Choosing the Right Polynomial Degree</h4>
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h5> Practical Guidelines:</h5>
                        <ul>
                            <li><strong>Degree 1:</strong> Linear relationship</li>
                            <li><strong>Degree 2:</strong> One curve (parabola) - good for many real-world phenomena</li>
                            <li><strong>Degree 3-4:</strong> More complex curves with multiple turns</li>
                            <li><strong>Degree >5:</strong> Usually overfitting unless you have lots of data</li>
                        </ul>

                        <h5> Selection Methods:</h5>
                        <ol>
                            <li><strong>Cross-Validation:</strong> Test different degrees, pick best CV score</li>
                            <li><strong>Learning Curves:</strong> Plot training vs validation error</li>
                            <li><strong>Information Criteria:</strong> AIC, BIC balance fit and complexity</li>
                            <li><strong>Domain Knowledge:</strong> Physics/business understanding of relationship</li>
                        </ol>

                        <div style="background: #fff3e0; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong> Pro Tip:</strong> Start simple (degree 1-2) and increase complexity only if validation performance improves!
                        </div>
                    </div>
                </div>

                <h2> Regression Evaluation Metrics</h2>
                <div class="azbn-card">
                    <h3>Measuring Model Performance</h3>
                    
                    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4> Essential Regression Metrics:</h4>
                        
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1rem 0;">
                            <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                                <h5>1Ô∏è‚É£ Mean Squared Error (MSE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤</strong>
                                </div>
                                <p><strong>Pros:</strong> Heavily penalizes large errors</p>
                                <p><strong>Cons:</strong> Same units as y¬≤, hard to interpret</p>
                                <p><strong>Use when:</strong> Large errors are especially bad</p>
                            </div>

                            <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                                <h5>2Ô∏è‚É£ Root Mean Squared Error (RMSE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>RMSE = ‚àöMSE</strong>
                                </div>
                                <p><strong>Pros:</strong> Same units as y, interpretable</p>
                                <p><strong>Cons:</strong> Still penalizes large errors heavily</p>
                                <p><strong>Use when:</strong> You want MSE benefits with interpretability</p>
                            </div>

                            <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                                <h5>3Ô∏è‚É£ Mean Absolute Error (MAE)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>MAE = (1/n) Œ£|y·µ¢ - ≈∑·µ¢|</strong>
                                </div>
                                <p><strong>Pros:</strong> Robust to outliers, easy to interpret</p>
                                <p><strong>Cons:</strong> Doesn't distinguish small vs large errors</p>
                                <p><strong>Use when:</strong> You have outliers or all errors are equally bad</p>
                            </div>

                            <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                                <h5>4Ô∏è‚É£ R-squared (R¬≤)</h5>
                                <div style="text-align: center; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                    <strong>R¬≤ = 1 - (SS_res / SS_tot)</strong>
                                </div>
                                <p><strong>Range:</strong> 0 to 1 (higher is better)</p>
                                <p><strong>Interpretation:</strong> % of variance explained</p>
                                <p><strong>Caveat:</strong> Can be misleading with non-linear relationships</p>
                            </div>
                        </div>

                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                            <h4> Which Metric to Use?</h4>
                            <ul style="margin: 0.5rem 0;">
                                <li><strong>RMSE:</strong> Most common, good for normally distributed errors</li>
                                <li><strong>MAE:</strong> When you have outliers or skewed error distribution</li>
                                <li><strong>R¬≤:</strong> For understanding model explanatory power</li>
                                <li><strong>Multiple metrics:</strong> Always use several metrics for complete picture!</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <h2>Ô∏è Regularization: Preventing Overfitting</h2>
                <div class="azbn-card">
                    <h3>Ridge and Lasso Regression</h3>
                    
                    <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4> Why Regularization?</h4>
                        <p>When we have many features or polynomial terms, the model can become too complex and overfit. Regularization adds a penalty term to prevent this.</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>General Regularized Objective:</strong></p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>Minimize: MSE + Œª √ó Penalty(Œ≤)</strong>
                            </div>
                            <p style="font-size: 0.9rem; text-align: center;">Where Œª (lambda) controls the strength of regularization</p>
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4> Ridge Regression (L2)</h4>
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Penalty = Œ£Œ≤‚±º¬≤</strong>
                            </div>
                            <h5>Characteristics:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Shrinks coefficients toward zero</li>
                                <li>Keeps all features (no feature selection)</li>
                                <li>Good when all features are somewhat relevant</li>
                                <li>Handles multicollinearity well</li>
                            </ul>
                            <div style="background: #f1f8e9; padding: 0.5rem; border-radius: 4px; margin-top: 0.5rem;">
                                <strong>Best for:</strong> Many relevant features
                            </div>
                        </div>

                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4> Lasso Regression (L1)</h4>
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Penalty = Œ£|Œ≤‚±º|</strong>
                            </div>
                            <h5>Characteristics:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Can set coefficients exactly to zero</li>
                                <li>Automatic feature selection</li>
                                <li>Produces sparse models</li>
                                <li>Good when only some features are relevant</li>
                            </ul>
                            <div style="background: #fef7e0; padding: 0.5rem; border-radius: 4px; margin-top: 0.5rem;">
                                <strong>Best for:</strong> Feature selection needed
                            </div>
                        </div>
                    </div>

                    <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                        <h4>Ô∏è Choosing Œª (Regularization Strength):</h4>
                        <ul>
                            <li><strong>Œª = 0:</strong> No regularization (standard regression)</li>
                            <li><strong>Small Œª:</strong> Light penalty, close to unregularized</li>
                            <li><strong>Large Œª:</strong> Heavy penalty, coefficients shrink toward zero</li>
                            <li><strong>Œª ‚Üí ‚àû:</strong> All coefficients approach zero (underfitting)</li>
                        </ul>
                        <div style="background: #e1bee7; padding: 0.8rem; border-radius: 4px; margin-top: 0.5rem;">
                            <strong> Selection Method:</strong> Use cross-validation to find optimal Œª that minimizes validation error!
                        </div>
                    </div>
                </div>

                <h2> Key Takeaways and Best Practices</h2>
                <div class="azbn-deployment-status">
                    <p><strong>‚úÖ Chapter 2 Mastery:</strong></p>
                    <p>‚Ä¢ Linear regression mathematical foundations and assumptions</p>
                    <p>‚Ä¢ Multiple regression with feature importance interpretation</p>
                    <p>‚Ä¢ Polynomial regression for non-linear relationships</p>
                    <p>‚Ä¢ Comprehensive evaluation metrics (MSE, RMSE, MAE, R¬≤)</p>
                    <p>‚Ä¢ Overfitting detection and regularization techniques</p>
                    <p>‚Ä¢ Practical model selection and validation strategies</p>
                </div>

                <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 2rem 0;">
                    <h3>üéì Practical Guidelines for Regression Success:</h3>
                    <ol>
                        <li><strong>Always start simple:</strong> Begin with linear regression before trying polynomial</li>
                        <li><strong>Check assumptions:</strong> Plot residuals to verify linearity and homoscedasticity</li>
                        <li><strong>Handle multicollinearity:</strong> Use correlation matrices and VIF</li>
                        <li><strong>Use multiple metrics:</strong> Don't rely on R¬≤ alone</li>
                        <li><strong>Validate properly:</strong> Use cross-validation for model selection</li>
                        <li><strong>Consider regularization:</strong> Especially with many features or limited data</li>
                        <li><strong>Understand your domain:</strong> Let business knowledge guide feature engineering</li>
                    </ol>
                </div>
            </div>
        </section>
    </main>
</body>
</html>

                <h2> Hands-On Python Implementation</h2>
                <div class="azbn-card">
                    <h3>Linear Regression from Scratch</h3>
                    
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        <h4> Complete Boston Housing Example</h4>
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Import essential libraries</div>
<div><span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">numpy</span> <span style="color: #f92672;">as</span> <span style="color: #f8f8f2;">np</span></div>
<div><span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">pandas</span> <span style="color: #f92672;">as</span> <span style="color: #f8f8f2;">pd</span></div>
<div><span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">matplotlib.pyplot</span> <span style="color: #f92672;">as</span> <span style="color: #f8f8f2;">plt</span></div>
<div><span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">seaborn</span> <span style="color: #f92672;">as</span> <span style="color: #f8f8f2;">sns</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.datasets</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">load_boston</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.model_selection</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">train_test_split</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.linear_model</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">LinearRegression, Ridge, Lasso</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.preprocessing</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">StandardScaler, PolynomialFeatures</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.metrics</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">mean_squared_error, mean_absolute_error, r2_score</span></div>
<div><span style="color: #f92672;">from</span> <span style="color: #f8f8f2;">sklearn.model_selection</span> <span style="color: #f92672;">import</span> <span style="color: #f8f8f2;">GridSearchCV, cross_val_score</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Load the dataset</div>
<div><span style="color: #f8f8f2;">boston</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">load_boston()</span></div>
<div><span style="color: #f8f8f2;">df</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">pd.DataFrame(boston.data, columns=boston.feature_names)</span></div>
<div><span style="color: #f8f8f2;">df</span><span style="color: #f92672;">[</span><span style="color: #e6db74;">'PRICE'</span><span style="color: #f92672;">]</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">boston.target</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Boston Housing Dataset Shape:"</span><span style="color: #f92672;">,</span> <span style="color: #f8f8f2;">df.shape</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nFeatures:"</span><span style="color: #f92672;">,</span> <span style="color: #66d9ef;">list</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">boston.feature_names</span><span style="color: #f92672;">))</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nFirst 5 rows:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">df.head()</span><span style="color: #f92672;">)</span></div>
                        </div>

                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Exploratory Data Analysis</div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Dataset Info:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">df.info()</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nMissing values:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">df.isnull().sum()</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Statistical summary</div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nStatistical Summary:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">df.describe()</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Correlation analysis</div>
<div><span style="color: #f8f8f2;">plt.figure(figsize=(12, 10))</span></div>
<div><span style="color: #f8f8f2;">correlation_matrix</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">df.corr()</span></div>
<div><span style="color: #f8f8f2;">sns.heatmap(correlation_matrix, annot=</span><span style="color: #ae81ff;">True</span><span style="color: #f8f8f2;">, cmap=</span><span style="color: #e6db74;">'coolwarm'</span><span style="color: #f8f8f2;">, center=0, fmt=</span><span style="color: #e6db74;">'.2f'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.title(</span><span style="color: #e6db74;">'Boston Housing: Feature Correlation Matrix'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.tight_layout()</span></div>
<div><span style="color: #f8f8f2;">plt.show()</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Features most correlated with price</div>
<div><span style="color: #f8f8f2;">price_corr</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">correlation_matrix[</span><span style="color: #e6db74;">'PRICE'</span><span style="color: #f8f8f2;">].abs().sort_values(ascending=</span><span style="color: #ae81ff;">False</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nFeatures most correlated with PRICE:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">price_corr[1:6]</span><span style="color: #f92672;">)</span></div>
                        </div>

                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Data Preparation</div>
<div><span style="color: #f8f8f2;">X</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">boston.data</span></div>
<div><span style="color: #f8f8f2;">y</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">boston.target</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Train-test split</div>
<div><span style="color: #f8f8f2;">X_train, X_test, y_train, y_test</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">train_test_split(</span></div>
<div><span style="color: #f8f8f2;">    X, y, test_size=0.2, random_state=42</span></div>
<div><span style="color: #f8f8f2;">)</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Training set: </span><span style="color: #e6db74;">{X_train.shape[0]}</span><span style="color: #e6db74;"> samples"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Test set: </span><span style="color: #e6db74;">{X_test.shape[0]}</span><span style="color: #e6db74;"> samples"</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Feature scaling</div>
<div><span style="color: #f8f8f2;">scaler</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">StandardScaler()</span></div>
<div><span style="color: #f8f8f2;">X_train_scaled</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">scaler.fit_transform(X_train)</span></div>
<div><span style="color: #f8f8f2;">X_test_scaled</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">scaler.transform(X_test)</span></div>
                        </div>
                    </div>

                    <h4> Linear Regression Implementation</h4>
                    <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># 1. Simple Linear Regression (using one feature)</div>
<div><span style="color: #f8f8f2;">simple_model</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">LinearRegression()</span></div>
<div><span style="color: #f8f8f2;">X_simple</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X_train[:, 5].reshape(-1, 1)</span>  <span style="color: #75715e;"># RM (average rooms)</span></div>
<div><span style="color: #f8f8f2;">X_simple_test</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X_test[:, 5].reshape(-1, 1)</span></div>

<div><span style="color: #f8f8f2;">simple_model.fit(X_simple, y_train)</span></div>
<div><span style="color: #f8f8f2;">y_pred_simple</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">simple_model.predict(X_simple_test)</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Simple Linear Regression (RM vs PRICE):"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Coefficient (slope): </span><span style="color: #e6db74;">{simple_model.coef_[0]:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Intercept: </span><span style="color: #e6db74;">{simple_model.intercept_:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"R¬≤ Score: </span><span style="color: #e6db74;">{r2_score(y_test, y_pred_simple):.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Visualization</div>
<div><span style="color: #f8f8f2;">plt.figure(figsize=(10, 6))</span></div>
<div><span style="color: #f8f8f2;">plt.scatter(X_simple_test, y_test, alpha=0.7, label=</span><span style="color: #e6db74;">'Actual'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.plot(X_simple_test, y_pred_simple, color=</span><span style="color: #e6db74;">'red'</span><span style="color: #f8f8f2;">, linewidth=2, label=</span><span style="color: #e6db74;">'Predicted'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.xlabel(</span><span style="color: #e6db74;">'Average Rooms (RM)'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.ylabel(</span><span style="color: #e6db74;">'House Price ($1000s)'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.title(</span><span style="color: #e6db74;">'Simple Linear Regression: Rooms vs Price'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.legend()</span></div>
<div><span style="color: #f8f8f2;">plt.show()</span></div>
                        </div>
                    </div>

                    <h4> Multiple Linear Regression</h4>
                    <div style="background: #fff8e1; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Multiple Linear Regression</div>
<div><span style="color: #f8f8f2;">mlr_model</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">LinearRegression()</span></div>
<div><span style="color: #f8f8f2;">mlr_model.fit(X_train_scaled, y_train)</span></div>
<div><span style="color: #f8f8f2;">y_pred_mlr</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">mlr_model.predict(X_test_scaled)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Calculate metrics</div>
<div><span style="color: #f8f8f2;">mse</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">mean_squared_error(y_test, y_pred_mlr)</span></div>
<div><span style="color: #f8f8f2;">rmse</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.sqrt(mse)</span></div>
<div><span style="color: #f8f8f2;">mae</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">mean_absolute_error(y_test, y_pred_mlr)</span></div>
<div><span style="color: #f8f8f2;">r2</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">r2_score(y_test, y_pred_mlr)</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Multiple Linear Regression Results:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"MSE: </span><span style="color: #e6db74;">{mse:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"RMSE: </span><span style="color: #e6db74;">{rmse:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"MAE: </span><span style="color: #e6db74;">{mae:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"R¬≤ Score: </span><span style="color: #e6db74;">{r2:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Feature importance (coefficient analysis)</div>
<div><span style="color: #f8f8f2;">feature_importance</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">pd.DataFrame({</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #e6db74;">'feature'</span><span style="color: #f8f8f2;">: boston.feature_names,</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #e6db74;">'coefficient'</span><span style="color: #f8f8f2;">: mlr_model.coef_,</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #e6db74;">'abs_coefficient'</span><span style="color: #f8f8f2;">: np.abs(mlr_model.coef_)</span></div>
<div><span style="color: #f8f8f2;">}).sort_values(</span><span style="color: #e6db74;">'abs_coefficient'</span><span style="color: #f8f8f2;">, ascending=</span><span style="color: #ae81ff;">False</span><span style="color: #f8f8f2;">)</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nFeature Importance (by coefficient magnitude):"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">feature_importance.head(10)</span><span style="color: #f92672;">)</span></div>
                        </div>
                    </div>

                    <h4> Polynomial Regression</h4>
                    <div style="background: #f3e5f5; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Polynomial Regression with different degrees</div>
<div><span style="color: #f8f8f2;">degrees</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">[1, 2, 3, 4]</span></div>
<div><span style="color: #f8f8f2;">poly_results</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{}</span></div>

<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">degree</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">degrees:</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #75715e;"># Create polynomial features</span></div>
<div><span style="color: #f8f8f2;">    poly_features</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">PolynomialFeatures(degree=degree, include_bias=</span><span style="color: #ae81ff;">False</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">    X_train_poly</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">poly_features.fit_transform(X_train_scaled)</span></div>
<div><span style="color: #f8f8f2;">    X_test_poly</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">poly_features.transform(X_test_scaled)</span></div>
    
<div><span style="color: #f8f8f2;">    </span><span style="color: #75715e;"># Train model</span></div>
<div><span style="color: #f8f8f2;">    poly_model</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">LinearRegression()</span></div>
<div><span style="color: #f8f8f2;">    poly_model.fit(X_train_poly, y_train)</span></div>
    
<div><span style="color: #f8f8f2;">    </span><span style="color: #75715e;"># Predictions</span></div>
<div><span style="color: #f8f8f2;">    y_train_pred</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">poly_model.predict(X_train_poly)</span></div>
<div><span style="color: #f8f8f2;">    y_test_pred</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">poly_model.predict(X_test_poly)</span></div>
    
<div><span style="color: #f8f8f2;">    </span><span style="color: #75715e;"># Calculate scores</span></div>
<div><span style="color: #f8f8f2;">    train_score</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">r2_score(y_train, y_train_pred)</span></div>
<div><span style="color: #f8f8f2;">    test_score</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">r2_score(y_test, y_test_pred)</span></div>
<div><span style="color: #f8f8f2;">    test_rmse</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.sqrt(mean_squared_error(y_test, y_test_pred))</span></div>
    
<div><span style="color: #f8f8f2;">    poly_results[degree]</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'train_r2'</span><span style="color: #f8f8f2;">: train_score,</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'test_r2'</span><span style="color: #f8f8f2;">: test_score,</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'test_rmse'</span><span style="color: #f8f8f2;">: test_rmse,</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'features'</span><span style="color: #f8f8f2;">: X_train_poly.shape[1]</span></div>
<div><span style="color: #f8f8f2;">    }</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Display results</div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Polynomial Regression Results:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Degree | Features | Train R¬≤ | Test R¬≤ | Test RMSE"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"-" * 50</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">degree, results</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">poly_results.items():</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"  </span><span style="color: #e6db74;">{degree}</span><span style="color: #e6db74;">    |   </span><span style="color: #e6db74;">{results['features']:3d}</span><span style="color: #e6db74;">   |  </span><span style="color: #e6db74;">{results['train_r2']:.3f}</span><span style="color: #e6db74;">  |  </span><span style="color: #e6db74;">{results['test_r2']:.3f}</span><span style="color: #e6db74;">  |   </span><span style="color: #e6db74;">{results['test_rmse']:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
                        </div>
                    </div>

                    <h4> Regularization: Ridge and Lasso</h4>
                    <div style="background: #e8f5e8; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Ridge Regression with different alpha values</div>
<div><span style="color: #f8f8f2;">alphas</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">[0.1, 1.0, 10.0, 100.0, 1000.0]</span></div>
<div><span style="color: #f8f8f2;">ridge_results</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{}</span></div>

<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">alpha</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">alphas:</span></div>
<div><span style="color: #f8f8f2;">    ridge_model</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">Ridge(alpha=alpha)</span></div>
<div><span style="color: #f8f8f2;">    ridge_model.fit(X_train_scaled, y_train)</span></div>
<div><span style="color: #f8f8f2;">    y_pred_ridge</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">ridge_model.predict(X_test_scaled)</span></div>
    
<div><span style="color: #f8f8f2;">    ridge_results[alpha]</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'r2'</span><span style="color: #f8f8f2;">: r2_score(y_test, y_pred_ridge),</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'rmse'</span><span style="color: #f8f8f2;">: np.sqrt(mean_squared_error(y_test, y_pred_ridge))</span></div>
<div><span style="color: #f8f8f2;">    }</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Lasso Regression</div>
<div><span style="color: #f8f8f2;">lasso_results</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{}</span></div>

<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">alpha</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">alphas:</span></div>
<div><span style="color: #f8f8f2;">    lasso_model</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">Lasso(alpha=alpha, max_iter=1000)</span></div>
<div><span style="color: #f8f8f2;">    lasso_model.fit(X_train_scaled, y_train)</span></div>
<div><span style="color: #f8f8f2;">    y_pred_lasso</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">lasso_model.predict(X_test_scaled)</span></div>
    
<div><span style="color: #f8f8f2;">    </span><span style="color: #75715e;"># Count non-zero coefficients</span></div>
<div><span style="color: #f8f8f2;">    non_zero_coefs</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.sum(lasso_model.coef_</span> <span style="color: #f92672;">!=</span> <span style="color: #ae81ff;">0</span><span style="color: #f8f8f2;">)</span></div>
    
<div><span style="color: #f8f8f2;">    lasso_results[alpha]</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'r2'</span><span style="color: #f8f8f2;">: r2_score(y_test, y_pred_lasso),</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'rmse'</span><span style="color: #f8f8f2;">: np.sqrt(mean_squared_error(y_test, y_pred_lasso)),</span></div>
<div><span style="color: #f8f8f2;">        </span><span style="color: #e6db74;">'features_selected'</span><span style="color: #f8f8f2;">: non_zero_coefs</span></div>
<div><span style="color: #f8f8f2;">    }</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Display regularization results</div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Ridge Regression Results:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Alpha  | R¬≤     | RMSE"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"-" * 20</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">alpha, results</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">ridge_results.items():</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"</span><span style="color: #e6db74;">{alpha:6.1f}</span><span style="color: #e6db74;"> | </span><span style="color: #e6db74;">{results['r2']:.3f}</span><span style="color: #e6db74;"> | </span><span style="color: #e6db74;">{results['rmse']:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>

<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"\nLasso Regression Results:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Alpha  | R¬≤     | RMSE  | Features Selected"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"-" * 35</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">alpha, results</span> <span style="color: #f92672;">in</span> <span style="color: #f8f8f2;">lasso_results.items():</span></div>
<div><span style="color: #f8f8f2;">    </span><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"</span><span style="color: #e6db74;">{alpha:6.1f}</span><span style="color: #e6db74;"> | </span><span style="color: #e6db74;">{results['r2']:.3f}</span><span style="color: #e6db74;"> | </span><span style="color: #e6db74;">{results['rmse']:.3f}</span><span style="color: #e6db74;"> | </span><span style="color: #e6db74;">{results['features_selected']:8d}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
                        </div>
                    </div>

                    <h4> Model Selection with GridSearchCV</h4>
                    <div style="background: #e3f2fd; padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
                        
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; margin: 1rem 0; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<div style="color: #75715e;"># Grid search for optimal Ridge alpha</div>
<div><span style="color: #f8f8f2;">param_grid</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">{</span><span style="color: #e6db74;">'alpha'</span><span style="color: #f8f8f2;">: [0.01, 0.1, 1, 10, 100, 1000]}</span></div>
<div><span style="color: #f8f8f2;">ridge_grid</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">GridSearchCV(</span></div>
<div><span style="color: #f8f8f2;">    Ridge(), param_grid, cv=5, scoring=</span><span style="color: #e6db74;">'r2'</span><span style="color: #f8f8f2;">, n_jobs=-1</span></div>
<div><span style="color: #f8f8f2;">)</span></div>

<div><span style="color: #f8f8f2;">ridge_grid.fit(X_train_scaled, y_train)</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Best Ridge parameters:"</span><span style="color: #f92672;">,</span> <span style="color: #f8f8f2;">ridge_grid.best_params_</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">"Best cross-validation score:"</span><span style="color: #f92672;">,</span> <span style="color: #f8f8f2;">ridge_grid.best_score_.round(3)</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Final model evaluation</div>
<div><span style="color: #f8f8f2;">best_ridge</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">ridge_grid.best_estimator_</span></div>
<div><span style="color: #f8f8f2;">final_predictions</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">best_ridge.predict(X_test_scaled)</span></div>
<div><span style="color: #f8f8f2;">final_r2</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">r2_score(y_test, final_predictions)</span></div>
<div><span style="color: #f8f8f2;">final_rmse</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.sqrt(mean_squared_error(y_test, final_predictions))</span></div>

<div style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"\nFinal Model Performance:"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Test R¬≤: </span><span style="color: #e6db74;">{final_r2:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>
<div><span style="color: #66d9ef;">print</span><span style="color: #f92672;">(</span><span style="color: #e6db74;">f"Test RMSE: </span><span style="color: #e6db74;">{final_rmse:.3f}</span><span style="color: #e6db74;">"</span><span style="color: #f92672;">)</span></div>

<div style="color: #75715e; margin-top: 1rem;"># Residual analysis</div>
<div><span style="color: #f8f8f2;">residuals</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">y_test</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">final_predictions</span></div>

<div><span style="color: #f8f8f2;">plt.figure(figsize=(12, 4))</span></div>

<div style="color: #75715e;"># Residual plot</div>
<div><span style="color: #f8f8f2;">plt.subplot(1, 2, 1)</span></div>
<div><span style="color: #f8f8f2;">plt.scatter(final_predictions, residuals, alpha=0.7)</span></div>
<div><span style="color: #f8f8f2;">plt.axhline(y=0, color=</span><span style="color: #e6db74;">'red'</span><span style="color: #f8f8f2;">, linestyle=</span><span style="color: #e6db74;">'--'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.xlabel(</span><span style="color: #e6db74;">'Predicted Values'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.ylabel(</span><span style="color: #e6db74;">'Residuals'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.title(</span><span style="color: #e6db74;">'Residual Plot'</span><span style="color: #f8f8f2;">)</span></div>

<div style="color: #75715e;"># Actual vs Predicted</div>
<div><span style="color: #f8f8f2;">plt.subplot(1, 2, 2)</span></div>
<div><span style="color: #f8f8f2;">plt.scatter(y_test, final_predictions, alpha=0.7)</span></div>
<div><span style="color: #f8f8f2;">plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], </span><span style="color: #e6db74;">'red'</span><span style="color: #f8f8f2;">, linewidth=2)</span></div>
<div><span style="color: #f8f8f2;">plt.xlabel(</span><span style="color: #e6db74;">'Actual Values'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.ylabel(</span><span style="color: #e6db74;">'Predicted Values'</span><span style="color: #f8f8f2;">)</span></div>
<div><span style="color: #f8f8f2;">plt.title(</span><span style="color: #e6db74;">'Actual vs Predicted'</span><span style="color: #f8f8f2;">)</span></div>

<div><span style="color: #f8f8f2;">plt.tight_layout()</span></div>
<div><span style="color: #f8f8f2;">plt.show()</span></div>
                        </div>

                        <div style="background: #fff3e0; padding: 1rem; border-radius: 6px; margin-top: 1rem;">
                            <strong> Expected Results:</strong>
                            <ul style="font-size: 0.9rem; margin: 0.5rem 0;">
                                <li>Linear Regression R¬≤ ‚âà 0.67</li>
                                <li>Polynomial features improve performance but risk overfitting</li>
                                <li>Ridge/Lasso regularization prevents overfitting</li>
                                <li>GridSearchCV finds optimal hyperparameters</li>
                                <li>Final RMSE around 4-5 (thousands of dollars)</li>
                            </ul>
                        </div>
                    </div>
                    <div style="text-align: center; margin: 40px 0;">
                        <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: blue; padding: 30px; border-radius: 15px;">
                            <h2> Congratulations!</h2>
                            <p style="font-size: 1.2em; margin: 20px 0;color: white">You've completed Chapter 2 and built a solid foundation in Regression!</p>
                
                            
                            <div style="display: flex; justify-content: space-between; margin-top: 30px;">
                                <!-- Left-aligned button -->
                                <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                                    <a href="./chapter1.html" class="azbn-btn azbn-secondary" style="text-decoration: none; color: inherit;">üìö Back to Chapter 1: Introduction</a>
                                </button>
                                
                                <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                                    <a href="#top" class="azbn-btn azbn-secondary" style="text-decoration: none; color: inherit;">  üîÑ Review This Chapter</a>
                                </button>
                                <!-- Right-aligned button -->
                                <button style="background: white; color: #667eea; border: none; padding: 15px 30px; border-radius: 8px; font-size: 1.1em; font-weight: bold; margin: 10px; cursor: pointer;">
                                    <a href="./chapter3.html" class="azbn-btn azbn-secondary" style="text-decoration: none; color: inherit;">üìö Start Chapter 3: Classification</a>
                                </button>
                            </div>
                        </div>

                        <div style="margin: 2rem 0; text-align: center;">
                            <a href="./chapter1.html" class="azbn-btn azbn-secondary" style="text-decoration: none; margin-right: 1rem;">‚Üê Chapter 1: Introduction</a>
                            <a href="./chapter3.html" class="azbn-btn" style="text-decoration: none;">Next: Chapter 3 - Classification ‚Üí</a>
                        </div>
                    </div>

        </div>

