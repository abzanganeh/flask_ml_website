<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: K-Means Clustering Theory - Comprehensive Clustering Analysis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering/clustering.css') }}">
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-tutorial-python.js') }}"></script>
    <!-- Chapter-specific functionality now handled by shared-tutorial-python.js -->
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering" class="course-link">
                    <span>Comprehensive Clustering Analysis</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 5: K-Means Clustering Theory</h1>
                <p class="chapter-subtitle">Master the mathematical foundations, algorithms, and theoretical properties of K-means clustering</p>
                
                <!-- Chapter Progress Bar (5/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="33.33"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/clustering/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/clustering/chapter5" class="chapter-nav-btn active">Chapter 5</a>
                    <a href="/tutorials/clustering/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/clustering/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/clustering/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering/chapter15" class="chapter-nav-btn">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="11.11"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="introduction">Introduction</button>
                    <button class="section-nav-btn azbn-btn" data-section="objective">Objective Function</button>
                    <button class="section-nav-btn azbn-btn" data-section="algorithm">Lloyd's Algorithm</button>
                    <button class="section-nav-btn azbn-btn" data-section="convergence">Convergence Theory</button>
                    <button class="section-nav-btn azbn-btn" data-section="complexity">Computational Analysis</button>
                    <button class="section-nav-btn azbn-btn" data-section="variants">Variants & Extensions</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="interactive">Interactive Demo</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the mathematical foundations of K-means clustering</li>
                        <li>Master the objective function and optimization problem</li>
                        <li>Learn Lloyd's algorithm and its implementation details</li>
                        <li>Analyze convergence properties and theoretical guarantees</li>
                        <li>Understand computational complexity and performance analysis</li>
                        <li>Explore algorithmic variants and extensions</li>
                        <li>Implement K-means with practical considerations</li>
                        <li>Experiment with K-means through interactive demonstrations</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <div class="tutorial-content">
                    <main class="chapter-main-content">
                    <!-- Introduction Section -->
                    <div id="introduction" class="content-section active">
                        <h2>K-Means Implementation: Building the Algorithm Step by Step</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of implementing K-means like building a smart organizer from scratch:</strong></p>
                            <ul>
                                <li><strong>You need to understand the blueprint:</strong> Like knowing how each part of the organizer works</li>
                                <li><strong>You need to code each component:</strong> Like building each piece of the organizer</li>
                                <li><strong>You need to test and debug:</strong> Like making sure everything works together properly</li>
                                <li><strong>You need to optimize performance:</strong> Like making the organizer work faster and more efficiently</li>
                            </ul>
                        </div>
                        
                        <p>K-means clustering stands as one of the most fundamental and widely-used unsupervised learning algorithms. Introduced by Stuart Lloyd at Bell Labs in 1957, it represents the archetypal partitional clustering method that seeks to divide data into k distinct, non-overlapping clusters by minimizing within-cluster variance.</p>

                        <h3>Why Implementation Understanding Matters</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Understanding the implementation helps you:</strong></p>
                            <ul>
                                <li><strong>Write better code:</strong> Know exactly what each part does and why</li>
                                <li><strong>Debug problems:</strong> Understand where things might go wrong</li>
                                <li><strong>Optimize performance:</strong> Make your algorithm run faster and use less memory</li>
                                <li><strong>Extend the algorithm:</strong> Modify K-means for specific applications</li>
                            </ul>
                        </div>

                        <h3>Core Concept and Intuition</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Think of K-means like organizing a classroom efficiently:</strong></p>
                            <ul>
                                <li><strong>You have students (data points) scattered around the room</li>
                                <li><strong>You want to create study groups (clusters) of similar students</li>
                                <li><strong>Each group needs a leader (centroid) who represents the group</li>
                                <li><strong>Students join the group with the leader most similar to them</li>
                                <li><strong>You keep adjusting group leaders until everyone is optimally placed</li>
                            </ul>
                        </div>
                        
                        <p>The central idea behind K-means is elegantly simple yet mathematically profound: given n data points in d-dimensional space, partition them into k clusters such that each point belongs to the cluster with the nearest centroid (cluster center).</p>

                        <div class="image-container">
                            <h4>Visualization: K-Means Core Concept</h4>
                            <img src="{{ url_for('static', filename='images/tutorials/clustering/chapter5/kmeans_concept.png') }}" alt="K-Means Core Concept" class="tutorial-image">
                            <p class="image-caption">A 2D scatter plot showing three distinct clusters of colored points (red, blue, green) with their respective centroids marked as larger symbols. Voronoi diagram lines separate the clusters, demonstrating how each region belongs to the nearest centroid. Animation shows the iterative process: initial random centroids, point assignments, centroid updates, and convergence to final positions.</p>
                        </div>

                        <h3>Mathematical Foundations</h3>
                        <p>K-means is fundamentally an optimization problem that seeks to minimize the total within-cluster sum of squares (WCSS), also known as inertia.</p>

                        <div class="formula-box">
                            <h4>K-Means Objective Function</h4>
                            <p>Given dataset X = &#123;x₁, x₂, ..., xₙ&#125; where xᵢ ∈ ℝᵈ, and k cluster centers μ₁, μ₂, ..., μₖ:</p>
                            
                            <div class="formula-display">
                                <strong>J(C, μ) = Σᵢ₌₁ⁿ Σⱼ₌₁ᵏ wᵢⱼ ||xᵢ - μⱼ||²</strong>
                            </div>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>wᵢⱼ ∈ &#123;0, 1&#125;:</strong> Assignment indicator (1 if xᵢ assigned to cluster j, 0 otherwise)</li>
                                <li><strong>C = &#123;C₁, C₂, ..., Cₖ&#125;:</strong> Cluster assignments</li>
                                <li><strong>μ = &#123;μ₁, μ₂, ..., μₖ&#125;:</strong> Cluster centroids</li>
                                <li><strong>||·||²:</strong> Squared Euclidean distance</li>
                            </ul>
                            
                            <p><strong>Goal:</strong> Find optimal C* and μ* that minimize J(C, μ)</p>
                        </div>

                        <h3>Problem Structure and Constraints</h3>
                        <p>The K-means optimization problem has a specific structure that makes it both tractable and challenging.</p>

                        <div class="model-box">
                            <h4>Mathematical Problem Formulation</h4>
                            
                            <h5>Optimization Problem:</h5>
                            <div class="formula-box">
                                <p><strong>minimize</strong> J(C, μ) = Σᵢ₌₁ⁿ Σⱼ₌₁ᵏ wᵢⱼ ||xᵢ - μⱼ||²</p>
                                <p><strong>subject to:</strong></p>
                                <ul>
                                    <li>Σⱼ₌₁ᵏ wᵢⱼ = 1 for all i = 1, ..., n (each point in exactly one cluster)</li>
                                    <li>wᵢⱼ ∈ &#123;0, 1&#125; for all i, j (binary assignment)</li>
                                    <li>Σᵢ₌₁ⁿ wᵢⱼ ≥ 1 for all j = 1, ..., k (no empty clusters)</li>
                                </ul>
                            </div>
                            
                            <h5>Two-Step Optimization:</h5>
                            <p>The problem is non-convex due to the discrete nature of assignments, but it becomes convex when we fix either C or μ:</p>
                            <ul>
                                <li><strong>Fixed μ:</strong> Optimal C found by nearest neighbor assignment</li>
                                <li><strong>Fixed C:</strong> Optimal μ are cluster centroids (means)</li>
                            </ul>
                            
                            <h5>Coordinate Descent Solution:</h5>
                            <p>Lloyd's algorithm alternates between these two convex subproblems, guaranteeing monotonic decrease in objective function.</p>
                        </div>

                        <h3>Historical Context and Significance</h3>
                        <p>Understanding the historical development helps appreciate K-means' importance in machine learning and data science.</p>

                        <div class="unsupervised-types-grid">
                            <div class="type-card">
                                <h4>Historical Development</h4>
                                <ul>
                                    <li><strong>1957:</strong> Stuart Lloyd develops algorithm at Bell Labs</li>
                                    <li><strong>1967:</strong> MacQueen coins term "K-means"</li>
                                    <li><strong>1982:</strong> Lloyd's work published</li>
                                    <li><strong>1990s:</strong> Computational improvements and variants</li>
                                    <li><strong>2000s:</strong> Large-scale applications and distributed versions</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Why K-Means Matters</h4>
                                <ul>
                                    <li><strong>Simplicity:</strong> Easy to understand and implement</li>
                                    <li><strong>Efficiency:</strong> Linear time complexity in n and d</li>
                                    <li><strong>Scalability:</strong> Works well on large datasets</li>
                                    <li><strong>Interpretability:</strong> Clear cluster centers and assignments</li>
                                    <li><strong>Foundation:</strong> Basis for many advanced methods</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Modern Applications</h4>
                                <ul>
                                    <li><strong>Customer segmentation:</strong> Marketing and e-commerce</li>
                                    <li><strong>Image processing:</strong> Color quantization and compression</li>
                                    <li><strong>Bioinformatics:</strong> Gene expression analysis</li>
                                    <li><strong>Computer vision:</strong> Feature clustering and object recognition</li>
                                    <li><strong>Recommendation systems:</strong> User and item clustering</li>
                                </ul>
                            </div>
                        </div>

                        <h3>Strengths and Limitations</h3>
                        <p>Like all algorithms, K-means has distinct advantages and limitations that determine its appropriate use cases.</p>

                        <div class="model-box">
                            <h4>Algorithm Characteristics</h4>
                            
                            <h5>Strengths:</h5>
                            <ul>
                                <li><strong>Computational Efficiency:</strong> O(nkd) per iteration</li>
                                <li><strong>Simplicity:</strong> Easy to understand and implement</li>
                                <li><strong>Scalability:</strong> Linear in dataset size</li>
                                <li><strong>Interpretability:</strong> Clear cluster centers</li>
                                <li><strong>Guaranteed Convergence:</strong> Finite number of iterations</li>
                            </ul>
                            
                            <h5>Limitations:</h5>
                            <ul>
                                <li><strong>Local Optima:</strong> Sensitive to initialization</li>
                                <li><strong>Spherical Clusters:</strong> Assumes circular/spherical shapes</li>
                                <li><strong>Fixed K:</strong> Number of clusters predetermined</li>
                                <li><strong>Sensitive to Outliers:</strong> Centroid-based method</li>
                                <li><strong>Equal Cluster Sizes:</strong> Bias toward similar-sized clusters</li>
                            </ul>
                            
                            <h5>Mitigation Strategies:</h5>
                            <ul>
                                <li>Multiple random restarts, K-means++</li>
                                <li>Feature transformation, kernel K-means</li>
                                <li>Elbow method, silhouette analysis</li>
                                <li>Robust variants, outlier detection</li>
                                <li>Weighted variants, different algorithms</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <h4>Visualization: K-Means Limitations</h4>
                            <img src="{{ url_for('static', filename='images/tutorials/clustering/chapter5/kmeans_limitations.png') }}" alt="K-Means Limitations" class="tutorial-image">
                            <p class="image-caption">Four 2D subplots showing K-means failures: (1) Non-spherical clusters: elongated elliptical clusters incorrectly partitioned, (2) Different densities: dense cluster split while sparse clusters merged, (3) Overlapping clusters: natural clusters with some overlap incorrectly separated, (4) Outliers: few extreme points pulling centroids away from natural cluster centers.</p>
                        </div>
                            
                            <h5>Two-Step Optimization:</h5>
                            <p>The problem is non-convex due to the discrete nature of assignments, but it becomes convex when we fix either C or μ:</p>
                            <ul>
                                <li><strong>Fixed μ:</strong> Optimal C found by nearest neighbor assignment</li>
                                <li><strong>Fixed C:</strong> Optimal μ are cluster centroids (means)</li>
                            </ul>
                            
                            <h5>Coordinate Descent Solution:</h5>
                            <p>Lloyd's algorithm alternates between these two convex subproblems, guaranteeing monotonic decrease in objective function.</p>
                        </div>

                        <h3>Historical Context and Significance</h3>
                        <p>Understanding the historical development helps appreciate K-means' importance in machine learning and data science.</p>

                        <div class="unsupervised-types-grid">
                            <div class="type-card">
                                <h4>Historical Development</h4>
                                <ul>
                                    <li><strong>1957:</strong> Stuart Lloyd develops algorithm at Bell Labs</li>
                                    <li><strong>1967:</strong> MacQueen coins term "K-means"</li>
                                    <li><strong>1982:</strong> Lloyd's work published</li>
                                    <li><strong>1990s:</strong> Computational improvements and variants</li>
                                    <li><strong>2000s:</strong> Large-scale applications and distributed versions</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Why K-Means Matters</h4>
                                <ul>
                                    <li><strong>Simplicity:</strong> Easy to understand and implement</li>
                                    <li><strong>Efficiency:</strong> Fast convergence and low computational cost</li>
                                    <li><strong>Versatility:</strong> Works well across many domains</li>
                                    <li><strong>Foundation:</strong> Basis for many advanced clustering methods</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Mathematical Foundation Section -->
                    <div id="objective" class="content-section">
                        <h2>Mathematical Deep Dive: The K-Means Objective Function</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of the objective function like a GPS that guides you to the best clustering:</strong></p>
                            <ul>
                                <li><strong>It tells you how good your current clustering is:</strong> Like a score that measures organization quality</li>
                                <li><strong>It guides the algorithm toward better solutions:</strong> Like a compass pointing to improvements</li>
                                <li><strong>It helps you compare different clusterings:</strong> Like a ruler that measures which is better</li>
                                <li><strong>It ensures the algorithm converges:</strong> Like brakes that stop when no improvement is possible</li>
                            </ul>
                        </div>
                        
                        <p>The objective function is the heart of K-means clustering, defining precisely what we want to optimize. Understanding its mathematical properties, geometric interpretation, and relationship to other clustering criteria is crucial for mastering the algorithm.</p>

                        <h3>Why Understanding the Objective Function Matters</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Understanding the objective function helps you:</strong></p>
                            <ul>
                                <li><strong>Implement the algorithm correctly:</strong> Know exactly what you're trying to minimize</li>
                                <li><strong>Debug clustering problems:</strong> Understand why results might be poor</li>
                                <li><strong>Optimize performance:</strong> Use mathematical properties to make algorithms faster</li>
                                <li><strong>Extend the algorithm:</strong> Modify the objective for specific applications</li>
                            </ul>
                        </div>

                        <h3>Detailed Mathematical Formulation</h3>
                        <p>Let's build the objective function step by step, starting from first principles and adding mathematical rigor.</p>

                        <div class="formula-box">
                            <h4>Complete Mathematical Setup</h4>
                            
                            <h5>Given Data:</h5>
                            <ul>
                                <li><strong>Dataset:</strong> X = {x₁, x₂, ..., xₙ} where xᵢ ∈ ℝᵈ</li>
                                <li><strong>Number of clusters:</strong> k ∈ ℕ, k ≤ n</li>
                                <li><strong>Cluster centers:</strong> μ = {μ₁, μ₂, ..., μₖ} where μⱼ ∈ ℝᵈ</li>
                                <li><strong>Assignment matrix:</strong> W ∈ {0,1}ⁿˣᵏ where wᵢⱼ = 1 if xᵢ ∈ Cⱼ</li>
                            </ul>
                            
                            <h5>Objective Function (Multiple Formulations):</h5>
                            
                            <div class="formula-box">
                                <p><strong>1. Matrix Form:</strong></p>
                                <div class="formula-display">
                                    <strong>J(W, μ) = Σᵢ₌₁ⁿ Σⱼ₌₁ᵏ wᵢⱼ ||xᵢ - μⱼ||²</strong>
                                </div>
                                
                                <p><strong>2. Cluster-wise Form:</strong></p>
                                <div class="formula-display">
                                    <strong>J(C) = Σⱼ₌₁ᵏ Σₓᵢ∈Cⱼ ||xᵢ - μⱼ||²</strong>
                                </div>
                                
                                <p><strong>3. Variance Form:</strong></p>
                                <div class="formula-display">
                                    <strong>J(C) = Σⱼ₌₁ᵏ |Cⱼ| · Var(Cⱼ)</strong>
                                </div>
                                
                                <p><strong>4. Expanded Euclidean Form:</strong></p>
                            <div class="formula-display">
                                    <strong>J(W, μ) = Σᵢ₌₁ⁿ Σⱼ₌₁ᵏ wᵢⱼ Σₗ₌₁ᵈ (xᵢₗ - μⱼₗ)²</strong>
                                </div>
                            </div>
                            
                            <h5>Constraints:</h5>
                            <ul>
                                <li><strong>Partition constraint:</strong> Σⱼ₌₁ᵏ wᵢⱼ = 1 ∀i (each point in exactly one cluster)</li>
                                <li><strong>Binary constraint:</strong> wᵢⱼ ∈ {0, 1} ∀i,j (binary assignment)</li>
                                <li><strong>Non-empty constraint:</strong> Σᵢ₌₁ⁿ wᵢⱼ ≥ 1 ∀j (no empty clusters)</li>
                            </ul>
                        </div>

                        <h3>Geometric Interpretation</h3>
                        <p>The objective function has a clear geometric meaning that provides intuition about what K-means actually optimizes.</p>

                        <div class="model-box">
                            <h4>Geometric Meaning of the Objective</h4>
                            
                            <h5>Within-Cluster Sum of Squares (WCSS):</h5>
                            <p>The objective function measures the total squared distance from each point to its assigned cluster center. This is equivalent to:</p>
                            
                            <ul>
                                <li><strong>Compactness:</strong> How tightly clustered the points are around their centers</li>
                                <li><strong>Homogeneity:</strong> How similar points within each cluster are</li>
                                <li><strong>Variance:</strong> The total within-cluster variance across all clusters</li>
                            </ul>
                            
                            <h5>Relationship to Total Sum of Squares:</h5>
                            <p>The total sum of squares can be decomposed as:</p>
                            <div class="formula-display">
                                <strong>TSS = WCSS + BSS</strong><br>
                                <span style="font-size: 0.9rem;">Total Sum of Squares = Within-Cluster SS + Between-Cluster SS</span>
                            </div>
                            
                            <p>Since TSS is constant for a given dataset, minimizing WCSS is equivalent to maximizing BSS (between-cluster separation).</p>
                            
                            <h5>Voronoi Tessellation:</h5>
                            <p>The optimal assignment for fixed centroids creates a Voronoi tessellation of the space, where each region contains points closest to one centroid.</p>
                        </div>
                    </div>

                    <!-- Initialization Methods Section -->
                    <div id="algorithm" class="content-section">
                        <h2>Lloyd's Algorithm: The K-Means Workhorse</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of Lloyd's algorithm like a smart organizer that keeps improving:</strong></p>
                            <ul>
                                <li><strong>Assignment step:</strong> Like reassigning students to better study groups</li>
                                <li><strong>Update step:</strong> Like moving group leaders to the center of their groups</li>
                                <li><strong>Iteration:</strong> Like repeating this process until everyone is optimally placed</li>
                                <li><strong>Convergence:</strong> Like knowing when the organization can't get any better</li>
                            </ul>
                        </div>
                        
                        <p>Lloyd's algorithm, also known as the K-means algorithm, is an iterative expectation-maximization style procedure that alternates between two steps: assigning points to clusters and updating cluster centers. Despite its simplicity, the algorithm has elegant mathematical properties and guaranteed convergence.</p>

                        <h3>Why Lloyd's Algorithm is So Effective</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Lloyd's algorithm works so well because:</strong></p>
                            <ul>
                                <li><strong>It's simple but powerful:</strong> Two easy steps that work together perfectly</li>
                                <li><strong>It's guaranteed to converge:</strong> You'll always reach a stable solution</li>
                                <li><strong>It's computationally efficient:</strong> Fast even with large datasets</li>
                                <li><strong>It's easy to implement:</strong> You can code it yourself in just a few lines</li>
                            </ul>
                        </div>

                        <h3>The Two-Step Iteration</h3>
                        <p>The genius of Lloyd's algorithm lies in its decomposition of the complex joint optimization into two simple, optimal subproblems.</p>

                        <div class="model-box">
                            <h4>Lloyd's Algorithm: Complete Specification</h4>
                            
                            <div class="formula-box">
                                <h5><strong>Input:</strong></h5>
                                <ul>
                                    <li>Dataset X = {x₁, x₂, ..., xₙ} ⊂ ℝᵈ</li>
                                    <li>Number of clusters k ∈ ℕ</li>
                                    <li>Initial centroids μ⁽⁰⁾ = {μ₁⁽⁰⁾, ..., μₖ⁽⁰⁾}</li>
                                    <li>Convergence tolerance ε > 0</li>
                                    <li>Maximum iterations T_max</li>
                                </ul>
                                
                                <h5><strong>Algorithm:</strong></h5>
                                <div class="code-box">
<strong>for</strong> t = 0, 1, 2, ... <strong>until</strong> convergence <strong>do</strong>
    <span style="color: #1976d2;">// Step 1: Assignment (E-step)</span>
    <strong>for</strong> i = 1 <strong>to</strong> n <strong>do</strong>
        j*(i) = argmin[j∈{1,...,k}] ||xᵢ - μⱼ⁽ᵗ⁾||²
        wᵢⱼ⁽ᵗ⁺¹⁾ = 1 <strong>if</strong> j = j*(i), <strong>else</strong> 0
    <strong>end for</strong>
    
    <span style="color: #1976d2;">// Step 2: Update (M-step)</span>
    <strong>for</strong> j = 1 <strong>to</strong> k <strong>do</strong>
        <strong>if</strong> Cⱼ⁽ᵗ⁺¹⁾ ≠ ∅ <strong>then</strong>
            μⱼ⁽ᵗ⁺¹⁾ = (1/|Cⱼ⁽ᵗ⁺¹⁾|) ∑[xᵢ∈Cⱼ⁽ᵗ⁺¹⁾] xᵢ
        <strong>else</strong>
            <span style="color: #d32f2f;">// Handle empty cluster</span>
            reinitialize μⱼ⁽ᵗ⁺¹⁾
        <strong>end if</strong>
    <strong>end for</strong>
    
    <span style="color: #1976d2;">// Check convergence</span>
    <strong>if</strong> ||μ⁽ᵗ⁺¹⁾ - μ⁽ᵗ⁾||₂ < ε <strong>or</strong> t ≥ T_max <strong>then</strong>
        <strong>break</strong>
    <strong>end if</strong>
<strong>end for</strong>

<strong>return</strong> C* = {C₁⁽ᵗ⁾, ..., Cₖ⁽ᵗ⁾}, μ* = {μ₁⁽ᵗ⁾, ..., μₖ⁽ᵗ⁾}
                                </div>
                            </div>
                        </div>

                        <h3>Mathematical Analysis of Each Step</h3>
                        <p>Let's analyze the mathematical optimality and properties of each step in Lloyd's algorithm.</p>

                        <div class="model-box">
                            <h4>Step-by-Step Mathematical Analysis</h4>
                            
                            <h5>Step 1: Assignment (E-step)</h5>
                            <div class="formula-box">
                                <p><strong>Problem:</strong> Given fixed centroids μ⁽ᵗ⁾, find optimal assignment W⁽ᵗ⁺¹⁾</p>
                                
                                <p><strong>Mathematical Formulation:</strong></p>
                                <div class="formula-display">
                                    W⁽ᵗ⁺¹⁾ = argmin[W] Σᵢⱼ wᵢⱼ ||xᵢ - μⱼ⁽ᵗ⁾||²
                                </div>
                                
                                <p><strong>Solution:</strong> This decomposes into n independent problems:</p>
                                <div class="formula-display">
                                    j*(i) = argmin[j∈{1,...,k}] ||xᵢ - μⱼ⁽ᵗ⁾||²
                                </div>
                                
                                <p><strong>Optimality:</strong> This is the nearest neighbor assignment, which is globally optimal for the fixed centroids.</p>
                                
                                <p><strong>Tie-breaking:</strong> When ||xᵢ - μⱼ₁|| = ||xᵢ - μⱼ₂||, any consistent rule works (e.g., smallest index j).</p>
                            </div>
                            
                            <h5>Step 2: Centroid Update (M-step)</h5>
                            <div class="formula-box">
                                <p><strong>Problem:</strong> Given fixed assignment W⁽ᵗ⁺¹⁾, find optimal centroids μ⁽ᵗ⁺¹⁾</p>
                                
                                <p><strong>Mathematical Formulation:</strong></p>
                                <div class="formula-display">
                                    μ⁽ᵗ⁺¹⁾ = argmin[μ] Σᵢⱼ wᵢⱼ⁽ᵗ⁺¹⁾ ||xᵢ - μⱼ||²
                                </div>
                                
                                <p><strong>Solution:</strong> Taking partial derivatives and setting to zero:</p>
                                <div class="formula-display">
                                    μⱼ⁽ᵗ⁺¹⁾ = (1/|Cⱼ⁽ᵗ⁺¹⁾|) ∑[xᵢ∈Cⱼ⁽ᵗ⁺¹⁾] xᵢ
                                </div>
                                
                                <p><strong>Optimality:</strong> The centroid is the arithmetic mean of assigned points, which minimizes the sum of squared distances.</p>
                            </div>
                        </div>

                        <div class="interactive-container">
                            <h3>Initialization Comparison Demo</h3>
                            <div class="demo-controls">
                                <label for="init-method">Initialization Method:</label>
                                <select id="init-method">
                                    <option value="random">Random</option>
                                    <option value="kmeans++">K-means++</option>
                                </select>
                                
                                <label for="num-clusters-init">Number of Clusters:</label>
                                <input type="range" id="num-clusters-init" min="2" max="6" value="3">
                                <span id="clusters-init-display">3</span>
                                
                                <button class="azbn-btn" onclick="runInitializationDemo()">Run Demo</button>
                                <button class="azbn-btn azbn-secondary" onclick="resetInitializationDemo()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="initialization-canvas">
                                <p>Click "Run Demo" to compare different initialization methods</p>
                            </div>
                        </div>
                    </div>


                    <!-- Convergence Analysis Section -->
                    <div id="convergence" class="content-section">
                        <h2>Convergence Analysis: When K-means Stops Improving</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of convergence like knowing when to stop reorganizing:</strong></p>
                            <ul>
                                <li><strong>Centroid movement:</strong> Like knowing when group leaders stop moving to better positions</li>
                                <li><strong>Assignment stability:</strong> Like knowing when students stop switching groups</li>
                                <li><strong>Objective improvement:</strong> Like knowing when the organization score stops getting better</li>
                                <li><strong>Maximum iterations:</strong> Like setting a time limit to avoid endless reorganization</li>
                            </ul>
                        </div>
                        
                        <p>Understanding convergence properties is essential for implementing K-means correctly and determining appropriate stopping criteria. The algorithm's convergence behavior affects both computational efficiency and clustering quality.</p>

                        <h3>Why Convergence Analysis Matters</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Understanding convergence helps you:</strong></p>
                            <ul>
                                <li><strong>Know when to stop:</strong> Avoid running the algorithm longer than necessary</li>
                                <li><strong>Ensure quality:</strong> Make sure the algorithm has found a good solution</li>
                                <li><strong>Optimize performance:</strong> Balance speed and accuracy</li>
                                <li><strong>Debug problems:</strong> Understand why the algorithm might not converge</li>
                            </ul>
                        </div>

                        <div class="model-box">
                            <h3>Convergence Criteria</h3>
                            <ul>
                                <li><strong>Centroid Movement:</strong> Stop when centroids move less than threshold</li>
                                <li><strong>Assignment Stability:</strong> Stop when cluster assignments don't change</li>
                                <li><strong>Objective Function:</strong> Stop when WCSS improvement is minimal</li>
                                <li><strong>Maximum Iterations:</strong> Stop after fixed number of iterations</li>
                            </ul>
                        </div>

                        <div class="formula-box">
                            <h3>Convergence Conditions</h3>
                            <div class="formula-display">
                                <h4>Centroid Movement Threshold</h4>
                                <div class="formula">maxᵢ ||μᵢ^(t+1) - μᵢ^(t)|| < ε</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>μᵢ^(t) is centroid i at iteration t</li>
                                    <li>ε is the convergence threshold (typically 1e-4)</li>
                                    <li>maxᵢ finds the maximum movement across all centroids</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Convergence Guarantees</h3>
                            <p>K-means is guaranteed to converge because:</p>
                            <ul>
                                <li>The objective function is bounded below by zero</li>
                                <li>Each iteration decreases or maintains the objective function</li>
                                <li>There are only finitely many possible cluster assignments</li>
                                <li>The algorithm cannot cycle due to strict improvement</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <h4>Visualization: Convergence Behavior</h4>
                            <div class="visualization-placeholder">
                                <p>Graph showing objective function value decreasing over iterations until convergence</p>
                            </div>
                            <p><strong>Convergence Pattern:</strong> Observe how the objective function decreases rapidly in early iterations and then stabilizes.</p>
                        </div>
                    </div>

                    <!-- Variants & Extensions Section -->
                    <div id="variants" class="content-section">
                        <h2>Variants and Extensions of K-Means</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of K-means variants like different types of organizers for different situations:</strong></p>
                            <ul>
                                <li><strong>K-medoids:</strong> Like an organizer that uses actual students as group leaders (more robust)</li>
                                <li><strong>Fuzzy C-means:</strong> Like an organizer that allows students to be in multiple groups</li>
                                <li><strong>Mini-batch K-means:</strong> Like an organizer that works with smaller groups at a time</li>
                                <li><strong>K-means++:</strong> Like an organizer that picks better starting group leaders</li>
                            </ul>
                        </div>
                        
                        <p>While standard K-means is powerful, numerous variants and extensions have been developed to address specific limitations and improve performance in various scenarios.</p>

                        <h3>Why Variants and Extensions Matter</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Understanding variants helps you:</strong></p>
                            <ul>
                                <li><strong>Choose the right tool:</strong> Pick the best variant for your specific problem</li>
                                <li><strong>Handle special cases:</strong> Deal with outliers, large datasets, or fuzzy boundaries</li>
                                <li><strong>Improve performance:</strong> Make the algorithm faster or more accurate</li>
                                <li><strong>Extend functionality:</strong> Add new features to the basic algorithm</li>
                            </ul>
                        </div>

                        <h3>K-Medoids (PAM)</h3>
                        <p>K-medoids uses actual data points as cluster centers instead of centroids, making it more robust to outliers.</p>

                        <div class="algorithm-box">
                            <h4>K-Medoids Algorithm</h4>
                            <ul>
                                <li><strong>Medoid selection:</strong> Choose actual data points as cluster centers</li>
                                <li><strong>Robust to outliers:</strong> Less sensitive to extreme values</li>
                                <li><strong>Arbitrary distance metrics:</strong> Works with any distance measure</li>
                                <li><strong>Higher computational cost:</strong> O(n²) complexity</li>
                            </ul>
                        </div>

                        <h3>Fuzzy C-Means</h3>
                        <p>Allows data points to belong to multiple clusters with different membership degrees.</p>

                        <div class="model-box">
                            <h4>Fuzzy C-Means Features</h4>
                            <ul>
                                <li><strong>Soft clustering:</strong> Points can belong to multiple clusters</li>
                                <li><strong>Membership degrees:</strong> Probabilistic cluster assignments</li>
                                <li><strong>Robust to noise:</strong> Less sensitive to outliers</li>
                                <li><strong>Overlapping clusters:</strong> Handles ambiguous boundaries</li>
                            </ul>
                        </div>

                        <h3>K-Means++</h3>
                        <p>Improved initialization method that provides better starting points for K-means.</p>

                        <div class="explanation-box">
                            <h4>K-Means++ Initialization</h4>
                            <ul>
                                <li><strong>Probabilistic selection:</strong> Choose initial centroids based on distance</li>
                                <li><strong>Better convergence:</strong> Faster convergence to good solutions</li>
                                <li><strong>Theoretical guarantees:</strong> O(log k) approximation ratio</li>
                                <li><strong>Widely adopted:</strong> Default in most implementations</li>
                            </ul>
                        </div>

                        <div class="visualization-container" id="kmeans-variants-viz">
                                <h4>Visualization: K-Means Variants Comparison</h4>
                                <div class="chart-container" id="kmeans-variants-chart">
                                    <div class="loading-spinner">
                                        <p>Loading K-means variants comparison...</p>
                                    </div>
                                </div>
                            </div>
                    </div>

                    <!-- Implementation Section -->
                    <div id="implementation" class="content-section">
                        <h2>Practical Implementation Considerations</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of implementation like building a reliable organizer that works in real-world conditions:</strong></p>
                            <ul>
                                <li><strong>Initialization strategies:</strong> Like choosing the best way to start organizing</li>
                                <li><strong>Stopping criteria:</strong> Like knowing when to stop reorganizing</li>
                                <li><strong>Error handling:</strong> Like dealing with unexpected situations</li>
                                <li><strong>Performance optimization:</strong> Like making the organizer work faster and more efficiently</li>
                            </ul>
                        </div>
                        
                        <p>Implementing K-means effectively requires careful consideration of various practical aspects that can significantly impact performance and results.</p>

                        <h3>Why Implementation Considerations Matter</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Good implementation practices help you:</strong></p>
                            <ul>
                                <li><strong>Get better results:</strong> Avoid common pitfalls that lead to poor clustering</li>
                                <li><strong>Run faster:</strong> Optimize the algorithm for your specific use case</li>
                                <li><strong>Handle edge cases:</strong> Deal with unusual data or situations</li>
                                <li><strong>Make it robust:</strong> Ensure the algorithm works reliably in production</li>
                            </ul>
                        </div>

                        <h3>Initialization Strategies</h3>
                        <div class="explanation-box">
                            <h4>Initialization Best Practices</h4>
                            <ul>
                                <li><strong>Multiple runs:</strong> Run algorithm multiple times with different initializations</li>
                                <li><strong>K-means++:</strong> Use probabilistic initialization for better starting points</li>
                                <li><strong>Random sampling:</strong> Simple but effective for many cases</li>
                                <li><strong>Domain knowledge:</strong> Use prior knowledge when available</li>
                            </ul>
                        </div>

                        <h3>Stopping Criteria</h3>
                        <p>Determining when the algorithm has converged is crucial for efficiency and accuracy.</p>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Criterion</th>
                                        <th>Advantages</th>
                                        <th>Disadvantages</th>
                                        <th>Use Case</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Centroid Movement</strong></td>
                                        <td>Intuitive, geometric meaning</td>
                                        <td>May not reflect objective improvement</td>
                                        <td>General purpose</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Objective Change</strong></td>
                                        <td>Direct optimization measure</td>
                                        <td>May be noisy</td>
                                        <td>Optimization focus</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Assignment Stability</strong></td>
                                        <td>Reflects clustering stability</td>
                                        <td>May converge slowly</td>
                                        <td>Stability focus</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Maximum Iterations</strong></td>
                                        <td>Guaranteed termination</td>
                                        <td>May stop too early or late</td>
                                        <td>Time-constrained</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h3>Numerical Stability</h3>
                        <p>Handling edge cases and numerical precision issues in real implementations.</p>

                        <div class="model-box">
                            <h4>Stability Considerations</h4>
                            <ul>
                                <li><strong>Empty clusters:</strong> Handle clusters with no assigned points</li>
                                <li><strong>Duplicate points:</strong> Manage identical data points</li>
                                <li><strong>Floating-point precision:</strong> Use appropriate tolerance values</li>
                                <li><strong>Memory efficiency:</strong> Optimize for large datasets</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Computational Analysis Section -->
                    <div id="complexity" class="content-section">
                        <h2>Computational Analysis: How Fast is K-means?</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of computational complexity like measuring how long it takes to organize a classroom:</strong></p>
                            <ul>
                                <li><strong>Time complexity:</strong> Like counting how many steps it takes to organize everyone</li>
                                <li><strong>Space complexity:</strong> Like measuring how much room you need to organize everyone</li>
                                <li><strong>Scalability:</strong> Like understanding how organization time changes with more students</li>
                                <li><strong>Optimization:</strong> Like finding ways to organize faster and more efficiently</li>
                            </ul>
                        </div>
                        
                        <p>Understanding the computational complexity of K-means is crucial for practical applications, especially when dealing with large datasets or real-time constraints.</p>

                        <h3>Why Understanding Complexity Matters</h3>
                        
                        <div class="explanation-box">
                            <p><strong>Understanding complexity helps you:</strong></p>
                            <ul>
                                <li><strong>Choose the right algorithm:</strong> Know when K-means is fast enough for your needs</li>
                                <li><strong>Optimize performance:</strong> Make the algorithm run faster on large datasets</li>
                                <li><strong>Plan resources:</strong> Know how much time and memory you'll need</li>
                                <li><strong>Compare alternatives:</strong> Understand when other algorithms might be better</li>
                            </ul>
                        </div>

                        <h3>Time Complexity Analysis</h3>
                        <div class="model-box">
                            <h4>Per-Iteration Complexity</h4>
                            <ul>
                                <li><strong>Assignment Step:</strong> O(nk) - assign n points to k clusters</li>
                                <li><strong>Update Step:</strong> O(n) - recalculate k centroids</li>
                                <li><strong>Total per iteration:</strong> O(nk)</li>
                            </ul>
                        </div>

                        <div class="algorithm-box">
                            <h4>Overall Complexity</h4>
                            <ul>
                                <li><strong>Best case:</strong> O(nk) - converges in 1 iteration</li>
                                <li><strong>Average case:</strong> O(nkt) - t iterations to converge</li>
                                <li><strong>Worst case:</strong> O(nk²) - exponential convergence</li>
                                <li><strong>Space complexity:</strong> O(n + k) - store points and centroids</li>
                            </ul>
                        </div>

                        <h3>Scalability Considerations</h3>
                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Dataset Size</th>
                                        <th>Time (seconds)</th>
                                        <th>Memory (MB)</th>
                                        <th>Recommendations</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Small (n < 1K)</strong></td>
                                        <td>< 0.1</td>
                                        <td>< 1</td>
                                        <td>Standard implementation</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Medium (1K < n < 100K)</strong></td>
                                        <td>0.1 - 10</td>
                                        <td>1 - 100</td>
                                        <td>Optimized implementation</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Large (100K < n < 1M)</strong></td>
                                        <td>10 - 1000</td>
                                        <td>100 - 1000</td>
                                        <td>Mini-batch K-means</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Very Large (n > 1M)</strong></td>
                                        <td>> 1000</td>
                                        <td>> 1000</td>
                                        <td>Distributed implementation</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h3>Performance Optimization</h3>
                        <div class="explanation-box">
                            <h4>Optimization Strategies</h4>
                            <ul>
                                <li><strong>Vectorization:</strong> Use matrix operations for distance calculations</li>
                                <li><strong>Early stopping:</strong> Stop when improvement is minimal</li>
                                <li><strong>Smart initialization:</strong> K-means++ reduces iterations</li>
                                <li><strong>Memory optimization:</strong> Process data in chunks</li>
                                <li><strong>Parallel processing:</strong> Distribute computation across cores</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Interactive Demo Section -->
                    <div id="interactive" class="content-section">
                        <h2>Interactive K-means Demo</h2>
                        
                        <div class="explanation-box">
                            <p>Experiment with the K-means algorithm using this interactive demo. Adjust parameters, try different initialization methods, and observe how they affect clustering results and convergence behavior.</p>
                        </div>

                        <div class="interactive-container">
                            <h3>K-means Clustering Demo</h3>
                            
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="demo-clusters">Number of Clusters:</label>
                                    <input type="range" id="demo-clusters" min="2" max="8" value="3">
                                    <span id="demo-clusters-display">3</span>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-init">Initialization:</label>
                                    <select id="demo-init">
                                        <option value="random">Random</option>
                                        <option value="kmeans++">K-means++</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-data">Data Type:</label>
                                    <select id="demo-data">
                                        <option value="blobs">Well-separated Blobs</option>
                                        <option value="random">Random Points</option>
                                        <option value="moons">Moon-shaped</option>
                                    </select>
                                </div>
                                
                                <div class="control-buttons">
                                    <button class="azbn-btn" onclick="generateDemoData()">Generate Data</button>
                                    <button class="azbn-btn" onclick="runKmeansDemo()">Run K-means</button>
                                    <button class="azbn-btn" onclick="stepKmeansDemo()">Step-by-Step</button>
                                    <button class="azbn-btn" onclick="resetDemo()">Reset</button>
                                </div>
                            </div>
                            
                            <div class="demo-status" id="demo-status">
                                <p>Click "Generate Data" to start the demo</p>
                            </div>
                            
                            <div class="metric-visualization" id="kmeans-demo-canvas">
                                <p>Interactive K-means clustering visualization will appear here</p>
                            </div>
                            
                            <div class="demo-metrics" id="demo-metrics" style="display: none;">
                                <h4>Clustering Metrics</h4>
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <span class="metric-label">WCSS:</span>
                                        <span class="metric-value" id="wcss-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Silhouette:</span>
                                        <span class="metric-value" id="silhouette-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Iterations:</span>
                                        <span class="metric-value" id="iterations-value">-</span>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="convergence-plot" id="convergencePlot" style="display: none;">
                                <h4>Convergence Plot</h4>
                                <p>WCSS reduction over iterations</p>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Test Your K-means Implementation Knowledge</h2>
                        
                        <div class="explanation-box">
                            <p><strong>Think of this quiz like a K-means implementation certification test:</strong></p>
                            <ul>
                                <li><strong>It's okay to get questions wrong:</strong> That's how you learn! Wrong answers help you identify what to review</li>
                                <li><strong>Each question teaches you something:</strong> Even if you get it right, the explanation reinforces your understanding</li>
                                <li><strong>It's not about the score:</strong> It's about making sure you understand the key concepts</li>
                                <li><strong>You can take it multiple times:</strong> Practice makes perfect!</li>
                            </ul>
                        </div>
                        
                        <p>Evaluate your understanding of K-means implementation, mathematical foundations, and practical considerations.</p>

                        <h3>What This Quiz Covers</h3>
                        
                        <div class="explanation-box">
                            <p><strong>This quiz tests your understanding of:</strong></p>
                            <ul>
                                <li><strong>Objective function:</strong> What the algorithm is trying to minimize</li>
                                <li><strong>Lloyd's algorithm:</strong> How the algorithm works step by step</li>
                                <li><strong>Convergence analysis:</strong> When and why the algorithm stops</li>
                                <li><strong>Computational complexity:</strong> How fast the algorithm runs</li>
                                <li><strong>Implementation considerations:</strong> Practical aspects of building the algorithm</li>
                            </ul>
                            <p><strong>Don't worry if you don't get everything right the first time - that's normal! The goal is to learn.</strong></p>
                        </div>
                        
                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 1: What is the primary objective function minimized by K-means?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Between-cluster sum of squares</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>Within-cluster sum of squares (WCSS)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Silhouette coefficient</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Calinski-Harabasz index</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means minimizes the within-cluster sum of squares (WCSS), which measures the total squared distance of all points from their cluster centroids.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 2: How are centroids updated in each K-means iteration?</h4>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>As the arithmetic mean of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the median of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the point closest to the cluster center</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As a weighted average based on point distances</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Centroids are updated as the arithmetic mean of all points assigned to that cluster, which minimizes the WCSS for that cluster.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 3: What is the main advantage of K-means++ initialization over random initialization?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It's faster to compute</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It guarantees global optimum</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>It provides better initialization leading to faster convergence</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It works better with non-spherical clusters</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means++ initialization probabilistically selects initial centroids that are well-separated, leading to better starting points and faster convergence to good local minima.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    </main>
                </div>
            </div>
        </section>
    </main>

    <!-- Sub-section Navigation Footer -->
    <div class="sub-section-nav-footer">
        <div class="sub-nav-buttons">
            <button id="prev-subsection" class="sub-nav-btn azbn-btn prev-btn" style="display: none;">
                <span>← Previous</span>
                <span class="sub-nav-label" id="prev-label"></span>
            </button>
            <button id="next-subsection" class="sub-nav-btn azbn-btn next-btn">
                <span class="sub-nav-label" id="next-label">Objective Function</span>
                <span>Next →</span>
            </button>
        </div>
    </div>

    <!-- Chapter Navigation Footer -->
    <div class="navigation-buttons">
        <a href="/tutorials/clustering/chapter4" class="azbn-btn azbn-secondary" onclick="scrollToTop()">← Chapter 4: Specialized Distance Metrics</a>
        <a href="/tutorials/clustering/chapter6" class="azbn-btn azbn-secondary" onclick="scrollToTop()">Chapter 6: K-Means Optimization →</a>
    </div>

    <!-- Visualization Loading Script -->
    <!-- Visualization loading now handled by shared-tutorial-python.js -->
    
    <!-- Visualization Styles -->
    <style>
    .visualization-container {
        margin: 2rem 0;
        padding: 1.5rem;
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        background-color: #fafafa;
    }

    .chart-container {
        min-height: 400px;
        margin: 1rem 0;
        border: 1px solid #ddd;
        border-radius: 4px;
        background-color: white;
    }

    .loading-spinner {
        display: flex;
        align-items: center;
        justify-content: center;
        height: 400px;
        color: #666;
    }

    .error-message {
        display: flex;
        align-items: center;
        justify-content: center;
        height: 400px;
        color: #d32f2f;
        background-color: #ffebee;
        border: 1px solid #ffcdd2;
        border-radius: 4px;
    }
    </style>
</body>
</html>
