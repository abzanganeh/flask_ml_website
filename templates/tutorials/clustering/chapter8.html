<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 8: Hierarchical Clustering Theory - Comprehensive Clustering Analysis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering/clustering.css') }}">
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-tutorial.js') }}"></script>
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-quiz.js') }}"></script>
    <script src="{{ url_for('static', filename='js/tutorials/clustering/chapter8.js') }}"></script>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering" class="course-link">
                    <span>Comprehensive Clustering Analysis</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 8: Hierarchical Clustering Theory</h1>
                <p class="chapter-subtitle">Master the mathematical foundations and theoretical principles of hierarchical clustering algorithms</p>
                
                <!-- Chapter Progress Bar (4/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="26.67"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/clustering/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/clustering/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/clustering/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/clustering/chapter8" class="chapter-nav-btn active">Chapter 8</a>
                    <a href="/tutorials/clustering/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering/chapter15" class="chapter-nav-btn">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.29"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn active" data-section="introduction">Introduction</button>
                    <button class="section-nav-btn" data-section="theory">Mathematical Theory</button>
                    <button class="section-nav-btn" data-section="agglomerative">Agglomerative Methods</button>
                    <button class="section-nav-btn" data-section="divisive">Divisive Methods</button>
                    <button class="section-nav-btn" data-section="dendrograms">Dendrogram Analysis</button>
                    <button class="section-nav-btn" data-section="complexity">Complexity Analysis</button>
                    <button class="section-nav-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn" data-section="interactive">Interactive Demos</button>
                    <button class="section-nav-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the mathematical foundations of hierarchical clustering</li>
                        <li>Master agglomerative and divisive clustering algorithms</li>
                        <li>Learn dendrogram construction and interpretation</li>
                        <li>Analyze computational complexity of hierarchical methods</li>
                        <li>Explore real-world applications and use cases</li>
                        <li>Compare hierarchical clustering with other approaches</li>
                        <li>Implement hierarchical clustering with interactive demos</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">
                    <!-- Introduction Section -->
                    <div id="introduction" class="content-section active">
                        <h2>Hierarchical Clustering: Revealing Nested Structure in Data</h2>
                        
                        <p>Hierarchical clustering represents a fundamentally different approach to clustering compared to partitional methods like K-means. Instead of producing a single flat partitioning, hierarchical methods construct a tree-like hierarchy of clusters, revealing structure at multiple scales simultaneously. This approach is particularly valuable when the natural granularity of clustering is unknown or when understanding relationships between clusters is important.</p>

                        <h3>Core Concepts and Motivation</h3>
                        <p>Hierarchical clustering addresses several limitations of flat clustering methods by providing a multi-resolution view of data structure.</p>

                        <div class="unsupervised-types-grid">
                            <div class="type-card">
                                <h4>Advantages of Hierarchical Approach</h4>
                                <ul>
                                    <li><strong>No k specification:</strong> Don't need to choose number of clusters a priori</li>
                                    <li><strong>Multi-scale structure:</strong> Reveals clustering at different resolutions</li>
                                    <li><strong>Deterministic results:</strong> Given distance matrix, results are reproducible</li>
                                    <li><strong>Natural interpretation:</strong> Tree structure is intuitive to understand</li>
                                    <li><strong>Nested clusters:</strong> Shows relationships between cluster groupings</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Types of Hierarchical Structure</h4>
                                <ul>
                                    <li><strong>Agglomerative:</strong> Bottom-up approach, merge similar clusters</li>
                                    <li><strong>Divisive:</strong> Top-down approach, split heterogeneous clusters</li>
                                    <li><strong>Nested partitions:</strong> Each level gives valid clustering</li>
                                    <li><strong>Binary trees:</strong> Most common structure with binary merges/splits</li>
                                    <li><strong>Ultrametric trees:</strong> Special case with meaningful distances</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Key Applications</h4>
                                <ul>
                                    <li><strong>Phylogenetic analysis:</strong> Evolutionary relationships in biology</li>
                                    <li><strong>Taxonomy construction:</strong> Scientific classification systems</li>
                                    <li><strong>Social network analysis:</strong> Community structure at multiple scales</li>
                                    <li><strong>Market segmentation:</strong> Customer hierarchy and sub-segments</li>
                                    <li><strong>Gene expression analysis:</strong> Co-expression patterns and pathways</li>
                                </ul>
                            </div>
                        </div>

                        <h3>Mathematical Framework</h3>
                        <p>Hierarchical clustering can be formalized through mathematical structures that capture the nested nature of cluster relationships.</p>

                        <div class="model-box">
                            <h4>Mathematical Foundations</h4>
                            
                            <h5>Hierarchical Clustering Definition:</h5>
                            <div class="formula-box">
                                <p>A hierarchical clustering of a set X = &#123;x₁, x₂, ..., xₙ&#125; is a sequence of partitions:</p>
                                <div class="formula-display">
                                    <strong>P₀, P₁, P₂, ..., Pₙ₋₁</strong>
                                </div>
                                <p>Where:</p>
                                <ul>
                                    <li><strong>P₀ = &#123;&#123;x₁&#125;, &#123;x₂&#125;, ..., &#123;xₙ&#125;&#125;:</strong> Each point is its own cluster</li>
                                    <li><strong>Pₙ₋₁ = &#123;X&#125;:</strong> All points in single cluster</li>
                                    <li><strong>Nested property:</strong> Each partition is a refinement of the next</li>
                                </ul>
                            </div>
                            
                            <h5>Dendrogram Representation:</h5>
                            <div class="formula-box">
                                <p>A dendrogram is a binary tree T where:</p>
                                <ul>
                                    <li><strong>Leaves:</strong> Correspond to individual data points</li>
                                    <li><strong>Internal nodes:</strong> Represent cluster merges (agglomerative) or splits (divisive)</li>
                                    <li><strong>Heights:</strong> Encode dissimilarity at which merges/splits occur</li>
                                    <li><strong>Cuts:</strong> Horizontal cuts through tree give different clusterings</li>
                                </ul>
                            </div>
                            
                            <h5>Ultrametric Property:</h5>
                            <div class="formula-box">
                                <p>For hierarchical clustering to be consistent, the distance function should satisfy:</p>
                                <div class="formula-display">
                                    <strong>d(x, z) ≤ max&#123;d(x, y), d(y, z)&#125;</strong>
                                </div>
                                <p>This ultrametric inequality is stronger than the triangle inequality and ensures hierarchical consistency.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Mathematical Foundation Section -->
                    <div id="theory" class="content-section">
                        <h2>Mathematical Theory of Hierarchical Clustering</h2>
                        
                        <p>The theoretical foundations of hierarchical clustering rest on concepts from metric geometry, graph theory, and discrete optimization. Understanding these mathematical principles provides insight into when hierarchical methods work well and what properties we can expect from the resulting cluster hierarchies.</p>

                        <h3>Ultrametric Spaces and Hierarchical Consistency</h3>
                        <p>The most important theoretical concept in hierarchical clustering is the relationship between ultrametric spaces and tree representations.</p>

                        <div class="theorem-box">
                            <h4>Ultrametric Spaces</h4>
                            
                            <h5>Definition:</h5>
                            <div class="formula-display">
                                <p>A metric space (X, d) is called ultrametric if for all x, y, z ∈ X:</p>
                                <strong>d(x, z) ≤ max{d(x, y), d(y, z)}</strong>
                                <p>This is stronger than the triangle inequality: d(x, z) ≤ d(x, y) + d(y, z)</p>
                            </div>
                            
                            <h5>Properties of Ultrametric Spaces:</h5>
                            <ul>
                                <li><strong>Strong triangle inequality:</strong> Distances are more constrained</li>
                                <li><strong>Isosceles triangles:</strong> Every triangle has two equal sides (the longer ones)</li>
                                <li><strong>Nested ball property:</strong> Any two balls are either disjoint or one contains the other</li>
                                <li><strong>Tree representation:</strong> Can be exactly represented as a tree with edge weights</li>
                            </ul>
                            
                            <h5>Fundamental Theorem:</h5>
                            <div class="formula-display">
                                <p><strong>Theorem (Ultrametric Tree Representation):</strong></p>
                                <p>A finite metric space (X, d) is ultrametric if and only if it can be isometrically embedded in a weighted tree where distances between leaves equal tree path lengths.</p>
                            </div>
                        </div>

                        <h3>Hierarchical Clustering Axioms</h3>
                        <p>Kleinberg's famous impossibility theorem characterizes hierarchical clustering through three natural axioms.</p>

                        <div class="proof-box">
                            <h4>Kleinberg's Impossibility Theorem (2003)</h4>
                            
                            <h5>The Three Axioms:</h5>
                            <div class="formula-display">
                                <p><strong>A1. Scale Invariance:</strong> Multiplying all distances by a positive constant doesn't change the clustering.</p>
                                <p><strong>A2. Richness:</strong> For any partition of the data, there exists a distance function that produces this partition.</p>
                                <p><strong>A3. Consistency:</strong> If distances within clusters decrease or distances between clusters increase, the clustering shouldn't change.</p>
                            </div>
                            
                            <h5>The Impossibility Result:</h5>
                            <div class="formula-display">
                                <p><strong>Theorem:</strong> No hierarchical clustering function can satisfy all three axioms simultaneously.</p>
                                <p><strong>Implication:</strong> Any hierarchical clustering algorithm must violate at least one intuitively reasonable property.</p>
                            </div>
                            
                            <h5>Practical Implications:</h5>
                            <ul>
                                <li><strong>No perfect algorithm:</strong> All methods have theoretical limitations</li>
                                <li><strong>Trade-offs necessary:</strong> Must choose which axiom to violate</li>
                                <li><strong>Context matters:</strong> Algorithm choice depends on application requirements</li>
                            </ul>
                        </div>

                        <h3>Linkage Criteria and Their Properties</h3>
                        <p>Different linkage criteria define how to measure distance between clusters, leading to different theoretical properties.</p>

                        <div class="linkage-box">
                            <h4>Mathematical Formulation of Linkage Criteria</h4>
                            
                            <h5>General Framework:</h5>
                            <p>For clusters A and B, define inter-cluster distance as:</p>
                            <div class="formula-display">
                                <strong>D(A, B) = f({d(a, b) : a ∈ A, b ∈ B})</strong>
                            </div>
                            
                            <h5>Specific Linkage Criteria:</h5>
                            <div class="comparison-table">
                                <table>
                                    <thead>
                                        <tr>
                                            <th>Linkage</th>
                                            <th>Formula</th>
                                            <th>Properties</th>
                                            <th>Cluster Shape Bias</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td><strong>Single</strong></td>
                                            <td>min{d(a,b) : a∈A, b∈B}</td>
                                            <td>Chaining effect, connects via closest points</td>
                                            <td>Elongated, irregular</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Complete</strong></td>
                                            <td>max{d(a,b) : a∈A, b∈B}</td>
                                            <td>Compact clusters, robust to outliers</td>
                                            <td>Spherical, compact</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Average</strong></td>
                                            <td>(1/|A||B|) Σ d(a,b)</td>
                                            <td>Balanced approach, moderate chaining</td>
                                            <td>Variable, balanced</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Ward</strong></td>
                                            <td>Minimize increase in WSS</td>
                                            <td>Minimizes variance, equal-sized clusters</td>
                                            <td>Spherical, equal-sized</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Mathematical Theory</h4>
                            <p><strong>Image Description:</strong> A 2x2 grid illustrating hierarchical clustering theory. Top-left: Ultrametric space showing the strong triangle inequality with three points where the longest side equals one of the shorter sides. Top-right: Tree representation of the same ultrametric space with edge weights. Bottom-left: Kleinberg's impossibility theorem demonstration showing how the three axioms lead to contradiction. Bottom-right: Comparison of different linkage criteria showing how they produce different cluster shapes on the same data.</p>
                            <p><em>This demonstrates the mathematical foundations that govern hierarchical clustering behavior</em></p>
                        </div>
                    </div>

                    <!-- Agglomerative Methods Section -->
                    <div id="agglomerative" class="content-section">
                        <h2>Agglomerative Methods</h2>
                        
                        <p>Agglomerative clustering, also known as bottom-up hierarchical clustering, starts with each data point as its own cluster and iteratively merges the most similar clusters until all points belong to a single cluster. This approach is the most commonly used hierarchical clustering method due to its conceptual simplicity and computational efficiency.</p>

                        <h3>Basic Agglomerative Algorithm</h3>
                        <p>The fundamental agglomerative clustering algorithm follows a simple but powerful iterative process.</p>

                        <div class="algorithm-box">
                            <h4>Agglomerative Clustering Algorithm</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
                                    <pre><code><strong>function</strong> agglomerative_clustering(X, linkage):
    n = X.shape[0]
    clusters = [{i} <strong>for</strong> i <strong>in</strong> range(n)]  <span class="comment">// Each point is its own cluster</span>
    dendrogram = []
    
    <span class="comment">// Step 1: Compute initial distance matrix</span>
    distance_matrix = compute_distances(X)
    
    <span class="comment">// Step 2: Iteratively merge closest clusters</span>
    <strong>for</strong> step = 1 <strong>to</strong> n-1:
        <span class="comment">// Find closest pair of clusters</span>
        min_distance = infinity
        merge_i, merge_j = -1, -1
        
        <strong>for</strong> i = 0 <strong>to</strong> len(clusters)-1:
            <strong>for</strong> j = i+1 <strong>to</strong> len(clusters)-1:
                dist = linkage_distance(clusters[i], clusters[j], distance_matrix, linkage)
                <strong>if</strong> dist < min_distance:
                    min_distance = dist
                    merge_i, merge_j = i, j
        
        <span class="comment">// Merge clusters and record in dendrogram</span>
        new_cluster = clusters[merge_i] ∪ clusters[merge_j]
        dendrogram.append((merge_i, merge_j, min_distance))
        
        <span class="comment">// Update cluster list</span>
        clusters.remove(clusters[merge_j])  <span class="comment">// Remove second cluster</span>
        clusters[merge_i] = new_cluster      <span class="comment">// Update first cluster</span>
    
    <strong>return</strong> dendrogram</code></pre>
                                </div>
                            </div>
                            
                            <h5>Key Steps:</h5>
                            <ul>
                                <li><strong>Initialization:</strong> Each data point starts as its own cluster</li>
                                <li><strong>Distance computation:</strong> Calculate pairwise distances between all points</li>
                                <li><strong>Iterative merging:</strong> Find and merge the closest pair of clusters</li>
                                <li><strong>Linkage criterion:</strong> Use specified method to measure cluster distances</li>
                                <li><strong>Dendrogram construction:</strong> Record merge history with heights</li>
                            </ul>
                            
                            <h5>Time Complexity:</h5>
                            <div class="formula-display">
                                <p><strong>O(n³):</strong> For each of n-1 merges, examine O(n²) cluster pairs</p>
                                <p>Can be optimized to O(n² log n) using efficient data structures</p>
                            </div>
                        </div>

                        <h3>Linkage Criteria in Agglomerative Clustering</h3>
                        <p>The choice of linkage criterion determines how distances between clusters are calculated, significantly affecting the resulting hierarchy.</p>

                        <div class="explanation-box">
                            <h4>Common Linkage Criteria</h4>
                            
                            <h5>Single Linkage (Minimum):</h5>
                            <div class="formula-display">
                                <strong>D(A,B) = min{d(a,b) : a ∈ A, b ∈ B}</strong>
                                <p>Uses the minimum distance between any two points in different clusters.</p>
                            </div>
                            
                            <h5>Complete Linkage (Maximum):</h5>
                            <div class="formula-display">
                                <strong>D(A,B) = max{d(a,b) : a ∈ A, b ∈ B}</strong>
                                <p>Uses the maximum distance between any two points in different clusters.</p>
                            </div>
                            
                            <h5>Average Linkage (UPGMA):</h5>
                            <div class="formula-display">
                                <strong>D(A,B) = (1/|A||B|) Σ_{a∈A} Σ_{b∈B} d(a,b)</strong>
                                <p>Uses the average distance between all pairs of points in different clusters.</p>
                            </div>
                            
                            <h5>Ward's Linkage:</h5>
                            <div class="formula-display">
                                <strong>D(A,B) = (|A||B|)/(|A|+|B|) ||μ_A - μ_B||²</strong>
                                <p>Minimizes the increase in within-cluster sum of squares.</p>
                            </div>
                        </div>

                        <h3>Computational Optimizations</h3>
                        <p>Several optimization techniques can significantly improve the efficiency of agglomerative clustering.</p>

                        <div class="explanation-box">
                            <h4>Efficiency Improvements</h4>
                            
                            <h5>Lance-Williams Formula:</h5>
                            <p>For updating distances after merging clusters A and B into cluster C:</p>
                            <div class="formula-display">
                                <strong>D(C,D) = α_A·D(A,D) + α_B·D(B,D) + β·D(A,B) + γ·|D(A,D) - D(B,D)|</strong>
                            </div>
                            <p>Where α_A, α_B, β, γ are coefficients that depend on the linkage criterion.</p>
                            
                            <h5>Heap-based Implementation:</h5>
                            <ul>
                                <li><strong>Priority queue:</strong> Maintain closest cluster pairs in a heap</li>
                                <li><strong>Lazy updates:</strong> Only update distances when necessary</li>
                                <li><strong>Complexity reduction:</strong> O(n² log n) instead of O(n³)</li>
                            </ul>
                            
                            <h5>Memory Optimization:</h5>
                            <ul>
                                <li><strong>Triangular storage:</strong> Store only upper triangle of distance matrix</li>
                                <li><strong>Incremental computation:</strong> Compute distances on-demand</li>
                                <li><strong>Chunked processing:</strong> Process large datasets in batches</li>
                            </ul>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Agglomerative Clustering Process</h4>
                            <p><strong>Image Description:</strong> A step-by-step visualization of agglomerative clustering. Top row: Initial state with each point as its own cluster, then first merge of closest points. Middle row: Progressive merging showing how clusters grow and merge. Bottom row: Final dendrogram showing the complete hierarchy with merge heights, and a comparison of different linkage criteria on the same data showing how they produce different cluster structures.</p>
                            <p><em>This demonstrates the bottom-up construction of hierarchical clusters</em></p>
                        </div>
                    </div>

                    <!-- Divisive Methods Section -->
                    <div id="divisive" class="content-section">
                        <h2>Divisive Methods</h2>
                        
                        <p>Divisive clustering, also known as top-down hierarchical clustering, takes the opposite approach to agglomerative methods. It starts with all data points in a single cluster and iteratively splits the most heterogeneous cluster until each point forms its own cluster. While less commonly used due to computational complexity, divisive methods can be more effective for certain types of data.</p>

                        <h3>Basic Divisive Algorithm</h3>
                        <p>The fundamental divisive clustering algorithm follows a top-down approach, starting with all points in one cluster.</p>

                        <div class="algorithm-box">
                            <h4>Divisive Clustering Algorithm</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
                                    <pre><code><strong>function</strong> divisive_clustering(X, split_criterion):
    n = X.shape[0]
    clusters = [set(range(n))]  <span class="comment">// All points in single cluster</span>
    dendrogram = []
    
    <span class="comment">// Step 1: Iteratively split most heterogeneous cluster</span>
    <strong>for</strong> step = 1 <strong>to</strong> n-1:
        <span class="comment">// Find cluster with highest heterogeneity</span>
        max_heterogeneity = -1
        split_cluster_idx = -1
        
        <strong>for</strong> i = 0 <strong>to</strong> len(clusters)-1:
            heterogeneity = compute_heterogeneity(clusters[i], X, split_criterion)
            <strong>if</strong> heterogeneity > max_heterogeneity:
                max_heterogeneity = heterogeneity
                split_cluster_idx = i
        
        <span class="comment">// Split the most heterogeneous cluster</span>
        cluster_to_split = clusters[split_cluster_idx]
        left_cluster, right_cluster = split_cluster(cluster_to_split, X, split_criterion)
        
        <span class="comment">// Record split in dendrogram</span>
        dendrogram.append((split_cluster_idx, left_cluster, right_cluster, max_heterogeneity))
        
        <span class="comment">// Update cluster list</span>
        clusters.remove(cluster_to_split)
        clusters.extend([left_cluster, right_cluster])
    
    <strong>return</strong> dendrogram</code></pre>
                                </div>
                            </div>
                            
                            <h5>Key Steps:</h5>
                            <ul>
                                <li><strong>Initialization:</strong> All points start in a single cluster</li>
                                <li><strong>Heterogeneity calculation:</strong> Measure how spread out each cluster is</li>
                                <li><strong>Cluster selection:</strong> Choose the most heterogeneous cluster to split</li>
                                <li><strong>Optimal splitting:</strong> Find the best way to divide the selected cluster</li>
                                <li><strong>Dendrogram construction:</strong> Record split history with heights</li>
                            </ul>
                        </div>

                        <h3>Split Criteria and Methods</h3>
                        <p>The choice of split criterion determines how clusters are divided, significantly affecting the resulting hierarchy.</p>

                        <div class="explanation-box">
                            <h4>Common Split Criteria</h4>
                            
                            <h5>Diameter-based Splitting:</h5>
                            <div class="formula-display">
                                <strong>Heterogeneity(C) = max{d(x,y) : x,y ∈ C}</strong>
                                <p>Measures the maximum distance between any two points in the cluster.</p>
                            </div>
                            
                            <h5>Radius-based Splitting:</h5>
                            <div class="formula-display">
                                <strong>Heterogeneity(C) = min_{c} max_{x∈C} d(x,c)</strong>
                                <p>Measures the radius of the smallest ball containing all points in the cluster.</p>
                            </div>
                            
                            <h5>Variance-based Splitting:</h5>
                            <div class="formula-display">
                                <strong>Heterogeneity(C) = Σ_{x∈C} ||x - μ_C||²</strong>
                                <p>Measures the within-cluster sum of squares (WCSS).</p>
                            </div>
                            
                            <h5>K-means Splitting:</h5>
                            <div class="formula-display">
                                <strong>Split using 2-means on cluster points</strong>
                                <p>Uses K-means with k=2 to find optimal binary split.</p>
                            </div>
                        </div>

                        <h3>Computational Challenges</h3>
                        <p>Divisive methods face significant computational challenges that limit their practical applicability.</p>

                        <div class="explanation-box">
                            <h4>Complexity Issues</h4>
                            
                            <h5>Exponential Complexity:</h5>
                            <ul>
                                <li><strong>Optimal splitting:</strong> Finding optimal binary split is NP-hard</li>
                                <li><strong>Exhaustive search:</strong> 2^n possible ways to split n points</li>
                                <li><strong>Heuristic required:</strong> Must use approximation algorithms</li>
                            </ul>
                            
                            <h5>Common Heuristics:</h5>
                            <ul>
                                <li><strong>K-means splitting:</strong> Use 2-means to find approximate optimal split</li>
                                <li><strong>Principal component splitting:</strong> Split along first principal component</li>
                                <li><strong>Furthest pair splitting:</strong> Use two most distant points as initial centroids</li>
                                <li><strong>Random splitting:</strong> Randomly assign points to two subclusters</li>
                            </ul>
                            
                            <h5>Time Complexity:</h5>
                            <div class="formula-display">
                                <p><strong>O(n²) to O(2^n):</strong> Depending on split method used</p>
                                <p>K-means splitting: O(n²) per split, O(n³) total</p>
                            </div>
                        </div>

                        <h3>Advantages and Disadvantages</h3>
                        <p>Divisive methods have specific strengths and weaknesses compared to agglomerative approaches.</p>

                        <div class="explanation-box">
                            <h4>Advantages of Divisive Methods</h4>
                            <ul>
                                <li><strong>Global perspective:</strong> Considers entire dataset when making splits</li>
                                <li><strong>Better for large clusters:</strong> Can identify major cluster boundaries early</li>
                                <li><strong>Natural for some data:</strong> Works well when data has clear hierarchical structure</li>
                                <li><strong>Interpretable splits:</strong> Each split can be understood in terms of data structure</li>
                            </ul>
                            
                            <h4>Disadvantages of Divisive Methods</h4>
                            <ul>
                                <li><strong>Computational cost:</strong> Much more expensive than agglomerative methods</li>
                                <li><strong>Heuristic dependence:</strong> Quality depends heavily on split method choice</li>
                                <li><strong>Local optima:</strong> Early splits can lead to poor overall hierarchy</li>
                                <li><strong>Limited scalability:</strong> Difficult to apply to large datasets</li>
                            </ul>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Divisive Clustering Process</h4>
                            <p><strong>Image Description:</strong> A step-by-step visualization of divisive clustering. Top row: Initial state with all points in one cluster, then first split showing how the most heterogeneous cluster is divided. Middle row: Progressive splitting showing how clusters are recursively divided. Bottom row: Final dendrogram showing the complete hierarchy with split heights, and a comparison with agglomerative clustering on the same data showing how the two approaches can produce different structures.</p>
                            <p><em>This demonstrates the top-down construction of hierarchical clusters</em></p>
                        </div>
                    </div>

                    <!-- Dendrogram Analysis Section -->
                    <div id="dendrograms" class="content-section">
                        <h2>Dendrogram Analysis</h2>
                        
                        <p>Dendrograms are the primary visualization tool for hierarchical clustering results, providing a comprehensive view of the clustering hierarchy. Understanding how to read, interpret, and analyze dendrograms is crucial for extracting meaningful insights from hierarchical clustering.</p>

                        <h3>Dendrogram Structure and Components</h3>
                        <p>A dendrogram is a tree-like diagram that represents the hierarchical clustering process, showing how clusters are merged or split at different levels.</p>

                        <div class="dendrogram-box">
                            <h4>Key Components of a Dendrogram</h4>
                            
                            <h5>Leaves (Terminal Nodes):</h5>
                            <ul>
                                <li><strong>Individual data points:</strong> Each leaf represents one data point</li>
                                <li><strong>Bottom level:</strong> Located at the bottom of the dendrogram</li>
                                <li><strong>Height zero:</strong> All leaves are at height 0</li>
                            </ul>
                            
                            <h5>Internal Nodes (Merge Points):</h5>
                            <ul>
                                <li><strong>Cluster merges:</strong> Represent the merging of two clusters</li>
                                <li><strong>Merge height:</strong> Height indicates dissimilarity at which merge occurred</li>
                                <li><strong>Binary structure:</strong> Each internal node has exactly two children</li>
                            </ul>
                            
                            <h5>Root (Top Node):</h5>
                            <ul>
                                <li><strong>Single cluster:</strong> Represents the cluster containing all data points</li>
                                <li><strong>Maximum height:</strong> Located at the highest point of the dendrogram</li>
                                <li><strong>Complete hierarchy:</strong> Root contains the entire clustering hierarchy</li>
                            </ul>
                        </div>

                        <h3>Reading and Interpreting Dendrograms</h3>
                        <p>Proper interpretation of dendrograms requires understanding the relationship between height, distance, and cluster structure.</p>

                        <div class="explanation-box">
                            <h4>Height and Distance Interpretation</h4>
                            
                            <h5>Height Meaning:</h5>
                            <ul>
                                <li><strong>Merge height:</strong> Distance between clusters when they were merged</li>
                                <li><strong>Cluster separation:</strong> Higher merges indicate more distinct clusters</li>
                                <li><strong>Relative importance:</strong> Height differences show cluster quality</li>
                            </ul>
                            
                            <h5>Cutting the Dendrogram:</h5>
                            <ul>
                                <li><strong>Horizontal cuts:</strong> Create flat clusterings at different levels</li>
                                <li><strong>Number of clusters:</strong> Determined by number of branches intersected</li>
                                <li><strong>Cluster membership:</strong> Points in same subtree belong to same cluster</li>
                            </ul>
                            
                            <h5>Cluster Quality Assessment:</h5>
                            <ul>
                                <li><strong>Compact clusters:</strong> Low merge heights indicate tight clusters</li>
                                <li><strong>Well-separated clusters:</strong> High merge heights indicate distinct clusters</li>
                                <li><strong>Natural number of clusters:</strong> Look for large height jumps</li>
                            </ul>
                        </div>

                        <h3>Dendrogram Cutting Strategies</h3>
                        <p>Determining where to cut the dendrogram to obtain a final clustering is a critical decision in hierarchical clustering analysis.</p>

                        <div class="explanation-box">
                            <h4>Common Cutting Methods</h4>
                            
                            <h5>Fixed Number of Clusters:</h5>
                            <ul>
                                <li><strong>K-cluster cut:</strong> Cut to obtain exactly k clusters</li>
                                <li><strong>Simple approach:</strong> Cut at height that produces k clusters</li>
                                <li><strong>Limitation:</strong> May not respect natural cluster boundaries</li>
                            </ul>
                            
                            <h5>Height-based Cutting:</h5>
                            <ul>
                                <li><strong>Fixed height:</strong> Cut at a specific dissimilarity threshold</li>
                                <li><strong>Natural breaks:</strong> Look for large gaps in merge heights</li>
                                <li><strong>Elbow method:</strong> Find point of maximum curvature in height profile</li>
                            </ul>
                            
                            <h5>Statistical Methods:</h5>
                            <ul>
                                <li><strong>Gap statistic:</strong> Compare within-cluster dispersion to random data</li>
                                <li><strong>Silhouette analysis:</strong> Maximize silhouette coefficient</li>
                                <li><strong>Bootstrap validation:</strong> Assess stability across resamples</li>
                            </ul>
                        </div>

                        <h3>Dendrogram Validation and Quality Assessment</h3>
                        <p>Evaluating the quality and reliability of dendrograms is essential for making informed clustering decisions.</p>

                        <div class="explanation-box">
                            <h4>Validation Techniques</h4>
                            
                            <h5>Internal Validation:</h5>
                            <ul>
                                <li><strong>Cophenetic correlation:</strong> Measure how well dendrogram preserves original distances</li>
                                <li><strong>Inconsistency coefficient:</strong> Identify potentially unreliable merges</li>
                                <li><strong>Height analysis:</strong> Examine distribution of merge heights</li>
                            </ul>
                            
                            <h5>External Validation:</h5>
                            <ul>
                                <li><strong>Known labels:</strong> Compare with ground truth if available</li>
                                <li><strong>Expert knowledge:</strong> Validate against domain expertise</li>
                                <li><strong>Cross-validation:</strong> Test stability on different data subsets</li>
                            </ul>
                            
                            <h5>Robustness Assessment:</h5>
                            <ul>
                                <li><strong>Bootstrap resampling:</strong> Test stability under data perturbations</li>
                                <li><strong>Noise sensitivity:</strong> Assess robustness to outliers</li>
                                <li><strong>Parameter sensitivity:</strong> Test sensitivity to linkage choice</li>
                            </ul>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Dendrogram Analysis</h4>
                            <p><strong>Image Description:</strong> A comprehensive dendrogram analysis visualization. Top panel: Complete dendrogram with different cutting levels highlighted in different colors. Middle panel: Height profile showing merge heights and potential cutting points. Bottom panel: Comparison of different cutting strategies showing how they produce different clusterings, with quality metrics displayed for each approach.</p>
                            <p><em>This demonstrates the comprehensive analysis of dendrogram structure and cutting strategies</em></p>
                        </div>
                    </div>

                    <!-- Complexity Analysis Section -->
                    <div id="complexity" class="content-section">
                        <h2>Complexity Analysis</h2>
                        
                        <p>Understanding the computational complexity of hierarchical clustering algorithms is crucial for assessing their scalability and practical applicability. The complexity varies significantly between different approaches and linkage criteria.</p>

                        <h3>Time Complexity Analysis</h3>
                        <p>The time complexity of hierarchical clustering depends on the specific algorithm and linkage criterion used.</p>

                        <div class="explanation-box">
                            <h4>Agglomerative Clustering Complexity</h4>
                            
                            <h5>Basic Algorithm:</h5>
                            <ul>
                                <li><strong>Distance matrix computation:</strong> O(n²) for n data points</li>
                                <li><strong>Iterative merging:</strong> O(n³) for n-1 merge operations</li>
                                <li><strong>Total complexity:</strong> O(n³) for most linkage criteria</li>
                            </ul>
                            
                            <h5>Linkage-specific Complexity:</h5>
                            <ul>
                                <li><strong>Single linkage:</strong> O(n²) using MST algorithms</li>
                                <li><strong>Complete linkage:</strong> O(n² log n) with efficient data structures</li>
                                <li><strong>Average linkage:</strong> O(n² log n) with heap-based implementation</li>
                                <li><strong>Ward's method:</strong> O(n² log n) with optimized updates</li>
                            </ul>
                            
                            <h5>Optimization Techniques:</h5>
                            <ul>
                                <li><strong>Heap-based implementation:</strong> Reduces complexity to O(n² log n)</li>
                                <li><strong>Lance-Williams formula:</strong> Enables efficient distance updates</li>
                                <li><strong>Memory optimization:</strong> Reduces space complexity</li>
                            </ul>
                        </div>

                        <h3>Space Complexity Analysis</h3>
                        <p>Memory requirements are a significant limiting factor for hierarchical clustering on large datasets.</p>

                        <div class="explanation-box">
                            <h4>Memory Requirements</h4>
                            
                            <h5>Distance Matrix Storage:</h5>
                            <ul>
                                <li><strong>Full matrix:</strong> O(n²) space for n×n distance matrix</li>
                                <li><strong>Triangular storage:</strong> O(n²/2) space for upper triangle only</li>
                                <li><strong>Memory bottleneck:</strong> Limits dataset size to ~10,000 points</li>
                            </ul>
                            
                            <h5>Optimization Strategies:</h5>
                            <ul>
                                <li><strong>Incremental computation:</strong> Compute distances on-demand</li>
                                <li><strong>Chunked processing:</strong> Process data in batches</li>
                                <li><strong>Approximate methods:</strong> Use sampling for large datasets</li>
                                <li><strong>External memory:</strong> Store matrix on disk for very large datasets</li>
                            </ul>
                        </div>

                        <h3>Scalability Challenges and Solutions</h3>
                        <p>Hierarchical clustering faces significant scalability challenges that require specialized approaches for large datasets.</p>

                        <div class="explanation-box">
                            <h4>Scalability Issues</h4>
                            
                            <h5>Computational Bottlenecks:</h5>
                            <ul>
                                <li><strong>Quadratic growth:</strong> Time complexity grows quadratically with data size</li>
                                <li><strong>Memory limitations:</strong> Distance matrix becomes prohibitively large</li>
                                <li><strong>Cache efficiency:</strong> Poor memory access patterns for large matrices</li>
                            </ul>
                            
                            <h5>Approximate Solutions:</h5>
                            <ul>
                                <li><strong>Sampling methods:</strong> Cluster a sample, assign remaining points</li>
                                <li><strong>Incremental clustering:</strong> Build hierarchy incrementally</li>
                                <li><strong>Parallel algorithms:</strong> Distribute computation across multiple cores</li>
                                <li><strong>GPU acceleration:</strong> Use parallel processing for distance computations</li>
                            </ul>
                        </div>

                        <h3>Comparison with Other Clustering Methods</h3>
                        <p>Understanding how hierarchical clustering compares to other methods helps in algorithm selection.</p>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>Time Complexity</th>
                                        <th>Space Complexity</th>
                                        <th>Scalability</th>
                                        <th>Output</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Hierarchical (Agglomerative)</strong></td>
                                        <td>O(n² log n)</td>
                                        <td>O(n²)</td>
                                        <td>Poor (n < 10,000)</td>
                                        <td>Complete hierarchy</td>
                                    </tr>
                                    <tr>
                                        <td><strong>K-means</strong></td>
                                        <td>O(nkt)</td>
                                        <td>O(n + k)</td>
                                        <td>Good (n < 1,000,000)</td>
                                        <td>Flat clustering</td>
                                    </tr>
                                    <tr>
                                        <td><strong>DBSCAN</strong></td>
                                        <td>O(n log n)</td>
                                        <td>O(n)</td>
                                        <td>Excellent</td>
                                        <td>Density-based clusters</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Gaussian Mixture</strong></td>
                                        <td>O(nkt)</td>
                                        <td>O(n + k)</td>
                                        <td>Good</td>
                                        <td>Probabilistic clusters</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Complexity Analysis</h4>
                            <p><strong>Image Description:</strong> A comprehensive complexity analysis visualization. Top panel: Time complexity comparison showing how different algorithms scale with dataset size. Middle panel: Memory usage comparison showing space requirements for different methods. Bottom panel: Scalability limits showing maximum dataset sizes for different approaches, with practical recommendations for algorithm selection.</p>
                            <p><em>This demonstrates the computational trade-offs in hierarchical clustering</em></p>
                        </div>
                    </div>

                    <!-- Applications Section -->
                    <div id="applications" class="content-section">
                        <h2>Applications</h2>
                        
                        <p>Hierarchical clustering finds applications across diverse domains where understanding data relationships and hierarchical structures is crucial. Its ability to provide complete dendrograms makes it valuable for exploratory data analysis and domain-specific clustering tasks.</p>

                        <h3>Biological and Medical Applications</h3>
                        <p>Hierarchical clustering is extensively used in bioinformatics and medical research for analyzing genetic and protein data.</p>

                        <div class="explanation-box">
                            <h4>Gene Expression Analysis</h4>
                            <p>Hierarchical clustering helps identify co-expressed genes and functional gene groups:</p>
                            <ul>
                                <li><strong>Microarray data analysis:</strong> Cluster genes with similar expression patterns</li>
                                <li><strong>Disease classification:</strong> Identify disease subtypes based on gene expression</li>
                                <li><strong>Drug discovery:</strong> Group compounds with similar mechanisms of action</li>
                                <li><strong>Pathway analysis:</strong> Discover biological pathways and regulatory networks</li>
                            </ul>
                            
                            <h4>Phylogenetic Analysis</h4>
                            <p>Used to construct evolutionary trees and study species relationships:</p>
                            <ul>
                                <li><strong>Species classification:</strong> Build phylogenetic trees from genetic data</li>
                                <li><strong>Evolutionary studies:</strong> Analyze evolutionary relationships and divergence</li>
                                <li><strong>Conservation biology:</strong> Identify genetically distinct populations</li>
                            </ul>
                        </div>

                        <h3>Social and Behavioral Sciences</h3>
                        <p>Hierarchical clustering provides insights into social structures and behavioral patterns.</p>

                        <div class="explanation-box">
                            <h4>Market Segmentation</h4>
                            <p>Businesses use hierarchical clustering to understand customer behavior:</p>
                            <ul>
                                <li><strong>Customer profiling:</strong> Group customers with similar purchasing patterns</li>
                                <li><strong>Product positioning:</strong> Identify market segments for targeted marketing</li>
                                <li><strong>Brand analysis:</strong> Understand brand perception and positioning</li>
                            </ul>
                            
                            <h4>Social Network Analysis</h4>
                            <p>Analyze social structures and community formation:</p>
                            <ul>
                                <li><strong>Community detection:</strong> Identify social groups and communities</li>
                                <li><strong>Influence analysis:</strong> Study information flow and influence patterns</li>
                                <li><strong>Behavioral clustering:</strong> Group users with similar online behavior</li>
                            </ul>
                        </div>

                        <h3>Image and Document Analysis</h3>
                        <p>Hierarchical clustering is valuable for organizing and analyzing large collections of images and documents.</p>

                        <div class="explanation-box">
                            <h4>Image Clustering</h4>
                            <p>Organize and categorize image collections:</p>
                            <ul>
                                <li><strong>Content-based retrieval:</strong> Group similar images for search systems</li>
                                <li><strong>Facial recognition:</strong> Cluster face images by identity</li>
                                <li><strong>Medical imaging:</strong> Classify medical images by pathology</li>
                                <li><strong>Satellite imagery:</strong> Analyze land use and environmental changes</li>
                            </ul>
                            
                            <h4>Text Mining and NLP</h4>
                            <p>Organize and analyze text documents:</p>
                            <ul>
                                <li><strong>Document clustering:</strong> Group similar documents for organization</li>
                                <li><strong>Topic modeling:</strong> Discover topics in large text collections</li>
                                <li><strong>Author identification:</strong> Group documents by writing style</li>
                                <li><strong>Sentiment analysis:</strong> Cluster text by emotional content</li>
                            </ul>
                        </div>

                        <h3>Geographic and Environmental Applications</h3>
                        <p>Hierarchical clustering helps analyze spatial patterns and environmental data.</p>

                        <div class="explanation-box">
                            <h4>Spatial Analysis</h4>
                            <p>Analyze geographic patterns and relationships:</p>
                            <ul>
                                <li><strong>Urban planning:</strong> Identify similar neighborhoods and districts</li>
                                <li><strong>Epidemiology:</strong> Study disease spread patterns</li>
                                <li><strong>Crime analysis:</strong> Identify crime hotspots and patterns</li>
                                <li><strong>Transportation:</strong> Optimize routes and service areas</li>
                            </ul>
                            
                            <h4>Environmental Monitoring</h4>
                            <p>Analyze environmental data and patterns:</p>
                            <ul>
                                <li><strong>Climate analysis:</strong> Group regions with similar climate patterns</li>
                                <li><strong>Ecosystem studies:</strong> Analyze species distribution and habitats</li>
                                <li><strong>Pollution monitoring:</strong> Identify pollution sources and patterns</li>
                            </ul>
                        </div>

                        <h3>Financial and Economic Applications</h3>
                        <p>Hierarchical clustering provides insights into financial markets and economic patterns.</p>

                        <div class="explanation-box">
                            <h4>Portfolio Management</h4>
                            <p>Analyze financial instruments and market behavior:</p>
                            <ul>
                                <li><strong>Asset clustering:</strong> Group similar financial instruments</li>
                                <li><strong>Risk analysis:</strong> Identify correlated risk factors</li>
                                <li><strong>Market segmentation:</strong> Understand market structure and dynamics</li>
                            </ul>
                            
                            <h4>Economic Analysis</h4>
                            <p>Study economic patterns and relationships:</p>
                            <ul>
                                <li><strong>Country clustering:</strong> Group countries by economic indicators</li>
                                <li><strong>Industry analysis:</strong> Identify similar industries and sectors</li>
                                <li><strong>Economic forecasting:</strong> Analyze economic cycles and trends</li>
                            </ul>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Application Domains</h4>
                            <p><strong>Image Description:</strong> A comprehensive overview of hierarchical clustering applications across different domains. The visualization shows six main application areas: Biological/Medical (gene expression, phylogenetics), Social/Behavioral (market segmentation, social networks), Image/Document (content retrieval, text mining), Geographic/Environmental (spatial analysis, climate), Financial/Economic (portfolio management, economic analysis), and Industrial/Manufacturing (quality control, process optimization). Each domain shows specific use cases with example datasets and clustering objectives.</p>
                            <p><em>This demonstrates the versatility of hierarchical clustering across diverse fields</em></p>
                        </div>
                    </div>

                    <!-- Interactive Demos Section -->
                    <div id="interactive" class="content-section">
                        <h2>Interactive Demos</h2>
                        
                        <p>Explore hierarchical clustering through interactive demonstrations that allow you to experiment with different algorithms, parameters, and datasets. These demos provide hands-on experience with the concepts discussed in this chapter.</p>

                        <h3>Demo 1: Linkage Criteria Comparison</h3>
                        <p>Compare different linkage criteria on the same dataset to understand their behavior and characteristics.</p>

                        <div class="interactive-container">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="linkage-dataset">Dataset:</label>
                                    <select id="linkage-dataset">
                                        <option value="blobs">Blob Clusters</option>
                                        <option value="moons">Moon Shapes</option>
                                        <option value="circles">Concentric Circles</option>
                                        <option value="random">Random Points</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="linkage-method">Linkage Method:</label>
                                    <select id="linkage-method">
                                        <option value="single">Single Linkage</option>
                                        <option value="complete">Complete Linkage</option>
                                        <option value="average">Average Linkage</option>
                                        <option value="ward">Ward's Method</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="linkage-clusters">Number of Clusters:</label>
                                    <input type="range" id="linkage-clusters" min="2" max="8" value="3">
                                    <span id="linkage-clusters-display">3</span>
                                </div>
                                
                                <button class="azbn-btn" onclick="generateLinkageDemo()">Generate Clustering</button>
                                <button class="azbn-btn azbn-secondary" onclick="resetLinkageDemo()">Reset</button>
                            </div>
                            
                            <div class="visualization-container">
                                <div class="visualization-panel">
                                    <h4>Data Points and Clusters</h4>
                                    <svg id="linkage-plot" width="400" height="300"></svg>
                                </div>
                                <div class="visualization-panel">
                                    <h4>Dendrogram</h4>
                                    <svg id="linkage-dendrogram" width="700" height="600"></svg>
                                </div>
                            </div>
                            
                            <div class="demo-metrics">
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <div class="metric-label">Silhouette Score</div>
                                        <div class="metric-value" id="linkage-silhouette">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Calinski-Harabasz Index</div>
                                        <div class="metric-value" id="linkage-calinski">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Davies-Bouldin Index</div>
                                        <div class="metric-value" id="linkage-davies">-</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <h3>Demo 2: Dendrogram Analysis</h3>
                        <p>Explore dendrogram construction and cutting strategies to understand hierarchical clustering results.</p>

                        <div class="interactive-container">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="dendro-dataset">Dataset:</label>
                                    <select id="dendro-dataset">
                                        <option value="blobs">Blob Clusters</option>
                                        <option value="moons">Moon Shapes</option>
                                        <option value="circles">Concentric Circles</option>
                                        <option value="random">Random Points</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="dendro-method">Linkage Method:</label>
                                    <select id="dendro-method">
                                        <option value="single">Single Linkage</option>
                                        <option value="complete">Complete Linkage</option>
                                        <option value="average">Average Linkage</option>
                                        <option value="ward">Ward's Method</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="dendro-cut">Cutting Strategy:</label>
                                    <select id="dendro-cut">
                                        <option value="height">Height-based</option>
                                        <option value="clusters">Number of Clusters</option>
                                        <option value="inconsistency">Inconsistency</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="dendro-threshold">Cutting Threshold:</label>
                                    <input type="range" id="dendro-threshold" min="0" max="100" value="50">
                                    <span id="dendro-threshold-display">50</span>
                                </div>
                                
                                <button class="azbn-btn" onclick="generateDendrogramDemo()">Generate Dendrogram</button>
                                <button class="azbn-btn azbn-secondary" onclick="resetDendrogramDemo()">Reset</button>
                            </div>
                            
                            <div class="visualization-container">
                                <div class="visualization-panel">
                                    <h4>Dendrogram with Cut Line</h4>
                                    <svg id="dendro-plot" width="700" height="600"></svg>
                                </div>
                                <div class="visualization-panel">
                                    <h4>Resulting Clusters</h4>
                                    <svg id="dendro-clusters" width="400" height="300"></svg>
                                </div>
                            </div>
                            
                            <div class="demo-metrics">
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <div class="metric-label">Number of Clusters</div>
                                        <div class="metric-value" id="dendro-num-clusters">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Cut Height</div>
                                        <div class="metric-value" id="dendro-cut-height">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Inconsistency Score</div>
                                        <div class="metric-value" id="dendro-inconsistency">-</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h4>Demo Instructions</h4>
                            <ul>
                                <li><strong>Linkage Criteria Comparison:</strong> Experiment with different linkage methods to see how they affect clustering results and dendrogram structure.</li>
                                <li><strong>Dendrogram Analysis:</strong> Explore different cutting strategies and thresholds to understand how to extract meaningful clusters from hierarchical structures.</li>
                                <li><strong>Parameter Effects:</strong> Observe how changing parameters affects the clustering quality metrics and visual results.</li>
                                <li><strong>Dataset Comparison:</strong> Test different datasets to understand how hierarchical clustering performs on various data structures.</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Test Your Hierarchical Clustering Knowledge</h2>
                        
                        <p>Evaluate your understanding of hierarchical clustering theory, linkage methods, and computational properties.</p>

                        <div class="enhanced-quiz-question">
                            <h4>Question 1: Agglomerative Clustering</h4>
                            <p>What is the main characteristic of agglomerative hierarchical clustering?</p>
                            <div class="margin-top">
                                <input type="radio" name="q1" value="a" id="q1a">
                                <label for="q1a">It starts with all points in separate clusters and merges them iteratively</label><br>
                                <input type="radio" name="q1" value="b" id="q1b">
                                <label for="q1b">It starts with one cluster and splits it iteratively</label><br>
                                <input type="radio" name="q1" value="c" id="q1c">
                                <label for="q1c">It uses a fixed number of clusters from the beginning</label><br>
                                <input type="radio" name="q1" value="d" id="q1d">
                                <label for="q1d">It only works with categorical data</label><br>
                            </div>
                            <button onclick="checkAnswer(1, 'a')" class="azbn-btn">Check Answer</button>
                            <div id="q1-result" class="margin-top"></div>
                        </div>

                        <div class="enhanced-quiz-question">
                            <h4>Question 2: Linkage Criteria</h4>
                            <p>Which linkage criterion is most sensitive to outliers?</p>
                            <div class="margin-top">
                                <input type="radio" name="q2" value="a" id="q2a">
                                <label for="q2a">Single linkage</label><br>
                                <input type="radio" name="q2" value="b" id="q2b">
                                <label for="q2b">Complete linkage</label><br>
                                <input type="radio" name="q2" value="c" id="q2c">
                                <label for="q2c">Average linkage</label><br>
                                <input type="radio" name="q2" value="d" id="q2d">
                                <label for="q2d">Ward's method</label><br>
                            </div>
                            <button onclick="checkAnswer(2, 'b')" class="azbn-btn">Check Answer</button>
                            <div id="q2-result" class="margin-top"></div>
                        </div>

                        <div class="enhanced-quiz-question">
                            <h4>Question 3: Time Complexity</h4>
                            <p>What is the time complexity of standard agglomerative hierarchical clustering?</p>
                            <div class="margin-top">
                                <input type="radio" name="q3" value="a" id="q3a">
                                <label for="q3a">O(n log n)</label><br>
                                <input type="radio" name="q3" value="b" id="q3b">
                                <label for="q3b">O(n²)</label><br>
                                <input type="radio" name="q3" value="c" id="q3c">
                                <label for="q3c">O(n² log n)</label><br>
                                <input type="radio" name="q3" value="d" id="q3d">
                                <label for="q3d">O(n³)</label><br>
                            </div>
                            <button onclick="checkAnswer(3, 'c')" class="azbn-btn">Check Answer</button>
                            <div id="q3-result" class="margin-top"></div>
                        </div>

                        <div class="enhanced-quiz-question">
                            <h4>Question 4: Dendrogram Properties</h4>
                            <p>What property makes dendrograms useful for understanding cluster relationships?</p>
                            <div class="margin-top">
                                <input type="radio" name="q4" value="a" id="q4a">
                                <label for="q4a">They show the complete hierarchy of cluster merges</label><br>
                                <input type="radio" name="q4" value="b" id="q4b">
                                <label for="q4b">They only show the final clustering result</label><br>
                                <input type="radio" name="q4" value="c" id="q4c">
                                <label for="q4c">They work only with binary data</label><br>
                                <input type="radio" name="q4" value="d" id="q4d">
                                <label for="q4d">They require pre-specified number of clusters</label><br>
                            </div>
                            <button onclick="checkAnswer(4, 'a')" class="azbn-btn">Check Answer</button>
                            <div id="q4-result" class="margin-top"></div>
                        </div>

                        <div class="enhanced-quiz-question">
                            <h4>Question 5: Ward's Method</h4>
                            <p>What does Ward's method minimize when merging clusters?</p>
                            <div class="margin-top">
                                <input type="radio" name="q5" value="a" id="q5a">
                                <label for="q5a">The maximum distance between cluster points</label><br>
                                <input type="radio" name="q5" value="b" id="q5b">
                                <label for="q5b">The increase in within-cluster sum of squares</label><br>
                                <input type="radio" name="q5" value="c" id="q5c">
                                <label for="q5c">The average distance between clusters</label><br>
                                <input type="radio" name="q5" value="d" id="q5d">
                                <label for="q5d">The minimum distance between cluster centroids</label><br>
                            </div>
                            <button onclick="checkAnswer(5, 'b')" class="azbn-btn">Check Answer</button>
                            <div id="q5-result" class="margin-top"></div>
                        </div>

                        <div class="quiz-section">
                            <h4>Quiz Score</h4>
                            <p>Correct answers: <span id="quiz-score">0</span> / 5</p>
                            <button onclick="resetQuiz()" class="azbn-btn azbn-secondary">Reset Quiz</button>
                        </div>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <!-- Sub-section Navigation Footer -->
    <div class="sub-section-nav-footer">
        <div class="sub-nav-buttons">
            <button id="prev-subsection" class="sub-nav-btn prev-btn" style="display: none;">
                <span>← Previous</span>
                <span class="sub-nav-label" id="prev-label"></span>
            </button>
            <button id="next-subsection" class="sub-nav-btn next-btn">
                <span class="sub-nav-label" id="next-label">Mathematical Theory</span>
                <span>Next →</span>
            </button>
        </div>
    </div>

    <!-- Chapter Navigation Footer -->
    <div class="navigation-buttons">
        <a href="/tutorials/clustering/chapter7" class="azbn-btn azbn-secondary" onclick="scrollToTop()">← Chapter 7: Optimal K Selection</a>
        <a href="/tutorials/clustering/chapter9" class="azbn-btn azbn-secondary" onclick="scrollToTop()">Chapter 9: Linkage Criteria Methods →</a>
    </div>

    <script src="{{ url_for('static', filename='js/tutorials/clustering/chapter8.js') }}"></script>
</body>
</html>
