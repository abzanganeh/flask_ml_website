<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 7: Optimal K Selection - Comprehensive Clustering Analysis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering/clustering.css') }}">
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-tutorial.js') }}"></script>
    <script src="{{ url_for('static', filename='js/tutorials/clustering/chapter7.js') }}"></script>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering" class="course-link">
                    <span>Comprehensive Clustering Analysis</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 7: Optimal K Selection</h1>
                <p class="chapter-subtitle">Master the mathematical techniques for determining the optimal number of clusters in K-means clustering</p>
                
                <!-- Chapter Progress Bar (7/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="46.67"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/clustering/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/clustering/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/clustering/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering/chapter7" class="chapter-nav-btn active">Chapter 7</a>
                    <a href="/tutorials/clustering/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/clustering/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering/chapter15" class="chapter-nav-btn">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="11.11"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn active" data-section="introduction">Introduction</button>
                    <button class="section-nav-btn" data-section="elbow">Elbow Method</button>
                    <button class="section-nav-btn" data-section="silhouette">Silhouette Analysis</button>
                    <button class="section-nav-btn" data-section="gap">Gap Statistic</button>
                    <button class="section-nav-btn" data-section="information">Information Criteria</button>
                    <button class="section-nav-btn" data-section="validation">Cross-Validation</button>
                    <button class="section-nav-btn" data-section="comparison">Method Comparison</button>
                    <button class="section-nav-btn" data-section="interactive">Interactive Demos</button>
                    <button class="section-nav-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the mathematical challenges of optimal K selection</li>
                        <li>Master the Elbow Method and its mathematical foundations</li>
                        <li>Learn Silhouette Analysis for cluster validation</li>
                        <li>Explore the Gap Statistic and its statistical principles</li>
                        <li>Understand Information Criteria (AIC, BIC) for model selection</li>
                        <li>Apply Cross-Validation techniques to clustering problems</li>
                        <li>Compare different K-selection methods and their trade-offs</li>
                        <li>Implement K-selection algorithms with interactive demonstrations</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">
                    <!-- Introduction Section -->
                    <div id="introduction" class="content-section active">
                        <h2>The Optimal K Problem: Choosing the Right Number of Clusters</h2>
                        
                        <p>One of the most challenging aspects of K-means clustering is determining the optimal number of clusters k. Unlike supervised learning where performance can be evaluated against known labels, clustering requires internal validation criteria to assess the quality of different clustering solutions. This chapter explores mathematically rigorous approaches to solve this fundamental problem.</p>

                        <h3>The Nature of the K-Selection Challenge</h3>
                        <p>The choice of k profoundly affects clustering results, yet there is no universally optimal solution across all datasets and applications.</p>

                        <div class="unsupervised-types-grid">
                            <div class="type-card">
                                <h4>Core Challenges</h4>
                                <ul>
                                    <li><strong>Objective function bias:</strong> WCSS always decreases with increasing k</li>
                                    <li><strong>Overfitting risk:</strong> Too many clusters create noise fitting</li>
                                    <li><strong>Underfitting risk:</strong> Too few clusters miss natural structure</li>
                                    <li><strong>Scale dependency:</strong> Different methods may give different answers</li>
                                    <li><strong>Data dependency:</strong> Optimal k varies with dataset characteristics</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Mathematical Frameworks</h4>
                                <ul>
                                    <li><strong>Variance decomposition:</strong> Within vs between cluster variance</li>
                                    <li><strong>Information theory:</strong> Model complexity vs data fit trade-offs</li>
                                    <li><strong>Statistical inference:</strong> Hypothesis testing for cluster existence</li>
                                    <li><strong>Geometric analysis:</strong> Cluster separation and compactness</li>
                                    <li><strong>Stability analysis:</strong> Robustness across perturbations</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Practical Considerations</h4>
                                <ul>
                                    <li><strong>Domain knowledge:</strong> Business or scientific constraints</li>
                                    <li><strong>Interpretability:</strong> Meaningful number of clusters</li>
                                    <li><strong>Computational cost:</strong> Processing time vs accuracy trade-offs</li>
                                    <li><strong>Downstream tasks:</strong> Impact on subsequent analysis</li>
                                    <li><strong>Robustness:</strong> Consistency across different methods</li>
                                </ul>
                            </div>
                        </div>

                        <h3>Mathematical Formulation of the K-Selection Problem</h3>
                        <p>The k-selection problem can be formulated as an optimization problem that balances model fit against model complexity.</p>

                        <div class="model-box">
                            <h4>General K-Selection Framework</h4>
                            
                            <h5>Objective Function Decomposition:</h5>
                            <p>For any clustering solution with k clusters, we can decompose the total variance:</p>
                            <div class="formula-box">
                                <div class="formula-display">
                                    <strong>TSS = WCSS(k) + BSS(k)</strong>
                                </div>
                                <p>Where:</p>
                                <ul>
                                    <li><strong>TSS:</strong> Total Sum of Squares (constant for given data)</li>
                                    <li><strong>WCSS(k):</strong> Within-Cluster Sum of Squares for k clusters</li>
                                    <li><strong>BSS(k):</strong> Between-Cluster Sum of Squares for k clusters</li>
                            </ul>
                            </div>
                            
                            <h5>The Fundamental Trade-off:</h5>
                            <div class="formula-box">
                                <p><strong>Model Fit:</strong> WCSS(k) decreases monotonically as k increases</p>
                                <p><strong>Model Complexity:</strong> More clusters increase overfitting risk</p>
                                <p><strong>Optimal k:</strong> Balance point between fit and complexity</p>
                            </div>
                            
                            <h5>General Selection Criterion:</h5>
                            <div class="formula-display">
                                <strong>k* = argmin[k] { f(WCSS(k), complexity(k)) }</strong>
                            </div>
                            <p>Different methods define f(·) and complexity(·) differently, leading to various k-selection criteria.</p>
                        </div>

                        <h3>Taxonomy of K-Selection Methods</h3>
                        <p>K-selection methods can be categorized by their underlying mathematical principles and computational approaches.</p>
                    </div>

                    <!-- Elbow Method Section -->
                    <div id="elbow" class="content-section">
                        <h2>The Elbow Method: Detecting Diminishing Returns</h2>
                        
                        <p>The Elbow Method is one of the most intuitive and widely-used approaches for determining optimal k. Based on the principle of diminishing returns, it identifies the point where increasing k yields progressively smaller improvements in clustering quality, typically visualized as an "elbow" in the WCSS vs k plot.</p>

                        <h3>Mathematical Foundation</h3>
                        <p>The Elbow Method relies on analyzing the rate of change in the objective function as k increases.</p>

                        <div class="explanation-box">
                            <h4>Elbow Method Mathematical Framework</h4>
                            
                            <h5>Within-Cluster Sum of Squares (WCSS):</h5>
                            <div class="formula-display">
                                <strong>WCSS(k) = Σⱼ₌₁ᵏ Σₓᵢ∈Cⱼ ||xᵢ - μⱼ||²</strong>
                            </div>
                            
                            <h5>Rate of Improvement:</h5>
                            <p>The improvement gained by adding one more cluster:</p>
                            <div class="formula-display">
                                <strong>Δ(k) = WCSS(k-1) - WCSS(k)</strong>
                            </div>
                            
                            <h5>Second Derivative (Curvature):</h5>
                            <p>The rate of change in improvement:</p>
                            <div class="formula-display">
                                <strong>Δ²(k) = Δ(k-1) - Δ(k) = WCSS(k-2) - 2·WCSS(k-1) + WCSS(k)</strong>
                            </div>
                            
                            <h5>Elbow Detection Criteria:</h5>
                            <ul>
                                <li><strong>Visual inspection:</strong> Identify sharp bend in WCSS curve</li>
                                <li><strong>Maximum curvature:</strong> k* = argmax[k] |Δ²(k)|</li>
                                <li><strong>Percentage threshold:</strong> k where improvement drops below threshold</li>
                                <li><strong>Knee detection algorithms:</strong> Automated elbow identification</li>
                            </ul>
                        </div>

                        <h3>Algorithm Implementation</h3>
                        <p>A systematic approach to implementing the Elbow Method with proper statistical considerations.</p>

                        <div class="algorithm-box">
                            <h4>Complete Elbow Method Algorithm</h4>
                            
                            <div class="code-box">
                                <pre><code><strong>function</strong> elbow_method(X, k_range, n_runs=10):
    wcss_values = []
    wcss_std = []
    
    <strong>for</strong> k in k_range:
        k_wcss = []
        
        <strong>for</strong> run in range(n_runs):
            # Multiple runs for stability
            centroids = initialize_centroids(X, k)
            clusters = kmeans(X, centroids)
            wcss = calculate_wcss(X, clusters)
            k_wcss.append(wcss)
        
        wcss_values.append(mean(k_wcss))
        wcss_std.append(std(k_wcss))
    
    # Find elbow point
    optimal_k = detect_elbow(k_range, wcss_values)
    
    <strong>return</strong> optimal_k, wcss_values, wcss_std</code></pre>
                            </div>
                    </div>

                        <h3>Advantages and Limitations</h3>
                        <p>Understanding when the Elbow Method works well and when it may fail.</p>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Advantages</th>
                                        <th>Limitations</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Intuitive and easy to understand</td>
                                        <td>Subjective elbow identification</td>
                                    </tr>
                                    <tr>
                                        <td>Computationally efficient</td>
                                        <td>May not work with unclear elbows</td>
                                    </tr>
                                    <tr>
                                        <td>Works well with spherical clusters</td>
                                        <td>Sensitive to data scaling</td>
                                    </tr>
                                    <tr>
                                        <td>Provides visual validation</td>
                                        <td>Less effective with overlapping clusters</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Elbow Method Example</h4>
                            <p><strong>Image Description:</strong> A comprehensive elbow method visualization showing WCSS values plotted against k (number of clusters) from k=1 to k=10. The plot clearly shows the characteristic "elbow" shape with a sharp decrease in WCSS for small k values, followed by a more gradual decline. The optimal k=3 is highlighted at the elbow point where the rate of improvement begins to level off. Additional panels show the second derivative curve for automated elbow detection and confidence intervals from multiple runs.</p>
                            <p><em>This demonstrates the classic elbow pattern used for optimal k selection</em></p>
                        </div>
                    </div>
                        <p>The simplest approach randomly selects k data points as initial centroids, but this method has significant limitations.</p>

                        <div class="model-box">
                            <h4>Random Initialization Algorithm</h4>
                            
                            <div class="formula-box">
                                <h5><strong>Basic Random Selection:</strong></h5>
                                <div class="code-box">
<strong>function</strong> random_init(X, k):
    n, d = X.shape
    indices = random_sample(n, k)  <span style="color: #1976d2;">// Sample k indices without replacement</span>
    centroids = X[indices]          <span style="color: #1976d2;">// Select corresponding data points</span>
    <strong>return</strong> centroids
                                </div>
                                
                                <h5><strong>Random Uniform in Feature Space:</strong></h5>
                                <div class="code-box">
<strong>function</strong> random_uniform_init(X, k):
    n, d = X.shape
    min_vals = min(X, axis=0)       <span style="color: #1976d2;">// Feature-wise minimum</span>
    max_vals = max(X, axis=0)       <span style="color: #1976d2;">// Feature-wise maximum</span>
    centroids = uniform(min_vals, max_vals, size=(k, d))
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Advantages of Random Initialization:</h5>
                            <ul>
                                <li><strong>Simplicity:</strong> Easy to implement and understand</li>
                                <li><strong>Speed:</strong> O(kd) time complexity</li>
                                <li><strong>Unbiased:</strong> No assumptions about data structure</li>
                                <li><strong>Baseline:</strong> Good reference for comparing other methods</li>
                            </ul>
                            
                            <h5>Disadvantages:</h5>
                            <ul>
                                <li><strong>High variance:</strong> Results vary significantly across runs</li>
                                <li><strong>Poor clustering:</strong> Often leads to suboptimal solutions</li>
                                <li><strong>Slow convergence:</strong> May require many iterations</li>
                                <li><strong>Empty clusters:</strong> Risk of centroids in sparse regions</li>
                            </ul>
                        </div>

                        <h3>Furthest-First Heuristic</h3>
                        <p>This method iteratively selects centroids that are as far as possible from previously selected ones, promoting good coverage of the data space.</p>

                        <div class="model-box">
                            <h4>Furthest-First Initialization</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
<strong>function</strong> furthest_first_init(X, k):
    n, d = X.shape
    centroids = []
    
    <span style="color: #1976d2;">// Step 1: Choose first centroid randomly</span>
    first_idx = random_choice(n)
    centroids.append(X[first_idx])
    
    <span style="color: #1976d2;">// Step 2: Iteratively choose furthest points</span>
    <strong>for</strong> i = 2 <strong>to</strong> k:
        max_distance = -1
        furthest_idx = -1
        
        <strong>for</strong> j = 1 <strong>to</strong> n:
            <span style="color: #1976d2;">// Find minimum distance to existing centroids</span>
            min_dist = min([distance(X[j], c) <strong>for</strong> c <strong>in</strong> centroids])
            
            <strong>if</strong> min_dist > max_distance:
                max_distance = min_dist
                furthest_idx = j
        
        centroids.append(X[furthest_idx])
    
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Advantages:</h5>
                            <ul>
                                <li><strong>Good coverage:</strong> Centroids spread across data space</li>
                                <li><strong>Deterministic:</strong> Same result for same first choice</li>
                                <li><strong>No empty clusters:</strong> Guarantees centroids on data points</li>
                                <li><strong>Better than random:</strong> Generally produces better initializations</li>
                            </ul>
                            
                            <h5>Disadvantages:</h5>
                            <ul>
                                <li><strong>Outlier sensitivity:</strong> May select extreme outliers</li>
                                <li><strong>Computational cost:</strong> O(nk) time complexity</li>
                                <li><strong>Still suboptimal:</strong> Not guaranteed to find good initializations</li>
                                <li><strong>First choice matters:</strong> Quality depends on initial random selection</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Silhouette Analysis Section -->
                    <div id="silhouette" class="content-section">
                        <h2>Silhouette Analysis: Measuring Cluster Quality</h2>
                        
                        <p>Silhouette Analysis provides a comprehensive method for evaluating both individual data points and overall clustering quality. Unlike the Elbow Method, which focuses solely on within-cluster variance, Silhouette Analysis considers both cluster cohesion and separation, providing a more nuanced view of clustering performance.</p>

                        <h3>Mathematical Foundation</h3>
                        <p>The silhouette coefficient quantifies how well each point fits within its assigned cluster compared to other clusters.</p>

                        <div class="explanation-box">
                            <h4>Silhouette Coefficient Mathematics</h4>
                            
                            <h5>Individual Point Silhouette:</h5>
                            <p>For each point i, calculate:</p>
                            <div class="formula-display">
                                <strong>s(i) = (b(i) - a(i)) / max(a(i), b(i))</strong>
                            </div>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>a(i)</strong> = average distance to points in the same cluster (cohesion)</li>
                                <li><strong>b(i)</strong> = minimum average distance to points in other clusters (separation)</li>
                            </ul>
                            
                            <h5>Cluster Silhouette:</h5>
                            <div class="formula-display">
                                <strong>S(C) = (1/|C|) Σᵢ∈C s(i)</strong>
                            </div>
                            
                            <h5>Overall Silhouette Score:</h5>
                            <div class="formula-display">
                                <strong>S = (1/n) Σᵢ₌₁ⁿ s(i)</strong>
                            </div>
                            
                            <h5>Interpretation:</h5>
                            <ul>
                                <li><strong>s(i) ≈ 1:</strong> Point is well-clustered (far from neighboring clusters)</li>
                                <li><strong>s(i) ≈ 0:</strong> Point is on or very close to decision boundary</li>
                                <li><strong>s(i) < 0:</strong> Point might be assigned to wrong cluster</li>
                            </ul>
                        </div>

                        <h3>Algorithm Implementation</h3>
                        <p>Step-by-step implementation of silhouette analysis for optimal k selection.</p>

                        <div class="algorithm-box">
                            <h4>Silhouette Analysis Algorithm</h4>
                            
                            <div class="code-box">
                                <pre><code><strong>function</strong> silhouette_analysis(X, k_range):
    silhouette_scores = []
    
    <strong>for</strong> k in k_range:
        # Perform clustering
        clusters = kmeans(X, k)
        
        # Calculate silhouette for each point
        point_silhouettes = []
        <strong>for</strong> i in range(len(X)):
            a_i = average_intra_cluster_distance(X[i], clusters)
            b_i = min_average_inter_cluster_distance(X[i], clusters)
            
            <strong>if</strong> a_i == 0 and b_i == 0:
                s_i = 0
            <strong>else</strong>:
                s_i = (b_i - a_i) / max(a_i, b_i)
            
            point_silhouettes.append(s_i)
        
        # Average silhouette score
        avg_silhouette = mean(point_silhouettes)
        silhouette_scores.append(avg_silhouette)
    
    # Find k with maximum silhouette score
    optimal_k = k_range[argmax(silhouette_scores)]
    
    <strong>return</strong> optimal_k, silhouette_scores</code></pre>
                            </div>
                        </div>

                        <h3>Advanced Silhouette Techniques</h3>
                        <p>Enhanced methods for more robust silhouette analysis.</p>

                        <div class="model-box">
                            <h4>Silhouette Plot Analysis</h4>
                            <ul>
                                <li><strong>Individual point analysis:</strong> Identify poorly clustered points</li>
                                <li><strong>Cluster comparison:</strong> Compare cluster quality within same k</li>
                                <li><strong>Thickness analysis:</strong> Evaluate cluster size consistency</li>
                                <li><strong>Below-average detection:</strong> Identify problematic clusters</li>
                            </ul>
                        </div>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Silhouette Range</th>
                                        <th>Interpretation</th>
                                        <th>Cluster Quality</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>0.7 - 1.0</td>
                                        <td>Strong, well-separated clusters</td>
                                        <td>Excellent</td>
                                    </tr>
                                    <tr>
                                        <td>0.5 - 0.7</td>
                                        <td>Reasonable clustering structure</td>
                                        <td>Good</td>
                                    </tr>
                                    <tr>
                                        <td>0.25 - 0.5</td>
                                        <td>Weak clustering structure</td>
                                        <td>Fair</td>
                                    </tr>
                                    <tr>
                                        <td>< 0.25</td>
                                        <td>No substantial clustering structure</td>
                                        <td>Poor</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="visualization-placeholder">
                            <h4>Visualization: Silhouette Analysis</h4>
                            <p><strong>Image Description:</strong> A comprehensive silhouette analysis visualization with three panels. Left: Silhouette plot showing individual point coefficients organized by cluster, with cluster averages and overall average marked. Center: Silhouette scores vs k plot showing the optimal k=3 with highest average silhouette score. Right: 2D scatter plot of the data colored by cluster assignments, demonstrating the cluster separation that leads to high silhouette scores.</p>
                            <p><em>This demonstrates how silhouette analysis provides detailed cluster quality assessment</em></p>
                        </div>
                    </div>
                        <p>K-means++ carefully selects initial centroids using a probability distribution that favors points far from existing centroids.</p>

                        <div class="model-box">
                            <h4>K-means++ Initialization Algorithm</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
<strong>function</strong> kmeans_plus_plus(X, k):
    n, d = X.shape
    centroids = []
    
    <span style="color: #1976d2;">// Step 1: Choose first centroid uniformly at random</span>
    first_idx = random_choice(n)
    centroids.append(X[first_idx])
    
    <span style="color: #1976d2;">// Step 2: Choose remaining k-1 centroids</span>
    <strong>for</strong> i = 2 <strong>to</strong> k:
        distances = []
        
        <span style="color: #1976d2;">// Compute squared distance to nearest existing centroid</span>
        <strong>for</strong> j = 1 <strong>to</strong> n:
            min_dist_sq = min([||X[j] - c||² <strong>for</strong> c <strong>in</strong> centroids])
            distances.append(min_dist_sq)
        
        <span style="color: #1976d2;">// Choose next centroid with probability proportional to squared distance</span>
        probabilities = distances / sum(distances)
        next_idx = weighted_random_choice(probabilities)
        centroids.append(X[next_idx])
    
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Key Insight:</h5>
                            <p>The probability of selecting a point as the next centroid is proportional to its squared distance from the nearest existing centroid. This creates a bias toward points that are far from current centroids, promoting good spatial distribution.</p>
                            
                            <h5>Mathematical Formulation:</h5>
                            <div class="formula-box">
                                <p>For selecting the (j+1)-th centroid, given j existing centroids C = {c₁, c₂, ..., cⱼ}:</p>
                                <div class="formula-display">
                                    <strong>P(xᵢ) = D²(xᵢ) / Σₖ D²(xₖ)</strong>
                                </div>
                                <p>Where D²(xᵢ) = min_{c∈C} ||xᵢ - c||² is the squared distance to the nearest centroid.</p>
                            </div>
                        </div>

                        <h3>Theoretical Analysis</h3>
                        <p>K-means++ comes with strong theoretical guarantees that explain its superior performance.</p>

                        <div class="model-box">
                            <h4>K-means++ Approximation Guarantee</h4>
                            
                            <h5>Main Theorem (Arthur & Vassilvitskii, 2007):</h5>
                            <div class="formula-box">
                                <p><strong>Theorem:</strong> K-means++ initialization followed by Lloyd's algorithm produces a solution with expected cost at most O(log k) times the optimal k-means cost.</p>
                                
                                <p><strong>Formally:</strong> E[cost(K-means++ solution)] ≤ 8(ln k + 2) × OPT</p>
                                
                                <p>Where OPT is the cost of the optimal k-means clustering.</p>
                            </div>
                            
                            <h5>Proof Sketch:</h5>
                            <ol>
                                <li><strong>Potential function:</strong> Define Φ = Σᵢ D²(xᵢ) as sum of squared distances to nearest centroids</li>
                                <li><strong>Expected reduction:</strong> Each K-means++ step reduces E[Φ] by a constant factor</li>
                                <li><strong>Concentration:</strong> Use probability tail bounds to show consistent performance</li>
                                <li><strong>Optimality bound:</strong> Relate final potential to optimal clustering cost</li>
                            </ol>
                            
                            <h5>Implications:</h5>
                            <ul>
                                <li><strong>Logarithmic guarantee:</strong> Performance degrades slowly with k</li>
                                <li><strong>Probabilistic bound:</strong> Guarantee holds in expectation</li>
                                <li><strong>Initialization only:</strong> Bound applies to initialization, Lloyd's improves it</li>
                                <li><strong>Practical relevance:</strong> Constant factors are reasonable in practice</li>
                            </ul>
                        </div>

                        <div class="interactive-container">
                            <h3>Initialization Comparison Demo</h3>
                            <div class="demo-controls">
                                <label for="init-method">Initialization Method:</label>
                                <select id="init-method">
                                    <option value="random">Random</option>
                                    <option value="kmeans++">K-means++</option>
                                </select>
                                
                                <label for="num-clusters-init">Number of Clusters:</label>
                                <input type="range" id="num-clusters-init" min="2" max="6" value="3">
                                <span id="clusters-init-display">3</span>
                                
                                <button onclick="runInitializationDemo()">Run Demo</button>
                                <button onclick="resetInitializationDemo()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="initialization-canvas">
                                <p>Click "Run Demo" to compare different initialization methods</p>
                            </div>
                        </div>
                    </div>

                    <!-- Gap Statistic Section -->
                    <div id="gap" class="content-section">
                        <h2>Gap Statistic: A Statistical Approach to K-Selection</h2>
                        
                        <div class="explanation-box">
                            <p>The Gap Statistic, introduced by Tibshirani, Walther, and Hastie (2001), provides a principled statistical method for estimating the optimal number of clusters by comparing the within-cluster dispersion of the data to that expected under a null reference distribution.</p>
                        </div>

                        <div class="formula-box">
                            <h3>Gap Statistic Formula</h3>
                            <div class="formula-display">
                                <h4>Gap Definition</h4>
                                <div class="formula">Gap(k) = E[log(W_k*)] - log(W_k)</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>W_k is the within-cluster sum of squares for k clusters</li>
                                    <li>W_k* is the expected WCSS under null reference distribution</li>
                                    <li>E[·] denotes expectation over reference datasets</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h3>Optimal K Selection</h3>
                            <div class="formula-display">
                                <h4>Selection Criterion</h4>
                                <div class="formula">k* = smallest k such that Gap(k) ≥ Gap(k+1) - s_{k+1}</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>s_k is the standard error of the gap statistic</li>
                                    <li>This ensures statistical significance of the gap</li>
                                    <li>Provides conservative estimate of optimal k</li>
                                </ul>
                            </div>
                        </div>

                        <div class="model-box">
                            <h3>Gap Statistic Properties</h3>
                            <ul>
                                <li><strong>Statistical Foundation:</strong> Based on formal hypothesis testing</li>
                                <li><strong>Reference Distribution:</strong> Compares to uniform random data</li>
                                <li><strong>Conservative Estimate:</strong> Tends to select smaller k values</li>
                                <li><strong>Computational Cost:</strong> Requires multiple reference datasets</li>
                            </ul>
                        </div>

                        <div class="algorithm-box">
                            <h3>Gap Statistic Algorithm</h3>
                            <div class="code-box">
                                <pre><code><strong>function</strong> gap_statistic(X, k_max, B=50):
    <span class="comment"># For each k, compute gap statistic</span>
    <strong>for</strong> k <strong>in</strong> range(1, k_max+1):
        <span class="comment"># Compute actual WCSS</span>
        W_k = compute_wcss(X, k)
        
        <span class="comment"># Generate B reference datasets</span>
        W_k_refs = []
        <strong>for</strong> b <strong>in</strong> range(B):
            X_ref = generate_uniform_reference(X)
            W_k_ref = compute_wcss(X_ref, k)
            W_k_refs.append(log(W_k_ref))
        
        <span class="comment"># Compute gap and standard error</span>
        E_log_W_k = mean(W_k_refs)
        gap_k = E_log_W_k - log(W_k)
        s_k = std(W_k_refs) * sqrt(1 + 1/B)
        
    <span class="comment"># Find optimal k</span>
    <strong>return</strong> find_optimal_k(gaps, standard_errors)</code></pre>
                            </div>
                        </div>
                    </div>

                    <!-- Information Criteria Section -->
                    <div id="information" class="content-section">
                        <h2>Information Criteria: AIC and BIC for Cluster Selection</h2>
                        
                        <div class="explanation-box">
                            <p>Information criteria, originally developed for model selection in statistics, can be adapted for clustering to provide principled methods for choosing the optimal number of clusters. These criteria balance model fit against complexity, penalizing solutions with too many clusters.</p>
                        </div>

                        <h3>Akaike Information Criterion (AIC)</h3>
                        <div class="formula-box">
                            <h4>AIC for Clustering</h4>
                            <div class="formula-display">
                                <div class="formula">AIC(k) = -2·log(L) + 2·p</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>L is the likelihood of the clustering model</li>
                                    <li>p is the number of parameters (typically k·d + k for centroids and cluster sizes)</li>
                                    <li>Lower AIC values indicate better models</li>
                            </ul>
                            </div>
                        </div>

                        <h3>Bayesian Information Criterion (BIC)</h3>
                        <div class="formula-box">
                            <h4>BIC for Clustering</h4>
                            <div class="formula-display">
                                <div class="formula">BIC(k) = -2·log(L) + p·log(n)</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>n is the number of data points</li>
                                    <li>BIC penalizes complexity more heavily than AIC</li>
                                    <li>Tends to select smaller k values</li>
                                </ul>
                            </div>
                        </div>

                        <div class="model-box">
                            <h3>Practical Considerations</h3>
                            <ul>
                                <li><strong>Likelihood Estimation:</strong> Requires assuming a probability model (e.g., Gaussian mixture)</li>
                                <li><strong>Parameter Counting:</strong> Must carefully count degrees of freedom</li>
                                <li><strong>AIC vs BIC:</strong> AIC tends to select more clusters, BIC is more conservative</li>
                                <li><strong>Computational Efficiency:</strong> Fast to compute once likelihood is available</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Cross-Validation Section -->
                    <div id="validation" class="content-section">
                        <h2>Cross-Validation for Clustering</h2>
                        
                        <p>Cross-validation techniques adapted for clustering problems provide robust methods for optimal k selection by evaluating clustering stability across different data subsets.</p>

                        <h3>Challenges in Clustering Cross-Validation</h3>
                        <p>Traditional cross-validation requires adaptation for unsupervised learning.</p>

                        <div class="explanation-box">
                            <h4>Clustering Cross-Validation Methods</h4>
                            <ul>
                                <li><strong>Stability-based validation:</strong> Measure clustering consistency across subsamples</li>
                                <li><strong>Prediction strength:</strong> Evaluate cluster membership prediction accuracy</li>
                                <li><strong>Bootstrap validation:</strong> Use resampling to assess clustering robustness</li>
                                <li><strong>Cross-validation stability:</strong> Compare clusterings from different data splits</li>
                            </ul>
                        </div>

                            <div class="visualization-placeholder">
                            <h4>Visualization: Cross-Validation Results</h4>
                            <p><strong>Image Description:</strong> Cross-validation stability scores plotted against k, showing how clustering consistency varies with the number of clusters. Higher stability scores indicate more robust clustering solutions.</p>
                            <p><em>This demonstrates stability-based k selection using cross-validation</em></p>
                            </div>
                    </div>

                    <!-- Method Comparison Section -->
                    <div id="comparison" class="content-section">
                        <h2>Method Comparison and Selection Guidelines</h2>
                        
                        <p>Different k-selection methods have varying strengths and weaknesses. Understanding when to use each method is crucial for effective clustering analysis.</p>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>Best Use Cases</th>
                                        <th>Limitations</th>
                                        <th>Computational Cost</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Elbow Method</strong></td>
                                        <td>Well-separated spherical clusters</td>
                                        <td>Subjective elbow detection</td>
                                        <td>Low</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Silhouette Analysis</strong></td>
                                        <td>Clusters with good separation</td>
                                        <td>Sensitive to cluster shape</td>
                                        <td>Medium</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Gap Statistic</strong></td>
                                        <td>Statistical significance testing</td>
                                        <td>Computationally expensive</td>
                                        <td>High</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Information Criteria</strong></td>
                                        <td>Model selection framework</td>
                                        <td>Assumes specific distributions</td>
                                        <td>Medium</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Cross-Validation</strong></td>
                                        <td>Stability assessment</td>
                                        <td>Complex implementation</td>
                                        <td>High</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h3>Practical Guidelines</h3>
                        <div class="model-box">
                            <h4>Method Selection Strategy</h4>
                            <ol>
                                <li><strong>Start with Elbow Method:</strong> Quick initial assessment</li>
                                <li><strong>Validate with Silhouette:</strong> Detailed quality analysis</li>
                                <li><strong>Use Gap Statistic:</strong> For statistical significance</li>
                                <li><strong>Apply multiple methods:</strong> Consensus-based selection</li>
                                <li><strong>Consider domain knowledge:</strong> Practical constraints</li>
                            </ol>
                        </div>
                    </div>

                    <!-- Interactive Demo Section -->
                    <div id="interactive" class="content-section">
                        <h2>Interactive K-Selection Demos</h2>
                        
                        <p>Explore optimal k selection methods through interactive demonstrations. Compare different approaches and understand their behavior on various datasets.</p>

                        <h3>Demo 1: Elbow Method Visualization</h3>
                        <div class="interactive-container">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="elbow-dataset">Dataset:</label>
                                    <select id="elbow-dataset">
                                        <option value="blobs">Blob Clusters</option>
                                        <option value="moons">Moon Shapes</option>
                                        <option value="circles">Concentric Circles</option>
                                        <option value="random">Random Points</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="elbow-max-k">Maximum K:</label>
                                    <input type="range" id="elbow-max-k" min="5" max="15" value="10">
                                    <span id="elbow-max-k-display">10</span>
                                </div>
                                
                                <button class="demo-button" onclick="generateElbowDemo()">Generate Elbow Plot</button>
                                <button class="demo-button" onclick="resetElbowDemo()">Reset</button>
                            </div>
                            
                            <div class="visualization-container">
                                <div class="visualization-panel">
                                    <h4>WCSS vs K Plot</h4>
                                    <svg id="elbow-plot" width="400" height="300"></svg>
                                </div>
                                <div class="visualization-panel">
                                    <h4>Optimal Clustering Result</h4>
                                    <svg id="elbow-clusters" width="400" height="300"></svg>
                                </div>
                            </div>
                        </div>

                        <h3>Demo 2: Silhouette Analysis Comparison</h3>
                        <div class="interactive-container">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="silhouette-dataset">Dataset:</label>
                                    <select id="silhouette-dataset">
                                        <option value="blobs">Blob Clusters</option>
                                        <option value="moons">Moon Shapes</option>
                                        <option value="circles">Concentric Circles</option>
                                        <option value="random">Random Points</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="silhouette-k">Number of Clusters:</label>
                                    <input type="range" id="silhouette-k" min="2" max="8" value="3">
                                    <span id="silhouette-k-display">3</span>
                                </div>
                                
                                <button class="demo-button" onclick="generateSilhouetteDemo()">Generate Analysis</button>
                                <button class="demo-button" onclick="resetSilhouetteDemo()">Reset</button>
                            </div>
                            
                            <div class="visualization-container">
                                <div class="visualization-panel">
                                    <h4>Silhouette Plot</h4>
                                    <svg id="silhouette-plot" width="400" height="300"></svg>
                                </div>
                                <div class="visualization-panel">
                                    <h4>Clustering Visualization</h4>
                                    <svg id="silhouette-clusters" width="400" height="300"></svg>
                                </div>
                            </div>
                            
                            <div class="demo-metrics">
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <div class="metric-label">Average Silhouette Score</div>
                                        <div class="metric-value" id="avg-silhouette">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Best Cluster Quality</div>
                                        <div class="metric-value" id="best-cluster">-</div>
                                    </div>
                                    <div class="metric-item">
                                        <div class="metric-label">Worst Cluster Quality</div>
                                        <div class="metric-value" id="worst-cluster">-</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        </div>

                        <div class="interactive-container">
                            <h3>K-means Clustering Demo</h3>
                            
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="demo-clusters">Number of Clusters:</label>
                                    <input type="range" id="demo-clusters" min="2" max="8" value="3">
                                    <span id="demo-clusters-display">3</span>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-init">Initialization:</label>
                                    <select id="demo-init">
                                        <option value="random">Random</option>
                                        <option value="kmeans++">K-means++</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-data">Data Type:</label>
                                    <select id="demo-data">
                                        <option value="blobs">Well-separated Blobs</option>
                                        <option value="random">Random Points</option>
                                        <option value="moons">Moon-shaped</option>
                                    </select>
                                </div>
                                
                                <div class="control-buttons">
                                    <button onclick="generateDemoData()">Generate Data</button>
                                    <button onclick="runKmeansDemo()">Run K-means</button>
                                    <button onclick="stepKmeansDemo()">Step-by-Step</button>
                                    <button onclick="resetDemo()">Reset</button>
                                </div>
                            </div>
                            
                            <div class="demo-status" id="demo-status">
                                <p>Click "Generate Data" to start the demo</p>
                            </div>
                            
                            <div class="metric-visualization" id="kmeans-demo-canvas">
                                <p>Interactive K-means clustering visualization will appear here</p>
                            </div>
                            
                            <div class="demo-metrics" id="demo-metrics" style="display: none;">
                                <h4>Clustering Metrics</h4>
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <span class="metric-label">WCSS:</span>
                                        <span class="metric-value" id="wcss-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Silhouette:</span>
                                        <span class="metric-value" id="silhouette-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Iterations:</span>
                                        <span class="metric-value" id="iterations-value">-</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Quiz</h2>
                        
                        <p>Test your understanding of optimal K selection methods with these comprehensive questions covering the key concepts discussed in this chapter.</p>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h3>Question 1: Elbow Method</h3>
                                <p>What does the "elbow" in the Elbow Method represent?</p>
                                <div class="enhanced-quiz-options">
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q1" value="a">
                                        <span class="option-text">The point where WCSS stops decreasing</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q1" value="b">
                                        <span class="option-text">The point where diminishing returns in WCSS reduction become apparent</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q1" value="c">
                                        <span class="option-text">The maximum value of WCSS</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q1" value="d">
                                        <span class="option-text">The point where WCSS increases</span>
                                    </label>
                                </div>
                            </div>

                            <div class="enhanced-quiz-question">
                                <h3>Question 2: Silhouette Analysis</h3>
                                <p>What does a silhouette coefficient of 0.8 for a data point indicate?</p>
                                <div class="enhanced-quiz-options">
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q2" value="a">
                                        <span class="option-text">The point is poorly clustered</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q2" value="b">
                                        <span class="option-text">The point is well-clustered with good separation from other clusters</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q2" value="c">
                                        <span class="option-text">The point is on the cluster boundary</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q2" value="d">
                                        <span class="option-text">The point should be assigned to a different cluster</span>
                                    </label>
                                </div>
                            </div>

                            <div class="enhanced-quiz-question">
                                <h3>Question 3: Gap Statistic</h3>
                                <p>What does the Gap Statistic compare to determine optimal k?</p>
                                <div class="enhanced-quiz-options">
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q3" value="a">
                                        <span class="option-text">WCSS values for different k</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q3" value="b">
                                        <span class="option-text">Silhouette scores for different k</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q3" value="c">
                                        <span class="option-text">Observed clustering quality against expected quality from random data</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q3" value="d">
                                        <span class="option-text">Between-cluster distances</span>
                                    </label>
                                </div>
                            </div>

                            <div class="enhanced-quiz-question">
                                <h3>Question 4: Information Criteria</h3>
                                <p>Which information criterion is more conservative in model selection?</p>
                                <div class="enhanced-quiz-options">
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q4" value="a">
                                        <span class="option-text">AIC (Akaike Information Criterion)</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q4" value="b">
                                        <span class="option-text">BIC (Bayesian Information Criterion)</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q4" value="c">
                                        <span class="option-text">Both are equally conservative</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q4" value="d">
                                        <span class="option-text">Neither is conservative</span>
                                    </label>
                                </div>
                            </div>

                            <div class="enhanced-quiz-question">
                                <h3>Question 5: Cross-Validation</h3>
                                <p>What is the main challenge in applying cross-validation to clustering?</p>
                                <div class="enhanced-quiz-options">
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q5" value="a">
                                        <span class="option-text">Clustering is too fast to require validation</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q5" value="b">
                                        <span class="option-text">There are no ground truth labels to validate against</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q5" value="c">
                                        <span class="option-text">Cross-validation cannot be applied to unsupervised learning</span>
                                    </label>
                                    <label class="enhanced-quiz-option">
                                        <input type="radio" name="q5" value="d">
                                        <span class="option-text">Clustering algorithms are deterministic</span>
                                    </label>
                                </div>
                            </div>

                            <button class="demo-button" onclick="checkQuizAnswers()">Submit Answers</button>
                            
                            <div id="quiz-results" class="quiz-score-container" style="display: none;">
                                <h3>Quiz Results</h3>
                                <div id="quiz-score"></div>
                                <div id="quiz-feedback"></div>
                            </div>
                        </div>
                    </div>
                        <h2>Chapter 4 Quiz</h2>
                        
                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 1: What is the primary objective function minimized by K-means?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Between-cluster sum of squares</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>Within-cluster sum of squares (WCSS)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Silhouette coefficient</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Calinski-Harabasz index</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means minimizes the within-cluster sum of squares (WCSS), which measures the total squared distance of all points from their cluster centroids.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 2: How are centroids updated in each K-means iteration?</h4>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>As the arithmetic mean of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the median of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the point closest to the cluster center</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As a weighted average based on point distances</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Centroids are updated as the arithmetic mean of all points assigned to that cluster, which minimizes the WCSS for that cluster.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 3: What is the main advantage of K-means++ initialization over random initialization?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It's faster to compute</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It guarantees global optimum</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>It provides better initialization leading to faster convergence</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It works better with non-spherical clusters</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means++ initialization probabilistically selects initial centroids that are well-separated, leading to better starting points and faster convergence to good local minima.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <!-- Sub-section Navigation Footer -->
    <div class="sub-section-nav-footer">
        <div class="sub-nav-buttons">
            <button id="prev-subsection" class="sub-nav-btn prev-btn" style="display: none;">
                <span>← Previous</span>
                <span class="sub-nav-label" id="prev-label"></span>
            </button>
            <button id="next-subsection" class="sub-nav-btn next-btn">
                <span class="sub-nav-label" id="next-label">Elbow Method</span>
                <span>Next →</span>
            </button>
        </div>
    </div>

    <!-- Chapter Navigation Footer -->
    <div class="navigation-buttons">
        <a href="/tutorials/clustering/chapter6" class="azbn-btn azbn-secondary" onclick="scrollToTop()">← Chapter 6: K-Means Optimization</a>
        <a href="/tutorials/clustering/chapter8" class="azbn-btn azbn-secondary" onclick="scrollToTop()">Chapter 8: Hierarchical Clustering →</a>
    </div>
</body>
</html>
