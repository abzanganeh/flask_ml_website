<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: K-means Clustering - Comprehensive Clustering Analysis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering/clustering.css') }}">
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-tutorial.js') }}"></script>
    <script src="{{ url_for('static', filename='js/tutorials/clustering/chapter4.js') }}"></script>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering" class="course-link">
                    <span>Comprehensive Clustering Analysis</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 4: K-means Clustering Algorithm</h1>
                <p class="chapter-subtitle">Master the most popular clustering algorithm from mathematical foundations to practical implementation</p>
                
                <!-- Chapter Progress Bar (4/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="26.67"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/clustering/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering/chapter4" class="chapter-nav-btn active">Chapter 4</a>
                    <a href="/tutorials/clustering/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/clustering/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/clustering/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/clustering/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering/chapter15" class="chapter-nav-btn">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.29"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn active" data-section="algorithm">Algorithm Overview</button>
                    <button class="section-nav-btn" data-section="mathematics">Mathematical Foundation</button>
                    <button class="section-nav-btn" data-section="initialization">Initialization Methods</button>
                    <button class="section-nav-btn" data-section="optimization">Optimization Process</button>
                    <button class="section-nav-btn" data-section="convergence">Convergence Analysis</button>
                    <button class="section-nav-btn" data-section="demo">Interactive Demo</button>
                    <button class="section-nav-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the K-means algorithm step-by-step process</li>
                        <li>Master the mathematical foundations and objective function</li>
                        <li>Learn different initialization methods and their impact</li>
                        <li>Analyze convergence properties and stopping criteria</li>
                        <li>Implement K-means from scratch with interactive demos</li>
                        <li>Compare different distance metrics in K-means clustering</li>
                        <li>Evaluate clustering quality using various metrics</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">
                    <!-- Algorithm Overview Section -->
                    <div id="algorithm" class="content-section active">
                        <h2>Beyond Minkowski: The World of Specialized Distance Metrics</h2>
                        
                        <p>While Minkowski distances provide a powerful foundation for measuring similarity, many real-world applications require specialized metrics that capture domain-specific notions of distance and similarity. This chapter explores advanced distance measures that excel in particular contexts, from high-dimensional sparse data to categorical variables and correlated features.</p>

                        <h3>Why Specialized Metrics Matter</h3>
                        <p>Different types of data and applications expose the limitations of general-purpose metrics like Euclidean and Manhattan distance:</p>

                        <div class="unsupervised-types-grid">
                            <div class="type-card">
                                <h4>Limitations of Standard Metrics</h4>
                                <ul>
                                    <li><strong>High dimensions:</strong> Euclidean distance loses discriminative power</li>
                                    <li><strong>Sparse data:</strong> Most components are zero, distances become meaningless</li>
                                    <li><strong>Categorical data:</strong> Ordering assumptions don't apply</li>
                                    <li><strong>Correlated features:</strong> Some directions more important than others</li>
                                    <li><strong>Scale differences:</strong> Features with different units dominate</li>
                                </ul>
                            </div>
                            
                            <div class="type-card">
                                <h4>Specialized Solutions</h4>
                                <ul>
                                    <li><strong>Cosine similarity:</strong> Direction matters more than magnitude</li>
                                    <li><strong>Hamming distance:</strong> Direct comparison of categorical values</li>
                                    <li><strong>Mahalanobis distance:</strong> Accounts for feature correlations</li>
                                    <li><strong>Edit distances:</strong> Sequence and string comparisons</li>
                                    <li><strong>Custom metrics:</strong> Domain-specific similarity measures</li>
                                </ul>
                            </div>
                        </div>

                        <h3>Taxonomy of Advanced Distance Metrics</h3>
                        <p>Advanced distance metrics can be categorized by the type of data they're designed for and the similarity concept they capture:</p>

                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Category</th>
                                    <th>Metrics</th>
                                    <th>Data Type</th>
                                    <th>Key Concept</th>
                                    <th>Primary Applications</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Angular Metrics</strong></td>
                                    <td>Cosine, Angular distance</td>
                                    <td>Continuous vectors</td>
                                    <td>Direction vs magnitude</td>
                                    <td>Text analysis, recommender systems</td>
                                </tr>
                                <tr>
                                    <td><strong>Edit Distances</strong></td>
                                    <td>Hamming, Levenshtein, Jaccard</td>
                                    <td>Discrete sequences</td>
                                    <td>Transformation cost</td>
                                    <td>DNA sequences, spell checking</td>
                                </tr>
                                <tr>
                                    <td><strong>Statistical Distances</strong></td>
                                    <td>Mahalanobis, Bhattacharyya</td>
                                    <td>Continuous features</td>
                                    <td>Statistical relationships</td>
                                    <td>Multivariate analysis, anomaly detection</td>
                                </tr>
                                <tr>
                                    <td><strong>Correlation Distances</strong></td>
                                    <td>Pearson, Spearman, Kendall</td>
                                    <td>Time series, rankings</td>
                                    <td>Co-variation patterns</td>
                                    <td>Gene expression, financial data</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <!-- Mathematical Foundation Section -->
                    <div id="mathematics" class="content-section">
                        <h2>Cosine Similarity: Angular Distance in High Dimensions</h2>
                        
                        <p>Cosine similarity is one of the most important and widely used similarity measures in machine learning, particularly for high-dimensional sparse data like text documents, user preferences, and recommendation systems. Unlike Euclidean distance, cosine similarity focuses on the angle between vectors rather than their magnitude.</p>

                        <h3>Mathematical Foundation</h3>
                        <p>Cosine similarity measures the cosine of the angle between two non-zero vectors, providing a measure of orientation rather than magnitude.</p>

                        <div class="formula-box">
                            <h3>Cosine Similarity Formula</h3>
                            <p>For two vectors x, y ∈ ℝⁿ, cosine similarity is defined as:</p>
                            
                            <div class="formula-display">
                                <strong>cos_sim(x, y) = (x · y) / (‖x‖₂ ‖y‖₂)</strong>
                            </div>
                            
                            <p>Expanded form:</p>
                            <div class="formula-display">
                                <strong>cos_sim(x, y) = (Σᵢ₌₁ⁿ xᵢyᵢ) / (√(Σᵢ₌₁ⁿ xᵢ²) √(Σᵢ₌₁ⁿ yᵢ²))</strong>
                            </div>
                            
                            <h4>Related Measures:</h4>
                            <ul>
                                <li><strong>Cosine Distance:</strong> cos_dist(x, y) = 1 - cos_sim(x, y)</li>
                                <li><strong>Angular Distance:</strong> θ = arccos(cos_sim(x, y))</li>
                                <li><strong>Angular Similarity:</strong> 1 - θ/π (normalized to [0, 1])</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <img src="{{ url_for('static', filename='images/tutorials/clustering/chapter4/cosine_similarity.png') }}" alt="Cosine Similarity Geometric Interpretation" class="tutorial-image">
                            <p class="image-caption">2D coordinate system showing vectors and their angles with cosine similarity values</p>
                        </div>

                        <h3>Properties and Applications</h3>
                        <div class="model-box">
                            <h4>Key Properties</h4>
                            <ul>
                                <li><strong>Range:</strong> [-1, 1] for any real vectors</li>
                                <li><strong>Scale invariant:</strong> cos_sim(αx, βy) = cos_sim(x, y) for α, β > 0</li>
                                <li><strong>Directional:</strong> Captures vector orientation, not magnitude</li>
                                <li><strong>High-dimensional friendly:</strong> Less affected by curse of dimensionality</li>
                            </ul>
                            
                            <h4>Primary Applications</h4>
                            <ul>
                                <li><strong>Text Analysis:</strong> Document similarity using TF-IDF vectors</li>
                                <li><strong>Recommendation Systems:</strong> User-item and item-item similarity</li>
                                <li><strong>Information Retrieval:</strong> Query-document matching</li>
                                <li><strong>Computer Vision:</strong> Feature vector comparison</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Initialization Methods Section -->
                    <div id="initialization" class="content-section">
                        <h2>Hamming Distance and Edit Distances</h2>
                        
                        <p>For categorical data, sequences, and discrete structures, traditional Lp distances often fail to capture meaningful similarity. Edit distances, led by Hamming distance, provide natural measures for comparing discrete sequences by counting the operations needed to transform one sequence into another.</p>

                        <h3>Hamming Distance: The Foundation</h3>
                        <p>Named after Richard Hamming, this distance measures the number of positions at which corresponding symbols differ between two strings of equal length.</p>

                        <div class="formula-box">
                            <h3>Hamming Distance Definition</h3>
                            <p>For two strings x, y of equal length n over alphabet Σ:</p>
                            
                            <div class="formula-display">
                                <strong>d_H(x, y) = Σᵢ₌₁ⁿ [xᵢ ≠ yᵢ]</strong>
                            </div>
                            
                            <p>Where [xᵢ ≠ yᵢ] is the Iverson bracket: 1 if xᵢ ≠ yᵢ, 0 otherwise.</p>
                            
                            <h4>Properties:</h4>
                            <ul>
                                <li><strong>Range:</strong> [0, n] where n is string length</li>
                                <li><strong>Valid metric:</strong> Satisfies all four metric axioms</li>
                                <li><strong>Discrete:</strong> Only integer values possible</li>
                                <li><strong>Position-dependent:</strong> Order matters in comparison</li>
                            </ul>
                        </div>

                        <h3>Levenshtein Distance: Edit Distance for Different Lengths</h3>
                        <p>While Hamming distance requires equal-length strings, Levenshtein distance allows insertions, deletions, and substitutions to compare strings of different lengths.</p>

                        <div class="model-box">
                            <h4>Levenshtein Distance Definition</h4>
                            <p>The minimum number of single-character edits (insertions, deletions, substitutions) needed to change one string into another.</p>
                            
                            <h5>Dynamic Programming Formulation:</h5>
                            <p>Let L(i, j) be the distance between the first i characters of string x and first j characters of string y:</p>
                            
                            <div class="formula-box">
                                <p><strong>Base cases:</strong></p>
                                <ul>
                                    <li>L(0, j) = j (insert j characters)</li>
                                    <li>L(i, 0) = i (delete i characters)</li>
                                </ul>
                                
                                <p><strong>Recursive relation:</strong></p>
                                <div class="formula-display">
                                    L(i, j) = min{<br>
                                    L(i-1, j) + 1,      (deletion)<br>
                                    L(i, j-1) + 1,      (insertion)<br>
                                    L(i-1, j-1) + cost  (substitution)
                                </div>
                                <p>Where cost = 0 if xᵢ = yⱼ, 1 otherwise</p>
                            </div>
                        </div>

                        <div class="interactive-container">
                            <h3>Initialization Comparison Demo</h3>
                            <div class="demo-controls">
                                <label for="init-method">Initialization Method:</label>
                                <select id="init-method">
                                    <option value="random">Random</option>
                                    <option value="kmeans++">K-means++</option>
                                </select>
                                
                                <label for="num-clusters-init">Number of Clusters:</label>
                                <input type="range" id="num-clusters-init" min="2" max="6" value="3">
                                <span id="clusters-init-display">3</span>
                                
                                <button onclick="runInitializationDemo()">Run Demo</button>
                                <button onclick="resetInitializationDemo()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="initialization-canvas">
                                <p>Click "Run Demo" to compare different initialization methods</p>
                            </div>
                        </div>
                    </div>

                    <!-- Optimization Process Section -->
                    <div id="optimization" class="content-section">
                        <h2>Optimization Process</h2>
                        
                        <div class="explanation-box">
                            <p>The K-means optimization process involves iteratively improving the clustering by alternating between assignment and update steps. Understanding this process helps in implementing efficient algorithms and analyzing convergence behavior.</p>
                        </div>

                        <div class="formula-box">
                            <h3>Assignment Step</h3>
                            <div class="formula-display">
                                <h4>Point-to-Cluster Assignment</h4>
                                <div class="formula">cᵢ = argminⱼ ||xᵢ - μⱼ||²</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>cᵢ is the cluster assignment for point xᵢ</li>
                                    <li>μⱼ is the centroid of cluster j</li>
                                    <li>argmin finds the cluster with minimum distance</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h3>Update Step</h3>
                            <div class="formula-display">
                                <h4>Centroid Recalculation</h4>
                                <div class="formula">μⱼ = (1/|Sⱼ|) Σᵢ∈Sⱼ xᵢ</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>Sⱼ is the set of points assigned to cluster j</li>
                                    <li>|Sⱼ| is the number of points in cluster j</li>
                                    <li>μⱼ is the new centroid for cluster j</li>
                                </ul>
                            </div>
                        </div>

                        <div class="model-box">
                            <h3>Optimization Properties</h3>
                            <ul>
                                <li><strong>Coordinate Descent:</strong> Alternates between optimizing assignments and centroids</li>
                                <li><strong>Monotonic Improvement:</strong> Objective function never increases</li>
                                <li><strong>Finite Convergence:</strong> Guaranteed to converge in finite steps</li>
                                <li><strong>Local Optima:</strong> May converge to local minimum</li>
                            </ul>
                        </div>

                        <div class="interactive-container">
                            <h3>Step-by-Step Optimization Demo</h3>
                            <div class="demo-controls">
                                <button onclick="stepOptimization()">Next Step</button>
                                <button onclick="runFullOptimization()">Run Full Algorithm</button>
                                <button onclick="resetOptimization()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="optimization-canvas">
                                <p>Click "Next Step" to see the optimization process step by step</p>
                            </div>
                        </div>
                    </div>

                    <!-- Convergence Analysis Section -->
                    <div id="convergence" class="content-section">
                        <h2>Convergence Analysis</h2>
                        
                        <div class="explanation-box">
                            <p>Understanding convergence properties is essential for implementing K-means correctly and determining appropriate stopping criteria. The algorithm's convergence behavior affects both computational efficiency and clustering quality.</p>
                        </div>

                        <div class="model-box">
                            <h3>Convergence Criteria</h3>
                            <ul>
                                <li><strong>Centroid Movement:</strong> Stop when centroids move less than threshold</li>
                                <li><strong>Assignment Stability:</strong> Stop when cluster assignments don't change</li>
                                <li><strong>Objective Function:</strong> Stop when WCSS improvement is minimal</li>
                                <li><strong>Maximum Iterations:</strong> Stop after fixed number of iterations</li>
                            </ul>
                        </div>

                        <div class="formula-box">
                            <h3>Convergence Conditions</h3>
                            <div class="formula-display">
                                <h4>Centroid Movement Threshold</h4>
                                <div class="formula">maxᵢ ||μᵢ^(t+1) - μᵢ^(t)|| < ε</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>μᵢ^(t) is centroid i at iteration t</li>
                                    <li>ε is the convergence threshold (typically 1e-4)</li>
                                    <li>maxᵢ finds the maximum movement across all centroids</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Convergence Guarantees</h3>
                            <p>K-means is guaranteed to converge because:</p>
                            <ul>
                                <li>The objective function is bounded below by zero</li>
                                <li>Each iteration decreases or maintains the objective function</li>
                                <li>There are only finitely many possible cluster assignments</li>
                                <li>The algorithm cannot cycle due to strict improvement</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <h4>Visualization: Convergence Behavior</h4>
                            <div class="visualization-placeholder">
                                <p>Graph showing objective function value decreasing over iterations until convergence</p>
                            </div>
                            <p><strong>Convergence Pattern:</strong> Observe how the objective function decreases rapidly in early iterations and then stabilizes.</p>
                        </div>
                    </div>

                    <!-- Interactive Demo Section -->
                    <div id="demo" class="content-section">
                        <h2>Interactive K-means Demo</h2>
                        
                        <div class="explanation-box">
                            <p>Experiment with the K-means algorithm using this interactive demo. Adjust parameters, try different initialization methods, and observe how they affect clustering results and convergence behavior.</p>
                        </div>

                        <div class="interactive-container">
                            <h3>K-means Clustering Demo</h3>
                            
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="demo-clusters">Number of Clusters:</label>
                                    <input type="range" id="demo-clusters" min="2" max="8" value="3">
                                    <span id="demo-clusters-display">3</span>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-init">Initialization:</label>
                                    <select id="demo-init">
                                        <option value="random">Random</option>
                                        <option value="kmeans++">K-means++</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-data">Data Type:</label>
                                    <select id="demo-data">
                                        <option value="blobs">Well-separated Blobs</option>
                                        <option value="random">Random Points</option>
                                        <option value="moons">Moon-shaped</option>
                                    </select>
                                </div>
                                
                                <div class="control-buttons">
                                    <button onclick="generateDemoData()">Generate Data</button>
                                    <button onclick="runKmeansDemo()">Run K-means</button>
                                    <button onclick="stepKmeansDemo()">Step-by-Step</button>
                                    <button onclick="resetDemo()">Reset</button>
                                </div>
                            </div>
                            
                            <div class="demo-status" id="demo-status">
                                <p>Click "Generate Data" to start the demo</p>
                            </div>
                            
                            <div class="metric-visualization" id="kmeans-demo-canvas">
                                <p>Interactive K-means clustering visualization will appear here</p>
                            </div>
                            
                            <div class="demo-metrics" id="demo-metrics" style="display: none;">
                                <h4>Clustering Metrics</h4>
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <span class="metric-label">WCSS:</span>
                                        <span class="metric-value" id="wcss-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Silhouette:</span>
                                        <span class="metric-value" id="silhouette-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Iterations:</span>
                                        <span class="metric-value" id="iterations-value">-</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Chapter 4 Quiz</h2>
                        
                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 1: What is the primary objective function minimized by K-means?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Between-cluster sum of squares</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>Within-cluster sum of squares (WCSS)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Silhouette coefficient</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>Calinski-Harabasz index</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means minimizes the within-cluster sum of squares (WCSS), which measures the total squared distance of all points from their cluster centroids.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 2: How are centroids updated in each K-means iteration?</h4>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>As the arithmetic mean of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the median of all points in the cluster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As the point closest to the cluster center</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>As a weighted average based on point distances</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Centroids are updated as the arithmetic mean of all points assigned to that cluster, which minimizes the WCSS for that cluster.</p>
                                </div>
                            </div>
                        </div>

                        <div class="enhanced-quiz-container">
                            <div class="enhanced-quiz-question">
                                <h4>Question 3: What is the main advantage of K-means++ initialization over random initialization?</h4>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It's faster to compute</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It guarantees global optimum</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>It provides better initialization leading to faster convergence</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>It works better with non-spherical clusters</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means++ initialization probabilistically selects initial centroids that are well-separated, leading to better starting points and faster convergence to good local minima.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <!-- Sub-section Navigation Footer -->
    <div class="sub-section-nav-footer">
        <div class="sub-nav-buttons">
            <button id="prev-subsection" class="sub-nav-btn prev-btn" style="display: none;">
                <span>← Previous</span>
                <span class="sub-nav-label" id="prev-label"></span>
            </button>
            <button id="next-subsection" class="sub-nav-btn next-btn" style="display: none;">
                <span class="sub-nav-label" id="next-label">Mathematical Foundation</span>
                <span>Next →</span>
            </button>
        </div>
    </div>

    <!-- Chapter Navigation Footer -->
    <div class="navigation-buttons">
        <a href="/tutorials/clustering/chapter3" class="azbn-btn azbn-secondary" onclick="scrollToTop()">← Chapter 3: Minkowski Distance</a>
        <a href="/tutorials/clustering/chapter5" class="azbn-btn azbn-secondary" onclick="scrollToTop()">Chapter 5: K-Means Clustering →</a>
    </div>
</body>
</html>
