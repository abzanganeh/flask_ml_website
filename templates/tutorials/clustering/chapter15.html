<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 15: Advanced Topics & Applications - Comprehensive Clustering Analysis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering/clustering.css') }}">
    <script src="{{ url_for('static', filename='js/tutorials/clustering/shared-tutorial.js') }}"></script>
    <script src="{{ url_for('static', filename='js/tutorials/clustering/chapter4.js') }}"></script>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering" class="course-link">
                    <span>Comprehensive Clustering Analysis</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <!-- Tutorial Header -->
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 4: K-means Clustering Algorithm</h1>
                <p class="chapter-subtitle">Master the most popular clustering algorithm from mathematical foundations to practical implementation</p>
                
                <!-- Chapter Progress Bar (4/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="26.67"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering/chapter1" class="chapter-nav-btn">Chapter 1</a>
                    <a href="/tutorials/clustering/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/clustering/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/clustering/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/clustering/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/clustering/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering/chapter15" class="chapter-nav-btn active">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.29"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="section-nav-btn active" data-section="algorithm">Algorithm Overview</button>
                    <button class="section-nav-btn" data-section="mathematics">Mathematical Foundation</button>
                    <button class="section-nav-btn" data-section="initialization">Initialization Methods</button>
                    <button class="section-nav-btn" data-section="optimization">Optimization Process</button>
                    <button class="section-nav-btn" data-section="convergence">Convergence Analysis</button>
                    <button class="section-nav-btn" data-section="demo">Interactive Demo</button>
                    <button class="section-nav-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives -->
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the K-means algorithm step-by-step process</li>
                        <li>Master the mathematical foundations and objective function</li>
                        <li>Learn different initialization methods and their impact</li>
                        <li>Analyze convergence properties and stopping criteria</li>
                        <li>Implement K-means from scratch with interactive demos</li>
                        <li>Compare different distance metrics in K-means clustering</li>
                        <li>Evaluate clustering quality using various metrics</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">
                    <!-- Algorithm Overview Section -->
                    <div id="algorithm" class="content-section active">
                        <h2>Real-World Clustering Applications</h2>
                        
                        <div class="learning-objectives-box">
                            <h3>Learning Objectives</h3>
                            <ul>
                                <li>Analyze real-world clustering applications across multiple domains</li>
                                <li>Master advanced clustering techniques: ensemble methods, deep clustering, streaming</li>
                                <li>Understand domain-specific challenges and solution strategies</li>
                                <li>Learn preprocessing and feature engineering for complex data types</li>
                                <li>Explore emerging trends in clustering research and applications</li>
                                <li>Develop skills for selecting appropriate methods for specific problems</li>
                                <li>Practice end-to-end clustering project implementation</li>
                                <li>Understand scalability challenges and big data clustering solutions</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h3>Evolution of Clustering Applications</h3>
                            <p>Clustering has evolved from simple taxonomy problems in the 1960s to sophisticated applications in modern AI systems. This chapter bridges the gap between theoretical knowledge and practical implementation across diverse domains.</p>
                            
                            <h4>Traditional Applications (1960s-1990s)</h4>
                            <ul>
                                <li><strong>Market Research:</strong> Customer segmentation and demographic analysis</li>
                                <li><strong>Biological Taxonomy:</strong> Species classification and evolutionary relationships</li>
                                <li><strong>Psychology:</strong> Personality profiling and behavioral grouping</li>
                                <li><strong>Manufacturing:</strong> Quality control and process optimization</li>
                            </ul>

                            <h4>Digital Era Applications (1990s-2010s)</h4>
                            <ul>
                                <li><strong>Web Mining:</strong> Document clustering and information retrieval</li>
                                <li><strong>Bioinformatics:</strong> Gene expression analysis and protein folding</li>
                                <li><strong>Computer Vision:</strong> Image segmentation and object recognition</li>
                                <li><strong>Network Analysis:</strong> Community detection in social networks</li>
                            </ul>

                            <h4>Big Data Era Applications (2010s-Present)</h4>
                            <ul>
                                <li><strong>Deep Learning:</strong> Representation learning and neural clustering</li>
                                <li><strong>IoT and Sensors:</strong> Real-time streaming data analysis</li>
                                <li><strong>Precision Medicine:</strong> Personalized treatment strategies</li>
                                <li><strong>Smart Cities:</strong> Urban planning and resource optimization</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <img src="{{ url_for('static', filename='images/tutorials/clustering/chapter15/timeline_evolution.png') }}" alt="Clustering Application Timeline" class="tutorial-image">
                            <p class="image-caption">Interactive timeline showing evolution of clustering applications from 1960s to present</p>
                        </div>

                        <div class="formula-box">
                            <h3>Framework for Application Analysis</h3>
                            
                            <h4>Step 1: Problem Understanding</h4>
                            <ul>
                                <li><strong>Domain expertise:</strong> Understand the field and its specific requirements</li>
                                <li><strong>Business objectives:</strong> Clarify what success looks like</li>
                                <li><strong>Data characteristics:</strong> Analyze data type, size, and quality</li>
                                <li><strong>Constraints:</strong> Identify computational, time, and resource limitations</li>
                            </ul>
                            
                            <h4>Step 2: Method Selection</h4>
                            <ul>
                                <li><strong>Algorithm choice:</strong> Select appropriate clustering method</li>
                                <li><strong>Parameter tuning:</strong> Optimize algorithm parameters</li>
                                <li><strong>Validation strategy:</strong> Choose evaluation metrics</li>
                                <li><strong>Implementation approach:</strong> Consider scalability and deployment</li>
                            </ul>
                            
                            <h4>Step 3: Implementation and Evaluation</h4>
                            <ul>
                                <li><strong>Preprocessing:</strong> Clean and prepare data</li>
                                <li><strong>Feature engineering:</strong> Create relevant features</li>
                                <li><strong>Model training:</strong> Execute clustering algorithm</li>
                                <li><strong>Results interpretation:</strong> Analyze and validate outcomes</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Mathematical Foundation Section -->
                    <div id="mathematics" class="content-section">
                        <h2>Initialization Methods: Setting the Stage for Success</h2>
                        
                        <p>The initialization phase of K-means clustering is critical for achieving high-quality results. Poor initialization can lead to suboptimal local minima, slow convergence, and inconsistent results across runs. This section explores various initialization strategies, from simple random selection to sophisticated heuristics.</p>

                        <h3>Random Initialization: The Baseline</h3>
                        <p>The simplest approach randomly selects k data points as initial centroids, but this method has significant limitations.</p>

                        <div class="model-box">
                            <h4>Random Initialization Algorithm</h4>
                            
                            <div class="formula-box">
                                <h5><strong>Basic Random Selection:</strong></h5>
                                <div class="code-box">
<strong>function</strong> random_init(X, k):
    n, d = X.shape
    indices = random_sample(n, k)  <span style="color: #1976d2;">// Sample k indices without replacement</span>
    centroids = X[indices]          <span style="color: #1976d2;">// Select corresponding data points</span>
    <strong>return</strong> centroids
                                </div>
                                
                                <h5><strong>Random Uniform in Feature Space:</strong></h5>
                                <div class="code-box">
<strong>function</strong> random_uniform_init(X, k):
    n, d = X.shape
    min_vals = min(X, axis=0)       <span style="color: #1976d2;">// Feature-wise minimum</span>
    max_vals = max(X, axis=0)       <span style="color: #1976d2;">// Feature-wise maximum</span>
    centroids = uniform(min_vals, max_vals, size=(k, d))
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Advantages of Random Initialization:</h5>
                            <ul>
                                <li><strong>Simplicity:</strong> Easy to implement and understand</li>
                                <li><strong>Speed:</strong> O(kd) time complexity</li>
                                <li><strong>Unbiased:</strong> No assumptions about data structure</li>
                                <li><strong>Baseline:</strong> Good reference for comparing other methods</li>
                            </ul>
                            
                            <h5>Disadvantages:</h5>
                            <ul>
                                <li><strong>High variance:</strong> Results vary significantly across runs</li>
                                <li><strong>Poor clustering:</strong> Often leads to suboptimal solutions</li>
                                <li><strong>Slow convergence:</strong> May require many iterations</li>
                                <li><strong>Empty clusters:</strong> Risk of centroids in sparse regions</li>
                            </ul>
                        </div>

                        <h3>Furthest-First Heuristic</h3>
                        <p>This method iteratively selects centroids that are as far as possible from previously selected ones, promoting good coverage of the data space.</p>

                        <div class="model-box">
                            <h4>Furthest-First Initialization</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
<strong>function</strong> furthest_first_init(X, k):
    n, d = X.shape
    centroids = []
    
    <span style="color: #1976d2;">// Step 1: Choose first centroid randomly</span>
    first_idx = random_choice(n)
    centroids.append(X[first_idx])
    
    <span style="color: #1976d2;">// Step 2: Iteratively choose furthest points</span>
    <strong>for</strong> i = 2 <strong>to</strong> k:
        max_distance = -1
        furthest_idx = -1
        
        <strong>for</strong> j = 1 <strong>to</strong> n:
            <span style="color: #1976d2;">// Find minimum distance to existing centroids</span>
            min_dist = min([distance(X[j], c) <strong>for</strong> c <strong>in</strong> centroids])
            
            <strong>if</strong> min_dist > max_distance:
                max_distance = min_dist
                furthest_idx = j
        
        centroids.append(X[furthest_idx])
    
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Advantages:</h5>
                            <ul>
                                <li><strong>Good coverage:</strong> Centroids spread across data space</li>
                                <li><strong>Deterministic:</strong> Same result for same first choice</li>
                                <li><strong>No empty clusters:</strong> Guarantees centroids on data points</li>
                                <li><strong>Better than random:</strong> Generally produces better initializations</li>
                            </ul>
                            
                            <h5>Disadvantages:</h5>
                            <ul>
                                <li><strong>Outlier sensitivity:</strong> May select extreme outliers</li>
                                <li><strong>Computational cost:</strong> O(nk) time complexity</li>
                                <li><strong>Still suboptimal:</strong> Not guaranteed to find good initializations</li>
                                <li><strong>First choice matters:</strong> Quality depends on initial random selection</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Initialization Methods Section -->
                    <div id="initialization" class="content-section">
                        <h2>K-means++: The Smart Initialization Revolution</h2>
                        
                        <p>K-means++ represents a breakthrough in K-means initialization, providing both theoretical guarantees and practical improvements. Developed by Arthur and Vassilvitskii in 2007, this method uses probabilistic selection to choose initial centroids that are likely to be well-separated, leading to better clustering results.</p>

                        <h3>The K-means++ Algorithm</h3>
                        <p>K-means++ carefully selects initial centroids using a probability distribution that favors points far from existing centroids.</p>

                        <div class="model-box">
                            <h4>K-means++ Initialization Algorithm</h4>
                            
                            <div class="formula-box">
                                <div class="code-box">
<strong>function</strong> kmeans_plus_plus(X, k):
    n, d = X.shape
    centroids = []
    
    <span style="color: #1976d2;">// Step 1: Choose first centroid uniformly at random</span>
    first_idx = random_choice(n)
    centroids.append(X[first_idx])
    
    <span style="color: #1976d2;">// Step 2: Choose remaining k-1 centroids</span>
    <strong>for</strong> i = 2 <strong>to</strong> k:
        distances = []
        
        <span style="color: #1976d2;">// Compute squared distance to nearest existing centroid</span>
        <strong>for</strong> j = 1 <strong>to</strong> n:
            min_dist_sq = min([||X[j] - c||² <strong>for</strong> c <strong>in</strong> centroids])
            distances.append(min_dist_sq)
        
        <span style="color: #1976d2;">// Choose next centroid with probability proportional to squared distance</span>
        probabilities = distances / sum(distances)
        next_idx = weighted_random_choice(probabilities)
        centroids.append(X[next_idx])
    
    <strong>return</strong> centroids
                                </div>
                            </div>
                            
                            <h5>Key Insight:</h5>
                            <p>The probability of selecting a point as the next centroid is proportional to its squared distance from the nearest existing centroid. This creates a bias toward points that are far from current centroids, promoting good spatial distribution.</p>
                            
                            <h5>Mathematical Formulation:</h5>
                            <div class="formula-box">
                                <p>For selecting the (j+1)-th centroid, given j existing centroids C = {c₁, c₂, ..., cⱼ}:</p>
                                <div class="formula-display">
                                    <strong>P(xᵢ) = D²(xᵢ) / Σₖ D²(xₖ)</strong>
                                </div>
                                <p>Where D²(xᵢ) = min_{c∈C} ||xᵢ - c||² is the squared distance to the nearest centroid.</p>
                            </div>
                        </div>

                        <h3>Theoretical Analysis</h3>
                        <p>K-means++ comes with strong theoretical guarantees that explain its superior performance.</p>

                        <div class="model-box">
                            <h4>K-means++ Approximation Guarantee</h4>
                            
                            <h5>Main Theorem (Arthur & Vassilvitskii, 2007):</h5>
                            <div class="formula-box">
                                <p><strong>Theorem:</strong> K-means++ initialization followed by Lloyd's algorithm produces a solution with expected cost at most O(log k) times the optimal k-means cost.</p>
                                
                                <p><strong>Formally:</strong> E[cost(K-means++ solution)] ≤ 8(ln k + 2) × OPT</p>
                                
                                <p>Where OPT is the cost of the optimal k-means clustering.</p>
                            </div>
                            
                            <h5>Proof Sketch:</h5>
                            <ol>
                                <li><strong>Potential function:</strong> Define Φ = Σᵢ D²(xᵢ) as sum of squared distances to nearest centroids</li>
                                <li><strong>Expected reduction:</strong> Each K-means++ step reduces E[Φ] by a constant factor</li>
                                <li><strong>Concentration:</strong> Use probability tail bounds to show consistent performance</li>
                                <li><strong>Optimality bound:</strong> Relate final potential to optimal clustering cost</li>
                            </ol>
                            
                            <h5>Implications:</h5>
                            <ul>
                                <li><strong>Logarithmic guarantee:</strong> Performance degrades slowly with k</li>
                                <li><strong>Probabilistic bound:</strong> Guarantee holds in expectation</li>
                                <li><strong>Initialization only:</strong> Bound applies to initialization, Lloyd's improves it</li>
                                <li><strong>Practical relevance:</strong> Constant factors are reasonable in practice</li>
                            </ul>
                        </div>

                        <div class="interactive-container">
                            <h3>Initialization Comparison Demo</h3>
                            <div class="demo-controls">
                                <label for="init-method">Initialization Method:</label>
                                <select id="init-method">
                                    <option value="random">Random</option>
                                    <option value="kmeans++">K-means++</option>
                                </select>
                                
                                <label for="num-clusters-init">Number of Clusters:</label>
                                <input type="range" id="num-clusters-init" min="2" max="6" value="3">
                                <span id="clusters-init-display">3</span>
                                
                                <button onclick="runInitializationDemo()">Run Demo</button>
                                <button onclick="resetInitializationDemo()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="initialization-canvas">
                                <p>Click "Run Demo" to compare different initialization methods</p>
                            </div>
                        </div>
                    </div>

                    <!-- Optimization Process Section -->
                    <div id="optimization" class="content-section">
                        <h2>Optimization Process</h2>
                        
                        <div class="explanation-box">
                            <p>The K-means optimization process involves iteratively improving the clustering by alternating between assignment and update steps. Understanding this process helps in implementing efficient algorithms and analyzing convergence behavior.</p>
                        </div>

                        <div class="formula-box">
                            <h3>Assignment Step</h3>
                            <div class="formula-display">
                                <h4>Point-to-Cluster Assignment</h4>
                                <div class="formula">cᵢ = argminⱼ ||xᵢ - μⱼ||²</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>cᵢ is the cluster assignment for point xᵢ</li>
                                    <li>μⱼ is the centroid of cluster j</li>
                                    <li>argmin finds the cluster with minimum distance</li>
                                </ul>
                            </div>
                        </div>

                        <div class="formula-box">
                            <h3>Update Step</h3>
                            <div class="formula-display">
                                <h4>Centroid Recalculation</h4>
                                <div class="formula">μⱼ = (1/|Sⱼ|) Σᵢ∈Sⱼ xᵢ</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>Sⱼ is the set of points assigned to cluster j</li>
                                    <li>|Sⱼ| is the number of points in cluster j</li>
                                    <li>μⱼ is the new centroid for cluster j</li>
                                </ul>
                            </div>
                        </div>

                        <div class="model-box">
                            <h3>Optimization Properties</h3>
                            <ul>
                                <li><strong>Coordinate Descent:</strong> Alternates between optimizing assignments and centroids</li>
                                <li><strong>Monotonic Improvement:</strong> Objective function never increases</li>
                                <li><strong>Finite Convergence:</strong> Guaranteed to converge in finite steps</li>
                                <li><strong>Local Optima:</strong> May converge to local minimum</li>
                            </ul>
                        </div>

                        <div class="interactive-container">
                            <h3>Step-by-Step Optimization Demo</h3>
                            <div class="demo-controls">
                                <button onclick="stepOptimization()">Next Step</button>
                                <button onclick="runFullOptimization()">Run Full Algorithm</button>
                                <button onclick="resetOptimization()">Reset</button>
                            </div>
                            
                            <div class="metric-visualization" id="optimization-canvas">
                                <p>Click "Next Step" to see the optimization process step by step</p>
                            </div>
                        </div>
                    </div>

                    <!-- Convergence Analysis Section -->
                    <div id="convergence" class="content-section">
                        <h2>Convergence Analysis</h2>
                        
                        <div class="explanation-box">
                            <p>Understanding convergence properties is essential for implementing K-means correctly and determining appropriate stopping criteria. The algorithm's convergence behavior affects both computational efficiency and clustering quality.</p>
                        </div>

                        <div class="model-box">
                            <h3>Convergence Criteria</h3>
                            <ul>
                                <li><strong>Centroid Movement:</strong> Stop when centroids move less than threshold</li>
                                <li><strong>Assignment Stability:</strong> Stop when cluster assignments don't change</li>
                                <li><strong>Objective Function:</strong> Stop when WCSS improvement is minimal</li>
                                <li><strong>Maximum Iterations:</strong> Stop after fixed number of iterations</li>
                            </ul>
                        </div>

                        <div class="formula-box">
                            <h3>Convergence Conditions</h3>
                            <div class="formula-display">
                                <h4>Centroid Movement Threshold</h4>
                                <div class="formula">maxᵢ ||μᵢ^(t+1) - μᵢ^(t)|| < ε</div>
                                <p><strong>Where:</strong></p>
                                <ul>
                                    <li>μᵢ^(t) is centroid i at iteration t</li>
                                    <li>ε is the convergence threshold (typically 1e-4)</li>
                                    <li>maxᵢ finds the maximum movement across all centroids</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Convergence Guarantees</h3>
                            <p>K-means is guaranteed to converge because:</p>
                            <ul>
                                <li>The objective function is bounded below by zero</li>
                                <li>Each iteration decreases or maintains the objective function</li>
                                <li>There are only finitely many possible cluster assignments</li>
                                <li>The algorithm cannot cycle due to strict improvement</li>
                            </ul>
                        </div>

                        <div class="image-container">
                            <h4>Visualization: Convergence Behavior</h4>
                            <div class="visualization-placeholder">
                                <p>Graph showing objective function value decreasing over iterations until convergence</p>
                            </div>
                            <p><strong>Convergence Pattern:</strong> Observe how the objective function decreases rapidly in early iterations and then stabilizes.</p>
                        </div>
                    </div>

                    <!-- Interactive Demo Section -->
                    <div id="demo" class="content-section">
                        <h2>Interactive K-means Demo</h2>
                        
                        <div class="explanation-box">
                            <p>Experiment with the K-means algorithm using this interactive demo. Adjust parameters, try different initialization methods, and observe how they affect clustering results and convergence behavior.</p>
                        </div>

                        <div class="interactive-container">
                            <h3>K-means Clustering Demo</h3>
                            
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="demo-clusters">Number of Clusters:</label>
                                    <input type="range" id="demo-clusters" min="2" max="8" value="3">
                                    <span id="demo-clusters-display">3</span>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-init">Initialization:</label>
                                    <select id="demo-init">
                                        <option value="random">Random</option>
                                        <option value="kmeans++">K-means++</option>
                                    </select>
                                </div>
                                
                                <div class="control-group">
                                    <label for="demo-data">Data Type:</label>
                                    <select id="demo-data">
                                        <option value="blobs">Well-separated Blobs</option>
                                        <option value="random">Random Points</option>
                                        <option value="moons">Moon-shaped</option>
                                    </select>
                                </div>
                                
                                <div class="control-buttons">
                                    <button onclick="generateDemoData()">Generate Data</button>
                                    <button onclick="runKmeansDemo()">Run K-means</button>
                                    <button onclick="stepKmeansDemo()">Step-by-Step</button>
                                    <button onclick="resetDemo()">Reset</button>
                                </div>
                            </div>
                            
                            <div class="demo-status" id="demo-status">
                                <p>Click "Generate Data" to start the demo</p>
                            </div>
                            
                            <div class="metric-visualization" id="kmeans-demo-canvas">
                                <p>Interactive K-means clustering visualization will appear here</p>
                            </div>
                            
                            <div class="demo-metrics" id="demo-metrics" style="display: none;">
                                <h4>Clustering Metrics</h4>
                                <div class="metrics-grid">
                                    <div class="metric-item">
                                        <span class="metric-label">WCSS:</span>
                                        <span class="metric-value" id="wcss-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Silhouette:</span>
                                        <span class="metric-value" id="silhouette-value">-</span>
                                    </div>
                                    <div class="metric-item">
                                        <span class="metric-label">Iterations:</span>
                                        <span class="metric-value" id="iterations-value">-</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz Section -->
                    <div id="quiz" class="content-section">
                        <h2>Comprehensive Clustering Knowledge Assessment</h2>
                        <p style="margin-bottom: 2rem;">Test your understanding of clustering concepts from basic fundamentals to advanced applications. This quiz includes interview-style questions commonly asked in data science positions.</p>

                        <!-- Quiz Questions -->
                        <div class="enhanced-quiz-container">
                            <!-- Question 1 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 1: Distance Metrics Fundamentals</h4>
                                <p><strong>What is the main difference between Manhattan and Euclidean distance, and when would you prefer one over the other?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Manhattan uses squares, Euclidean uses absolute values</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Euclidean measures straight-line distance, Manhattan measures grid-based distance</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Manhattan is always faster to compute than Euclidean</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) They are mathematically equivalent for clustering</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Euclidean distance measures straight-line distance, while Manhattan distance measures grid-based distance. Manhattan is preferred when movement is constrained to grid-like paths or when features have different units/scales.</p>
                                </div>
                            </div>

                            <!-- Question 2 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 2: K-Means Algorithm Theory</h4>
                                <p><strong>What is the computational complexity of the K-means algorithm?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) O(n log n)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) O(n²)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>c) O(nkti) where n=samples, k=clusters, t=iterations, i=dimensions</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) O(k²n)</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means has O(nkti) complexity where n is the number of samples, k is the number of clusters, t is the number of iterations, and i is the number of dimensions. This is because each iteration requires computing distances between all points and all centroids.</p>
                                </div>
                            </div>

                            <!-- Question 3 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 3: Optimal K Selection</h4>
                                <p><strong>You observe an elbow curve that shows a gradual decline without a clear elbow. What does this suggest?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) The data has very clear cluster structure</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) The data may not have well-defined clusters or has hierarchical structure</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) K-means is the wrong algorithm for this data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) More data is needed to find the optimal k</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> A gradual decline without a clear elbow suggests the data may not have well-defined clusters or has hierarchical structure. This indicates that K-means might not be the best choice, and you should consider hierarchical clustering or other methods.</p>
                                </div>
                            </div>

                            <!-- Question 4 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 4: DBSCAN Parameters</h4>
                                <p><strong>In DBSCAN, what happens if you set eps too small?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) All points become core points</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Most points become noise/outliers</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) The algorithm runs faster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Clusters become more spherical</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> If eps is too small, most points won't have enough neighbors within the eps radius to become core points, so they'll be classified as noise/outliers. This results in very few or no clusters being formed.</p>
                                </div>
                            </div>

                            <!-- Question 5 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 5: Hierarchical Clustering</h4>
                                <p><strong>What is the key advantage of Ward linkage over single linkage in hierarchical clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Ward linkage is faster to compute</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Ward linkage minimizes within-cluster variance and avoids chaining effect</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Ward linkage works with any distance metric</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Ward linkage produces more clusters</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Ward linkage minimizes within-cluster variance and avoids the chaining effect that single linkage can create. This results in more compact, spherical clusters compared to the elongated chains that single linkage often produces.</p>
                                </div>
                            </div>

                            <!-- Question 6 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 6: Gaussian Mixture Models</h4>
                                <p><strong>What is the main assumption that GMM makes about cluster shape that K-means doesn't?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Clusters must be of equal size</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Clusters can have different covariance structures (elliptical shapes)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Clusters must be linearly separable</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Data points can only belong to one cluster</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> GMM allows clusters to have different covariance structures, enabling elliptical shapes of varying sizes and orientations. K-means assumes spherical clusters with equal variance, making GMM more flexible for complex cluster shapes.</p>
                                </div>
                            </div>

                            <!-- Question 7 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 7: Evaluation Metrics</h4>
                                <p><strong>Which clustering evaluation metric requires ground truth labels?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Silhouette coefficient</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Davies-Bouldin index</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>c) Adjusted Rand Index (ARI)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Calinski-Harabasz index</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> The Adjusted Rand Index (ARI) is an external validation metric that requires ground truth labels to compare the clustering results with the true cluster assignments. The other metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz) are internal metrics that don't require ground truth.</p>
                                </div>
                            </div>

                            <!-- Question 8 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 8: Preprocessing Decisions</h4>
                                <p><strong>Why is feature scaling particularly important for K-means clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) It makes the algorithm converge faster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) K-means uses Euclidean distance, so features with larger scales dominate</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) It reduces the number of required clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) It's not important for K-means</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means uses Euclidean distance, so features with larger scales will dominate the distance calculations. Without scaling, a feature with values in the thousands will have much more influence than a feature with values between 0-1, leading to biased clustering results.</p>
                                </div>
                            </div>

                            <!-- Question 9 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 9: Interview Question - Algorithm Selection</h4>
                                <p><strong>You have a dataset with 100,000 points, unknown number of clusters, and clusters of varying densities. Which algorithm would you start with and why?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) K-means because it's fast and scalable</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) DBSCAN because it can find varying density clusters and doesn't require pre-specifying k</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Hierarchical clustering because it shows all possible cluster numbers</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) GMM because it handles complex cluster shapes</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> DBSCAN is ideal for this scenario because it can automatically find clusters of varying densities without requiring you to specify the number of clusters beforehand. It also handles noise/outliers well and is reasonably scalable for large datasets.</p>
                                </div>
                            </div>

                            <!-- Question 10 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 10: Curse of Dimensionality</h4>
                                <p><strong>How does high dimensionality affect distance-based clustering algorithms?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) It makes clustering more accurate</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Distance between points becomes less meaningful as all pairs become equidistant</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) It only affects K-means but not other algorithms</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) It reduces computational complexity</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> In high dimensions, the curse of dimensionality causes all pairwise distances to become similar, making it difficult for distance-based algorithms to distinguish between different clusters. This affects all distance-based clustering methods, not just K-means.</p>
                                </div>
                            </div>

                            <!-- Question 11 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 11: Practical Application</h4>
                                <p><strong>For customer segmentation in e-commerce, which features would be most appropriate for clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Only demographic information (age, location)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Purchase behavior, frequency, monetary value, and product preferences</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only transaction timestamps</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Customer ID and registration date</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> For customer segmentation, behavioral features like purchase patterns, frequency, monetary value, and product preferences are most meaningful. These features capture actual customer behavior and preferences, making them ideal for creating actionable customer segments.</p>
                                </div>
                            </div>

                            <!-- Question 12 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 12: Algorithm Limitations</h4>
                                <p><strong>Which clustering algorithm struggles most with clusters of different sizes?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) DBSCAN</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) K-means</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Hierarchical clustering</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Mean shift</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means assumes clusters of roughly equal size and tends to create clusters of similar sizes. When clusters have very different sizes, K-means may split large clusters or merge small ones, leading to poor clustering results.</p>
                                </div>
                            </div>

                            <!-- Question 13 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 13: Interview Question - Debugging</h4>
                                <p><strong>Your K-means results are inconsistent across runs. What are the most likely causes and solutions?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Poor data quality - clean the data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Random initialization causing local optima - use K-means++ or multiple runs</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Wrong distance metric - try different metrics</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Dataset is too small - collect more data</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means can get stuck in local optima due to random initialization. The solution is to use K-means++ initialization or run the algorithm multiple times with different initializations and choose the best result based on the objective function.</p>
                                </div>
                            </div>

                            <!-- Question 14 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 14: Spectral Clustering</h4>
                                <p><strong>What is the key insight behind spectral clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) It uses spectrum analysis of colors in images</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) It uses eigenvalues of similarity matrices to find clusters in lower dimensions</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) It only works with numerical data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) It finds clusters by spectral decomposition of distance matrices</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Spectral clustering uses the eigenvalues and eigenvectors of similarity matrices to transform the data into a lower-dimensional space where clusters are more easily separable. This approach can handle non-convex clusters that traditional methods struggle with.</p>
                                </div>
                            </div>

                            <!-- Question 15 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 15: Big Data Considerations</h4>
                                <p><strong>For clustering a dataset with 10 million samples, which approach would be most practical?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Hierarchical clustering with complete linkage</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Mini-batch K-means or distributed clustering frameworks</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) DBSCAN with high eps value</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Gaussian Mixture Models with many components</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> For large datasets, mini-batch K-means or distributed clustering frameworks are most practical. Hierarchical clustering has O(n³) complexity and is infeasible for 10 million samples. Mini-batch methods provide good approximations with much lower computational cost.</p>
                                </div>
                            </div>

                            <!-- Question 16 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 16: Time Series Clustering</h4>
                                <p><strong>What distance metric is most appropriate for clustering time series with similar shapes but different phases?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Euclidean distance</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Dynamic Time Warping (DTW)</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Manhattan distance</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Cosine similarity</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Dynamic Time Warping (DTW) is ideal for time series with similar shapes but different phases because it can align sequences by warping the time axis, allowing it to find similarities even when the patterns are shifted in time.</p>
                                </div>
                            </div>

                            <!-- Question 17 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 17: Deep Clustering</h4>
                                <p><strong>What is the main advantage of deep clustering over traditional clustering methods?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) It's always faster</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) It learns representations and clusters jointly, potentially finding better clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) It doesn't require any hyperparameter tuning</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) It works only with image data</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Deep clustering learns feature representations and clustering assignments jointly, which can discover better clusters by finding optimal representations for the clustering task. This is particularly powerful for high-dimensional data where traditional methods struggle.</p>
                                </div>
                            </div>

                            <!-- Question 18 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 18: Interview Question - Business Impact</h4>
                                <p><strong>How would you measure the success of a customer segmentation clustering project?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Only silhouette score and other internal metrics</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Business metrics like campaign response rates, customer lifetime value improvement</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Number of clusters found</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Algorithm execution time</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> While technical metrics like silhouette score are important, the ultimate measure of success for customer segmentation is business impact. Metrics like improved campaign response rates, increased customer lifetime value, and better targeting effectiveness demonstrate real value.</p>
                                </div>
                            </div>

                            <!-- Question 19 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 19: Ensemble Clustering</h4>
                                <p><strong>What is the main benefit of ensemble clustering methods?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) They always produce more clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) They improve robustness and stability by combining multiple clustering results</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) They only work with K-means</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) They reduce computational complexity</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Ensemble clustering combines multiple clustering results to create more robust and stable clusters. By aggregating different algorithms or different runs, ensemble methods can reduce the impact of individual algorithm weaknesses and produce more reliable results.</p>
                                </div>
                            </div>

                            <!-- Question 20 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 20: Categorical Data Clustering</h4>
                                <p><strong>Why can't you directly use K-means on categorical data?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) K-means requires computing means, which is undefined for categorical data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Categorical data has too many dimensions</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) K-means only works with normalized data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) There's no reason; K-means works fine with categorical data</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> K-means requires computing centroids (means) of clusters, which is mathematically undefined for categorical data. You need to use algorithms like K-modes, K-prototypes, or convert categorical data to numerical representations first.</p>
                                </div>
                            </div>

                            <!-- Question 21 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 21: Network/Graph Clustering</h4>
                                <p><strong>What does modularity measure in network clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) The speed of the clustering algorithm</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) The strength of division into communities compared to a random network</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) The number of clusters found</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) The balance between cluster sizes</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Modularity measures how much better the clustering is compared to a random network. It quantifies the strength of community structure by comparing the number of edges within communities to what would be expected in a random network with the same degree distribution.</p>
                                </div>
                            </div>

                            <!-- Question 22 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 22: Interview Question - Performance Optimization</h4>
                                <p><strong>Your clustering algorithm is taking too long on a large dataset. What optimization strategies would you try?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Only use sampling to reduce dataset size</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Sampling, approximation methods, distributed computing, or algorithm substitution</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Just use fewer clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Remove all preprocessing steps</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Multiple optimization strategies should be considered: sampling for initial exploration, approximation methods like mini-batch algorithms, distributed computing frameworks, or switching to more scalable algorithms like DBSCAN or streaming clustering methods.</p>
                                </div>
                            </div>

                            <!-- Question 23 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 23: Semi-supervised Clustering</h4>
                                <p><strong>In semi-supervised clustering, what additional information is typically provided?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) More unlabeled data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Pairwise constraints (must-link or cannot-link) or limited labeled examples</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Different distance metrics</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) The optimal number of clusters</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Semi-supervised clustering uses additional information like pairwise constraints (must-link/cannot-link) or limited labeled examples to guide the clustering process. This helps improve clustering quality by incorporating domain knowledge.</p>
                                </div>
                            </div>

                            <!-- Question 24 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 24: Clustering Stability</h4>
                                <p><strong>What does bootstrap resampling tell you about clustering results?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) The optimal number of clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) How stable/reliable the clusters are across different samples</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) The best algorithm to use</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) The computational complexity</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Bootstrap resampling helps assess clustering stability by repeatedly sampling from the data and checking how consistent the clustering results are. Stable clusters that appear consistently across different samples are more reliable and meaningful.</p>
                                </div>
                            </div>

                            <!-- Question 25 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 25: Interview Question - Real-world Challenges</h4>
                                <p><strong>You're clustering customer data and find that 80% of customers fall into one large cluster. What might be happening and how would you address it?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) This is normal and expected</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Possible issues: poor feature selection, need for data transformation, or hierarchical clustering within the large group</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Just use more clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) The algorithm is working incorrectly</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> A large cluster containing 80% of customers suggests poor feature selection, inadequate data transformation, or that the large group needs further subdivision. Solutions include feature engineering, data transformation, or applying hierarchical clustering within the large cluster.</p>
                                </div>
                            </div>

                            <!-- Question 26 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 26: Multi-view Clustering</h4>
                                <p><strong>What is the main challenge in multi-view clustering?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) Combining information from different data representations while preserving their unique characteristics</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) It requires more computational resources</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) It only works with image data</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) It always produces worse results than single-view clustering</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> The main challenge in multi-view clustering is effectively combining information from different data representations (views) while preserving their unique characteristics. This requires careful balance between view-specific information and consensus across views.</p>
                                </div>
                            </div>

                            <!-- Question 27 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 27: Anomaly Detection vs Clustering</h4>
                                <p><strong>How can clustering be used for anomaly detection?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) Points far from cluster centers or in very small clusters can be considered anomalies</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Clustering cannot be used for anomaly detection</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only DBSCAN can be used for this purpose</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Anomalies are always in the largest cluster</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Clustering can identify anomalies by finding points that are far from cluster centers or belong to very small clusters. DBSCAN explicitly identifies noise points, while other algorithms can flag outliers based on distance to centroids or cluster size.</p>
                                </div>
                            </div>

                            <!-- Question 28 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 28: Interview Question - Data Drift</h4>
                                <p><strong>How would you detect if your deployed clustering model needs to be retrained due to data drift?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) Monitor cluster assignment distributions and data statistics over time</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Retrain every week regardless</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only retrain when users complain</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Data drift doesn't affect clustering models</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Monitor cluster assignment distributions, data statistics, and clustering quality metrics over time. Significant changes in these metrics indicate data drift and the need for model retraining. This proactive approach ensures the model remains effective.</p>
                                </div>
                            </div>

                            <!-- Question 29 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 29: Clustering for Recommendation Systems</h4>
                                <p><strong>In a recommendation system, how can clustering be used to address the cold start problem?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) Cluster existing users and assign new users to similar clusters for initial recommendations</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Clustering cannot help with cold start problems</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only use demographic data for clustering</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Wait until new users provide enough interaction data</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> By clustering existing users based on their preferences and behavior, you can assign new users to similar clusters and provide recommendations based on the preferences of users in the same cluster. This helps address the cold start problem for new users.</p>
                                </div>
                            </div>

                            <!-- Question 30 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 30: Interview Question - Ethical Considerations</h4>
                                <p><strong>What ethical considerations should you keep in mind when using clustering for customer segmentation?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) There are no ethical considerations in clustering</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Avoiding discriminatory features, ensuring fairness across groups, and transparency in decision-making</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only consider accuracy metrics</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Ethics only matter in supervised learning</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Ethical considerations in clustering include avoiding discriminatory features, ensuring fairness across demographic groups, maintaining transparency in decision-making processes, and being aware of potential biases in the data and algorithms used.</p>
                                </div>
                            </div>

                            <!-- Question 31 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 31: Feature Engineering for Clustering</h4>
                                <p><strong>When preparing features for clustering, which preprocessing step is most critical?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>a) Feature scaling/normalization</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>b) Adding more features</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Removing all missing values</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Converting all features to categorical</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Feature scaling/normalization is critical for clustering because distance-based algorithms are sensitive to feature scales. Without proper scaling, features with larger scales will dominate the distance calculations, leading to biased clustering results.</p>
                                </div>
                            </div>

                            <!-- Question 32 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 32: Interview Question - Model Selection</h4>
                                <p><strong>A stakeholder asks you to explain why you chose DBSCAN over K-means for their customer data. What would be your key points?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) DBSCAN is newer and therefore better</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) DBSCAN handles noise, finds arbitrary shaped clusters, and doesn't require predetermining cluster count</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) DBSCAN is always faster than K-means</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) K-means doesn't work with customer data</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> DBSCAN's key advantages include: handling noise/outliers explicitly, finding clusters of arbitrary shapes (not just spherical), and automatically determining the number of clusters. This makes it particularly suitable for customer data where clusters may have irregular shapes and contain outliers.</p>
                                </div>
                            </div>

                            <!-- Question 33 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 33: Validation and Interpretation</h4>
                                <p><strong>After clustering your data, you find clusters that don't align with domain expert expectations. What should you do?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Ignore the expert opinions and trust the algorithm</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Investigate feature selection, preprocessing, and algorithm parameters; consider semi-supervised approaches</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Manually override the clustering results</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Use different evaluation metrics</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> When results don't align with domain expertise, investigate the methodology: check feature selection, preprocessing steps, algorithm parameters, and consider semi-supervised approaches that incorporate domain knowledge. This collaborative approach often leads to better, more interpretable results.</p>
                                </div>
                            </div>

                            <!-- Question 34 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 34: Production Deployment</h4>
                                <p><strong>What is the most important consideration when deploying a clustering model to production?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Using the most complex algorithm available</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Monitoring for data drift and model performance degradation</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Maximizing the number of clusters</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Minimizing computational cost only</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> The most critical aspect of production deployment is monitoring for data drift and model performance degradation. Clustering models can become outdated as data patterns change, so continuous monitoring ensures the model remains effective and relevant over time.</p>
                                </div>
                            </div>

                            <!-- Question 35 -->
                            <div class="enhanced-quiz-question">
                                <h4>Question 35: Interview Question - Technical Communication</h4>
                                <p><strong>How would you explain clustering results to a non-technical business stakeholder?</strong></p>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>a) Focus on mathematical details and algorithm complexity</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="correct">
                                    <p>b) Use business language, visualizations, and concrete examples relevant to their domain</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>c) Only provide the final cluster assignments</p>
                                </div>
                                <div class="enhanced-quiz-option" data-answer="incorrect">
                                    <p>d) Recommend they take a machine learning course first</p>
                                </div>
                                <div class="enhanced-quiz-explanation">
                                    <p><strong>Correct!</strong> Effective communication with non-technical stakeholders requires using business language, clear visualizations, and concrete examples relevant to their domain. Focus on business value, actionable insights, and practical implications rather than technical details.</p>
                                </div>
                            </div>

                            <!-- Submit Button -->
                            <div style="margin: 2rem 0; text-align: center;">
                                <button onclick="checkQuizAnswers()" class="azbn-btn" style="font-size: 1.1rem; padding: 1rem 2rem;">
                                    Submit Quiz and View Results
                                </button>
                            </div>

                            <!-- Quiz Results -->
                            <div id="quiz-results" style="display: none;" class="explanation-box">
                                <h3>Quiz Results and Explanations</h3>
                                <div id="score-display" style="font-size: 1.2rem; font-weight: bold; margin: 1rem 0;"></div>
                                <div id="detailed-results"></div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <!-- Sub-section Navigation Footer -->
    <div class="sub-section-nav-footer">
        <div class="sub-nav-buttons">
            <button id="prev-subsection" class="sub-nav-btn prev-btn" style="display: none;">
                <span>← Previous</span>
                <span class="sub-nav-label" id="prev-label"></span>
            </button>
            <button id="next-subsection" class="sub-nav-btn next-btn" style="display: none;">
                <span class="sub-nav-label" id="next-label">Mathematical Foundation</span>
                <span>Next →</span>
            </button>
        </div>
    </div>

    <!-- Chapter Navigation Footer -->
    <div class="navigation-buttons">
        <a href="/tutorials/clustering/chapter14" class="azbn-btn azbn-secondary" onclick="scrollToTop()">← Chapter 14: Clustering Evaluation</a>
        <a href="/tutorials/clustering/" class="azbn-btn azbn-secondary" onclick="scrollToTop()">Back to Tutorial Index →</a>
    </div>

    <script>
        // Section navigation functionality
        function showSection(sectionName, clickedElement) {
            // Hide all sections
            document.querySelectorAll('.content-section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            
            // Add active class to clicked button
            if (clickedElement) {
                clickedElement.classList.add('active');
            }
        }

        // Quiz functionality
        function checkQuizAnswers() {
            const correctAnswers = {
                'q1': 'b', 'q2': 'c', 'q3': 'b', 'q4': 'b', 'q5': 'b',
                'q6': 'b', 'q7': 'c', 'q8': 'b', 'q9': 'b', 'q10': 'b',
                'q11': 'b', 'q12': 'b', 'q13': 'b', 'q14': 'b', 'q15': 'b',
                'q16': 'b', 'q17': 'b', 'q18': 'b', 'q19': 'b', 'q20': 'a',
                'q21': 'b', 'q22': 'b', 'q23': 'b', 'q24': 'b', 'q25': 'b',
                'q26': 'a', 'q27': 'a', 'q28': 'a', 'q29': 'a', 'q30': 'b',
                'q31': 'a', 'q32': 'b', 'q33': 'b', 'q34': 'b', 'q35': 'b'
            };

            const explanations = {
                'q1': 'Euclidean distance measures straight-line distance, while Manhattan distance measures grid-based distance. Manhattan is preferred when movement is constrained to grid-like paths or when features have different units/scales.',
                'q2': 'K-means complexity is O(nkti) where n=number of samples, k=number of clusters, t=number of iterations, i=number of dimensions.',
                'q3': 'A gradual decline without a clear elbow suggests the data may not have well-defined natural clusters, or has hierarchical/nested cluster structure.',
                'q4': 'Setting eps too small means fewer points will be within the neighborhood radius, leading to most points being classified as noise/outliers.',
                'q5': 'Ward linkage minimizes within-cluster sum of squared errors and tends to create compact clusters while avoiding the chaining effect.',
                'q6': 'GMM assumes each cluster follows a Gaussian distribution with its own covariance matrix, allowing for elliptical cluster shapes.',
                'q7': 'Adjusted Rand Index (ARI) compares clustering results with ground truth labels to measure agreement.',
                'q8': 'K-means uses Euclidean distance, so features with larger scales will dominate the distance calculation, leading to biased results.',
                'q9': 'DBSCAN is ideal because it doesn\'t require pre-specifying k, can handle varying densities, and is reasonably scalable.',
                'q10': 'In high dimensions, the curse of dimensionality causes all pairwise distances to become similar, making distance-based similarity less meaningful.',
                'q11': 'Effective customer segmentation requires behavioral and transactional features that reflect purchasing patterns and customer value.',
                'q12': 'K-means tends to create clusters of similar sizes due to its objective function, struggling when natural clusters have very different sizes.',
                'q13': 'K-means\' random initialization can lead to different local optima. K-means++ initialization and multiple runs help achieve consistency.',
                'q14': 'Spectral clustering uses eigenvalue decomposition of similarity matrices to project data into lower-dimensional space where clusters are more separable.',
                'q15': 'For very large datasets, traditional algorithms become impractical. Mini-batch K-means or distributed frameworks are necessary.',
                'q16': 'Dynamic Time Warping (DTW) allows elastic matching of time series, handling phase shifts while preserving shape similarity.',
                'q17': 'Deep clustering jointly optimizes representation learning and clustering, potentially discovering better cluster structures.',
                'q18': 'Business success should be measured by actionable outcomes like improved campaign performance or operational efficiency.',
                'q19': 'Ensemble clustering combines multiple clustering results to improve robustness, stability, and often quality.',
                'q20': 'K-means requires computing cluster centroids as the mean of member points, but the mean is undefined for categorical variables.',
                'q21': 'Modularity measures how much more connected nodes within communities are compared to a random network with the same degree distribution.',
                'q22': 'Multiple optimization strategies include sampling, approximate algorithms, distributed computing, or switching to more scalable algorithms.',
                'q23': 'Semi-supervised clustering incorporates limited labeled examples or pairwise constraints to guide the clustering process.',
                'q24': 'Bootstrap resampling assesses how consistent clusters are across different data samples, measuring stability.',
                'q25': 'A large dominant cluster often indicates poor feature selection, need for feature transformation, or hierarchical analysis within the large group.',
                'q26': 'Multi-view clustering must balance preserving unique information from each view while finding consistent cluster structure.',
                'q27': 'Clustering-based anomaly detection identifies outliers as points far from cluster centers or in very small clusters.',
                'q28': 'Data drift monitoring includes tracking cluster assignment distributions, feature statistics, and model performance over time.',
                'q29': 'For new users with no interaction history, clustering existing users allows assignment to similar groups for initial recommendations.',
                'q30': 'Ethical considerations include avoiding discriminatory features, ensuring fairness across groups, and transparency in decision-making.',
                'q31': 'Feature scaling/normalization is critical because many clustering algorithms are sensitive to feature scales, especially distance-based methods.',
                'q32': 'DBSCAN\'s key advantages include handling noise, finding arbitrary shaped clusters, and not requiring predetermined cluster count.',
                'q33': 'When clusters don\'t align with expert expectations, investigate preprocessing, feature selection, and parameters; consider domain knowledge integration.',
                'q34': 'Monitoring for data drift and model performance degradation is crucial for maintaining clustering model effectiveness in production.',
                'q35': 'Effective communication with non-technical stakeholders requires business language, visualizations, and domain-relevant examples.'
            };

            // Collect user answers
            let userAnswers = {};
            let score = 0;
            
            for (let i = 1; i <= 35; i++) {
                const questionName = 'q' + i;
                const selectedOption = document.querySelector(`input[name="${questionName}"]:checked`);
                if (selectedOption) {
                    userAnswers[questionName] = selectedOption.value;
                    if (selectedOption.value === correctAnswers[questionName]) {
                        score++;
                    }
                }
            }

            // Display results
            const percentage = Math.round((score / 35) * 100);
            let performanceLevel = '';
            let feedback = '';

            if (percentage >= 90) {
                performanceLevel = 'Excellent';
                feedback = 'Outstanding! You have mastered clustering concepts and are ready for advanced applications and leadership roles.';
            } else if (percentage >= 80) {
                performanceLevel = 'Very Good';
                feedback = 'Great job! You have strong clustering knowledge. Review the missed questions to reach expert level.';
            } else if (percentage >= 70) {
                performanceLevel = 'Good';
                feedback = 'Good understanding! You grasp most clustering concepts. Focus on the practical application questions.';
            } else if (percentage >= 60) {
                performanceLevel = 'Fair';
                feedback = 'Decent foundation, but review fundamental concepts and algorithm details for better understanding.';
            } else {
                performanceLevel = 'Needs Improvement';
                feedback = 'Consider reviewing the course materials and practicing with real datasets to strengthen your understanding.';
            }

            const scoreColor = percentage >= 70 ? '#4caf50' : percentage >= 60 ? '#ff9800' : '#f44336';
            
            document.getElementById('score-display').innerHTML = 
                `<div style="background: ${scoreColor}; color: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                    Score: ${score}/35 (${percentage}%) - ${performanceLevel}
                    <br><small>${feedback}</small>
                </div>`;

            // Show detailed results
            let detailedHTML = '<h4>Detailed Answer Review:</h4>';
            for (let i = 1; i <= 35; i++) {
                const questionName = 'q' + i;
                const userAnswer = userAnswers[questionName] || 'Not answered';
                const correctAnswer = correctAnswers[questionName];
                const isCorrect = userAnswer === correctAnswer;
                
                detailedHTML += `
                    <div style="background: ${isCorrect ? '#e8f5e8' : '#ffebee'}; border: 1px solid ${isCorrect ? '#4caf50' : '#f44336'}; border-radius: 6px; padding: 1rem; margin: 0.5rem 0;">
                        <strong>Question ${i}:</strong> ${isCorrect ? '✓ Correct' : '✗ Incorrect'}
                        <br><small>Your answer: ${userAnswer} | Correct answer: ${correctAnswer}</small>
                        <br><em>${explanations[questionName]}</em>
                    </div>
                `;
            }

            document.getElementById('detailed-results').innerHTML = detailedHTML;
            document.getElementById('quiz-results').style.display = 'block';
            
            // Scroll to results
            document.getElementById('quiz-results').scrollIntoView({ behavior: 'smooth' });
        }

        // Sub-section navigation functionality
        function navigateSubSection(direction) {
            const sections = ['algorithm', 'mathematics', 'optimization', 'convergence', 'demo', 'quiz'];
            const currentSection = document.querySelector('.content-section.active').id;
            const currentIndex = sections.indexOf(currentSection);
            
            if (direction === 'next' && currentIndex < sections.length - 1) {
                const nextSection = sections[currentIndex + 1];
                showSection(nextSection, document.querySelector(`button[onclick*="${nextSection}"]`));
            } else if (direction === 'prev' && currentIndex > 0) {
                const prevSection = sections[currentIndex - 1];
                showSection(prevSection, document.querySelector(`button[onclick*="${prevSection}"]`));
            }
            
            // Update navigation button labels
            updateSubSectionNavigation(sections, currentIndex);
        }
        
        function updateSubSectionNavigation(sections, currentIndex) {
            const prevBtn = document.getElementById('prev-subsection');
            const nextBtn = document.getElementById('next-subsection');
            const prevLabel = document.getElementById('prev-label');
            const nextLabel = document.getElementById('next-label');
            
            if (currentIndex > 0) {
                prevBtn.style.display = 'block';
                prevLabel.textContent = sections[currentIndex - 1].charAt(0).toUpperCase() + sections[currentIndex - 1].slice(1);
            } else {
                prevBtn.style.display = 'none';
            }
            
            if (currentIndex < sections.length - 1) {
                nextBtn.style.display = 'block';
                nextLabel.textContent = sections[currentIndex + 1].charAt(0).toUpperCase() + sections[currentIndex + 1].slice(1);
            } else {
                nextBtn.style.display = 'none';
            }
        }

        // Initialize with default section
        window.addEventListener('load', function() {
            showSection('algorithm');
            updateSubSectionNavigation(['algorithm', 'mathematics', 'optimization', 'convergence', 'demo', 'quiz'], 0);
        });
    </script>
</body>
</html>
