<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Document Processing & Chunking - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 3: Document Processing & Chunking</h1>
                <p class="chapter-subtitle">Preparing Documents for Retrieval</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="42"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn active">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand document processing & chunking fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Document Processing & Chunking</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Document Processing is Critical in RAG</h3>
                            <p>Before you can retrieve relevant information, you need to prepare your documents. Real-world documents come in many formats (PDFs, Word docs, HTML pages, markdown files, databases) and sizes (from short paragraphs to entire books with thousands of pages). <strong>Document processing and chunking</strong> is the crucial first step that transforms raw documents into a format that RAG systems can effectively search and retrieve.</p>
                            
                            <p><strong>The fundamental challenge:</strong> Most documents are too large to process as a single unit. LLMs have context window limits (typically 4K-128K tokens), and embedding models work best with text segments of 128-512 tokens. A single research paper might be 10,000+ words, and a technical manual could be hundreds of pages. You can't embed or process these as single units.</p>
                            
                            <h4>What Document Processing & Chunking Involves</h4>
                            <ol>
                                <li><strong>Document Loading:</strong> Extract text from various formats (PDF, HTML, Word, etc.) while preserving structure and metadata</li>
                                <li><strong>Text Cleaning:</strong> Remove formatting artifacts, handle special characters, normalize whitespace</li>
                                <li><strong>Chunking:</strong> Split large documents into smaller, semantically meaningful pieces (chunks) that fit within context windows</li>
                                <li><strong>Metadata Extraction:</strong> Capture document properties (title, author, date, section, category) for filtering and organization</li>
                                <li><strong>Context Preservation:</strong> Maintain relationships between chunks (overlap, hierarchy) so retrieved chunks have sufficient context</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Real-World Example:</h5>
                                <p>Imagine you have a 200-page technical manual about machine learning. Without proper processing:</p>
                                <ul>
                                    <li>❌ The entire document is too large to embed meaningfully (loses semantic precision)</li>
                                    <li>❌ Retrieval returns the whole manual even for specific questions (wastes tokens, poor accuracy)</li>
                                    <li>❌ LLM struggles to find relevant information in 200 pages of text</li>
                                </ul>
                                <p>With proper chunking:</p>
                                <ul>
                                    <li>✅ Manual is split into 500 focused chunks (one per section/topic)</li>
                                    <li>✅ Each chunk is embedded separately and stored in vector database</li>
                                    <li>✅ Query about "gradient descent" retrieves only the 2-3 most relevant chunks</li>
                                    <li>✅ LLM receives focused, relevant context and generates accurate answers</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Chunking Strategies:</strong> Fixed-size, sentence-based, semantic, and recursive chunking - when to use each and why</li>
                                <li><strong>Chunk Overlap:</strong> Why overlapping chunks preserve context at boundaries and how to choose the right overlap percentage</li>
                                <li><strong>Optimal Chunk Size:</strong> Balancing retrieval precision (smaller chunks) with context completeness (larger chunks)</li>
                                <li><strong>Document Type Handling:</strong> Processing PDFs, HTML, markdown, and structured documents with appropriate parsers</li>
                                <li><strong>Metadata Management:</strong> Extracting and storing document properties for filtering and organization</li>
                                <li><strong>Context Preservation:</strong> Techniques to maintain semantic relationships between chunks</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> Poor chunking leads to poor retrieval. If chunks are too large, retrieval is imprecise. If chunks are too small, context is lost. If chunks break at wrong boundaries, semantic meaning is destroyed. Getting chunking right is foundational to RAG system performance.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Chunking is Critical for RAG Systems</h3>
                            
                            <h4>The Core Problem</h4>
                            <p>Raw documents in real-world RAG systems are often <strong>extremely long</strong>—think research papers (10,000+ words), legal documents (hundreds of pages), technical documentation (thousands of sections), or entire books. These documents present several fundamental challenges:</p>
                            
                            <ul>
                                <li><strong>Context Window Limits:</strong> Most LLMs have fixed context windows (e.g., GPT-4: 8K-128K tokens, Claude: 100K-200K tokens). A single large document can easily exceed these limits, making it impossible to process the entire document at once.</li>
                                <li><strong>Embedding Model Constraints:</strong> Embedding models like SentenceTransformers work best with text segments of 128-512 tokens. Very long documents produce embeddings that lose semantic precision—the model struggles to capture the meaning of a 10,000-word document in a single 384-dimensional vector.</li>
                                <li><strong>Retrieval Precision:</strong> When you retrieve a 50-page document for a specific question, most of that document is irrelevant. The LLM has to sift through thousands of words to find the answer, leading to poor performance and high costs.</li>
                                <li><strong>Computational Efficiency:</strong> Processing entire documents is computationally expensive. Smaller chunks allow for faster embedding generation, more efficient storage, and quicker retrieval.</li>
                            </ul>
                            
                            <h4>The Solution: Intelligent Chunking</h4>
                            <p>Chunking splits large documents into smaller, manageable pieces that:</p>
                            
                            <ul>
                                <li><strong>Fit within context limits:</strong> Each chunk is small enough to fit comfortably in the LLM's context window, even when combined with the query and other chunks.</li>
                                <li><strong>Are semantically meaningful:</strong> Each chunk represents a coherent unit of information (a paragraph, a section, a concept) rather than arbitrary text splits.</li>
                                <li><strong>Can be retrieved independently:</strong> Each chunk can be embedded and stored separately, allowing the retrieval system to find the most relevant chunk(s) for a specific query.</li>
                                <li><strong>Maintain context when possible:</strong> Chunks preserve surrounding context (through overlap or metadata) so the LLM understands the broader context when generating answers.</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Real-World Example:</h5>
                                <p>Imagine you have a 200-page technical manual about machine learning. Without chunking:</p>
                                <ul>
                                    <li>❌ The entire document is too large to embed meaningfully</li>
                                    <li>❌ Retrieval returns the whole manual, even if the question is about a specific algorithm</li>
                                    <li>❌ The LLM wastes tokens processing irrelevant sections</li>
                                </ul>
                                <p>With intelligent chunking:</p>
                                <ul>
                                    <li>✅ The manual is split into 500 chunks (one per section/topic)</li>
                                    <li>✅ Each chunk is embedded separately and stored in the vector database</li>
                                    <li>✅ A query about "gradient descent" retrieves only the 2-3 most relevant chunks</li>
                                    <li>✅ The LLM receives focused, relevant context and generates accurate answers</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Chunking Strategies: Choosing the Right Approach</h3>
                            
                            <p>Different chunking strategies serve different purposes. The choice depends on your document type, use case, and performance requirements.</p>
                            
                            <h4>1. Fixed-Size Chunking</h4>
                            <p><strong>What it is:</strong> Splits documents into chunks of a fixed size (measured in characters or tokens), regardless of content structure.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Divide the document into equal-sized segments (e.g., 500 characters or 200 tokens)</li>
                                <li>Each chunk has exactly the same size (except possibly the last chunk)</li>
                                <li>No consideration for sentence boundaries, paragraphs, or semantic meaning</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Simple and fast:</strong> Very easy to implement, no complex logic needed</li>
                                <li>✅ <strong>Predictable:</strong> You know exactly how many chunks you'll get for any document size</li>
                                <li>✅ <strong>Efficient storage:</strong> Uniform chunk sizes make storage and indexing straightforward</li>
                                <li>✅ <strong>Good for uniform content:</strong> Works well when documents have consistent structure</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>May break sentences:</strong> A chunk might end mid-sentence, losing meaning</li>
                                <li>❌ <strong>Ignores semantic boundaries:</strong> A single concept might be split across two chunks</li>
                                <li>❌ <strong>Poor for structured content:</strong> Doesn't respect paragraphs, sections, or logical divisions</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you have uniform, unstructured text where semantic boundaries don't matter much, or when you need maximum speed and simplicity.</p>
                            
                            <h4>2. Sentence-Based Chunking</h4>
                            <p><strong>What it is:</strong> Splits documents at sentence boundaries, grouping multiple sentences into chunks of roughly equal size.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Identify sentence boundaries using NLP tools (NLTK, spaCy, or regex)</li>
                                <li>Group sentences together until reaching a target chunk size (e.g., 5-10 sentences or 200-500 tokens)</li>
                                <li>Each chunk contains complete sentences, never breaking mid-sentence</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Preserves sentence integrity:</strong> Sentences remain intact, maintaining grammatical and semantic coherence</li>
                                <li>✅ <strong>Better semantic coherence:</strong> Related sentences stay together, improving embedding quality</li>
                                <li>✅ <strong>Respects natural boundaries:</strong> Works with how humans structure information</li>
                                <li>✅ <strong>Good for narrative content:</strong> Excellent for articles, stories, and prose</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Variable chunk sizes:</strong> Chunks may vary significantly in size depending on sentence length</li>
                                <li>❌ <strong>May split related concepts:</strong> A concept spanning multiple sentences might be split across chunks</li>
                                <li>❌ <strong>Requires sentence detection:</strong> Needs reliable sentence segmentation (can fail with abbreviations, decimals, etc.)</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For narrative text, articles, blog posts, or any content where sentence boundaries matter. This is often the default choice for general-purpose RAG systems.</p>
                            
                            <h4>3. Semantic Chunking (Advanced)</h4>
                            <p><strong>What it is:</strong> Uses embeddings and similarity calculations to group semantically related sentences together, creating chunks based on meaning rather than size.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Embed each sentence (or small group of sentences) into vector space</li>
                                <li>Calculate similarity between consecutive sentences</li>
                                <li>When similarity drops below a threshold, that's a chunk boundary (new topic/concept)</li>
                                <li>Group similar sentences together until reaching a maximum chunk size</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Most semantically coherent:</strong> Chunks represent complete concepts or topics</li>
                                <li>✅ <strong>Adaptive to content:</strong> Automatically adjusts to document structure</li>
                                <li>✅ <strong>Better retrieval quality:</strong> Chunks are more likely to be fully relevant or fully irrelevant</li>
                                <li>✅ <strong>Respects topic boundaries:</strong> Natural breaks occur at topic transitions</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Computationally expensive:</strong> Requires embedding every sentence, then calculating similarities</li>
                                <li>❌ <strong>More complex to implement:</strong> Needs careful tuning of similarity thresholds</li>
                                <li>❌ <strong>Variable chunk sizes:</strong> Can produce very small or very large chunks</li>
                                <li>❌ <strong>Requires embedding model:</strong> Needs a good sentence embedding model to work well</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For high-quality RAG systems where retrieval precision matters more than speed. Ideal for technical documentation, research papers, or any content with clear topic boundaries.</p>
                            
                            <h4>4. Recursive Chunking (Hierarchical)</h4>
                            <p><strong>What it is:</strong> A hybrid approach that tries multiple chunking strategies in a hierarchy (e.g., try paragraphs first, then sentences, then fixed-size).</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>First, try to split by paragraphs (if they exist and are reasonable size)</li>
                                <li>If paragraphs are too large, split by sentences</li>
                                <li>If sentences are still too large, use fixed-size chunking as fallback</li>
                                <li>Maintains hierarchy: parent chunks contain metadata about child chunks</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Adaptive:</strong> Automatically chooses the best strategy for each part of the document</li>
                                <li>✅ <strong>Respects structure:</strong> Uses document structure when available</li>
                                <li>✅ <strong>Robust:</strong> Falls back gracefully when structure is missing</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you have diverse document types with varying structures. Popular in production RAG systems (used by LangChain, LlamaIndex).</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Chunk Overlap: Preserving Context at Boundaries</h3>
                            
                            <h4>Why Overlap is Essential</h4>
                            <p>When you split a document into chunks, you create <strong>boundaries</strong> between chunks. These boundaries are artificial—they don't exist in the original document. This creates a critical problem:</p>
                            
                            <p><strong>The Boundary Problem:</strong> Important information often appears at the edges of chunks. Consider this example:</p>
                            
                            <div class="example-box">
                                <h5>Example: The Boundary Problem</h5>
                                <p><strong>Original Document:</strong></p>
                                <p>"Machine learning models require careful tuning. <strong>Hyperparameters like learning rate and batch size significantly impact model performance.</strong> Regularization techniques help prevent overfitting."</p>
                                
                                <p><strong>Without Overlap (Bad):</strong></p>
                                <ul>
                                    <li><strong>Chunk 1:</strong> "Machine learning models require careful tuning. Hyperparameters like learning rate and batch size significantly impact model performance."</li>
                                    <li><strong>Chunk 2:</strong> "Regularization techniques help prevent overfitting."</li>
                                </ul>
                                <p>❌ If a query asks about "hyperparameters and regularization," Chunk 1 might be retrieved (mentions hyperparameters) but Chunk 2 (mentions regularization) might not be, even though they're related concepts.</p>
                                
                                <p><strong>With Overlap (Good):</strong></p>
                                <ul>
                                    <li><strong>Chunk 1:</strong> "Machine learning models require careful tuning. Hyperparameters like learning rate and batch size significantly impact model performance. Regularization techniques help prevent overfitting."</li>
                                    <li><strong>Chunk 2:</strong> "Hyperparameters like learning rate and batch size significantly impact model performance. Regularization techniques help prevent overfitting."</li>
                                </ul>
                                <p>✅ Now both chunks contain information about hyperparameters AND regularization, improving retrieval quality.</p>
                            </div>
                            
                            <h4>How Overlap Works</h4>
                            <p>Overlap means that consecutive chunks share some content at their boundaries. For example, with 20% overlap:</p>
                            
                            <ul>
                                <li>If chunk size is 500 tokens, overlap is 100 tokens</li>
                                <li>Chunk 1: tokens 1-500</li>
                                <li>Chunk 2: tokens 401-900 (starts at token 401, overlapping the last 100 tokens of Chunk 1)</li>
                                <li>Chunk 3: tokens 801-1300 (overlaps with Chunk 2)</li>
                            </ul>
                            
                            <h4>Choosing the Right Overlap</h4>
                            <p><strong>Typical overlap:</strong> 10-20% of chunk size is standard. Here's why:</p>
                            
                            <ul>
                                <li><strong>Too little overlap (0-5%):</strong> ❌ Doesn't solve the boundary problem. Important context at boundaries is still lost. Not recommended.</li>
                                <li><strong>Moderate overlap (10-20%):</strong> ✅ Good balance. Preserves context without excessive storage overhead. This is the sweet spot for most use cases.</li>
                                <li><strong>High overlap (30-50%):</strong> ⚠️ Better context preservation but significantly increases storage costs. Use only when context preservation is critical (e.g., legal documents, medical records).</li>
                                <li><strong>Very high overlap (50%+):</strong> ❌ Wasteful. You're essentially storing the document twice. Rarely justified.</li>
                            </ul>
                            
                            <h4>Trade-offs</h4>
                            <p><strong>Benefits of overlap:</strong></p>
                            <ul>
                                <li>✅ Preserves context at boundaries</li>
                                <li>✅ Improves retrieval quality (related concepts stay together)</li>
                                <li>✅ Reduces risk of missing relevant information</li>
                                <li>✅ Better for queries that span multiple topics</li>
                            </ul>
                            
                            <p><strong>Costs of overlap:</strong></p>
                            <ul>
                                <li>❌ <strong>Increased storage:</strong> More chunks = more embeddings to store</li>
                                <li>❌ <strong>Higher embedding costs:</strong> More chunks to embed (if using paid APIs)</li>
                                <li>❌ <strong>Potential redundancy:</strong> Same information retrieved multiple times (though this is usually acceptable)</li>
                            </ul>
                            
                            <h4>Best Practices</h4>
                            <ul>
                                <li><strong>Start with 10-20% overlap:</strong> This works well for most documents</li>
                                <li><strong>Increase for critical documents:</strong> Use 20-30% for legal, medical, or financial documents where context is crucial</li>
                                <li><strong>Decrease for uniform content:</strong> Use 5-10% for structured data or lists where boundaries are less important</li>
                                <li><strong>Test and measure:</strong> Evaluate retrieval quality with different overlap percentages on your specific documents</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Chunking Mathematics Overview</h3>
                            <p>Chunking involves several mathematical considerations: calculating the number of chunks needed, determining overlap, and understanding storage efficiency. These formulas help you make informed decisions about chunk size and overlap for optimal RAG performance.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. Chunk Count Calculation</h4>
                            <div class="formula-display">
                                \[\text{num\_chunks} = \left\lceil \frac{\text{document\_length}}{\text{chunk\_size} - \text{overlap}} \right\rceil\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Calculates:</h5>
                                <p>This formula determines how many chunks you'll get when splitting a document of a given length into chunks of a specified size with overlap. The ceiling function (\(\lceil \rceil\)) ensures you round up to include any partial chunk at the end.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{document\_length}\):</strong> Total length of the document measured in characters or tokens (e.g., 10,000 characters or 2,500 tokens)</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Desired size of each chunk (e.g., 500 characters or 200 tokens)</li>
                                    <li><strong>\(\text{overlap}\):</strong> Number of characters/tokens that consecutive chunks share at their boundaries (e.g., 100 characters or 20 tokens)</li>
                                    <li><strong>\(\text{chunk\_size} - \text{overlap}\):</strong> Effective chunk size - the amount of new content each chunk adds (e.g., 500 - 100 = 400 characters of new content per chunk)</li>
                                    <li><strong>\(\left\lceil \ldots \right\rceil\):</strong> Ceiling function - rounds up to the nearest integer (ensures partial chunks are counted)</li>
                                </ul>
                                
                                <h5>Why Subtract Overlap?</h5>
                                <p>If chunks overlap by 100 characters, then each chunk after the first only adds 400 new characters (500 - 100 = 400). The overlap is shared between chunks, so it doesn't count as "new" content for the chunk count calculation.</p>
                                
                                <h5>Example:</h5>
                                <p>Document: 5,000 characters<br>
                                Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Effective chunk size: 500 - 100 = 400 characters<br>
                                Number of chunks: \(\lceil 5000 / 400 \rceil = \lceil 12.5 \rceil = 13\) chunks</p>
                                
                                <p><strong>Chunk distribution:</strong></p>
                                <ul>
                                    <li>Chunk 1: characters 1-500</li>
                                    <li>Chunk 2: characters 401-900 (overlaps 401-500 with Chunk 1)</li>
                                    <li>Chunk 3: characters 801-1300 (overlaps 801-900 with Chunk 2)</li>
                                    <li>... and so on</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. Overlap Percentage</h4>
                            <div class="formula-display">
                                \[\text{overlap\_percentage} = \frac{\text{overlap}}{\text{chunk\_size}} \times 100\%\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>This formula calculates what percentage of each chunk is shared with the next chunk. It helps you understand the trade-off between context preservation and storage efficiency.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{overlap}\):</strong> Number of overlapping characters/tokens between consecutive chunks</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Total size of each chunk</li>
                                    <li><strong>\(\frac{\text{overlap}}{\text{chunk\_size}}\):</strong> Fraction of chunk that overlaps (e.g., 100/500 = 0.2 = 20%)</li>
                                    <li><strong>\(\times 100\%\):</strong> Converts fraction to percentage</li>
                                </ul>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>10-20%:</strong> Standard overlap, good balance between context preservation and storage efficiency</li>
                                    <li><strong>5-10%:</strong> Low overlap, minimal storage overhead but less context preservation</li>
                                    <li><strong>20-30%:</strong> High overlap, better context preservation but significant storage increase</li>
                                    <li><strong>30%+:</strong> Very high overlap, rarely justified except for critical documents</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Overlap percentage: \(\frac{100}{500} \times 100\% = 20\%\)</p>
                                <p>This means 20% of each chunk (100 out of 500 characters) is shared with the next chunk, ensuring context is preserved at boundaries.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Storage Efficiency Ratio</h4>
                            <div class="formula-display">
                                \[\text{storage\_ratio} = \frac{\text{total\_chunks} \times \text{chunk\_size}}{\text{document\_length}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>This formula calculates how much storage space is needed for chunks compared to the original document. With overlap, you store more data than the original document (ratio > 1), but this preserves context at boundaries.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{total\_chunks}\):</strong> Number of chunks created from the document</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Size of each chunk (characters or tokens)</li>
                                    <li><strong>\(\text{total\_chunks} \times \text{chunk\_size}\):</strong> Total storage needed for all chunks (includes overlap)</li>
                                    <li><strong>\(\text{document\_length}\):</strong> Original document size</li>
                                    <li><strong>Ratio:</strong> How many times more storage is needed compared to original</li>
                                </ul>
                                
                                <h5>Interpreting the Ratio:</h5>
                                <ul>
                                    <li><strong>Ratio = 1.0:</strong> No overlap, storage equals original document size (rare in practice)</li>
                                    <li><strong>Ratio = 1.1-1.3:</strong> Moderate overlap (10-20%), typical for most RAG systems</li>
                                    <li><strong>Ratio = 1.3-1.5:</strong> High overlap (20-30%), better context but more storage</li>
                                    <li><strong>Ratio > 1.5:</strong> Very high overlap, usually not justified</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Document: 10,000 characters<br>
                                Chunk size: 500 characters<br>
                                Overlap: 100 characters (20%)<br>
                                Number of chunks: \(\lceil 10000 / (500-100) \rceil = \lceil 25 \rceil = 25\) chunks<br>
                                Storage ratio: \(\frac{25 \times 500}{10000} = \frac{12500}{10000} = 1.25\)</p>
                                <p>This means you need 25% more storage than the original document, but you preserve context at all chunk boundaries.</p>
                                
                                <h5>Trade-off:</h5>
                                <p>Higher storage ratio = better context preservation but more storage costs and embedding costs. Lower storage ratio = less storage but risk of losing context at boundaries.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Effective Chunk Size (New Content Per Chunk)</h4>
                            <div class="formula-display">
                                \[\text{effective\_chunk\_size} = \text{chunk\_size} - \text{overlap}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>The effective chunk size is the amount of <strong>new</strong> content each chunk adds, excluding the overlap that's shared with the previous chunk. This is what actually "advances" you through the document.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Total size of each chunk</li>
                                    <li><strong>\(\text{overlap}\):</strong> Amount shared with previous chunk</li>
                                    <li><strong>\(\text{chunk\_size} - \text{overlap}\):</strong> New content unique to this chunk</li>
                                </ul>
                                
                                <h5>Why This Matters:</h5>
                                <p>When calculating how many chunks you need, you use the effective chunk size, not the total chunk size, because overlap doesn't advance you through the document.</p>
                                
                                <h5>Example:</h5>
                                <p>Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Effective chunk size: 500 - 100 = 400 characters</p>
                                <p>This means each chunk after the first adds 400 new characters of content, while 100 characters are shared with the previous chunk for context.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Total Storage with Overlap</h4>
                            <div class="formula-display">
                                \[\text{total\_storage} = \text{num\_chunks} \times \text{chunk\_size} = \left\lceil \frac{\text{document\_length}}{\text{chunk\_size} - \text{overlap}} \right\rceil \times \text{chunk\_size}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Calculates:</h5>
                                <p>Total storage space needed to store all chunks, including overlap. This helps you estimate storage costs and embedding API costs.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>First, calculate number of chunks using the chunk count formula</li>
                                    <li>Then multiply by chunk size to get total storage</li>
                                    <li>Result includes all overlap, so it's larger than the original document</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Document: 20,000 characters<br>
                                Chunk size: 1000 characters<br>
                                Overlap: 200 characters (20%)<br>
                                Number of chunks: \(\lceil 20000 / (1000-200) \rceil = \lceil 25 \rceil = 25\) chunks<br>
                                Total storage: \(25 \times 1000 = 25,000\) characters</p>
                                <p>Original document: 20,000 characters<br>
                                Storage overhead: 25,000 - 20,000 = 5,000 characters (25% increase)</p>
                                
                                <h5>Cost Implications:</h5>
                                <p>If you're using a paid embedding API (e.g., OpenAI), you pay per token/character embedded. With 25% storage overhead, you pay 25% more for embeddings. This is usually worth it for better retrieval quality, but it's important to be aware of the cost.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example 1: Fixed-Size Chunking - Detailed Walkthrough</h4>
                            <p><strong>Scenario:</strong> You have a long technical document that needs to be chunked for RAG.</p>
                            
                            <p><strong>Original Document (200 characters):</strong></p>
                            <p>"Machine learning is a subset of artificial intelligence. It enables computers to learn from data without explicit programming. Deep learning uses neural networks with multiple layers. Natural language processing helps computers understand text."</p>
                            
                            <p><strong>Parameters:</strong></p>
                            <ul>
                                <li>Chunk size: 80 characters</li>
                                <li>Overlap: 20 characters (25% overlap)</li>
                                <li>Effective chunk size: 80 - 20 = 60 new characters per chunk</li>
                            </ul>
                            
                            <p><strong>Chunking Process:</strong></p>
                            <p><strong>Chunk 1 (characters 1-80):</strong><br>
                            "Machine learning is a subset of artificial intelligence. It enables computers to learn from data without"</p>
                            
                            <p><strong>Chunk 2 (characters 61-140, overlaps 61-80 with Chunk 1):</strong><br>
                            "from data without explicit programming. Deep learning uses neural networks with multiple layers. Natural"</p>
                            
                            <p><strong>Chunk 3 (characters 121-200, overlaps 121-140 with Chunk 2):</strong><br>
                            "multiple layers. Natural language processing helps computers understand text."</p>
                            
                            <p><strong>Analysis:</strong></p>
                            <ul>
                                <li>Total chunks: 3</li>
                                <li>Overlap preserved: "from data without" appears in both Chunk 1 and Chunk 2</li>
                                <li>Overlap preserved: "multiple layers. Natural" appears in both Chunk 2 and Chunk 3</li>
                                <li>✅ Context at boundaries is preserved</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 2: Sentence-Based Chunking - Preserving Semantic Coherence</h4>
                            <p><strong>Scenario:</strong> A narrative document where sentence boundaries matter for meaning.</p>
                            
                            <p><strong>Original Document:</strong></p>
                            <p>"Python is a versatile programming language. It is widely used in data science and machine learning. Many libraries like NumPy and Pandas make Python powerful for data analysis. Machine learning frameworks such as scikit-learn and TensorFlow are built on Python."</p>
                            
                            <p><strong>Parameters:</strong></p>
                            <ul>
                                <li>Chunk size: 3 sentences</li>
                                <li>Overlap: 1 sentence</li>
                            </ul>
                            
                            <p><strong>Sentence Identification:</strong></p>
                            <ol>
                                <li>"Python is a versatile programming language."</li>
                                <li>"It is widely used in data science and machine learning."</li>
                                <li>"Many libraries like NumPy and Pandas make Python powerful for data analysis."</li>
                                <li>"Machine learning frameworks such as scikit-learn and TensorFlow are built on Python."</li>
                            </ol>
                            
                            <p><strong>Chunking Result:</strong></p>
                            <p><strong>Chunk 1 (sentences 1-3):</strong><br>
                            "Python is a versatile programming language. It is widely used in data science and machine learning. Many libraries like NumPy and Pandas make Python powerful for data analysis."</p>
                            
                            <p><strong>Chunk 2 (sentences 3-4, overlaps sentence 3):</strong><br>
                            "Many libraries like NumPy and Pandas make Python powerful for data analysis. Machine learning frameworks such as scikit-learn and TensorFlow are built on Python."</p>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ Sentences remain intact (no mid-sentence breaks)</li>
                                <li>✅ Better semantic coherence (related sentences stay together)</li>
                                <li>✅ Overlap preserves context (sentence 3 appears in both chunks)</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 3: Semantic Chunking - Topic-Based Boundaries</h4>
                            <p><strong>Scenario:</strong> A document with clear topic transitions that semantic chunking can identify.</p>
                            
                            <p><strong>Original Document:</strong></p>
                            <p>"Machine learning algorithms learn patterns from data. Supervised learning uses labeled examples. Unsupervised learning finds patterns without labels. Reinforcement learning learns through trial and error. Neural networks are inspired by the brain. They consist of interconnected nodes called neurons. Deep learning uses networks with many layers."</p>
                            
                            <p><strong>Semantic Chunking Process:</strong></p>
                            <p><strong>Step 1: Embed each sentence</strong></p>
                            <ul>
                                <li>Sentence 1: [0.45, -0.23, 0.67, ...] (about ML algorithms)</li>
                                <li>Sentence 2: [0.48, -0.25, 0.65, ...] (about supervised learning)</li>
                                <li>Sentence 3: [0.46, -0.24, 0.66, ...] (about unsupervised learning)</li>
                                <li>Sentence 4: [0.47, -0.26, 0.64, ...] (about reinforcement learning)</li>
                                <li>Sentence 5: [0.12, 0.34, -0.21, ...] (about neural networks - different topic!)</li>
                                <li>Sentence 6: [0.13, 0.35, -0.20, ...] (about neural networks)</li>
                                <li>Sentence 7: [0.14, 0.36, -0.19, ...] (about deep learning)</li>
                            </ul>
                            
                            <p><strong>Step 2: Calculate similarity between consecutive sentences</strong></p>
                            <ul>
                                <li>Sentences 1-4: High similarity (0.85-0.92) - all about ML learning types</li>
                                <li>Sentence 4 to 5: Low similarity (0.35) - topic change from learning types to neural networks</li>
                                <li>Sentences 5-7: High similarity (0.88-0.90) - all about neural networks</li>
                            </ul>
                            
                            <p><strong>Step 3: Identify chunk boundaries</strong></p>
                            <ul>
                                <li>Boundary detected between sentences 4 and 5 (similarity drops below threshold 0.5)</li>
                            </ul>
                            
                            <p><strong>Resulting Chunks:</strong></p>
                            <p><strong>Chunk 1 (sentences 1-4):</strong> "Machine learning algorithms learn patterns from data. Supervised learning uses labeled examples. Unsupervised learning finds patterns without labels. Reinforcement learning learns through trial and error."<br>
                            <strong>Topic:</strong> Types of machine learning</p>
                            
                            <p><strong>Chunk 2 (sentences 5-7):</strong> "Neural networks are inspired by the brain. They consist of interconnected nodes called neurons. Deep learning uses networks with many layers."<br>
                            <strong>Topic:</strong> Neural networks and deep learning</p>
                            
                            <p><strong>Key Advantage:</strong> Semantic chunking automatically identified the topic boundary, creating chunks that represent complete concepts rather than arbitrary text splits.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 4: Recursive Chunking - Handling Nested Structures</h4>
                            <p><strong>Scenario:</strong> A structured document with chapters, sections, and paragraphs.</p>
                            
                            <p><strong>Document Structure:</strong></p>
                            <pre>
Chapter 1: Introduction
  Section 1.1: Overview (500 words)
  Section 1.2: History (800 words)
Chapter 2: Methods
  Section 2.1: Approach A (300 words)
  Section 2.2: Approach B (400 words)
                            </pre>
                            
                            <p><strong>Recursive Chunking Process:</strong></p>
                            <p><strong>Level 1: Try chapters</strong></p>
                            <ul>
                                <li>Chapter 1: 1,300 words (too large for 500-word chunks)</li>
                                <li>Chapter 2: 700 words (too large)</li>
                                <li>→ Move to next level</li>
                            </ul>
                            
                            <p><strong>Level 2: Try sections</strong></p>
                            <ul>
                                <li>Section 1.1: 500 words (perfect size!) → Chunk 1</li>
                                <li>Section 1.2: 800 words (too large) → Move to next level</li>
                                <li>Section 2.1: 300 words (good size) → Chunk 2</li>
                                <li>Section 2.2: 400 words (good size) → Chunk 3</li>
                            </ul>
                            
                            <p><strong>Level 3: Try paragraphs (for Section 1.2)</strong></p>
                            <ul>
                                <li>Paragraph 1: 200 words → Chunk 4</li>
                                <li>Paragraph 2: 250 words → Chunk 5</li>
                                <li>Paragraph 3: 350 words → Chunk 6</li>
                            </ul>
                            
                            <p><strong>Final Result:</strong></p>
                            <ul>
                                <li>6 chunks total, each respecting document structure</li>
                                <li>Chunks maintain hierarchy (metadata links chunks to their parent sections/chapters)</li>
                                <li>✅ Structure is preserved while meeting size constraints</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 5: Impact of Chunk Size on Retrieval</h4>
                            <p><strong>Scenario:</strong> Same document chunked with different sizes to show how chunk size affects retrieval precision.</p>
                            
                            <p><strong>Document:</strong> "Machine learning uses algorithms. Supervised learning requires labeled data. Unsupervised learning finds patterns. Neural networks have multiple layers. Deep learning uses many layers."</p>
                            
                            <p><strong>Query:</strong> "What is supervised learning?"</p>
                            
                            <p><strong>Small Chunks (50 characters each):</strong></p>
                            <ul>
                                <li>Chunk 1: "Machine learning uses algorithms. Supervised learning"</li>
                                <li>Chunk 2: "Supervised learning requires labeled data. Unsupervised"</li>
                                <li>Chunk 3: "Unsupervised learning finds patterns. Neural networks"</li>
                                <li>Chunk 4: "Neural networks have multiple layers. Deep learning"</li>
                                <li>Chunk 5: "Deep learning uses many layers."</li>
                            </ul>
                            <p><strong>Retrieval:</strong> Chunk 2 is retrieved (contains "supervised learning" and "labeled data")<br>
                            <strong>Precision:</strong> High - chunk is highly relevant<br>
                            <strong>Context:</strong> Limited - only mentions supervised learning briefly</p>
                            
                            <p><strong>Large Chunks (150 characters each):</strong></p>
                            <ul>
                                <li>Chunk 1: "Machine learning uses algorithms. Supervised learning requires labeled data. Unsupervised learning finds patterns."</li>
                                <li>Chunk 2: "Neural networks have multiple layers. Deep learning uses many layers."</li>
                            </ul>
                            <p><strong>Retrieval:</strong> Chunk 1 is retrieved (contains supervised learning)<br>
                            <strong>Precision:</strong> Lower - chunk also contains unrelated info (unsupervised learning, algorithms)<br>
                            <strong>Context:</strong> Rich - includes related concepts</p>
                            
                            <p><strong>Trade-off:</strong> Smaller chunks = more precise retrieval but less context. Larger chunks = more context but less precise retrieval. Choose based on your needs.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="explanation-box">
                            <h3>Implementation Overview</h3>
                            <p>This section provides practical Python code examples for implementing document processing and chunking in RAG systems. The examples use popular libraries like <strong>LangChain</strong> and <strong>NLTK</strong> for text processing, and demonstrate different chunking strategies with real-world scenarios.</p>
                        </div>
                        
                        <div class="code-box">
                            <h4>1. Fixed-Size Chunking Implementation</h4>
                            <p><strong>What this does:</strong> Splits documents into chunks of fixed size (characters or tokens) with configurable overlap. Simple and fast, good for uniform content.</p>
                            <pre><code class="language-python">from langchain.text_splitter import CharacterTextSplitter
import re

class FixedSizeChunker:
    """
    Fixed-size chunking implementation with overlap support.
    
    This class splits documents into equal-sized chunks, preserving
    overlap at boundaries to maintain context.
    """
    
    def __init__(self, chunk_size=500, chunk_overlap=100):
        """
        Initialize chunker with size and overlap parameters.
        
        Args:
            chunk_size: Size of each chunk in characters
            chunk_overlap: Number of overlapping characters between chunks
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.effective_size = chunk_size - chunk_overlap
    
    def chunk(self, text):
        """
        Split text into fixed-size chunks with overlap.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of chunk strings
        """
        if len(text) <= self.chunk_size:
            return [text]  # Text fits in one chunk
        
        chunks = []
        start = 0
        
        while start < len(text):
            # Calculate end position
            end = start + self.chunk_size
            
            # Extract chunk
            chunk = text[start:end]
            chunks.append(chunk)
            
            # Move start position (accounting for overlap)
            start += self.effective_size
        
        return chunks
    
    def chunk_with_metadata(self, text, metadata=None):
        """
        Chunk text and preserve metadata for each chunk.
        
        Args:
            text: Input text
            metadata: Dictionary of metadata (e.g., {'title': 'Doc1', 'author': 'John'})
            
        Returns:
            List of dictionaries with 'text' and 'metadata' keys
        """
        chunks = self.chunk(text)
        chunked_docs = []
        
        for i, chunk_text in enumerate(chunks):
            chunk_metadata = {
                **(metadata or {}),
                'chunk_index': i,
                'total_chunks': len(chunks)
            }
            chunked_docs.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunked_docs

# Example usage
chunker = FixedSizeChunker(chunk_size=100, chunk_overlap=20)

document = "Machine learning is a subset of artificial intelligence. " \
           "It enables computers to learn from data without explicit programming. " \
           "Deep learning uses neural networks with multiple layers."

chunks = chunker.chunk(document)
print(f"Number of chunks: {len(chunks)}")
for i, chunk in enumerate(chunks, 1):
    print(f"\nChunk {i} ({len(chunk)} chars): {chunk}")

# Output:
# Number of chunks: 2
# Chunk 1 (100 chars): Machine learning is a subset of artificial intelligence. It enables computers to learn from data without explicit
# Chunk 2 (100 chars): from data without explicit programming. Deep learning uses neural networks with multiple layers.
# Note: "from data without explicit" appears in both chunks (overlap)</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Simple implementation:</strong> Easy to understand and implement</li>
                                    <li><strong>Overlap handling:</strong> Each chunk after the first starts `chunk_size - overlap` characters into the previous chunk</li>
                                    <li><strong>Metadata preservation:</strong> Can attach metadata to each chunk for filtering and organization</li>
                                    <li><strong>Use case:</strong> Good for uniform text where structure doesn't matter</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>2. Sentence-Based Chunking Implementation</h4>
                            <p><strong>What this does:</strong> Splits documents at sentence boundaries, grouping multiple sentences into chunks. Preserves sentence integrity and improves semantic coherence.</p>
                            <pre><code class="language-python">import nltk
from nltk.tokenize import sent_tokenize
import re

# Download required NLTK data (run once)
# nltk.download('punkt')

class SentenceBasedChunker:
    """
    Sentence-based chunking that respects sentence boundaries.
    
    Groups sentences into chunks of approximately target size,
    ensuring no sentence is split across chunks.
    """
    
    def __init__(self, chunk_size=500, chunk_overlap=100):
        """
        Initialize sentence-based chunker.
        
        Args:
            chunk_size: Target chunk size in characters
            chunk_overlap: Overlap in characters (applied at sentence boundaries)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk(self, text):
        """
        Split text into sentence-based chunks.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of chunk strings
        """
        # Split into sentences
        sentences = sent_tokenize(text)
        
        if not sentences:
            return [text]
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # If adding this sentence would exceed chunk size, finalize current chunk
            if current_size + sentence_size > self.chunk_size and current_chunk:
                # Join sentences into chunk
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text)
                
                # Start new chunk with overlap (last few sentences)
                overlap_sentences = self._get_overlap_sentences(current_chunk)
                current_chunk = overlap_sentences + [sentence]
                current_size = sum(len(s) for s in current_chunk)
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        # Add final chunk
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _get_overlap_sentences(self, sentences):
        """
        Get sentences for overlap based on target overlap size.
        
        Args:
            sentences: List of sentences in current chunk
            
        Returns:
            List of sentences to include in next chunk for overlap
        """
        overlap_sentences = []
        overlap_size = 0
        
        # Add sentences from the end until we reach overlap size
        for sentence in reversed(sentences):
            if overlap_size + len(sentence) <= self.chunk_overlap:
                overlap_sentences.insert(0, sentence)
                overlap_size += len(sentence)
            else:
                break
        
        return overlap_sentences

# Example usage
chunker = SentenceBasedChunker(chunk_size=150, chunk_overlap=30)

document = "Python is a versatile programming language. " \
           "It is widely used in data science and machine learning. " \
           "Many libraries like NumPy and Pandas make Python powerful. " \
           "Machine learning frameworks such as scikit-learn are built on Python."

chunks = chunker.chunk(document)
print(f"Number of chunks: {len(chunks)}")
for i, chunk in enumerate(chunks, 1):
    print(f"\nChunk {i} ({len(chunk)} chars):")
    print(chunk)

# Output:
# Number of chunks: 2
# Chunk 1: Python is a versatile programming language. It is widely used in data science and machine learning.
# Chunk 2: It is widely used in data science and machine learning. Many libraries like NumPy and Pandas make Python powerful.
# Note: Overlap preserves context at sentence boundaries</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Sentence preservation:</strong> Never breaks sentences, maintaining grammatical integrity</li>
                                    <li><strong>Smart overlap:</strong> Overlap is applied at sentence boundaries, not arbitrary character positions</li>
                                    <li><strong>Better semantics:</strong> Related sentences stay together, improving embedding quality</li>
                                    <li><strong>Use case:</strong> Ideal for narrative text, articles, and prose where sentence structure matters</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>3. Recursive Chunking Implementation (LangChain)</h4>
                            <p><strong>What this does:</strong> Tries multiple chunking strategies in hierarchy (paragraphs → sentences → characters) until chunks fit size requirements. Handles nested document structures intelligently.</p>
                            <pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentChunker:
    """
    Recursive chunking that tries multiple separators in order.
    
    This is the most robust chunking approach, handling various
    document structures automatically.
    """
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        """
        Initialize recursive chunker.
        
        Args:
            chunk_size: Target chunk size in characters
            chunk_overlap: Overlap between chunks
        """
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            # Try these separators in order (most to least preferred)
            separators=[
                "\n\n",  # Paragraphs (double newline)
                "\n",    # Lines (single newline)
                ". ",    # Sentences (period + space)
                " ",     # Words (spaces)
                ""       # Characters (fallback)
            ]
        )
    
    def chunk_document(self, text, metadata=None):
        """
        Chunk document with metadata preservation.
        
        Args:
            text: Document text
            metadata: Optional metadata dictionary
            
        Returns:
            List of chunk dictionaries with text and metadata
        """
        # Split into chunks
        chunks = self.splitter.split_text(text)
        
        # Add metadata to each chunk
        chunked_docs = []
        for i, chunk_text in enumerate(chunks):
            chunk_metadata = {
                **(metadata or {}),
                'chunk_index': i,
                'total_chunks': len(chunks),
                'chunk_size': len(chunk_text)
            }
            chunked_docs.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunked_docs
    
    def chunk_multiple_documents(self, documents):
        """
        Chunk multiple documents, preserving document-level metadata.
        
        Args:
            documents: List of dicts with 'text' and optional 'metadata'
            
        Returns:
            List of all chunks with preserved metadata
        """
        all_chunks = []
        
        for doc_idx, doc in enumerate(documents):
            text = doc.get('text', '')
            metadata = doc.get('metadata', {})
            metadata['document_index'] = doc_idx
            
            chunks = self.chunk_document(text, metadata)
            all_chunks.extend(chunks)
        
        return all_chunks

# Example usage
chunker = DocumentChunker(chunk_size=200, chunk_overlap=50)

# Example: Structured document with paragraphs
document = """Introduction

Machine learning is transforming industries. It enables computers to learn from data.

Methods

We use neural networks for pattern recognition. Deep learning models achieve state-of-the-art results.

Conclusion

The future of AI looks promising. Machine learning will continue to evolve."""

chunks = chunker.chunk_document(
    document,
    metadata={'title': 'ML Overview', 'author': 'John Doe', 'date': '2024-01-15'}
)

print(f"Number of chunks: {len(chunks)}")
for chunk in chunks:
    print(f"\nChunk {chunk['metadata']['chunk_index'] + 1}:")
    print(f"Text: {chunk['text'][:100]}...")
    print(f"Metadata: {chunk['metadata']}")

# The recursive splitter will:
# 1. Try splitting by "\n\n" (paragraphs) - succeeds for this document
# 2. If paragraphs are too large, try "\n" (lines)
# 3. If lines are too large, try ". " (sentences)
# 4. And so on...</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Adaptive:</strong> Automatically chooses the best separator based on document structure</li>
                                    <li><strong>Hierarchical:</strong> Respects document hierarchy (paragraphs → sentences → words)</li>
                                    <li><strong>Robust:</strong> Handles various document formats (markdown, plain text, structured)</li>
                                    <li><strong>Metadata preservation:</strong> Maintains document-level and chunk-level metadata</li>
                                    <li><strong>Use case:</strong> Best for diverse document types or when you're unsure of document structure</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>4. Complete Document Processing Pipeline</h4>
                            <p><strong>What this does:</strong> A complete implementation that loads documents from various formats, processes them, chunks them, and prepares them for embedding and storage in a vector database.</p>
                            <pre><code class="language-python">import os
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredHTMLLoader

class DocumentProcessor:
    """
    Complete document processing pipeline for RAG systems.
    
    Handles loading, cleaning, chunking, and metadata extraction
    from various document formats.
    """
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len
        )
    
    def load_document(self, file_path):
        """
        Load document from file based on extension.
        
        Args:
            file_path: Path to document file
            
        Returns:
            Document text and metadata
        """
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        
        # Extract base metadata
        metadata = {
            'source': str(file_path),
            'filename': file_path.name,
            'file_type': extension
        }
        
        # Load based on file type
        if extension == '.pdf':
            loader = PyPDFLoader(str(file_path))
            pages = loader.load()
            text = '\n\n'.join([page.page_content for page in pages])
            metadata['page_count'] = len(pages)
        
        elif extension in ['.txt', '.md']:
            loader = TextLoader(str(file_path), encoding='utf-8')
            document = loader.load()
            text = document[0].page_content
        
        elif extension in ['.html', '.htm']:
            loader = UnstructuredHTMLLoader(str(file_path))
            document = loader.load()
            text = document[0].page_content
        
        else:
            raise ValueError(f"Unsupported file type: {extension}")
        
        return text, metadata
    
    def process_document(self, file_path):
        """
        Complete processing: load, chunk, and prepare for embedding.
        
        Args:
            file_path: Path to document file
            
        Returns:
            List of chunk dictionaries ready for embedding
        """
        # Step 1: Load document
        text, doc_metadata = self.load_document(file_path)
        
        # Step 2: Clean text (remove excessive whitespace, normalize)
        text = self._clean_text(text)
        
        # Step 3: Chunk document
        chunks = self.splitter.split_text(text)
        
        # Step 4: Create chunk documents with metadata
        chunked_docs = []
        for i, chunk_text in enumerate(chunks):
            chunk_metadata = {
                **doc_metadata,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'chunk_size': len(chunk_text)
            }
            chunked_docs.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunked_docs
    
    def _clean_text(self, text):
        """Clean and normalize text."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters that might interfere
        text = text.strip()
        return text
    
    def process_directory(self, directory_path, file_patterns=None):
        """
        Process all documents in a directory.
        
        Args:
            directory_path: Path to directory containing documents
            file_patterns: List of file patterns to include (e.g., ['*.pdf', '*.txt'])
            
        Returns:
            List of all chunks from all documents
        """
        directory = Path(directory_path)
        all_chunks = []
        
        # Default patterns
        if file_patterns is None:
            file_patterns = ['*.pdf', '*.txt', '*.md', '*.html']
        
        # Find all matching files
        files = []
        for pattern in file_patterns:
            files.extend(directory.glob(pattern))
        
        # Process each file
        for file_path in files:
            try:
                chunks = self.process_document(file_path)
                all_chunks.extend(chunks)
                print(f"Processed {file_path.name}: {len(chunks)} chunks")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
        
        return all_chunks

# Example usage
processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)

# Process a single document
chunks = processor.process_document('document.pdf')
print(f"Created {len(chunks)} chunks from document")

# Process entire directory
all_chunks = processor.process_directory('./documents/', file_patterns=['*.pdf', '*.txt'])
print(f"Total chunks from all documents: {len(all_chunks)}")

# Now chunks are ready for:
# 1. Embedding generation
# 2. Storage in vector database
# 3. Retrieval in RAG system</code></pre>
                            
                            <div class="example-box">
                                <h5>Complete Pipeline Steps:</h5>
                                <ol>
                                    <li><strong>Document Loading:</strong> Loads from PDF, TXT, MD, HTML formats</li>
                                    <li><strong>Text Cleaning:</strong> Normalizes whitespace and removes artifacts</li>
                                    <li><strong>Chunking:</strong> Splits into manageable chunks with overlap</li>
                                    <li><strong>Metadata Extraction:</strong> Preserves document properties (filename, type, page count)</li>
                                    <li><strong>Ready for Embedding:</strong> Chunks are prepared for vector database storage</li>
                                </ol>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Installation Requirements</h3>
                            <p>To run these examples, install the required packages:</p>
                            <pre><code class="language-bash">pip install langchain nltk pypdf unstructured</code></pre>
                            <p>For NLTK sentence tokenization, download the punkt tokenizer (run once):</p>
                            <pre><code class="language-python">import nltk
nltk.download('punkt')</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Retrieval Strategy Selection</h3>
                            <p><strong>Use dense retrieval when:</strong></p>
                            <ul>
                                <li>Semantic understanding is important</li>
                                <li>Users may phrase queries differently</li>
                                <li>Domain-specific terminology</li>
                            </ul>
                            
                            <p><strong>Use sparse retrieval when:</strong></p>
                            <ul>
                                <li>Exact keyword matching is important</li>
                                <li>Speed is critical</li>
                                <li>Technical documentation with specific terms</li>
                            </ul>
                            
                            <p><strong>Use hybrid when:</strong></p>
                            <ul>
                                <li>You want best of both worlds</li>
                                <li>High accuracy is required</li>
                                <li>Can afford extra computation</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Reranking Benefits</h3>
                            <p><strong>When to use reranking:</strong></p>
                            <ul>
                                <li>Initial retrieval returns many candidates</li>
                                <li>Need high precision in top results</li>
                                <li>Can afford additional latency</li>
                                <li>Quality is more important than speed</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: Why is document chunking important in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) LLMs have context limits, chunking breaks documents into manageable pieces that fit in context windows while preserving semantic meaning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To make documents smaller</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To reduce storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What are the different chunking strategies and when would you use each?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Fixed-size: Simple, fast, good for uniform text. Semantic: Preserves meaning, better for varied content. Recursive: Handles nested structures. Use fixed-size for speed, semantic for quality, recursive for structured documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only fixed-size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only semantic</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking strategy doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is chunk overlap and why is it used?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Overlapping chunks share some content to preserve context at boundaries, preventing information loss when sentences/paragraphs are split</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To reduce storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To make chunks smaller</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Overlap is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "How do you determine optimal chunk size?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance LLM context window, retrieval precision (smaller = more precise), and semantic completeness (larger = more context). Common: 200-1000 tokens. Test on your data and downstream task</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use 100 tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use 5000 tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunk size doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is semantic chunking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Chunking based on semantic boundaries (sentences, paragraphs, topics) rather than fixed sizes, preserving meaning and context</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Chunking by file size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chunking</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking by word count only</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: Interview question: "How do you handle different document types (PDF, HTML, Markdown) in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use appropriate parsers (PyPDF2, BeautifulSoup, markdown), extract text while preserving structure, handle metadata, and apply document-type-specific chunking strategies</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Convert all to text first</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only support one format</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No special handling needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is metadata extraction in document processing?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Extracting document properties (title, author, date, source, section) to enable filtering and better retrieval in vector databases</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Extracting all text</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Compressing documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Metadata is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "How do you preserve context when chunking documents?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use chunk overlap, preserve sentence/paragraph boundaries, include surrounding context in metadata, and use semantic chunking to keep related content together</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No context preservation needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only use fixed-size chunks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Split randomly</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What are the trade-offs between small and large chunk sizes?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Small chunks: More precise retrieval but may lose context. Large chunks: More context but less precise retrieval. Balance based on query type and document structure</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Small is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Large is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Size doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How would you handle very long documents (e.g., books) in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use hierarchical chunking (chapters → sections → paragraphs), maintain document structure in metadata, use multi-level retrieval, and consider document summarization for overview</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Split into equal chunks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use only first part</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Skip long documents</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is recursive chunking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Chunking that tries multiple strategies in order (e.g., paragraphs → sentences → characters) until chunks fit size requirements, handling nested document structures</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Chunking twice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chunking</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference from fixed-size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "How do you evaluate chunking quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Measure retrieval performance (precision@k, recall@k), test downstream RAG quality, check if relevant information is preserved, and evaluate chunk coherence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check chunk size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No evaluation needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only check number of chunks</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 2</a>
                <a href="/tutorials/rag/chapter4" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 4 →</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
    </script>
</body>
</html>