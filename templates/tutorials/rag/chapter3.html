<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Document Processing & Chunking - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 3: Document Processing & Chunking</h1>
                <p class="chapter-subtitle">Preparing Documents for Retrieval</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="42"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn active">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand document processing & chunking fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Document Processing & Chunking</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Document Processing is Critical in RAG</h3>
                            <p>Before you can retrieve relevant information, you need to prepare your documents. Real-world documents come in many formats (PDFs, Word docs, HTML pages, markdown files, databases) and sizes (from short paragraphs to entire books with thousands of pages). <strong>Document processing and chunking</strong> is the crucial first step that transforms raw documents into a format that RAG systems can effectively search and retrieve.</p>
                            
                            <p><strong>The fundamental challenge:</strong> Most documents are too large to process as a single unit. LLMs have context window limits (typically 4K-128K tokens), and embedding models work best with text segments of 128-512 tokens. A single research paper might be 10,000+ words, and a technical manual could be hundreds of pages. You can't embed or process these as single units.</p>
                            
                            <h4>What Document Processing & Chunking Involves</h4>
                            <ol>
                                <li><strong>Document Loading:</strong> Extract text from various formats (PDF, HTML, Word, etc.) while preserving structure and metadata</li>
                                <li><strong>Text Cleaning:</strong> Remove formatting artifacts, handle special characters, normalize whitespace</li>
                                <li><strong>Chunking:</strong> Split large documents into smaller, semantically meaningful pieces (chunks) that fit within context windows</li>
                                <li><strong>Metadata Extraction:</strong> Capture document properties (title, author, date, section, category) for filtering and organization</li>
                                <li><strong>Context Preservation:</strong> Maintain relationships between chunks (overlap, hierarchy) so retrieved chunks have sufficient context</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Real-World Example:</h5>
                                <p>Imagine you have a 200-page technical manual about machine learning. Without proper processing:</p>
                                <ul>
                                    <li>❌ The entire document is too large to embed meaningfully (loses semantic precision)</li>
                                    <li>❌ Retrieval returns the whole manual even for specific questions (wastes tokens, poor accuracy)</li>
                                    <li>❌ LLM struggles to find relevant information in 200 pages of text</li>
                                </ul>
                                <p>With proper chunking:</p>
                                <ul>
                                    <li>✅ Manual is split into 500 focused chunks (one per section/topic)</li>
                                    <li>✅ Each chunk is embedded separately and stored in vector database</li>
                                    <li>✅ Query about "gradient descent" retrieves only the 2-3 most relevant chunks</li>
                                    <li>✅ LLM receives focused, relevant context and generates accurate answers</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Chunking Strategies:</strong> Fixed-size, sentence-based, semantic, and recursive chunking - when to use each and why</li>
                                <li><strong>Chunk Overlap:</strong> Why overlapping chunks preserve context at boundaries and how to choose the right overlap percentage</li>
                                <li><strong>Optimal Chunk Size:</strong> Balancing retrieval precision (smaller chunks) with context completeness (larger chunks)</li>
                                <li><strong>Document Type Handling:</strong> Processing PDFs, HTML, markdown, and structured documents with appropriate parsers</li>
                                <li><strong>Metadata Management:</strong> Extracting and storing document properties for filtering and organization</li>
                                <li><strong>Context Preservation:</strong> Techniques to maintain semantic relationships between chunks</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> Poor chunking leads to poor retrieval. If chunks are too large, retrieval is imprecise. If chunks are too small, context is lost. If chunks break at wrong boundaries, semantic meaning is destroyed. Getting chunking right is foundational to RAG system performance.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Chunking is Critical for RAG Systems</h3>
                            
                            <h4>The Core Problem</h4>
                            <p>Raw documents in real-world RAG systems are often <strong>extremely long</strong>—think research papers (10,000+ words), legal documents (hundreds of pages), technical documentation (thousands of sections), or entire books. These documents present several fundamental challenges:</p>
                            
                            <ul>
                                <li><strong>Context Window Limits:</strong> Most LLMs have fixed context windows (e.g., GPT-4: 8K-128K tokens, Claude: 100K-200K tokens). A single large document can easily exceed these limits, making it impossible to process the entire document at once.</li>
                                <li><strong>Embedding Model Constraints:</strong> Embedding models like SentenceTransformers work best with text segments of 128-512 tokens. Very long documents produce embeddings that lose semantic precision—the model struggles to capture the meaning of a 10,000-word document in a single 384-dimensional vector.</li>
                                <li><strong>Retrieval Precision:</strong> When you retrieve a 50-page document for a specific question, most of that document is irrelevant. The LLM has to sift through thousands of words to find the answer, leading to poor performance and high costs.</li>
                                <li><strong>Computational Efficiency:</strong> Processing entire documents is computationally expensive. Smaller chunks allow for faster embedding generation, more efficient storage, and quicker retrieval.</li>
                            </ul>
                            
                            <h4>The Solution: Intelligent Chunking</h4>
                            <p>Chunking splits large documents into smaller, manageable pieces that:</p>
                            
                            <ul>
                                <li><strong>Fit within context limits:</strong> Each chunk is small enough to fit comfortably in the LLM's context window, even when combined with the query and other chunks.</li>
                                <li><strong>Are semantically meaningful:</strong> Each chunk represents a coherent unit of information (a paragraph, a section, a concept) rather than arbitrary text splits.</li>
                                <li><strong>Can be retrieved independently:</strong> Each chunk can be embedded and stored separately, allowing the retrieval system to find the most relevant chunk(s) for a specific query.</li>
                                <li><strong>Maintain context when possible:</strong> Chunks preserve surrounding context (through overlap or metadata) so the LLM understands the broader context when generating answers.</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Real-World Example:</h5>
                                <p>Imagine you have a 200-page technical manual about machine learning. Without chunking:</p>
                                <ul>
                                    <li>❌ The entire document is too large to embed meaningfully</li>
                                    <li>❌ Retrieval returns the whole manual, even if the question is about a specific algorithm</li>
                                    <li>❌ The LLM wastes tokens processing irrelevant sections</li>
                                </ul>
                                <p>With intelligent chunking:</p>
                                <ul>
                                    <li>✅ The manual is split into 500 chunks (one per section/topic)</li>
                                    <li>✅ Each chunk is embedded separately and stored in the vector database</li>
                                    <li>✅ A query about "gradient descent" retrieves only the 2-3 most relevant chunks</li>
                                    <li>✅ The LLM receives focused, relevant context and generates accurate answers</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Chunking Strategies: Choosing the Right Approach</h3>
                            
                            <p>Different chunking strategies serve different purposes. The choice depends on your document type, use case, and performance requirements.</p>
                            
                            <h4>1. Fixed-Size Chunking</h4>
                            <p><strong>What it is:</strong> Splits documents into chunks of a fixed size (measured in characters or tokens), regardless of content structure.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Divide the document into equal-sized segments (e.g., 500 characters or 200 tokens)</li>
                                <li>Each chunk has exactly the same size (except possibly the last chunk)</li>
                                <li>No consideration for sentence boundaries, paragraphs, or semantic meaning</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Simple and fast:</strong> Very easy to implement, no complex logic needed</li>
                                <li>✅ <strong>Predictable:</strong> You know exactly how many chunks you'll get for any document size</li>
                                <li>✅ <strong>Efficient storage:</strong> Uniform chunk sizes make storage and indexing straightforward</li>
                                <li>✅ <strong>Good for uniform content:</strong> Works well when documents have consistent structure</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>May break sentences:</strong> A chunk might end mid-sentence, losing meaning</li>
                                <li>❌ <strong>Ignores semantic boundaries:</strong> A single concept might be split across two chunks</li>
                                <li>❌ <strong>Poor for structured content:</strong> Doesn't respect paragraphs, sections, or logical divisions</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you have uniform, unstructured text where semantic boundaries don't matter much, or when you need maximum speed and simplicity.</p>
                            
                            <h4>2. Sentence-Based Chunking</h4>
                            <p><strong>What it is:</strong> Splits documents at sentence boundaries, grouping multiple sentences into chunks of roughly equal size.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Identify sentence boundaries using NLP tools (NLTK, spaCy, or regex)</li>
                                <li>Group sentences together until reaching a target chunk size (e.g., 5-10 sentences or 200-500 tokens)</li>
                                <li>Each chunk contains complete sentences, never breaking mid-sentence</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Preserves sentence integrity:</strong> Sentences remain intact, maintaining grammatical and semantic coherence</li>
                                <li>✅ <strong>Better semantic coherence:</strong> Related sentences stay together, improving embedding quality</li>
                                <li>✅ <strong>Respects natural boundaries:</strong> Works with how humans structure information</li>
                                <li>✅ <strong>Good for narrative content:</strong> Excellent for articles, stories, and prose</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Variable chunk sizes:</strong> Chunks may vary significantly in size depending on sentence length</li>
                                <li>❌ <strong>May split related concepts:</strong> A concept spanning multiple sentences might be split across chunks</li>
                                <li>❌ <strong>Requires sentence detection:</strong> Needs reliable sentence segmentation (can fail with abbreviations, decimals, etc.)</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For narrative text, articles, blog posts, or any content where sentence boundaries matter. This is often the default choice for general-purpose RAG systems.</p>
                            
                            <h4>3. Semantic Chunking (Advanced)</h4>
                            <p><strong>What it is:</strong> Uses embeddings and similarity calculations to group semantically related sentences together, creating chunks based on meaning rather than size.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Embed each sentence (or small group of sentences) into vector space</li>
                                <li>Calculate similarity between consecutive sentences</li>
                                <li>When similarity drops below a threshold, that's a chunk boundary (new topic/concept)</li>
                                <li>Group similar sentences together until reaching a maximum chunk size</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Most semantically coherent:</strong> Chunks represent complete concepts or topics</li>
                                <li>✅ <strong>Adaptive to content:</strong> Automatically adjusts to document structure</li>
                                <li>✅ <strong>Better retrieval quality:</strong> Chunks are more likely to be fully relevant or fully irrelevant</li>
                                <li>✅ <strong>Respects topic boundaries:</strong> Natural breaks occur at topic transitions</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Computationally expensive:</strong> Requires embedding every sentence, then calculating similarities</li>
                                <li>❌ <strong>More complex to implement:</strong> Needs careful tuning of similarity thresholds</li>
                                <li>❌ <strong>Variable chunk sizes:</strong> Can produce very small or very large chunks</li>
                                <li>❌ <strong>Requires embedding model:</strong> Needs a good sentence embedding model to work well</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For high-quality RAG systems where retrieval precision matters more than speed. Ideal for technical documentation, research papers, or any content with clear topic boundaries.</p>
                            
                            <h4>4. Recursive Chunking (Hierarchical)</h4>
                            <p><strong>What it is:</strong> A hybrid approach that tries multiple chunking strategies in a hierarchy (e.g., try paragraphs first, then sentences, then fixed-size).</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>First, try to split by paragraphs (if they exist and are reasonable size)</li>
                                <li>If paragraphs are too large, split by sentences</li>
                                <li>If sentences are still too large, use fixed-size chunking as fallback</li>
                                <li>Maintains hierarchy: parent chunks contain metadata about child chunks</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Adaptive:</strong> Automatically chooses the best strategy for each part of the document</li>
                                <li>✅ <strong>Respects structure:</strong> Uses document structure when available</li>
                                <li>✅ <strong>Robust:</strong> Falls back gracefully when structure is missing</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you have diverse document types with varying structures. Popular in production RAG systems (used by LangChain, LlamaIndex).</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Chunk Overlap: Preserving Context at Boundaries</h3>
                            
                            <h4>Why Overlap is Essential</h4>
                            <p>When you split a document into chunks, you create <strong>boundaries</strong> between chunks. These boundaries are artificial—they don't exist in the original document. This creates a critical problem:</p>
                            
                            <p><strong>The Boundary Problem:</strong> Important information often appears at the edges of chunks. Consider this example:</p>
                            
                            <div class="example-box">
                                <h5>Example: The Boundary Problem</h5>
                                <p><strong>Original Document:</strong></p>
                                <p>"Machine learning models require careful tuning. <strong>Hyperparameters like learning rate and batch size significantly impact model performance.</strong> Regularization techniques help prevent overfitting."</p>
                                
                                <p><strong>Without Overlap (Bad):</strong></p>
                                <ul>
                                    <li><strong>Chunk 1:</strong> "Machine learning models require careful tuning. Hyperparameters like learning rate and batch size significantly impact model performance."</li>
                                    <li><strong>Chunk 2:</strong> "Regularization techniques help prevent overfitting."</li>
                                </ul>
                                <p>❌ If a query asks about "hyperparameters and regularization," Chunk 1 might be retrieved (mentions hyperparameters) but Chunk 2 (mentions regularization) might not be, even though they're related concepts.</p>
                                
                                <p><strong>With Overlap (Good):</strong></p>
                                <ul>
                                    <li><strong>Chunk 1:</strong> "Machine learning models require careful tuning. Hyperparameters like learning rate and batch size significantly impact model performance. Regularization techniques help prevent overfitting."</li>
                                    <li><strong>Chunk 2:</strong> "Hyperparameters like learning rate and batch size significantly impact model performance. Regularization techniques help prevent overfitting."</li>
                                </ul>
                                <p>✅ Now both chunks contain information about hyperparameters AND regularization, improving retrieval quality.</p>
                            </div>
                            
                            <h4>How Overlap Works</h4>
                            <p>Overlap means that consecutive chunks share some content at their boundaries. For example, with 20% overlap:</p>
                            
                            <ul>
                                <li>If chunk size is 500 tokens, overlap is 100 tokens</li>
                                <li>Chunk 1: tokens 1-500</li>
                                <li>Chunk 2: tokens 401-900 (starts at token 401, overlapping the last 100 tokens of Chunk 1)</li>
                                <li>Chunk 3: tokens 801-1300 (overlaps with Chunk 2)</li>
                            </ul>
                            
                            <h4>Choosing the Right Overlap</h4>
                            <p><strong>Typical overlap:</strong> 10-20% of chunk size is standard. Here's why:</p>
                            
                            <ul>
                                <li><strong>Too little overlap (0-5%):</strong> ❌ Doesn't solve the boundary problem. Important context at boundaries is still lost. Not recommended.</li>
                                <li><strong>Moderate overlap (10-20%):</strong> ✅ Good balance. Preserves context without excessive storage overhead. This is the sweet spot for most use cases.</li>
                                <li><strong>High overlap (30-50%):</strong> ⚠️ Better context preservation but significantly increases storage costs. Use only when context preservation is critical (e.g., legal documents, medical records).</li>
                                <li><strong>Very high overlap (50%+):</strong> ❌ Wasteful. You're essentially storing the document twice. Rarely justified.</li>
                            </ul>
                            
                            <h4>Trade-offs</h4>
                            <p><strong>Benefits of overlap:</strong></p>
                            <ul>
                                <li>✅ Preserves context at boundaries</li>
                                <li>✅ Improves retrieval quality (related concepts stay together)</li>
                                <li>✅ Reduces risk of missing relevant information</li>
                                <li>✅ Better for queries that span multiple topics</li>
                            </ul>
                            
                            <p><strong>Costs of overlap:</strong></p>
                            <ul>
                                <li>❌ <strong>Increased storage:</strong> More chunks = more embeddings to store</li>
                                <li>❌ <strong>Higher embedding costs:</strong> More chunks to embed (if using paid APIs)</li>
                                <li>❌ <strong>Potential redundancy:</strong> Same information retrieved multiple times (though this is usually acceptable)</li>
                            </ul>
                            
                            <h4>Best Practices</h4>
                            <ul>
                                <li><strong>Start with 10-20% overlap:</strong> This works well for most documents</li>
                                <li><strong>Increase for critical documents:</strong> Use 20-30% for legal, medical, or financial documents where context is crucial</li>
                                <li><strong>Decrease for uniform content:</strong> Use 5-10% for structured data or lists where boundaries are less important</li>
                                <li><strong>Test and measure:</strong> Evaluate retrieval quality with different overlap percentages on your specific documents</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Chunking Mathematics Overview</h3>
                            <p>Chunking involves several mathematical considerations: calculating the number of chunks needed, determining overlap, and understanding storage efficiency. These formulas help you make informed decisions about chunk size and overlap for optimal RAG performance.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. Chunk Count Calculation</h4>
                            <div class="formula-display">
                                \[\text{num\_chunks} = \left\lceil \frac{\text{document\_length}}{\text{chunk\_size} - \text{overlap}} \right\rceil\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Calculates:</h5>
                                <p>This formula determines how many chunks you'll get when splitting a document of a given length into chunks of a specified size with overlap. The ceiling function (\(\lceil \rceil\)) ensures you round up to include any partial chunk at the end.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{document\_length}\):</strong> Total length of the document measured in characters or tokens (e.g., 10,000 characters or 2,500 tokens)</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Desired size of each chunk (e.g., 500 characters or 200 tokens)</li>
                                    <li><strong>\(\text{overlap}\):</strong> Number of characters/tokens that consecutive chunks share at their boundaries (e.g., 100 characters or 20 tokens)</li>
                                    <li><strong>\(\text{chunk\_size} - \text{overlap}\):</strong> Effective chunk size - the amount of new content each chunk adds (e.g., 500 - 100 = 400 characters of new content per chunk)</li>
                                    <li><strong>\(\left\lceil \ldots \right\rceil\):</strong> Ceiling function - rounds up to the nearest integer (ensures partial chunks are counted)</li>
                                </ul>
                                
                                <h5>Why Subtract Overlap?</h5>
                                <p>If chunks overlap by 100 characters, then each chunk after the first only adds 400 new characters (500 - 100 = 400). The overlap is shared between chunks, so it doesn't count as "new" content for the chunk count calculation.</p>
                                
                                <h5>Example:</h5>
                                <p>Document: 5,000 characters<br>
                                Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Effective chunk size: 500 - 100 = 400 characters<br>
                                Number of chunks: \(\lceil 5000 / 400 \rceil = \lceil 12.5 \rceil = 13\) chunks</p>
                                
                                <p><strong>Chunk distribution:</strong></p>
                                <ul>
                                    <li>Chunk 1: characters 1-500</li>
                                    <li>Chunk 2: characters 401-900 (overlaps 401-500 with Chunk 1)</li>
                                    <li>Chunk 3: characters 801-1300 (overlaps 801-900 with Chunk 2)</li>
                                    <li>... and so on</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. Overlap Percentage</h4>
                            <div class="formula-display">
                                \[\text{overlap\_percentage} = \frac{\text{overlap}}{\text{chunk\_size}} \times 100\%\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>This formula calculates what percentage of each chunk is shared with the next chunk. It helps you understand the trade-off between context preservation and storage efficiency.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{overlap}\):</strong> Number of overlapping characters/tokens between consecutive chunks</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Total size of each chunk</li>
                                    <li><strong>\(\frac{\text{overlap}}{\text{chunk\_size}}\):</strong> Fraction of chunk that overlaps (e.g., 100/500 = 0.2 = 20%)</li>
                                    <li><strong>\(\times 100\%\):</strong> Converts fraction to percentage</li>
                                </ul>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>10-20%:</strong> Standard overlap, good balance between context preservation and storage efficiency</li>
                                    <li><strong>5-10%:</strong> Low overlap, minimal storage overhead but less context preservation</li>
                                    <li><strong>20-30%:</strong> High overlap, better context preservation but significant storage increase</li>
                                    <li><strong>30%+:</strong> Very high overlap, rarely justified except for critical documents</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Overlap percentage: \(\frac{100}{500} \times 100\% = 20\%\)</p>
                                <p>This means 20% of each chunk (100 out of 500 characters) is shared with the next chunk, ensuring context is preserved at boundaries.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Storage Efficiency Ratio</h4>
                            <div class="formula-display">
                                \[\text{storage\_ratio} = \frac{\text{total\_chunks} \times \text{chunk\_size}}{\text{document\_length}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>This formula calculates how much storage space is needed for chunks compared to the original document. With overlap, you store more data than the original document (ratio > 1), but this preserves context at boundaries.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{total\_chunks}\):</strong> Number of chunks created from the document</li>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Size of each chunk (characters or tokens)</li>
                                    <li><strong>\(\text{total\_chunks} \times \text{chunk\_size}\):</strong> Total storage needed for all chunks (includes overlap)</li>
                                    <li><strong>\(\text{document\_length}\):</strong> Original document size</li>
                                    <li><strong>Ratio:</strong> How many times more storage is needed compared to original</li>
                                </ul>
                                
                                <h5>Interpreting the Ratio:</h5>
                                <ul>
                                    <li><strong>Ratio = 1.0:</strong> No overlap, storage equals original document size (rare in practice)</li>
                                    <li><strong>Ratio = 1.1-1.3:</strong> Moderate overlap (10-20%), typical for most RAG systems</li>
                                    <li><strong>Ratio = 1.3-1.5:</strong> High overlap (20-30%), better context but more storage</li>
                                    <li><strong>Ratio > 1.5:</strong> Very high overlap, usually not justified</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Document: 10,000 characters<br>
                                Chunk size: 500 characters<br>
                                Overlap: 100 characters (20%)<br>
                                Number of chunks: \(\lceil 10000 / (500-100) \rceil = \lceil 25 \rceil = 25\) chunks<br>
                                Storage ratio: \(\frac{25 \times 500}{10000} = \frac{12500}{10000} = 1.25\)</p>
                                <p>This means you need 25% more storage than the original document, but you preserve context at all chunk boundaries.</p>
                                
                                <h5>Trade-off:</h5>
                                <p>Higher storage ratio = better context preservation but more storage costs and embedding costs. Lower storage ratio = less storage but risk of losing context at boundaries.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Effective Chunk Size (New Content Per Chunk)</h4>
                            <div class="formula-display">
                                \[\text{effective\_chunk\_size} = \text{chunk\_size} - \text{overlap}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>The effective chunk size is the amount of <strong>new</strong> content each chunk adds, excluding the overlap that's shared with the previous chunk. This is what actually "advances" you through the document.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{chunk\_size}\):</strong> Total size of each chunk</li>
                                    <li><strong>\(\text{overlap}\):</strong> Amount shared with previous chunk</li>
                                    <li><strong>\(\text{chunk\_size} - \text{overlap}\):</strong> New content unique to this chunk</li>
                                </ul>
                                
                                <h5>Why This Matters:</h5>
                                <p>When calculating how many chunks you need, you use the effective chunk size, not the total chunk size, because overlap doesn't advance you through the document.</p>
                                
                                <h5>Example:</h5>
                                <p>Chunk size: 500 characters<br>
                                Overlap: 100 characters<br>
                                Effective chunk size: 500 - 100 = 400 characters</p>
                                <p>This means each chunk after the first adds 400 new characters of content, while 100 characters are shared with the previous chunk for context.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Total Storage with Overlap</h4>
                            <div class="formula-display">
                                \[\text{total\_storage} = \text{num\_chunks} \times \text{chunk\_size} = \left\lceil \frac{\text{document\_length}}{\text{chunk\_size} - \text{overlap}} \right\rceil \times \text{chunk\_size}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Calculates:</h5>
                                <p>Total storage space needed to store all chunks, including overlap. This helps you estimate storage costs and embedding API costs.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>First, calculate number of chunks using the chunk count formula</li>
                                    <li>Then multiply by chunk size to get total storage</li>
                                    <li>Result includes all overlap, so it's larger than the original document</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Document: 20,000 characters<br>
                                Chunk size: 1000 characters<br>
                                Overlap: 200 characters (20%)<br>
                                Number of chunks: \(\lceil 20000 / (1000-200) \rceil = \lceil 25 \rceil = 25\) chunks<br>
                                Total storage: \(25 \times 1000 = 25,000\) characters</p>
                                <p>Original document: 20,000 characters<br>
                                Storage overhead: 25,000 - 20,000 = 5,000 characters (25% increase)</p>
                                
                                <h5>Cost Implications:</h5>
                                <p>If you're using a paid embedding API (e.g., OpenAI), you pay per token/character embedded. With 25% storage overhead, you pay 25% more for embeddings. This is usually worth it for better retrieval quality, but it's important to be aware of the cost.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Dense vs Sparse Retrieval</h4>
                            <p><strong>Query:</strong> "How do I reset my password?"</p>
                            
                            <p><strong>Dense retrieval (semantic):</strong></p>
                            <ul>
                                <li>Finds: "Password recovery steps", "Reset account access", "Forgot password guide"</li>
                                <li>Understands synonyms: "reset" = "recovery" = "forgot"</li>
                            </ul>
                            
                            <p><strong>Sparse retrieval (keyword):</strong></p>
                            <ul>
                                <li>Finds: Documents containing "reset" AND "password"</li>
                                <li>Misses: "Password recovery" (no "reset" keyword)</li>
                            </ul>
                            
                            <p><strong>Hybrid:</strong> Combines both, gets best results</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Reranking</h4>
                            <p><strong>Initial retrieval (top-10):</strong></p>
                            <ul>
                                <li>Doc 1: "Password reset" (score: 0.85)</li>
                                <li>Doc 2: "Account settings" (score: 0.82)</li>
                                <li>Doc 3: "Password recovery" (score: 0.80)</li>
                                <li>...</li>
                            </ul>
                            
                            <p><strong>After reranking:</strong></p>
                            <ul>
                                <li>Doc 3: "Password recovery" (score: 0.92) ← More relevant!</li>
                                <li>Doc 1: "Password reset" (score: 0.88)</li>
                                <li>Doc 2: "Account settings" (score: 0.65) ← Less relevant</li>
                            </ul>
                            
                            <p><strong>Result:</strong> Better ordering improves final answer quality</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Hybrid Retrieval</h4>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import numpy as np

class HybridRetriever:
    """Hybrid dense + sparse retrieval"""
    
    def __init__(self, documents, alpha=0.5):
        self.documents = documents
        self.alpha = alpha  # Weight for BM25 vs dense
        
        # Dense retriever
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.embeddings = self.embedder.encode(documents)
        
        # Sparse retriever (BM25)
        tokenized_docs = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)
    
    def retrieve(self, query, top_k=5):
        """Retrieve using hybrid approach"""
        # Dense retrieval
        query_embedding = self.embedder.encode([query])
        dense_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-8)
        
        # Sparse retrieval
        tokenized_query = query.split()
        sparse_scores = self.bm25.get_scores(tokenized_query)
        sparse_scores = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-8)
        
        # Combine
        hybrid_scores = self.alpha * sparse_scores + (1 - self.alpha) * dense_scores
        
        # Get top-k
        top_indices = np.argsort(hybrid_scores)[-top_k:][::-1]
        return [self.documents[i] for i in top_indices]

# Example
docs = ["Password reset instructions", "Account settings guide", "Password recovery steps"]
retriever = HybridRetriever(docs)
results = retriever.retrieve("How do I reset my password?")
print(results)</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Reranking with Cross-Encoder</h4>
                            <pre><code class="language-python">from sentence_transformers import CrossEncoder

class Reranker:
    """Rerank retrieved documents"""
    
    def __init__(self):
        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def rerank(self, query, documents, top_k=3):
        """Rerank documents for query"""
        # Create query-document pairs
        pairs = [[query, doc] for doc in documents]
        
        # Get scores
        scores = self.model.predict(pairs)
        
        # Sort by score
        ranked_indices = np.argsort(scores)[::-1]
        
        # Return top-k
        return [documents[i] for i in ranked_indices[:top_k]]

# Example
reranker = Reranker()
docs = ["Password reset", "Account settings", "Password recovery"]
reranked = reranker.rerank("How do I reset my password?", docs)
print(reranked)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Retrieval Strategy Selection</h3>
                            <p><strong>Use dense retrieval when:</strong></p>
                            <ul>
                                <li>Semantic understanding is important</li>
                                <li>Users may phrase queries differently</li>
                                <li>Domain-specific terminology</li>
                            </ul>
                            
                            <p><strong>Use sparse retrieval when:</strong></p>
                            <ul>
                                <li>Exact keyword matching is important</li>
                                <li>Speed is critical</li>
                                <li>Technical documentation with specific terms</li>
                            </ul>
                            
                            <p><strong>Use hybrid when:</strong></p>
                            <ul>
                                <li>You want best of both worlds</li>
                                <li>High accuracy is required</li>
                                <li>Can afford extra computation</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Reranking Benefits</h3>
                            <p><strong>When to use reranking:</strong></p>
                            <ul>
                                <li>Initial retrieval returns many candidates</li>
                                <li>Need high precision in top results</li>
                                <li>Can afford additional latency</li>
                                <li>Quality is more important than speed</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: Why is document chunking important in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) LLMs have context limits, chunking breaks documents into manageable pieces that fit in context windows while preserving semantic meaning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To make documents smaller</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To reduce storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What are the different chunking strategies and when would you use each?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Fixed-size: Simple, fast, good for uniform text. Semantic: Preserves meaning, better for varied content. Recursive: Handles nested structures. Use fixed-size for speed, semantic for quality, recursive for structured documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only fixed-size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only semantic</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking strategy doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is chunk overlap and why is it used?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Overlapping chunks share some content to preserve context at boundaries, preventing information loss when sentences/paragraphs are split</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To reduce storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To make chunks smaller</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Overlap is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "How do you determine optimal chunk size?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance LLM context window, retrieval precision (smaller = more precise), and semantic completeness (larger = more context). Common: 200-1000 tokens. Test on your data and downstream task</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use 100 tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use 5000 tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunk size doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is semantic chunking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Chunking based on semantic boundaries (sentences, paragraphs, topics) rather than fixed sizes, preserving meaning and context</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Chunking by file size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chunking</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Chunking by word count only</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: Interview question: "How do you handle different document types (PDF, HTML, Markdown) in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use appropriate parsers (PyPDF2, BeautifulSoup, markdown), extract text while preserving structure, handle metadata, and apply document-type-specific chunking strategies</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Convert all to text first</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only support one format</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No special handling needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is metadata extraction in document processing?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Extracting document properties (title, author, date, source, section) to enable filtering and better retrieval in vector databases</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Extracting all text</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Compressing documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Metadata is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "How do you preserve context when chunking documents?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use chunk overlap, preserve sentence/paragraph boundaries, include surrounding context in metadata, and use semantic chunking to keep related content together</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No context preservation needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only use fixed-size chunks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Split randomly</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What are the trade-offs between small and large chunk sizes?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Small chunks: More precise retrieval but may lose context. Large chunks: More context but less precise retrieval. Balance based on query type and document structure</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Small is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Large is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Size doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How would you handle very long documents (e.g., books) in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use hierarchical chunking (chapters → sections → paragraphs), maintain document structure in metadata, use multi-level retrieval, and consider document summarization for overview</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Split into equal chunks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use only first part</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Skip long documents</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is recursive chunking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Chunking that tries multiple strategies in order (e.g., paragraphs → sentences → characters) until chunks fit size requirements, handling nested document structures</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Chunking twice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chunking</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference from fixed-size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "How do you evaluate chunking quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Measure retrieval performance (precision@k, recall@k), test downstream RAG quality, check if relevant information is preserved, and evaluate chunk coherence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check chunk size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No evaluation needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only check number of chunks</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 2</a>
                <a href="/tutorials/rag/chapter4" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 4 →</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
    </script>
</body>
</html>