<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: Advanced RAG Techniques - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 6: Advanced RAG Techniques</h1>
                <p class="chapter-subtitle">Improving Performance</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="85"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn active">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand advanced rag techniques fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Advanced RAG Techniques</h2>
                        
                        <div class="explanation-box">
                            <h3>Beyond Basic RAG: Optimizing for Real-World Performance</h3>
                            <p>Basic RAG (retrieve top-k documents, pass to LLM, generate answer) works well for simple queries, but real-world applications face complex challenges: ambiguous queries, context window limits, multi-part questions, and the need for higher accuracy. <strong>Advanced RAG techniques</strong> address these challenges to significantly improve system performance.</p>
                            
                            <p><strong>Common problems with basic RAG:</strong></p>
                            <ul>
                                <li>❌ <strong>Query ambiguity:</strong> Short queries like "Python ML" are unclear - does the user want Python machine learning libraries, Python ML algorithms, or something else?</li>
                                <li>❌ <strong>Context overflow:</strong> Retrieved chunks might exceed LLM context window limits, forcing truncation and loss of information</li>
                                <li>❌ <strong>Incomplete answers:</strong> Complex questions require information from multiple documents, but basic RAG retrieves once and generates once</li>
                                <li>❌ <strong>Precision vs context trade-off:</strong> Small chunks are precise but lack context; large chunks have context but are less precise</li>
                                <li>❌ <strong>Single retrieval limitation:</strong> One retrieval pass might miss relevant documents that use different terminology</li>
                            </ul>
                            
                            <h4>Advanced Techniques That Solve These Problems</h4>
                            <ol>
                                <li><strong>Query Expansion:</strong> Automatically expand or rewrite queries to include synonyms, related terms, and alternative phrasings before retrieval</li>
                                <li><strong>Multi-Query Retrieval:</strong> Generate multiple query variations, retrieve for each, and combine results for comprehensive coverage</li>
                                <li><strong>Parent-Child Chunking:</strong> Store small chunks for precise retrieval, but include parent document context when generating answers</li>
                                <li><strong>Context Compression:</strong> Summarize or extract only relevant parts of retrieved chunks to fit within context windows</li>
                                <li><strong>Iterative Retrieval:</strong> If initial answer is incomplete, generate follow-up queries and retrieve more context (multi-hop retrieval)</li>
                                <li><strong>Self-RAG:</strong> Autonomous system where the model decides when to retrieve, when to generate, and when to stop</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Example: Multi-Query Retrieval in Action</h5>
                                <p><strong>Original query:</strong> "How to train a neural network?"</p>
                                <p><strong>Generated variations:</strong></p>
                                <ul>
                                    <li>"neural network training process"</li>
                                    <li>"how to train deep learning models"</li>
                                    <li>"backpropagation and gradient descent for neural networks"</li>
                                </ul>
                                <p><strong>Result:</strong> Each variation retrieves slightly different documents. Combined, you get comprehensive coverage of the topic - you don't miss relevant information that uses different terminology.</p>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Query Expansion:</strong> Techniques to improve retrieval by expanding queries with synonyms and related terms</li>
                                <li><strong>Multi-Query Retrieval:</strong> Generating multiple query variations and combining results for better coverage</li>
                                <li><strong>Parent-Child Chunking:</strong> Hierarchical chunking strategy that balances retrieval precision with generation context</li>
                                <li><strong>Context Compression:</strong> Methods to reduce retrieved context size (summarization, sentence extraction) while preserving important information</li>
                                <li><strong>Iterative Retrieval:</strong> Multi-hop retrieval where follow-up queries retrieve additional context if initial answer is incomplete</li>
                                <li><strong>Self-RAG:</strong> Advanced autonomous RAG where the model controls retrieval and generation decisions</li>
                                <li><strong>Metadata Filtering:</strong> Combining vector search with traditional filters to reduce search space and improve precision</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> These advanced techniques can improve RAG system accuracy by 20-40% compared to basic RAG. They're essential for production systems where accuracy, completeness, and user experience are critical. Understanding when and how to apply these techniques is what separates basic RAG implementations from production-grade systems.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Advanced RAG Techniques: Beyond Basic Retrieval</h3>
                            
                            <p>Basic RAG (retrieve top-k documents, pass to LLM) works well for simple queries, but real-world applications often need more sophisticated techniques to handle complex queries, improve accuracy, and optimize performance.</p>
                            
                            <h4>1. Query Expansion: Improving Retrieval Coverage</h4>
                            <p><strong>What it is:</strong> Query expansion rewrites or augments the user's query to include synonyms, related terms, or alternative phrasings before retrieval. This helps find documents that use different terminology than the original query.</p>
                            
                            <p><strong>Why it's needed:</strong> Users often write short, ambiguous queries. A query like "Python ML" could mean many things, and documents might use different terminology ("machine learning," "deep learning," "scikit-learn," etc.).</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Synonym expansion:</strong> Add synonyms for key terms (e.g., "ML" → "machine learning," "deep learning")</li>
                                <li><strong>Related term expansion:</strong> Add related concepts (e.g., "Python ML" → "scikit-learn," "TensorFlow," "PyTorch")</li>
                                <li><strong>LLM-based expansion:</strong> Use an LLM to generate query variations or rewrite the query more clearly</li>
                                <li><strong>Retrieve with expanded query:</strong> Use the expanded query for retrieval (or retrieve for each variation and combine)</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Example: Query Expansion</h5>
                                <p><strong>Original query:</strong> "Python ML"</p>
                                <p><strong>Expanded query:</strong> "Python machine learning libraries scikit-learn TensorFlow PyTorch data science"</p>
                                <p><strong>Result:</strong> ✅ Retrieves documents that mention "scikit-learn" or "TensorFlow" even if they don't contain "Python ML" as a phrase.</p>
                            </div>
                            
                            <p><strong>When to use:</strong> When queries are short, ambiguous, or use abbreviations. Particularly useful for technical domains with lots of synonyms and jargon.</p>
                            
                            <h4>2. Multi-Query Retrieval: Generating Multiple Query Variations</h4>
                            <p><strong>What it is:</strong> Instead of retrieving with a single query, generate multiple query variations, retrieve documents for each variation, then combine and deduplicate the results.</p>
                            
                            <p><strong>Why it works:</strong> Different phrasings of the same question might retrieve different relevant documents. By generating multiple variations, you increase the chance of finding all relevant information.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Generate query variations:</strong> Use an LLM to generate 3-5 different phrasings of the original query</li>
                                <li><strong>Retrieve for each:</strong> Run retrieval for each query variation</li>
                                <li><strong>Combine results:</strong> Merge all retrieved documents, removing duplicates</li>
                                <li><strong>Rerank:</strong> Rerank the combined results to get the most relevant documents</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Example: Multi-Query Retrieval</h5>
                                <p><strong>Original query:</strong> "How to train a neural network?"</p>
                                <p><strong>Generated variations:</strong></p>
                                <ul>
                                    <li>"neural network training process"</li>
                                    <li>"how to train deep learning models"</li>
                                    <li>"backpropagation and gradient descent for neural networks"</li>
                                </ul>
                                <p><strong>Result:</strong> ✅ Each variation might retrieve slightly different documents, giving you comprehensive coverage of the topic.</p>
                            </div>
                            
                            <p><strong>When to use:</strong> For complex queries where a single phrasing might miss relevant documents. Particularly effective when combined with query expansion.</p>
                            
                            <h4>3. Parent-Child Chunking: Balancing Precision and Context</h4>
                            <p><strong>What it is:</strong> A hierarchical chunking strategy where you store small, precise chunks for retrieval (children), but also store larger parent chunks that contain surrounding context.</p>
                            
                            <p><strong>The problem it solves:</strong> Small chunks are better for precise retrieval (more likely to be fully relevant), but they lack context. Large chunks have more context but are less precise (may contain irrelevant information).</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Create parent chunks:</strong> Split documents into larger chunks (e.g., 1000 tokens) that preserve context</li>
                                <li><strong>Create child chunks:</strong> Split each parent into smaller chunks (e.g., 200 tokens) for precise retrieval</li>
                                <li><strong>Store both:</strong> Embed and store both parent and child chunks, with metadata linking children to parents</li>
                                <li><strong>Retrieve children:</strong> Use small child chunks for retrieval (precise matching)</li>
                                <li><strong>Include parent context:</strong> When a child is retrieved, also include its parent chunk in the context sent to the LLM</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Example: Parent-Child Chunking</h5>
                                <p><strong>Parent chunk (1000 tokens):</strong> "Machine learning models require careful tuning. Hyperparameters like learning rate significantly impact performance. Regularization techniques help prevent overfitting..."</p>
                                <p><strong>Child chunk 1 (200 tokens):</strong> "Hyperparameters like learning rate significantly impact performance."</p>
                                <p><strong>Query:</strong> "What is learning rate?"</p>
                                <p><strong>Result:</strong> ✅ Child chunk 1 is retrieved (precise match), but parent chunk is also included, providing context about hyperparameters and regularization.</p>
                            </div>
                            
                            <p><strong>When to use:</strong> When you need both precise retrieval (small chunks) and rich context (large chunks). Common in production RAG systems.</p>
                            
                            <h4>4. Metadata Filtering: Reducing Search Space</h4>
                            <p><strong>What it is:</strong> Apply filters based on document metadata (date, author, category, source) <strong>before</strong> performing vector similarity search. This reduces the number of documents to search.</p>
                            
                            <p><strong>Why it's important:</strong> Searching 1 million documents is slow. If you can filter to 50,000 relevant documents first (e.g., "only documents from 2023"), search becomes much faster and more accurate.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Extract metadata:</strong> When indexing, extract and store metadata (date, category, author, etc.)</li>
                                <li><strong>Apply filters:</strong> Before vector search, filter documents by metadata criteria</li>
                                <li><strong>Search filtered set:</strong> Perform vector similarity search only on filtered documents</li>
                                <li><strong>Return results:</strong> Return top-k documents from the filtered search</li>
                            </ol>
                            
                            <p><strong>When to use:</strong> When documents have meaningful metadata and queries can be scoped (e.g., "recent articles," "technical documentation," "from specific author").</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Context Compression: Fitting Retrieved Information into LLM Windows</h3>
                            
                            <h4>The Problem</h4>
                            <p>After retrieving top-k documents, you might have 10,000+ tokens of context. But LLMs have context window limits (e.g., GPT-4: 8K-128K tokens, Claude: 100K-200K tokens). If your query + context + answer exceeds the limit, you need to compress the context.</p>
                            
                            <h4>Context Compression Techniques</h4>
                            
                            <h5>1. Summarization</h5>
                            <p><strong>What it is:</strong> Use an LLM to summarize each retrieved chunk, reducing token count while preserving key information.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>For each retrieved chunk, create a summary prompt: "Summarize this text in 2-3 sentences: [chunk text]"</li>
                                <li>Generate summaries (much shorter than originals)</li>
                                <li>Use summaries as context instead of full chunks</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong> ✅ Reduces tokens significantly. ❌ May lose important details. ⚠️ Adds latency (need to summarize each chunk).</p>
                            
                            <h5>2. Sentence Extraction</h5>
                            <p><strong>What it is:</strong> Extract only the most relevant sentences from retrieved chunks, discarding the rest.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Score each sentence in retrieved chunks for relevance to the query (using embeddings or LLM)</li>
                                <li>Select top-N most relevant sentences</li>
                                <li>Use only these sentences as context</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong> ✅ Very fast. ✅ Preserves exact information (no summarization loss). ❌ May lose context between sentences.</p>
                            
                            <h5>3. LLM-Based Compression</h5>
                            <p><strong>What it is:</strong> Use an LLM to compress the entire retrieved context into a shorter version that preserves information relevant to the query.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Prompt: "Compress this context to [target tokens] while preserving all information relevant to: [query]"</li>
                                <li>LLM generates compressed version</li>
                                <li>Use compressed context for final answer generation</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong> ✅ Most intelligent compression. ✅ Preserves query-relevant information. ❌ Expensive and slow. ❌ May introduce hallucinations.</p>
                            
                            <h5>4. Relevance-Based Prioritization</h5>
                            <p><strong>What it is:</strong> Instead of compressing, prioritize chunks by relevance score and include only the most relevant ones until you hit the token limit.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Sort retrieved chunks by similarity score (most relevant first)</li>
                                <li>Add chunks to context one by one until you reach token limit</li>
                                <li>Discard remaining chunks</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong> ✅ Fast and simple. ✅ No information loss (just truncation). ❌ May miss important information in lower-ranked chunks.</p>
                            
                            <h4>Best Practices</h4>
                            <ul>
                                <li><strong>Combine techniques:</strong> Use summarization for less relevant chunks, full text for most relevant</li>
                                <li><strong>Reserve tokens for answer:</strong> Leave 20-30% of context window for the LLM's answer</li>
                                <li><strong>Monitor compression quality:</strong> Track answer quality with and without compression</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Iterative Retrieval: Multi-Hop and Self-RAG</h3>
                            
                            <h4>1. Multi-Hop Retrieval</h4>
                            <p><strong>What it is:</strong> If the initial answer is incomplete or the model needs more information, generate follow-up queries, retrieve additional context, and repeat until the answer is complete.</p>
                            
                            <p><strong>Why it's needed:</strong> Complex questions often require information from multiple documents. A single retrieval might not find all necessary information.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Initial retrieval:</strong> Retrieve top-k documents for the original query</li>
                                <li><strong>Generate answer:</strong> Attempt to generate answer from retrieved context</li>
                                <li><strong>Check completeness:</strong> Determine if answer is complete (using LLM or heuristics)</li>
                                <li><strong>Generate follow-up query:</strong> If incomplete, generate a new query to find missing information</li>
                                <li><strong>Retrieve again:</strong> Retrieve documents for the follow-up query</li>
                                <li><strong>Combine context:</strong> Add new context to existing context</li>
                                <li><strong>Generate final answer:</strong> Generate answer from combined context</li>
                                <li><strong>Repeat if needed:</strong> Continue until answer is complete or max iterations reached</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Example: Multi-Hop Retrieval</h5>
                                <p><strong>Query:</strong> "What is the capital of France and what is its population?"</p>
                                <p><strong>Hop 1:</strong> Query "capital of France" → Retrieves document about Paris being the capital</p>
                                <p><strong>Hop 2:</strong> Query "Paris population" → Retrieves document with population statistics</p>
                                <p><strong>Result:</strong> ✅ Combines information from both hops to answer the complete question.</p>
                            </div>
                            
                            <p><strong>When to use:</strong> For complex, multi-part questions that require information from multiple sources. Common in question-answering systems.</p>
                            
                            <h4>2. Self-RAG: Autonomous Retrieval and Generation</h4>
                            <p><strong>What it is:</strong> An advanced technique where the model itself decides when to retrieve, what to retrieve, when to generate, and when to stop—making the RAG system more autonomous.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li><strong>Retrieve decision:</strong> Model decides if retrieval is needed (some queries can be answered from its training data)</li>
                                <li><strong>Query generation:</strong> If retrieval needed, model generates the retrieval query</li>
                                <li><strong>Retrieval:</strong> Retrieve documents using generated query</li>
                                <li><strong>Relevance check:</strong> Model evaluates if retrieved documents are relevant</li>
                                <li><strong>Generate or retrieve again:</strong> If relevant, generate answer; if not, retrieve again with different query</li>
                                <li><strong>Self-critique:</strong> Model evaluates its own answer for completeness and accuracy</li>
                                <li><strong>Iterate or stop:</strong> If answer is good, stop; if not, continue retrieving and generating</li>
                            </ol>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>More autonomous:</strong> Doesn't always retrieve (saves cost when not needed)</li>
                                <li>✅ <strong>Adaptive:</strong> Adjusts retrieval strategy based on query complexity</li>
                                <li>✅ <strong>Self-correcting:</strong> Can identify when answers are incomplete and retrieve more</li>
                            </ul>
                            
                            <p><strong>Challenges:</strong></p>
                            <ul>
                                <li>❌ <strong>Complex to implement:</strong> Requires fine-tuning or prompt engineering</li>
                                <li>❌ <strong>Higher latency:</strong> Multiple decision points add time</li>
                                <li>❌ <strong>Cost:</strong> More LLM calls (retrieve decisions, query generation, self-critique)</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For high-value applications where accuracy is critical and you can afford the complexity and cost. Still experimental but promising.</p>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Advanced RAG Mathematical Models</h3>
                            <p>Advanced RAG techniques involve mathematical models for generation, context management, and quality evaluation. Understanding these formulas helps you optimize context usage, implement iterative retrieval, and measure answer quality.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. RAG Generation Probability</h4>
                            <div class="formula-display">
                                \[P(\text{answer} | \text{query}, \text{context}) = \prod_{i=1}^{n} P(\text{token}_i | \text{query}, \text{context}, \text{tokens}_{<i})\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Represents:</h5>
                                <p>This is the fundamental probability model for RAG generation. The LLM generates the answer token by token, where each token's probability depends on the query, retrieved context, and all previously generated tokens.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(P(\text{answer} | \text{query}, \text{context})\):</strong> Probability of generating the entire answer given the query and retrieved context</li>
                                    <li><strong>\(\prod_{i=1}^{n}\):</strong> Product over all \(n\) tokens in the answer (multiplication of probabilities)</li>
                                    <li><strong>\(P(\text{token}_i | \text{query}, \text{context}, \text{tokens}_{<i})\):</strong> Probability of generating token \(i\) given:
                                        <ul>
                                            <li>The query (user's question)</li>
                                            <li>The context (retrieved documents)</li>
                                            <li>All previous tokens (\(\text{tokens}_{<i}\) means tokens before position \(i\))</li>
                                        </ul>
                                    </li>
                                </ul>
                                
                                <h5>Why It's a Product:</h5>
                                <p>To generate the full answer, the model must generate token 1, THEN token 2 given token 1, THEN token 3 given tokens 1 and 2, and so on. The probability of the entire sequence is the product of these conditional probabilities (chain rule of probability).</p>
                                
                                <h5>Key Insight:</h5>
                                <p>The formula shows that the answer depends on BOTH the query AND the context. This is what makes RAG different from standard LLM generation - the context from retrieved documents directly influences each token's probability.</p>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is the capital of France?"<br>
                                Context: "France is a country in Europe. Its capital city is Paris."</p>
                                
                                <p>The model generates:</p>
                                <ul>
                                    <li>Token 1 ("The"): \(P(\text{"The"} | \text{query}, \text{context})\) - high probability because context mentions "capital"</li>
                                    <li>Token 2 ("capital"): \(P(\text{"capital"} | \text{query}, \text{context}, \text{"The"})\) - high probability, matches context</li>
                                    <li>Token 3 ("of"): \(P(\text{"of"} | \text{query}, \text{context}, \text{"The capital"})\) - high probability, grammatical continuation</li>
                                    <li>Token 4 ("France"): \(P(\text{"France"} | \text{query}, \text{context}, \text{"The capital of"})\) - high probability, matches query</li>
                                    <li>Token 5 ("is"): \(P(\text{"is"} | \text{query}, \text{context}, \text{"The capital of France"})\) - high probability</li>
                                    <li>Token 6 ("Paris"): \(P(\text{"Paris"} | \text{query}, \text{context}, \text{"The capital of France is"})\) - very high probability, directly from context!</li>
                                </ul>
                                
                                <p>The final answer "The capital of France is Paris" has high probability because each token is likely given the context.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. Context Window Constraint</h4>
                            <div class="formula-display">
                                \[|\text{query}| + |\text{context}| + |\text{answer}| \leq \text{max\_context\_length}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Constraint Enforces:</h5>
                                <p>LLMs have fixed context window limits. The total number of tokens (query + retrieved context + generated answer) must fit within this limit. This constraint determines how much context you can include and how long answers can be.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(|\text{query}|\):</strong> Number of tokens in the user's query (typically 10-50 tokens)</li>
                                    <li><strong>\(|\text{context}|\):</strong> Number of tokens in retrieved documents (can be 500-4000+ tokens depending on top-k and chunk size)</li>
                                    <li><strong>\(|\text{answer}|\):</strong> Number of tokens in the generated answer (variable, depends on query complexity)</li>
                                    <li><strong>\(\text{max\_context\_length}\):</strong> Maximum tokens the LLM can process (e.g., GPT-4: 8K-128K, Claude: 100K-200K, GPT-3.5: 4K-16K)</li>
                                </ul>
                                
                                <h5>Practical Implications:</h5>
                                <p>If your context window is 4,000 tokens and your query is 50 tokens, you have ~3,950 tokens available for context + answer. If you retrieve 3,000 tokens of context, you only have ~950 tokens left for the answer. This is why context compression is important!</p>
                                
                                <h5>Example:</h5>
                                <p><strong>Scenario:</strong> GPT-3.5-turbo with 4,096 token context window</p>
                                <ul>
                                    <li>Query: 30 tokens</li>
                                    <li>Retrieved context: 3,500 tokens (5 chunks × 700 tokens each)</li>
                                    <li>Available for answer: 4,096 - 30 - 3,500 = 566 tokens</li>
                                    <li>✅ Fits, but close to limit</li>
                                </ul>
                                
                                <p><strong>Problem scenario:</strong></p>
                                <ul>
                                    <li>Query: 30 tokens</li>
                                    <li>Retrieved context: 4,200 tokens (too much!)</li>
                                    <li>Available for answer: 4,096 - 30 - 4,200 = -134 tokens ❌</li>
                                    <li><strong>Solution:</strong> Compress context to 3,000 tokens, leaving 1,066 tokens for answer</li>
                                </ul>
                                
                                <h5>Strategies to Handle This:</h5>
                                <ul>
                                    <li><strong>Context compression:</strong> Summarize or extract relevant parts to reduce \(|\text{context}|\)</li>
                                    <li><strong>Prioritize chunks:</strong> Include only top-k most relevant chunks</li>
                                    <li><strong>Reserve tokens:</strong> Leave 20-30% of context window for answer generation</li>
                                    <li><strong>Use larger context windows:</strong> GPT-4 (128K) or Claude (200K) for very long contexts</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Answer Quality Score</h4>
                            <div class="formula-display">
                                \[\text{quality} = \alpha \cdot \text{relevance} + \beta \cdot \text{faithfulness} + \gamma \cdot \text{completeness}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>Answer quality in RAG systems is multi-dimensional. This formula combines three critical dimensions into a single quality score, allowing you to evaluate and optimize RAG system performance.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{relevance}\):</strong> How well the answer addresses the query. Measured by semantic similarity between query and answer embeddings, or by human/LM evaluation. Range: [0, 1]</li>
                                    <li><strong>\(\text{faithfulness}\):</strong> How well the answer is grounded in the retrieved context (not hallucinated). Measured as fraction of answer claims supported by context. Range: [0, 1]</li>
                                    <li><strong>\(\text{completeness}\):</strong> How complete the answer is - does it fully address all parts of the query? Measured by coverage of query aspects. Range: [0, 1]</li>
                                    <li><strong>\(\alpha, \beta, \gamma\):</strong> Weighting factors that sum to 1.0, controlling the relative importance of each dimension</li>
                                </ul>
                                
                                <h5>Typical Weightings:</h5>
                                <ul>
                                    <li><strong>Balanced:</strong> \(\alpha = 0.4, \beta = 0.4, \gamma = 0.2\) - Equal emphasis on relevance and faithfulness, less on completeness</li>
                                    <li><strong>Faithfulness-focused:</strong> \(\alpha = 0.3, \beta = 0.5, \gamma = 0.2\) - Emphasizes avoiding hallucinations (critical for factual domains)</li>
                                    <li><strong>Completeness-focused:</strong> \(\alpha = 0.3, \beta = 0.3, \gamma = 0.4\) - Emphasizes comprehensive answers (good for complex queries)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What are the main advantages of RAG?"</p>
                                <p><strong>Answer 1:</strong> "RAG is good."<br>
                                Relevance: 0.6 (somewhat relevant but vague)<br>
                                Faithfulness: 1.0 (supported by context)<br>
                                Completeness: 0.2 (very incomplete)<br>
                                Quality (α=0.4, β=0.4, γ=0.2): \(0.4 \times 0.6 + 0.4 \times 1.0 + 0.2 \times 0.2 = 0.68\)</p>
                                
                                <p><strong>Answer 2:</strong> "RAG has several advantages: no training required, easy to update knowledge, can cite sources, works with any LLM."<br>
                                Relevance: 0.95 (highly relevant)<br>
                                Faithfulness: 0.9 (mostly supported, minor elaboration)<br>
                                Completeness: 0.9 (covers main advantages)<br>
                                Quality (α=0.4, β=0.4, γ=0.2): \(0.4 \times 0.95 + 0.4 \times 0.9 + 0.2 \times 0.9 = 0.92\)</p>
                                
                                <p>✅ Answer 2 scores much higher due to better relevance and completeness.</p>
                                
                                <h5>Using This for Optimization:</h5>
                                <p>Track quality scores over time. If faithfulness drops, you might need better retrieval or context. If relevance drops, you might need better query understanding. If completeness drops, you might need to retrieve more documents or use iterative retrieval.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Multi-Query Retrieval Coverage</h4>
                            <div class="formula-display">
                                \[\text{coverage} = \frac{|\bigcup_{i=1}^{m} \text{Retrieve}(q_i, D)|}{|D_{\text{relevant}}|}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Multi-query retrieval generates \(m\) query variations and retrieves documents for each. This formula measures what fraction of all relevant documents were found by at least one query variation.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(q_1, q_2, \ldots, q_m\):</strong> \(m\) different query variations (e.g., 3-5 variations)</li>
                                    <li><strong>\(\text{Retrieve}(q_i, D)\):</strong> Documents retrieved for query variation \(q_i\)</li>
                                    <li><strong>\(\bigcup_{i=1}^{m}\):</strong> Union of all retrieved document sets (removes duplicates)</li>
                                    <li><strong>\(|D_{\text{relevant}}|\):</strong> Total number of relevant documents in the knowledge base</li>
                                    <li><strong>Coverage:</strong> Fraction of relevant documents found by at least one query variation</li>
                                </ul>
                                
                                <h5>Why Multi-Query Improves Coverage:</h5>
                                <p>Different query phrasings might retrieve different relevant documents. By generating multiple variations and taking the union, you increase the chance of finding all relevant documents.</p>
                                
                                <h5>Example:</h5>
                                <p>Original query: "How to train a neural network?"</p>
                                <p>Query variations:</p>
                                <ul>
                                    <li>\(q_1\): "neural network training process" → retrieves docs: {A, B, C}</li>
                                    <li>\(q_2\): "how to train deep learning models" → retrieves docs: {B, D, E}</li>
                                    <li>\(q_3\): "backpropagation and gradient descent" → retrieves docs: {C, F, G}</li>
                                </ul>
                                
                                <p>Union: {A, B, C, D, E, F, G} (7 unique documents)</p>
                                <p>If single query \(q_1\) only retrieved {A, B, C}, multi-query found 4 additional relevant documents (D, E, F, G).</p>
                                
                                <p>Coverage improvement: From 3/7 = 43% to 7/7 = 100% of relevant documents found.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Context Compression Ratio</h4>
                            <div class="formula-display">
                                \[\text{compression\_ratio} = \frac{|\text{original\_context}|}{|\text{compressed\_context}|}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>When retrieved context exceeds the LLM's context window, you need to compress it. This formula measures the compression achieved (how much smaller the compressed context is compared to original).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(|\text{original\_context}|\):</strong> Token count of original retrieved context</li>
                                    <li><strong>\(|\text{compressed\_context}|\):</strong> Token count after compression (summarization, extraction, etc.)</li>
                                    <li><strong>Compression ratio:</strong> How many times smaller the compressed version is</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Original context: 5,000 tokens (5 retrieved chunks × 1,000 tokens each)<br>
                                Compressed context: 2,000 tokens (summarized chunks)<br>
                                Compression ratio: \(\frac{5000}{2000} = 2.5x\)</p>
                                <p>This means the compressed context is 2.5 times smaller, allowing you to fit more chunks or leave more room for the answer.</p>
                                
                                <h5>Trade-off:</h5>
                                <p>Higher compression ratio = more space saved but risk of losing important details. Lower compression ratio = preserves more information but less space saved. Typical compression ratios: 2-5x for summarization, 3-10x for sentence extraction.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Step-by-Step Examples</h4>
                            <h4>Example: RAG Generation Process</h4>
                            <p><strong>Query:</strong> "What is the capital of France?"</p>
                            
                            <p><strong>Step 1: Retrieval</strong></p>
                            <ul>
                                <li>Retrieved context: "France is a country in Europe. Its capital city is Paris."</li>
                            </ul>
                            
                            <p><strong>Step 2: Prompt Construction</strong></p>
                            <ul>
                                <li>Prompt: "Context: France is a country in Europe. Its capital city is Paris. Question: What is the capital of France? Answer:"</li>
                            </ul>
                            
                            <p><strong>Step 3: Generation</strong></p>
                            <ul>
                                <li>LLM generates: "The capital of France is Paris."</li>
                                <li>Answer is grounded in retrieved context</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Context Truncation</h4>
                            <p><strong>Scenario:</strong> Retrieved 5 documents, each 500 tokens. Context window: 4000 tokens. Query: 50 tokens.</p>
                            
                            <p><strong>Problem:</strong> 5 × 500 + 50 = 2550 tokens (fits, but close to limit)</p>
                            
                            <p><strong>Solution:</strong> Prioritize most relevant documents, truncate if needed</p>
                            
                            <p><strong>Strategy:</strong> Include top-3 documents (1500 tokens), truncate others if needed</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Code Implementation</h4>
                            <pre><code class="language-python">import numpy as np

from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# RAG generation with custom prompt
template = """Context: {context}

Question: {question}

Answer based on the context above. If the answer is not in the context, say "I don't know":"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

# Create RAG chain
llm = OpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# Query
query = "What is the capital of France?"
result = qa_chain({"query": query})
print(f"Answer: {result['result']}")
print(f"Sources: {result['source_documents']}")

# Alternative: Manual generation
def generate_answer(query, retrieved_docs, llm):
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    answer = llm.generate([prompt])
    return answer</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Where This Is Used</h3>
                            <h3>Generation Strategies</h3>
                            <p><strong>Stuff:</strong> Put all context in single prompt. Simple but limited by context window.</p>
                            <p><strong>Map-reduce:</strong> Generate answer for each document, then combine. Handles large contexts.</p>
                            <p><strong>Refine:</strong> Iteratively refine answer with each document. Most accurate but slowest.</p>
                            <p><strong>Map-rerank:</strong> Generate and score answers, return best. Good balance.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Answer Quality</h3>
                            <p><strong>Relevance:</strong> Answer addresses the query</p>
                            <p><strong>Faithfulness:</strong> Answer is grounded in retrieved context (not hallucinated)</p>
                            <p><strong>Completeness:</strong> Answer is comprehensive</p>
                            <p><strong>Citation:</strong> Can cite source documents</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are advanced RAG techniques?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Methods to improve RAG performance: query expansion, multi-query retrieval, parent-child chunking, context compression, iterative retrieval, and metadata filtering</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only using better models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only using more data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No advanced techniques exist</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What is query expansion and why is it useful?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Rewriting or expanding queries with synonyms, related terms, or context to improve retrieval. Helps find relevant documents that use different terminology than the query</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Making queries longer</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Shortening queries</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Query expansion is not useful</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is parent-child chunking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Store small chunks for precise retrieval, but include parent document context when generating answers. Balances retrieval precision with generation context</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Chunking by file hierarchy</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chunking</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No parent-child relationship</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "How do you handle context window limitations in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use context compression (summarize chunks, extract relevant sentences), prioritize most relevant chunks, use parent-child chunking, or implement iterative retrieval for complex queries</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Ignore context limits</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only use first chunks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No solution exists</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is multi-query retrieval?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Generating multiple query variations, retrieving documents for each, and combining results to improve coverage and find more relevant information</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Using multiple databases</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Asking multiple users</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference from single query</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: Interview question: "What is iterative retrieval and when would you use it?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Multi-hop retrieval where if initial answer is incomplete, generate follow-up queries and retrieve more context. Repeat until complete. Useful for complex, multi-part questions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Retrieving once</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Retrieving randomly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Iterative retrieval is never needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: In the context window constraint \(|\text{query}| + |\text{context}| + |\text{answer}| \leq \text{max\_context\_length}\), what does this mean?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Total tokens (query + context + answer) must fit within LLM's context window, limiting how much retrieved context can be included</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only query length matters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Context length is unlimited</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Answer length doesn't count</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "What is Self-RAG and how does it differ from standard RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Self-RAG is an autonomous system where the model decides when to retrieve, when to generate, and when to stop. More flexible than fixed retrieval-then-generate pipeline</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Same as standard RAG</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No retrieval needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only generation, no retrieval</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the answer quality score formula measuring?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Combined score of relevance (addresses query), faithfulness (grounded in context), and completeness (comprehensive answer) with weighting factors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only answer length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only retrieval score</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random score</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How do you implement metadata filtering in RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Store document metadata (date, type, source) with embeddings, filter by metadata before or after similarity search, use vector database metadata filters, and combine with semantic search</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only filter after search</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Metadata is not used</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only use metadata, no search</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is context compression and why is it needed?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Reducing retrieved context size (summarize, extract relevant parts) to fit in LLM context window while preserving important information. Needed when retrieved chunks exceed context limits</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Compressing files</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Reducing embedding size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Context compression is not needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "How would you optimize RAG for better answer quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Improve retrieval (better embeddings, hybrid search, reranking), use query expansion, implement parent-child chunking, add context compression, use iterative retrieval for complex queries, and evaluate with quality metrics</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only use better LLM</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only use more documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No optimization possible</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter5" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 5</a>
                <a href="/tutorials/rag/chapter7" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 7 →</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
