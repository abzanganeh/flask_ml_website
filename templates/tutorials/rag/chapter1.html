<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Introduction to RAG - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 1: Introduction to RAG</h1>
                <p class="chapter-subtitle">Retrieval-Augmented Generation</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="14"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn active">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand introduction to rag fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Introduction to RAG</h2>
                        
                        <div class="explanation-box">
                            <h3>What is RAG?</h3>
                            <p><strong>Retrieval-Augmented Generation (RAG) combines information retrieval with language generation.</strong> Instead of relying solely on the LLM's training data, RAG retrieves relevant information from external knowledge sources and uses it to generate more accurate, up-to-date responses.</p>
                            
                            <p><strong>Think of RAG like a research assistant:</strong></p>
                            <ul>
                                <li><strong>Traditional LLM:</strong> Like answering from memory - might be outdated or incomplete</li>
                                <li><strong>RAG System:</strong> Like a researcher who looks up current information, then answers based on what they found</li>
                                <li><strong>Result:</strong> More accurate, factual, and up-to-date responses</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h4>‚ö†Ô∏è The Problem with LLMs</h4>
                            <p><strong>LLMs have three critical limitations:</strong></p>
                            
                            <div class="example-box">
                                <h5>1. Hallucination</h5>
                                <p><strong>LLMs can generate plausible-sounding but incorrect information:</strong></p>
                                <ul>
                                    <li>Question: "What is the capital of France?"</li>
                                    <li>LLM might say: "The capital of France is Paris" (correct)</li>
                                    <li>But also: "The capital of France is Lyon" (incorrect, but sounds plausible)</li>
                                    <li><strong>Problem:</strong> No way to verify without external knowledge</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. Outdated Information</h5>
                                <p><strong>LLMs are trained on data up to a cutoff date:</strong></p>
                                <ul>
                                    <li>GPT-3.5 trained on data up to September 2021</li>
                                    <li>Cannot know about events after that date</li>
                                    <li>Question: "Who won the 2024 World Cup?" ‚Üí Might not know or hallucinate</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>3. Limited Context Window</h5>
                                <p><strong>LLMs have fixed context limits:</strong></p>
                                <ul>
                                    <li>Cannot store entire knowledge bases in context</li>
                                    <li>Cannot access private/internal documents</li>
                                    <li>Limited to what fits in the prompt</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h4>‚úÖ How RAG Solves These Problems</h4>
                            <p><strong>RAG architecture:</strong></p>
                            <ol>
                                <li><strong>Retrieval:</strong> Search external knowledge base for relevant information</li>
                                <li><strong>Augmentation:</strong> Add retrieved information to the prompt</li>
                                <li><strong>Generation:</strong> LLM generates answer based on retrieved context</li>
                            </ol>
                            
                            <p><strong>Benefits:</strong></p>
                            <ul>
                                <li>‚úÖ Reduces hallucination (grounded in retrieved facts)</li>
                                <li>‚úÖ Provides up-to-date information (can update knowledge base)</li>
                                <li>‚úÖ Accesses private documents (can index internal docs)</li>
                                <li>‚úÖ More transparent (can cite sources)</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h4>üìä RAG Data Flow Diagram</h4>
                            <p>The following diagram shows how data flows through a RAG system, including document indexing and query processing:</p>
                            
                            <div class="rag-flow-diagram">
                                <!-- Document Indexing Phase (Left Side) -->
                                <div class="flow-phase">
                                    <div class="phase-title">üìö Document Indexing Phase (One-time Setup)</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box documents">
                                            <div class="flow-icon">üìÑ</div>
                                            <div class="flow-title">Raw Documents</div>
                                            <div class="flow-content">"France is a country...", "Germany is...", etc.</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box embedding">
                                            <div class="flow-icon">üî¢</div>
                                            <div class="flow-title">Document Embedding</div>
                                            <div class="flow-library">SentenceTransformer</div>
                                            <div class="flow-content">Convert each document to vector: [0.1, -0.3, 0.7, ...]</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box storage">
                                            <div class="flow-icon">üíæ</div>
                                            <div class="flow-title">Store in Vector Database</div>
                                            <div class="flow-library">Pinecone / Weaviate / FAISS</div>
                                            <div class="flow-content">Store document embeddings for fast retrieval</div>
                                        </div>
                                    </div>
                                </div>
                                
                                <!-- Query Processing Phase (Right Side) -->
                                <div class="flow-phase">
                                    <div class="phase-title">üîç Query Processing Phase (Per Query)</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box user-query">
                                            <div class="flow-icon">üë§</div>
                                            <div class="flow-title">User Query</div>
                                            <div class="flow-content">"What is the capital of France?"</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box embedding">
                                            <div class="flow-icon">üî¢</div>
                                            <div class="flow-title">Query Embedding</div>
                                            <div class="flow-library">SentenceTransformer</div>
                                            <div class="flow-content">Convert query to vector: [0.2, -0.5, 0.8, ...]</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box similarity">
                                            <div class="flow-icon">üìê</div>
                                            <div class="flow-title">Cosine Similarity</div>
                                            <div class="flow-library">NumPy</div>
                                            <div class="flow-content">cos(Œ∏) = (q¬∑d) / (||q|| √ó ||d||)<br>Compare query vector with all document vectors</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box retrieval">
                                            <div class="flow-icon">üîç</div>
                                            <div class="flow-title">Top-k Retrieval</div>
                                            <div class="flow-library">NumPy argsort / Vector DB</div>
                                            <div class="flow-content">Select top 3-5 documents with highest similarity scores</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box context">
                                            <div class="flow-icon">üìù</div>
                                            <div class="flow-title">Context Assembly</div>
                                            <div class="flow-library">Python String</div>
                                            <div class="flow-content">Build prompt: Context + Question</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box generation">
                                            <div class="flow-icon">ü§ñ</div>
                                            <div class="flow-title">LLM Generation</div>
                                            <div class="flow-library">OpenAI API / Transformers</div>
                                            <div class="flow-content">Generate answer based on context</div>
                                        </div>
                                    </div>
                                    
                                    <div class="flow-arrow">‚Üì</div>
                                    
                                    <div class="flow-step">
                                        <div class="flow-box answer">
                                            <div class="flow-icon">‚úÖ</div>
                                            <div class="flow-title">Final Answer</div>
                                            <div class="flow-content">"The capital of France is Paris."</div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="flow-legend">
                                <h5>Key Libraries & Components:</h5>
                                <ul>
                                    <li><strong>SentenceTransformer:</strong> Converts text (queries and documents) into dense vector embeddings. Used in both indexing (documents) and query processing phases.</li>
                                    <li><strong>Vector Database:</strong> Pinecone, Weaviate, Chroma, or FAISS stores document embeddings for fast similarity search. Documents are embedded once during indexing.</li>
                                    <li><strong>NumPy:</strong> Computes cosine similarity using the formula: cos(Œ∏) = (q¬∑d) / (||q|| √ó ||d||). Compares query embedding with all stored document embeddings.</li>
                                    <li><strong>NumPy argsort:</strong> Sorts similarity scores to find top-k documents with highest cosine similarity values.</li>
                                    <li><strong>OpenAI API / Transformers:</strong> Language model for generating answers based on retrieved context</li>
                                    <li><strong>Python String Operations:</strong> Assembles the final prompt with context and question</li>
                                </ul>
                                
                                <h5>Key Process Steps:</h5>
                                <ol>
                                    <li><strong>Document Indexing (One-time):</strong> All documents are embedded using SentenceTransformer and stored in a vector database. This happens once when building the knowledge base.</li>
                                    <li><strong>Query Processing (Per Query):</strong> Each user query is embedded, then cosine similarity is computed against all stored document embeddings to find the most relevant documents.</li>
                                    <li><strong>Retrieval:</strong> Top-k documents with highest similarity scores are retrieved and used as context.</li>
                                    <li><strong>Generation:</strong> LLM generates the final answer using the retrieved context.</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>RAG Architecture</h3>
                            <p><strong>Components:</strong></p>
                            <ul>
                                <li><strong>Knowledge Base:</strong> Collection of documents (vector database)</li>
                                <li><strong>Retriever:</strong> Finds relevant documents for query</li>
                                <li><strong>LLM:</strong> Generates answer using retrieved context</li>
                            </ul>
                            
                            <p><strong>Process:</strong></p>
                            <ol>
                                <li>User asks question</li>
                                <li>Retriever searches knowledge base</li>
                                <li>Top-k relevant documents retrieved</li>
                                <li>Documents added to LLM prompt as context</li>
                                <li>LLM generates answer based on context</li>
                            </ol>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>RAG vs Fine-tuning</h3>
                            <p><strong>RAG advantages:</strong></p>
                            <ul>
                                <li>No training required</li>
                                <li>Easy to update knowledge (just add documents)</li>
                                <li>Can cite sources</li>
                                <li>Works with any LLM</li>
                            </ul>
                            
                            <p><strong>Fine-tuning advantages:</strong></p>
                            <ul>
                                <li>Better for learning specific patterns</li>
                                <li>No retrieval latency</li>
                                <li>More consistent behavior</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>RAG Generation Process</h4>
                            <div class="formula-display">
                                \[P(y | q) = P(y | q, \text{Retrieve}(q, D))\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Means:</h5>
                                <p>This formula represents the core principle of RAG: the probability of generating answer \(y\) given query \(q\) is equal to the probability of generating \(y\) given both the query \(q\) and the retrieved documents from the knowledge base.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(P(y | q)\)</strong>: Traditional LLM approach - probability of answer \(y\) given only the query \(q\). This relies solely on the model's training data.</li>
                                    <li><strong>\(P(y | q, \text{Retrieve}(q, D))\)</strong>: RAG approach - probability of answer \(y\) given both the query \(q\) AND the retrieved context from knowledge base \(D\).</li>
                                    <li><strong>\(\text{Retrieve}(q, D)\)</strong>: Function that searches knowledge base \(D\) and returns the most relevant documents for query \(q\).</li>
                                </ul>
                                
                                <h5>Key Insight:</h5>
                                <p>The formula shows that RAG <strong>augments</strong> the generation process by conditioning on retrieved documents. Instead of generating from memory alone, the LLM generates based on both the query and the retrieved factual context, leading to more accurate and up-to-date responses.</p>
                                
                                <h5>Example:</h5>
                                <p>If a user asks "What happened in Q4 2024?", the traditional model might say "I don't have information about that" (because its training data cuts off earlier). But with RAG, \(\text{Retrieve}(q, D)\) finds the Q4 2024 report in the knowledge base, and the model generates an answer based on that actual document.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Retrieval Score (Cosine Similarity)</h4>
                            <div class="formula-display">
                                \[\text{score}(q, d) = \frac{q \cdot d}{\|q\| \|d\|} = \cos(\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>Cosine similarity measures how similar two vectors are in direction, regardless of their magnitude. It's the cosine of the angle \(\theta\) between the query embedding vector \(q\) and document embedding vector \(d\).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(q \cdot d\)</strong>: Dot product of query and document vectors. Measures how much the vectors point in the same direction.</li>
                                    <li><strong>\(\|q\|\)</strong>: Magnitude (length) of query vector = \(\sqrt{q_1^2 + q_2^2 + \ldots + q_n^2}\)</li>
                                    <li><strong>\(\|d\|\)</strong>: Magnitude (length) of document vector</li>
                                    <li><strong>\(\frac{q \cdot d}{\|q\| \|d\|}\)</strong>: Normalizes the dot product by dividing by the product of magnitudes, giving us the cosine of the angle.</li>
                                    <li><strong>\(\cos(\theta)\)</strong>: The cosine of the angle between vectors. When vectors point in the same direction, \(\theta = 0¬∞\) and \(\cos(0¬∞) = 1\) (maximum similarity).</li>
                                </ul>
                                
                                <h5>Why Cosine Similarity?</h5>
                                <ul>
                                    <li><strong>Range:</strong> Values range from -1 to 1, but for normalized embeddings (common in RAG), values are typically between 0 and 1.</li>
                                    <li><strong>Scale-invariant:</strong> Only cares about direction, not magnitude. A document about "machine learning" will have high similarity to a query about "ML" even if one is longer.</li>
                                    <li><strong>Semantic meaning:</strong> Embeddings capture semantic meaning, so similar meanings = similar directions = high cosine similarity.</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is artificial intelligence?"<br>
                                Document 1: "AI is the simulation of human intelligence by machines" ‚Üí High similarity (0.92)<br>
                                Document 2: "The weather today is sunny" ‚Üí Low similarity (0.15)<br>
                                The retrieval system ranks Document 1 higher because its embedding vector points in a similar direction to the query embedding.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Top-k Retrieval</h4>
                            <div class="formula-display">
                                \[D_{\text{retrieved}} = \text{argmax}_k \{\text{score}(q, d) : d \in D\}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Does:</h5>
                                <p>This formula selects the top \(k\) documents from knowledge base \(D\) that have the highest similarity scores with query \(q\). The \(\text{argmax}_k\) function finds the \(k\) documents that maximize the score function.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(D\)</strong>: The entire knowledge base (all documents available for retrieval)</li>
                                    <li><strong>\(d \in D\)</strong>: Each document \(d\) in the knowledge base \(D\)</li>
                                    <li><strong>\(\text{score}(q, d)\)</strong>: Similarity score between query \(q\) and document \(d\) (typically cosine similarity)</li>
                                    <li><strong>\(\text{argmax}_k\)</strong>: Returns the \(k\) documents with the highest scores (not just the maximum, but the top \(k\))</li>
                                    <li><strong>\(D_{\text{retrieved}}\)</strong>: The final set of \(k\) documents selected for context</li>
                                </ul>
                                
                                <h5>Why Top-k Instead of Just the Best?</h5>
                                <ul>
                                    <li><strong>Context completeness:</strong> A single document might not contain all relevant information. Multiple documents provide richer context.</li>
                                    <li><strong>Redundancy:</strong> Multiple sources can confirm information, reducing hallucination risk.</li>
                                    <li><strong>Coverage:</strong> Different documents might cover different aspects of the query.</li>
                                </ul>
                                
                                <h5>Choosing k:</h5>
                                <ul>
                                    <li><strong>Small k (3-5):</strong> Faster, lower cost, but might miss relevant information. Good for simple queries.</li>
                                    <li><strong>Medium k (5-10):</strong> Balanced approach, most common in production RAG systems.</li>
                                    <li><strong>Large k (10+):</strong> More comprehensive but increases latency, cost, and may include irrelevant documents that confuse the LLM.</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "How does RAG work?"<br>
                                Knowledge base has 1000 documents. The system calculates similarity scores for all 1000, then selects the top 5 documents with scores: [0.95, 0.92, 0.89, 0.87, 0.85]. These 5 documents are passed to the LLM as context for generating the answer.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: RAG Question Answering</h4>
                            <p><strong>User query:</strong> "What is the capital of France?"</p>
                            
                            <p><strong>Step 1: Query Embedding</strong></p>
                            <ul>
                                <li>Convert query to embedding vector: [0.2, -0.5, 0.8, ...]</li>
                            </ul>
                            
                            <p><strong>Step 2: Retrieval</strong></p>
                            <ul>
                                <li>Search knowledge base for similar embeddings</li>
                                <li>Find document: "France is a country in Europe. Its capital is Paris."</li>
                                <li>Similarity score: 0.92</li>
                            </ul>
                            
                            <p><strong>Step 3: Context Augmentation</strong></p>
                            <ul>
                                <li>Build prompt: "Context: France is a country in Europe. Its capital is Paris. Question: What is the capital of France?"</li>
                            </ul>
                            
                            <p><strong>Step 4: Generation</strong></p>
                            <ul>
                                <li>LLM generates: "The capital of France is Paris."</li>
                                <li>Answer is grounded in retrieved context</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Without RAG vs With RAG</h4>
                            <p><strong>Query:</strong> "What happened in the company Q4 2024 earnings?"</p>
                            
                            <p><strong>Without RAG:</strong></p>
                            <ul>
                                <li>LLM only knows training data (cutoff date)</li>
                                <li>May hallucinate or say "I don't have information about that"</li>
                            </ul>
                            
                            <p><strong>With RAG:</strong></p>
                            <ul>
                                <li>Retrieves Q4 2024 earnings report from knowledge base</li>
                                <li>LLM generates answer based on actual report</li>
                                <li>Accurate, up-to-date information</li>
                            </ul>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Simple RAG Implementation</h4>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np
from openai import OpenAI  # Or use transformers pipeline for local models

class SimpleRAG:
    """
    Basic RAG implementation with proper cosine similarity calculation.
    
    This class implements the core RAG pipeline:
    1. Document embedding and storage
    2. Query embedding and retrieval
    3. Context augmentation and generation
    """
    
    def __init__(self, embedding_model='all-MiniLM-L6-v2'):
        """
        Initialize RAG system with embedding model.
        
        Args:
            embedding_model: Sentence transformer model name
        """
        self.embedder = SentenceTransformer(embedding_model)
        # For production, use OpenAI API or better local models
        # self.client = OpenAI(api_key="your-api-key")
        self.documents = []
        self.embeddings = None
    
    def add_documents(self, docs):
        """
        Add documents to knowledge base and compute embeddings.
        
        Args:
            docs: List of document strings to add
        """
        if not docs:
            raise ValueError("Documents list cannot be empty")
        
        self.documents = docs
        # Encode all documents into embeddings (vectors)
        self.embeddings = self.embedder.encode(docs, show_progress_bar=False)
        print(f"Added {len(docs)} documents to knowledge base")
    
    def retrieve(self, query, top_k=3):
        """
        Retrieve top-k most relevant documents using cosine similarity.
        
        Args:
            query: User query string
            top_k: Number of documents to retrieve
            
        Returns:
            List of top-k most relevant document strings
        """
        if self.embeddings is None or len(self.documents) == 0:
            raise ValueError("No documents in knowledge base. Call add_documents() first.")
        
        # Encode query into embedding vector
        query_embedding = self.embedder.encode([query])
        
        # Compute cosine similarity: (q ¬∑ d) / (||q|| * ||d||)
        # Step 1: Compute dot product between query and all document embeddings
        dot_products = np.dot(self.embeddings, query_embedding.T).flatten()
        
        # Step 2: Compute norms (magnitudes) of embeddings
        query_norm = np.linalg.norm(query_embedding)
        doc_norms = np.linalg.norm(self.embeddings, axis=1)
        
        # Step 3: Normalize to get cosine similarity (range: -1 to 1, typically 0 to 1)
        cosine_similarities = dot_products / (query_norm * doc_norms)
        
        # Step 4: Get indices of top-k documents with highest similarity
        top_indices = np.argsort(cosine_similarities)[-top_k:][::-1]
        
        # Return the actual documents (not just indices)
        retrieved_docs = [self.documents[i] for i in top_indices]
        
        print(f"Retrieved {len(retrieved_docs)} documents with similarities: {cosine_similarities[top_indices]}")
        return retrieved_docs
    
    def generate(self, query, top_k=3, use_openai=False):
        """
        Generate answer using RAG pipeline.
        
        Args:
            query: User query
            top_k: Number of documents to retrieve
            use_openai: Whether to use OpenAI API (requires API key)
            
        Returns:
            Generated answer string
        """
        # Step 1: Retrieve relevant documents
        context_docs = self.retrieve(query, top_k)
        
        # Step 2: Build prompt with retrieved context
        context = "\n\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(context_docs)])
        prompt = f"""Based on the following context documents, answer the question.

Context:
{context}

Question: {query}

Answer:"""
        
        # Step 3: Generate answer using LLM
        if use_openai:
            # Using OpenAI API (recommended for production)
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )
            return response.choices[0].message.content
        else:
            # Using local model (for demonstration)
            # Note: GPT-2 is not ideal for Q&A. Use better models in production.
            from transformers import pipeline
            llm = pipeline("text-generation", model="gpt2")
            result = llm(prompt, max_length=len(prompt.split()) + 100, num_return_sequences=1)
            # Extract only the generated part (after the prompt)
            generated_text = result[0]['generated_text']
            answer = generated_text[len(prompt):].strip()
            return answer

# Example usage
if __name__ == "__main__":
    # Initialize RAG system
    rag = SimpleRAG()
    
    # Add documents to knowledge base
    rag.add_documents([
        "France is a country in Europe. Its capital is Paris, a major cultural and economic center.",
        "Germany is a country in Central Europe. Its capital is Berlin, known for its history and technology sector.",
        "Italy is a country in Southern Europe. Its capital is Rome, famous for its ancient history and art."
    ])
    
    # Ask a question
    query = "What is the capital of France?"
    answer = rag.generate(query, top_k=2)
    print(f"\nQuestion: {query}")
    print(f"Answer: {answer}")</code></pre>
                            
                            <div class="code-explanation">
                                <h5>Key Implementation Details:</h5>
                                <ul>
                                    <li><strong>Cosine Similarity Calculation:</strong> Properly normalized by dividing dot product by the product of vector norms. This ensures values are in the range [-1, 1] and represent true cosine similarity.</li>
                                    <li><strong>Document Embedding:</strong> All documents are embedded once when added to the knowledge base, making retrieval fast.</li>
                                    <li><strong>Top-k Retrieval:</strong> Uses <code>np.argsort</code> to efficiently find the k documents with highest similarity scores.</li>
                                    <li><strong>Prompt Engineering:</strong> Structured prompt that clearly separates context from question, helping the LLM understand what to answer.</li>
                                    <li><strong>Error Handling:</strong> Checks for empty documents and missing knowledge base before operations.</li>
                                </ul>
                                
                                <h5>Production Considerations:</h5>
                                <ul>
                                    <li>Use OpenAI API, Anthropic Claude, or better local models (like Llama 2) instead of GPT-2 for better Q&A performance.</li>
                                    <li>Implement vector database (Pinecone, Weaviate) for large-scale document storage.</li>
                                    <li>Add caching for frequently asked queries.</li>
                                    <li>Implement proper logging and monitoring.</li>
                                    <li>Add source citation to show which documents were used.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="example-box">
                            <h4>1. Customer Support Chatbots - Real-World Example</h4>
                            <p><strong>Company:</strong> Major e-commerce platform (similar to Amazon, Shopify)</p>
                            
                            <p><strong>Problem:</strong> Customer support receives thousands of questions daily about product features, shipping policies, returns, and technical issues. Traditional chatbots with fixed responses couldn't handle the variety of questions, and human agents were overwhelmed.</p>
                            
                            <p><strong>RAG Solution:</strong></p>
                            <ul>
                                <li><strong>Knowledge Base:</strong> Indexed all product documentation, FAQ pages, shipping policies, return procedures, and troubleshooting guides (50,000+ documents)</li>
                                <li><strong>Implementation:</strong> Built RAG system that retrieves relevant documentation for each customer query and generates accurate, up-to-date answers</li>
                                <li><strong>Results:</strong>
                                    <ul>
                                        <li>‚úÖ 70% of customer queries resolved automatically (up from 30% with rule-based chatbot)</li>
                                        <li>‚úÖ Response time reduced from 5 minutes (human agent) to 10 seconds (RAG chatbot)</li>
                                        <li>‚úÖ Customer satisfaction increased by 25%</li>
                                        <li>‚úÖ Support costs reduced by 40%</li>
                                    </ul>
                                </li>
                            </ul>
                            
                            <p><strong>Example Interaction:</strong></p>
                            <p><strong>Customer:</strong> "How do I return an item I ordered last week?"</p>
                            <p><strong>RAG System:</strong> Retrieves return policy document, shipping policy, and order management guide. Generates answer:</p>
                            <p><em>"You can return items within 30 days of purchase. To initiate a return: 1) Go to 'My Orders' in your account, 2) Select the item and click 'Return', 3) Print the prepaid return label, 4) Package the item and drop it off at any carrier location. Refunds are processed within 5-7 business days after we receive the item. [Source: Return Policy v2.3, Shipping Guide 2024]"</em></p>
                            
                            <p><strong>Why RAG Works Here:</strong> Policies change frequently, products are added daily, and customers ask questions in many different ways. RAG can handle all of this without retraining the model.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>2. Enterprise Knowledge Base - Real-World Example</h4>
                            <p><strong>Company:</strong> Large technology company (similar to Microsoft, Google)</p>
                            
                            <p><strong>Problem:</strong> Company has 10,000+ employees across multiple departments. Internal documentation includes technical specs, API documentation, HR policies, project documentation, and meeting notes. Employees spent hours searching through documents to find information, and often couldn't find what they needed.</p>
                            
                            <p><strong>RAG Solution:</strong></p>
                            <ul>
                                <li><strong>Knowledge Base:</strong> Indexed all internal documentation (200,000+ documents) including:
                                    <ul>
                                        <li>Technical documentation and API references</li>
                                        <li>HR policies and employee handbooks</li>
                                        <li>Project documentation and meeting notes</li>
                                        <li>Code documentation and architecture decisions</li>
                                    </ul>
                                </li>
                                <li><strong>Implementation:</strong> Built internal RAG-powered search system accessible via Slack, Teams, and web interface</li>
                                <li><strong>Results:</strong>
                                    <ul>
                                        <li>‚úÖ 60% reduction in time spent searching for information</li>
                                        <li>‚úÖ 80% of queries successfully answered (vs. 40% with traditional search)</li>
                                        <li>‚úÖ Knowledge sharing improved - employees discover relevant documentation they didn't know existed</li>
                                        <li>‚úÖ Onboarding time for new employees reduced by 30%</li>
                                    </ul>
                                </li>
                            </ul>
                            
                            <p><strong>Example Interaction:</strong></p>
                            <p><strong>Employee:</strong> "What's the process for deploying to production?"</p>
                            <p><strong>RAG System:</strong> Retrieves deployment guide, CI/CD documentation, and recent deployment notes. Generates comprehensive answer with step-by-step process, links to relevant documentation, and recent changes.</p>
                            
                            <p><strong>Why RAG Works Here:</strong> Documentation is constantly updated, spread across many systems, and employees need to find information quickly. RAG understands semantic meaning, so it finds relevant docs even when exact keywords don't match.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>3. Legal Document Analysis - Real-World Example</h4>
                            <p><strong>Company:</strong> Law firm specializing in contract review</p>
                            
                            <p><strong>Problem:</strong> Lawyers spend hours reviewing contracts, legal documents, and case files to answer client questions. Each contract review takes 4-6 hours, and lawyers need to reference previous cases, legal precedents, and regulatory documents.</p>
                            
                            <p><strong>RAG Solution:</strong></p>
                            <ul>
                                <li><strong>Knowledge Base:</strong> Indexed:
                                    <ul>
                                        <li>All previous contracts and case files (100,000+ documents)</li>
                                        <li>Legal precedents and case law</li>
                                        <li>Regulatory documents and compliance guidelines</li>
                                        <li>Client-specific documentation</li>
                                    </ul>
                                </li>
                                <li><strong>Implementation:</strong> RAG system that helps lawyers quickly find relevant precedents, similar contracts, and regulatory requirements</li>
                                <li><strong>Results:</strong>
                                    <ul>
                                        <li>‚úÖ Contract review time reduced from 4-6 hours to 1-2 hours</li>
                                        <li>‚úÖ 90% accuracy in finding relevant precedents (vs. 60% with manual search)</li>
                                        <li>‚úÖ Lawyers can handle 2x more cases</li>
                                        <li>‚úÖ Better consistency - all lawyers have access to same knowledge base</li>
                                    </ul>
                                </li>
                            </ul>
                            
                            <p><strong>Example Interaction:</strong></p>
                            <p><strong>Lawyer:</strong> "What are the standard terms for data privacy clauses in software licensing agreements?"</p>
                            <p><strong>RAG System:</strong> Retrieves relevant contracts, GDPR compliance documents, and previous case analyses. Generates answer with common terms, variations, and relevant precedents, all with source citations.</p>
                            
                            <p><strong>Why RAG Works Here:</strong> Legal documents use precise terminology, but questions can be phrased in many ways. RAG understands semantic relationships and can find relevant documents even when exact phrases don't match. Source attribution is critical for legal work, which RAG provides.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>4. Medical Information System - Real-World Example</h4>
                            <p><strong>Organization:</strong> Hospital system with multiple facilities</p>
                            
                            <p><strong>Problem:</strong> Doctors and medical staff need quick access to medical guidelines, drug information, treatment protocols, and research papers. Medical information changes frequently, and staff need the most up-to-date information to make critical decisions.</p>
                            
                            <p><strong>RAG Solution:</strong></p>
                            <ul>
                                <li><strong>Knowledge Base:</strong> Indexed:
                                    <ul>
                                        <li>Medical guidelines and treatment protocols</li>
                                        <li>Drug databases and interaction information</li>
                                        <li>Recent research papers and clinical trials</li>
                                        <li>Hospital-specific procedures and policies</li>
                                    </ul>
                                </li>
                                <li><strong>Implementation:</strong> RAG-powered medical assistant accessible via tablets and computers in patient rooms</li>
                                <li><strong>Results:</strong>
                                    <ul>
                                        <li>‚úÖ Doctors get answers in 10-15 seconds (vs. 5-10 minutes searching databases)</li>
                                        <li>‚úÖ Always up-to-date - new research and guidelines added immediately</li>
                                        <li>‚úÖ Better patient care - doctors have access to latest treatment options</li>
                                        <li>‚úÖ Reduced medical errors - system provides evidence-based recommendations with citations</li>
                                    </ul>
                                </li>
                            </ul>
                            
                            <p><strong>Example Interaction:</strong></p>
                            <p><strong>Doctor:</strong> "What are the latest treatment options for Type 2 diabetes in elderly patients with kidney complications?"</p>
                            <p><strong>RAG System:</strong> Retrieves latest diabetes treatment guidelines, kidney disease management protocols, and recent research on elderly patients. Generates comprehensive answer with treatment options, dosages, contraindications, and links to full guidelines.</p>
                            
                            <p><strong>Why RAG Works Here:</strong> Medical information is constantly evolving. New research, updated guidelines, and drug approvals happen frequently. RAG allows the system to stay current without retraining. Source citations are essential for medical decisions, which RAG provides.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Key Advantages of RAG in Production</h3>
                            <p><strong>1. Up-to-Date Information:</strong> Unlike traditional LLMs with fixed training data, RAG systems can be updated instantly by adding new documents. This is critical for industries where information changes rapidly (legal, medical, technology).</p>
                            
                            <p><strong>2. Source Attribution:</strong> RAG systems can cite the exact documents used to generate answers. This is essential for:
                                <ul>
                                    <li>Legal work (need to cite precedents)</li>
                                    <li>Medical decisions (need evidence-based sources)</li>
                                    <li>Enterprise knowledge (need to verify information)</li>
                                    <li>Customer support (need to reference policy documents)</li>
                                </ul>
                            </p>
                            
                            <p><strong>3. Reduced Hallucination:</strong> By grounding answers in retrieved documents, RAG systems are much less likely to "make up" information. This is critical for applications where accuracy is essential (medical, legal, financial).</p>
                            
                            <p><strong>4. Domain-Specific Knowledge:</strong> RAG systems can work with any domain-specific knowledge base without fine-tuning the LLM. This makes it cost-effective and fast to deploy for specialized use cases.</p>
                            
                            <p><strong>5. Easy Updates:</strong> Adding new information is as simple as adding documents to the knowledge base. No model retraining required, making it practical for production systems that need to stay current.</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is RAG (Retrieval-Augmented Generation)?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A system that combines information retrieval with language generation, retrieving relevant information from external sources before generating responses</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A method to train LLMs faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) A type of neural network architecture</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A database system</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What are the main limitations of traditional LLMs that RAG addresses?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Speed and cost only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Hallucination, outdated information, lack of source attribution, and knowledge cutoff dates</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Training time</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: In the RAG formula \(P(y \mid q) = P(y \mid q, \text{Retrieve}(q, D))\), what does \(\text{Retrieve}(q, D)\) represent?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) The generated answer</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The query embedding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) The retrieved relevant documents from knowledge base D for query q</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The LLM model</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "What are the key advantages of RAG over fine-tuning?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Faster inference</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Smaller model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Lower cost</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) No training required, easy to update knowledge by adding documents, can cite sources, works with any LLM, and no retraining needed for new information</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is cosine similarity used for in RAG?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Training the LLM</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Generating embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Chunking documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Measuring semantic similarity between query and document embeddings to find relevant documents</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What does top-k retrieval mean in RAG?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Using k different models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Selecting the k documents with highest similarity scores (typically k=3-10)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Training k times</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Processing k queries</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Interview question: "When would you choose RAG over fine-tuning?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Always choose RAG</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always choose fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) When you need up-to-date information, want source attribution, have frequently changing knowledge, or need to work with domain-specific documents without retraining</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) When you need faster inference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is the typical RAG pipeline flow?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Query ‚Üí LLM ‚Üí Response</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Documents ‚Üí Training ‚Üí Model</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Query ‚Üí Database ‚Üí Response</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Query ‚Üí Embedding ‚Üí Retrieval ‚Üí Context Assembly ‚Üí LLM Generation ‚Üí Response</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the main difference between RAG and traditional LLM responses?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) RAG is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) RAG uses smaller models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) RAG retrieves relevant context from external knowledge sources before generating, while traditional LLMs only use training data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) There's no difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How does RAG reduce hallucination?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) By using smaller models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) By training more</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) By using faster inference</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) By grounding responses in retrieved documents from the knowledge base, providing factual context that constrains the LLM's generation</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the cosine similarity formula \(\text{score}(q, d) = \frac{q \cdot d}{\|q\| \|d\|}\) measuring?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Euclidean distance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Dot product only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Vector magnitude</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) The cosine of the angle between query and document vectors, indicating semantic similarity (range: -1 to 1, higher = more similar)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "What are the key components of a RAG system?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Just an LLM</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Just a database</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) Document knowledge base, embedding model, vector database, retrieval mechanism, LLM for generation, and context assembly logic</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Just embeddings</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/rag/chapter2" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 2 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
