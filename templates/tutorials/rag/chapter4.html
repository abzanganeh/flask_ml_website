<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: Vector Databases - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 4: Vector Databases</h1>
                <p class="chapter-subtitle">Storing and Searching Embeddings</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="57"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn active">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand vector databases fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Vector Databases</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Vector Databases are Essential for RAG</h3>
                            <p>Once you've created embeddings for your documents, you need to store them somewhere and search through them efficiently. A traditional database (like PostgreSQL or MySQL) is designed for exact matches and structured queries, not for finding "similar" vectors. <strong>Vector databases</strong> are specialized databases optimized for storing and searching high-dimensional vectors using similarity metrics like cosine similarity.</p>
                            
                            <p><strong>The scale problem:</strong> In production RAG systems, you might have millions or billions of document chunks, each with a 384-1536 dimensional embedding vector. Searching through all of them to find the most similar to a query vector would take hours using brute-force methods. Vector databases use sophisticated indexing algorithms (like HNSW, IVF, or LSH) to enable <strong>sub-millisecond</strong> similarity search even across billions of vectors.</p>
                            
                            <h4>What Vector Databases Provide</h4>
                            <ul>
                                <li><strong>Fast Similarity Search:</strong> Find top-k most similar vectors in milliseconds, even with millions of documents</li>
                                <li><strong>Scalable Storage:</strong> Efficiently store and index billions of high-dimensional vectors</li>
                                <li><strong>Metadata Filtering:</strong> Combine vector similarity search with traditional filters (date, category, author, etc.)</li>
                                <li><strong>Real-time Updates:</strong> Add, update, or delete vectors without rebuilding entire indexes</li>
                                <li><strong>Approximate Nearest Neighbor (ANN):</strong> Trade some accuracy for massive speed improvements (1000-10000x faster than exact search)</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Performance Comparison:</h5>
                                <p><strong>Brute-force search (NumPy):</strong> To find top-5 most similar vectors among 1 million documents:</p>
                                <ul>
                                    <li>❌ Compute 1 million cosine similarities: ~16 minutes</li>
                                    <li>❌ Sort results: Additional time</li>
                                    <li>❌ Total: ~16+ minutes per query (completely impractical)</li>
                                </ul>
                                <p><strong>Vector database (HNSW index):</strong> Same task:</p>
                                <ul>
                                    <li>✅ Find top-5 similar vectors: 50-200 milliseconds</li>
                                    <li>✅ Speedup: ~5,000-20,000x faster!</li>
                                    <li>✅ Enables real-time RAG systems</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Indexing Algorithms:</strong> HNSW (Hierarchical Navigable Small World), IVF (Inverted File Index), Product Quantization, and LSH - how they work and when to use each</li>
                                <li><strong>Vector Database Options:</strong> Pinecone, Weaviate, Chroma, Qdrant, FAISS, Milvus - comparing features, performance, and use cases</li>
                                <li><strong>Metadata Filtering:</strong> Combining vector similarity search with traditional database filters for precise retrieval</li>
                                <li><strong>Exact vs Approximate Search:</strong> Understanding the trade-offs between accuracy and speed</li>
                                <li><strong>Scaling Strategies:</strong> Sharding, distributed indexing, and optimization techniques for billion-scale systems</li>
                                <li><strong>Production Considerations:</strong> Update strategies, index maintenance, and performance monitoring</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> Vector databases are the infrastructure that makes RAG systems practical at scale. Without them, you're limited to small document collections or unacceptably slow query times. Choosing the right vector database and indexing strategy directly impacts your RAG system's performance, cost, and scalability.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Database Indexing Strategies: How Fast Retrieval Works</h3>
                            
                            <p>Vector databases use sophisticated indexing algorithms to enable fast similarity search across millions of vectors. Understanding these indexing strategies is crucial for choosing the right database and optimizing performance.</p>
                            
                            <h4>1. HNSW (Hierarchical Navigable Small World)</h4>
                            <p><strong>What it is:</strong> HNSW is one of the most popular and effective indexing algorithms for approximate nearest neighbor (ANN) search. It builds a multi-layer graph where each layer is a subset of the previous layer, creating a "small world" network that allows efficient navigation.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li><strong>Multi-layer structure:</strong> Creates multiple layers (levels) of graphs, with the top layer having few nodes and bottom layer having all nodes</li>
                                <li><strong>Greedy search:</strong> Starts at the top layer, finds the nearest neighbor, then moves to the next layer and continues</li>
                                <li><strong>Small world property:</strong> Each node is connected to a small number of "long-range" connections, allowing fast navigation across the graph</li>
                                <li><strong>Dynamic insertion:</strong> New vectors can be added without rebuilding the entire index</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Very fast:</strong> Sub-millisecond search times even for millions of vectors</li>
                                <li>✅ <strong>High accuracy:</strong> Can achieve 95%+ recall (finds 95% of true nearest neighbors)</li>
                                <li>✅ <strong>Scalable:</strong> Works well with billions of vectors</li>
                                <li>✅ <strong>Used by major databases:</strong> Pinecone, Weaviate, Qdrant all use HNSW variants</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong></p>
                            <ul>
                                <li>⚠️ <strong>Memory intensive:</strong> Stores the graph structure in memory (though can be optimized)</li>
                                <li>⚠️ <strong>Indexing time:</strong> Building the index takes time, though it's a one-time cost</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For production RAG systems where speed and accuracy are critical. This is the default choice for most vector databases.</p>
                            
                            <h4>2. IVF (Inverted File Index)</h4>
                            <p><strong>What it is:</strong> IVF partitions the vector space into clusters (Voronoi cells) and creates an inverted index mapping each cluster to its vectors.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li><strong>Clustering:</strong> Uses k-means or similar to partition vectors into clusters</li>
                                <li><strong>Inverted index:</strong> For each cluster, stores a list of vectors belonging to that cluster</li>
                                <li><strong>Search:</strong> Finds the nearest cluster(s) to the query vector, then searches only within those clusters</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Memory efficient:</strong> Lower memory footprint than HNSW</li>
                                <li>✅ <strong>Fast for large datasets:</strong> Only searches relevant clusters, not all vectors</li>
                                <li>✅ <strong>Used by FAISS:</strong> FAISS's IVF-Flat and IVF-PQ use this approach</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong></p>
                            <ul>
                                <li>⚠️ <strong>Lower accuracy:</strong> May miss vectors near cluster boundaries</li>
                                <li>⚠️ <strong>Requires tuning:</strong> Number of clusters needs careful selection</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For very large datasets (billions of vectors) where memory is a constraint, or when using FAISS.</p>
                            
                            <h4>3. Product Quantization (PQ)</h4>
                            <p><strong>What it is:</strong> A compression technique that reduces vector storage by quantizing (discretizing) vector components into a smaller number of values.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li><strong>Vector splitting:</strong> Splits each vector into multiple sub-vectors</li>
                                <li><strong>Quantization:</strong> Each sub-vector is mapped to a "codebook" (set of representative vectors)</li>
                                <li><strong>Compression:</strong> Stores only the codebook indices, not full vectors</li>
                                <li><strong>Fast distance:</strong> Uses lookup tables for fast approximate distance calculations</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Massive storage reduction:</strong> Can reduce storage by 10-100x</li>
                                <li>✅ <strong>Fast search:</strong> Approximate distances computed quickly using lookup tables</li>
                                <li>✅ <strong>Enables billion-scale:</strong> Makes it feasible to store billions of vectors</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong></p>
                            <ul>
                                <li>⚠️ <strong>Accuracy loss:</strong> Compression introduces approximation errors</li>
                                <li>⚠️ <strong>Training required:</strong> Codebooks need to be trained on representative data</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For extremely large datasets where storage is a primary concern. Often combined with IVF (IVF-PQ).</p>
                            
                            <h4>4. LSH (Locality-Sensitive Hashing)</h4>
                            <p><strong>What it is:</strong> Uses hash functions that map similar vectors to the same hash buckets, enabling fast approximate search.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li><strong>Hash functions:</strong> Creates multiple hash functions that preserve similarity (similar vectors hash to same bucket)</li>
                                <li><strong>Bucketing:</strong> Vectors are placed into hash buckets</li>
                                <li><strong>Search:</strong> Query vector is hashed, then only vectors in the same bucket(s) are searched</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Very fast:</strong> Constant-time hash lookup</li>
                                <li>✅ <strong>Simple:</strong> Easy to understand and implement</li>
                            </ul>
                            
                            <p><strong>Trade-offs:</strong></p>
                            <ul>
                                <li>⚠️ <strong>Lower accuracy:</strong> May miss some similar vectors</li>
                                <li>⚠️ <strong>Parameter tuning:</strong> Number of hash functions and buckets needs tuning</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For very fast, approximate search when some accuracy loss is acceptable. Less common in modern RAG systems.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Metadata Filtering: Combining Vector Search with Traditional Filters</h3>
                            
                            <p>Real-world RAG systems often need to filter documents by metadata (date, author, category, etc.) in addition to semantic similarity. Vector databases support this through <strong>metadata filtering</strong>.</p>
                            
                            <h4>How Metadata Filtering Works</h4>
                            <p><strong>Two-stage process:</strong></p>
                            <ol>
                                <li><strong>Filter first:</strong> Apply metadata filters to reduce the search space (e.g., "only documents from 2023")</li>
                                <li><strong>Search in filtered set:</strong> Perform vector similarity search only on the filtered documents</li>
                            </ol>
                            
                            <p><strong>Example:</strong></p>
                            <div class="example-box">
                                <h5>Query with Metadata Filtering</h5>
                                <p><strong>Query:</strong> "machine learning best practices"</p>
                                <p><strong>Metadata filters:</strong> 
                                    <ul>
                                        <li>Category = "Technical Blog"</li>
                                        <li>Date >= "2023-01-01"</li>
                                        <li>Author = "John Doe"</li>
                                    </ul>
                                </p>
                                <p><strong>Process:</strong></p>
                                <ol>
                                    <li>Filter 1M documents → 50K documents matching metadata</li>
                                    <li>Vector search in 50K documents → Top 5 most similar</li>
                                </ol>
                                <p>✅ Much faster than searching all 1M documents!</p>
                            </div>
                            
                            <h4>Types of Metadata Filters</h4>
                            <ul>
                                <li><strong>Equality filters:</strong> <code>author = "John Doe"</code></li>
                                <li><strong>Range filters:</strong> <code>date >= "2023-01-01" AND date <= "2023-12-31"</code></li>
                                <li><strong>In filters:</strong> <code>category IN ["Tech", "Science"]</code></li>
                                <li><strong>Boolean combinations:</strong> <code>(category = "Tech" OR category = "Science") AND date >= "2023"</code></li>
                            </ul>
                            
                            <h4>Benefits of Metadata Filtering</h4>
                            <ul>
                                <li>✅ <strong>Faster search:</strong> Reduces the number of vectors to search</li>
                                <li>✅ <strong>More relevant results:</strong> Ensures results match business constraints</li>
                                <li>✅ <strong>Better user experience:</strong> Users can narrow down by date, source, etc.</li>
                                <li>✅ <strong>Compliance:</strong> Can filter by access permissions, data retention policies</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Vector Database Operations: Indexing, Querying, and Maintenance</h3>
                            
                            <h4>1. Indexing (One-Time Setup)</h4>
                            <p><strong>What happens:</strong> When you add documents to a vector database, they go through an indexing process:</p>
                            
                            <ol>
                                <li><strong>Embedding generation:</strong> Each document chunk is converted to a vector using an embedding model</li>
                                <li><strong>Metadata extraction:</strong> Extract and store metadata (title, date, author, etc.)</li>
                                <li><strong>Index building:</strong> Vector is inserted into the index structure (HNSW, IVF, etc.)</li>
                                <li><strong>Storage:</strong> Vector, metadata, and original text are stored</li>
                            </ol>
                            
                            <p><strong>Performance considerations:</strong></p>
                            <ul>
                                <li>⚠️ <strong>Indexing is slow:</strong> Building indexes takes time (minutes to hours for large datasets)</li>
                                <li>⚠️ <strong>Batch processing:</strong> More efficient to index in batches rather than one-by-one</li>
                                <li>⚠️ <strong>Incremental updates:</strong> Some databases support adding vectors without rebuilding (HNSW), others require full rebuild (IVF)</li>
                            </ul>
                            
                            <h4>2. Querying (Per-Request)</h4>
                            <p><strong>What happens:</strong> When a user query arrives:</p>
                            
                            <ol>
                                <li><strong>Query embedding:</strong> Convert query to vector using the same embedding model</li>
                                <li><strong>Metadata filtering (optional):</strong> Apply metadata filters to reduce search space</li>
                                <li><strong>Vector search:</strong> Use the index to find top-k most similar vectors</li>
                                <li><strong>Result retrieval:</strong> Return document IDs, metadata, and similarity scores</li>
                                <li><strong>Document fetching:</strong> Retrieve actual document text using IDs</li>
                            </ol>
                            
                            <p><strong>Performance considerations:</strong></p>
                            <ul>
                                <li>✅ <strong>Very fast:</strong> Sub-100ms for millions of vectors with good indexes</li>
                                <li>✅ <strong>Scalable:</strong> Query time grows slowly with dataset size (logarithmic for HNSW)</li>
                                <li>⚠️ <strong>First query slower:</strong> May need to load index into memory</li>
                            </ul>
                            
                            <h4>3. Maintenance Operations</h4>
                            <p><strong>Index updates:</strong> When documents are added, updated, or deleted:</p>
                            <ul>
                                <li><strong>Add:</strong> Insert new vector into index (fast for HNSW, may require rebuild for IVF)</li>
                                <li><strong>Update:</strong> Delete old vector, insert new one (or update in-place if supported)</li>
                                <li><strong>Delete:</strong> Remove vector from index (mark as deleted or physically remove)</li>
                            </ul>
                            
                            <p><strong>Index optimization:</strong></p>
                            <ul>
                                <li><strong>Rebuilding:</strong> Periodically rebuild index to optimize structure (especially for IVF)</li>
                                <li><strong>Compaction:</strong> Remove deleted vectors and optimize storage</li>
                                <li><strong>Monitoring:</strong> Track index size, query performance, accuracy metrics</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Database Performance Metrics</h3>
                            <p>Vector databases use sophisticated indexing algorithms to enable fast similarity search. Understanding the mathematical foundations helps you choose the right database, configure indexes, and optimize performance. These formulas describe how vector databases achieve sub-millisecond search times even with millions of vectors.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. HNSW Search Complexity</h4>
                            <div class="formula-display">
                                \[T_{\text{search}} = O(\log N)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>HNSW (Hierarchical Navigable Small World) achieves logarithmic search time complexity, meaning search time grows very slowly as the number of vectors increases. This is why it can search millions of vectors in milliseconds.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(T_{\text{search}}\):</strong> Time complexity of search operation</li>
                                    <li><strong>\(O(\log N)\):</strong> Big-O notation indicating logarithmic time complexity</li>
                                    <li><strong>\(N\):</strong> Number of vectors in the database</li>
                                </ul>
                                
                                <h5>What Logarithmic Means:</h5>
                                <p>If you double the number of vectors, search time increases by a constant amount (not doubled). For example:</p>
                                <ul>
                                    <li>1,000 vectors: ~10 operations</li>
                                    <li>10,000 vectors: ~13 operations (only 30% more!)</li>
                                    <li>1,000,000 vectors: ~20 operations (only 100% more for 1000x more data!)</li>
                                </ul>
                                
                                <h5>Comparison to Brute-Force:</h5>
                                <p><strong>Brute-force:</strong> \(O(N)\) - linear time. To search 1M vectors, you must compare query with all 1M vectors.</p>
                                <p><strong>HNSW:</strong> \(O(\log N)\) - logarithmic time. To search 1M vectors, you only need ~20 comparisons by navigating the graph structure.</p>
                                
                                <h5>Why HNSW is Fast:</h5>
                                <p>HNSW builds a multi-layer graph where each layer has fewer nodes. Search starts at the top (few nodes), finds approximate location, then refines in lower layers. This hierarchical approach dramatically reduces comparisons needed.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. IVF Cluster Search Reduction</h4>
                            <div class="formula-display">
                                \[T_{\text{search}} = O(\sqrt{N} + k)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>IVF (Inverted File Index) partitions vectors into clusters. Instead of searching all \(N\) vectors, you only search within the nearest cluster(s), dramatically reducing search space.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(O(\sqrt{N})\):</strong> Time to find the nearest cluster(s) - grows with square root of N</li>
                                    <li><strong>\(O(k)\):</strong> Time to search within the cluster(s) - constant or linear in cluster size</li>
                                    <li><strong>Total:</strong> Much faster than \(O(N)\) brute-force search</li>
                                </ul>
                                
                                <h5>How It Works:</h5>
                                <ol>
                                    <li>Partition all vectors into \(\sqrt{N}\) clusters using k-means</li>
                                    <li>For a query, find the nearest cluster(s): \(O(\sqrt{N})\) operations</li>
                                    <li>Search only within those clusters: \(O(k)\) where k is cluster size</li>
                                    <li>Total: \(O(\sqrt{N} + k)\) instead of \(O(N)\)</li>
                                </ol>
                                
                                <h5>Example:</h5>
                                <p>1,000,000 vectors partitioned into 1,000 clusters (1,000 vectors per cluster):</p>
                                <ul>
                                    <li><strong>Brute-force:</strong> Compare with all 1,000,000 vectors</li>
                                    <li><strong>IVF:</strong> Find nearest cluster (1,000 comparisons) + search within cluster (1,000 comparisons) = 2,000 total comparisons</li>
                                    <li><strong>Speedup:</strong> 500x faster!</li>
                                </ul>
                                
                                <h5>Trade-off:</h5>
                                <p>IVF is faster than brute-force but may miss vectors near cluster boundaries. HNSW is more accurate but uses more memory. Choose based on your accuracy vs speed requirements.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Product Quantization Compression Ratio</h4>
                            <div class="formula-display">
                                \[\text{compression\_ratio} = \frac{\text{original\_size}}{\text{compressed\_size}} = \frac{d \times 4 \text{ bytes}}{m \times \log_2(k) \text{ bits}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Product Quantization (PQ) compresses vectors by quantizing sub-vectors into codebooks. This formula calculates the compression ratio achieved.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(d\):</strong> Original vector dimension (e.g., 384)</li>
                                    <li><strong>\(4 \text{ bytes}\):</strong> Size per float32 value (original storage)</li>
                                    <li><strong>\(m\):</strong> Number of sub-vectors (e.g., 8 sub-vectors of 48 dimensions each)</li>
                                    <li><strong>\(k\):</strong> Codebook size (number of quantization levels, e.g., 256)</li>
                                    <li><strong>\(\log_2(k) \text{ bits}\):</strong> Bits needed to store codebook index (e.g., \(\log_2(256) = 8\) bits = 1 byte)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Original: 384-dimensional vector = 384 × 4 bytes = 1,536 bytes<br>
                                PQ compressed: 8 sub-vectors × 1 byte = 8 bytes<br>
                                Compression ratio: \(\frac{1536}{8} = 192x\) reduction!</p>
                                
                                <h5>Trade-off:</h5>
                                <p>Higher compression = less storage but some accuracy loss. Typical PQ achieves 10-100x compression with minimal accuracy degradation (95%+ recall maintained).</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Vector Database Query Time</h4>
                            <div class="formula-display">
                                \[T_{\text{query}} = T_{\text{embed}} + T_{\text{search}} + T_{\text{retrieve}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>Total query time in a RAG system is the sum of embedding generation, vector search, and document retrieval times. Understanding this breakdown helps you optimize each component.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(T_{\text{embed}}\):</strong> Time to convert query text to embedding vector (typically 10-50ms for local models, 50-200ms for API calls)</li>
                                    <li><strong>\(T_{\text{search}}\):</strong> Time to find top-k similar vectors in the database (typically 10-100ms with HNSW for millions of vectors)</li>
                                    <li><strong>\(T_{\text{retrieve}}\):</strong> Time to fetch actual document text using retrieved IDs (typically 1-10ms if documents are cached)</li>
                                </ul>
                                
                                <h5>Typical Breakdown:</h5>
                                <p>For a query in a system with 1M documents:</p>
                                <ul>
                                    <li>Embedding: 20ms (local model) or 100ms (API)</li>
                                    <li>Vector search: 50ms (HNSW index)</li>
                                    <li>Document retrieval: 5ms (cached)</li>
                                    <li><strong>Total: 75ms (local) or 155ms (API)</strong></li>
                                </ul>
                                
                                <h5>Optimization Strategies:</h5>
                                <ul>
                                    <li><strong>Reduce \(T_{\text{embed}}\):</strong> Cache query embeddings for common queries, use faster embedding models</li>
                                    <li><strong>Reduce \(T_{\text{search}}\):</strong> Use efficient indexes (HNSW), limit search space with metadata filters</li>
                                    <li><strong>Reduce \(T_{\text{retrieve}}\):</strong> Cache documents in memory, use fast storage (SSD, in-memory cache)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Index Build Time</h4>
                            <div class="formula-display">
                                \[T_{\text{build}} = O(N \log N)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>Building an HNSW index takes \(O(N \log N)\) time, where \(N\) is the number of vectors. This is a one-time cost when indexing documents, but it's important to understand for planning indexing operations.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(N\):</strong> Number of vectors to index</li>
                                    <li><strong>\(O(N \log N)\):</strong> Time complexity - grows faster than linear but slower than quadratic</li>
                                    <li>For each vector, the algorithm needs to find its position in the graph structure</li>
                                </ul>
                                
                                <h5>Practical Times:</h5>
                                <ul>
                                    <li>10,000 vectors: ~1-5 seconds</li>
                                    <li>100,000 vectors: ~30-120 seconds (1-2 minutes)</li>
                                    <li>1,000,000 vectors: ~10-30 minutes</li>
                                    <li>10,000,000 vectors: ~2-8 hours</li>
                                </ul>
                                
                                <h5>Strategies for Large Datasets:</h5>
                                <ul>
                                    <li><strong>Batch indexing:</strong> Index in batches rather than one-by-one</li>
                                    <li><strong>Incremental updates:</strong> Use databases that support adding vectors without full rebuild (HNSW supports this)</li>
                                    <li><strong>Parallel indexing:</strong> Use multiple CPU cores to speed up index building</li>
                                    <li><strong>Background indexing:</strong> Build index in background while serving queries from old index</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example 1: Vector Database Indexing and Querying - Complete Workflow</h4>
                            <p><strong>Scenario:</strong> Setting up a vector database for a RAG system with 10,000 document chunks.</p>
                            
                            <p><strong>Step 1: Document Preparation</strong></p>
                            <p>You have 10,000 document chunks, each already embedded:</p>
                            <ul>
                                <li>Chunk 1: "Machine learning is..." → Embedding: [0.45, -0.23, 0.67, ...] (384-dim)</li>
                                <li>Chunk 2: "Deep learning uses..." → Embedding: [0.48, -0.25, 0.65, ...]</li>
                                <li>... (9,998 more chunks)</li>
                            </ul>
                            
                            <p><strong>Step 2: Index Building</strong></p>
                            <ul>
                                <li>All 10,000 embeddings are inserted into the vector database</li>
                                <li>HNSW index is built (takes ~2-5 minutes for 10K vectors)</li>
                                <li>Index creates a multi-layer graph structure for fast search</li>
                                <li>Total storage: 10,000 × 384 × 4 bytes = ~15 MB (just for embeddings)</li>
                            </ul>
                            
                            <p><strong>Step 3: Query Processing</strong></p>
                            <p>User query: "What is neural network training?"</p>
                            <ul>
                                <li>Query embedding: [0.46, -0.24, 0.66, ...]</li>
                                <li>Vector database searches the HNSW index</li>
                                <li>Finds top-5 most similar vectors in ~10-50 milliseconds</li>
                                <li>Returns document IDs: [doc_123, doc_456, doc_789, doc_234, doc_567]</li>
                            </ul>
                            
                            <p><strong>Step 4: Document Retrieval</strong></p>
                            <ul>
                                <li>Using document IDs, fetch actual text from storage</li>
                                <li>Returns: ["Neural networks are trained using...", "Training involves...", ...]</li>
                                <li>Total query time: ~50-100ms (embedding + search + retrieval)</li>
                            </ul>
                            
                            <p><strong>Performance:</strong> Without vector database (brute-force), this would take ~16 seconds. With HNSW index, it takes ~50ms - a 320x speedup!</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 2: Metadata Filtering in Action</h4>
                            <p><strong>Scenario:</strong> A knowledge base with documents from different years, and you want to filter by date before similarity search.</p>
                            
                            <p><strong>Knowledge Base:</strong> 1,000,000 documents</p>
                            <ul>
                                <li>500,000 documents from 2023</li>
                                <li>300,000 documents from 2024</li>
                                <li>200,000 documents from 2025</li>
                            </ul>
                            
                            <p><strong>Query:</strong> "Latest machine learning trends"</p>
                            
                            <p><strong>Without Metadata Filtering:</strong></p>
                            <ul>
                                <li>Search all 1,000,000 documents</li>
                                <li>Time: ~200ms</li>
                                <li>Results might include outdated 2023 documents</li>
                            </ul>
                            
                            <p><strong>With Metadata Filtering (date >= 2024):</strong></p>
                            <ul>
                                <li>Filter to 500,000 documents (2024 + 2025)</li>
                                <li>Search only in filtered set</li>
                                <li>Time: ~100ms (50% faster!)</li>
                                <li>Results are more recent and relevant</li>
                            </ul>
                            
                            <p><strong>Combined Filter Example:</strong></p>
                            <p>Query: "Python machine learning tutorials from 2024"</p>
                            <ul>
                                <li>Metadata filters: category="tutorial", year=2024, language="Python"</li>
                                <li>Filter from 1M → 50,000 documents</li>
                                <li>Vector search in 50K documents: ~30ms</li>
                                <li>✅ Much faster and more precise than searching all 1M documents</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 3: HNSW vs Brute-Force Performance Comparison</h4>
                            <p><strong>Scenario:</strong> Comparing search performance for different database sizes.</p>
                            
                            <p><strong>Test Setup:</strong> Find top-5 most similar vectors to a query</p>
                            
                            <p><strong>1,000 Documents:</strong></p>
                            <ul>
                                <li><strong>Brute-force:</strong> Compare query with all 1,000 vectors = 1,000 comparisons = ~16ms</li>
                                <li><strong>HNSW:</strong> Navigate graph structure = ~10 comparisons = ~2ms</li>
                                <li><strong>Speedup:</strong> 8x faster</li>
                            </ul>
                            
                            <p><strong>100,000 Documents:</strong></p>
                            <ul>
                                <li><strong>Brute-force:</strong> 100,000 comparisons = ~1.6 seconds</li>
                                <li><strong>HNSW:</strong> ~15 comparisons = ~5ms</li>
                                <li><strong>Speedup:</strong> 320x faster!</li>
                            </ul>
                            
                            <p><strong>1,000,000 Documents:</strong></p>
                            <ul>
                                <li><strong>Brute-force:</strong> 1,000,000 comparisons = ~16 seconds (unacceptable!)</li>
                                <li><strong>HNSW:</strong> ~20 comparisons = ~50ms</li>
                                <li><strong>Speedup:</strong> 320x faster!</li>
                            </ul>
                            
                            <p><strong>10,000,000 Documents:</strong></p>
                            <ul>
                                <li><strong>Brute-force:</strong> 10,000,000 comparisons = ~2.7 minutes (completely impractical!)</li>
                                <li><strong>HNSW:</strong> ~25 comparisons = ~100ms</li>
                                <li><strong>Speedup:</strong> 1,600x faster!</li>
                            </ul>
                            
                            <p><strong>Key Insight:</strong> As database size grows, brute-force becomes exponentially slower, while HNSW search time grows only logarithmically. This is why vector databases are essential for production RAG systems.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 4: Incremental Updates to Vector Database</h4>
                            <p><strong>Scenario:</strong> Adding new documents to an existing vector database without rebuilding the entire index.</p>
                            
                            <p><strong>Initial State:</strong></p>
                            <ul>
                                <li>Database has 100,000 documents indexed</li>
                                <li>HNSW index is built and optimized</li>
                                <li>Query time: ~50ms</li>
                            </ul>
                            
                            <p><strong>New Documents Arrive:</strong></p>
                            <ul>
                                <li>1,000 new documents need to be added</li>
                                <li>Each document is embedded: [0.45, -0.23, 0.67, ...]</li>
                            </ul>
                            
                            <p><strong>Incremental Update Process:</strong></p>
                            <p><strong>Step 1: Embed new documents</strong></p>
                            <ul>
                                <li>Generate embeddings for 1,000 new documents</li>
                                <li>Time: ~10-30 seconds (depending on embedding model)</li>
                            </ul>
                            
                            <p><strong>Step 2: Insert into HNSW index</strong></p>
                            <ul>
                                <li>For each new vector, find its position in the graph</li>
                                <li>Connect it to nearest neighbors in each layer</li>
                                <li>Time: ~5-10 seconds for 1,000 vectors</li>
                                <li>✅ No need to rebuild entire index!</li>
                            </ul>
                            
                            <p><strong>Step 3: Verify</strong></p>
                            <ul>
                                <li>Database now has 101,000 documents</li>
                                <li>Query time: ~52ms (slightly slower, but still fast)</li>
                                <li>New documents are immediately searchable</li>
                            </ul>
                            
                            <p><strong>Comparison to Full Rebuild:</strong></p>
                            <ul>
                                <li><strong>Incremental update:</strong> ~15-40 seconds total</li>
                                <li><strong>Full rebuild:</strong> ~10-30 minutes (would require rebuilding entire index)</li>
                                <li>✅ Incremental updates are 15-120x faster!</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 5: Choosing Between Vector Database Options</h4>
                            <p><strong>Scenario:</strong> You need to choose a vector database for your RAG system. Here's how different options compare for a specific use case.</p>
                            
                            <p><strong>Use Case:</strong> 5 million documents, need sub-100ms query time, cloud deployment preferred</p>
                            
                            <p><strong>Option 1: Pinecone (Managed Cloud)</strong></p>
                            <ul>
                                <li>✅ Setup: 5 minutes (just create account, API key)</li>
                                <li>✅ Scaling: Automatic, handles 5M+ documents easily</li>
                                <li>✅ Performance: ~50ms query time</li>
                                <li>✅ Maintenance: Zero (fully managed)</li>
                                <li>❌ Cost: ~$70-200/month for 5M vectors</li>
                                <li>❌ Vendor lock-in: Data stored in Pinecone's cloud</li>
                                <li><strong>Best for:</strong> Quick deployment, teams without DevOps resources</li>
                            </ul>
                            
                            <p><strong>Option 2: Chroma (Self-Hosted)</strong></p>
                            <ul>
                                <li>✅ Setup: 30-60 minutes (install, configure, deploy)</li>
                                <li>✅ Scaling: Manual (need to set up infrastructure)</li>
                                <li>✅ Performance: ~60-80ms query time</li>
                                <li>✅ Cost: ~$20-50/month (server costs only)</li>
                                <li>✅ Control: Full control over data and infrastructure</li>
                                <li>❌ Maintenance: You manage servers, updates, backups</li>
                                <li><strong>Best for:</strong> Cost-sensitive, need data control, have DevOps team</li>
                            </ul>
                            
                            <p><strong>Option 3: FAISS (Library, Self-Hosted)</strong></p>
                            <ul>
                                <li>✅ Setup: 1-2 hours (integrate into your application)</li>
                                <li>✅ Performance: ~40-60ms query time (very fast)</li>
                                <li>✅ Cost: Server costs only</li>
                                <li>✅ Flexibility: Full control, can customize</li>
                                <li>❌ Scaling: Manual, need to implement sharding yourself</li>
                                <li>❌ Features: No built-in metadata filtering, need to implement yourself</li>
                                <li><strong>Best for:</strong> Research, maximum performance, full customization needed</li>
                            </ul>
                            
                            <p><strong>Recommendation for this use case:</strong> Pinecone for fastest deployment, Chroma for cost savings, FAISS for maximum performance and control.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="explanation-box">
                            <h3>Implementation Overview</h3>
                            <p>This section provides practical Python code examples for working with vector databases in RAG systems. The examples demonstrate how to set up, index documents, query, and manage vector databases using popular options like <strong>Chroma</strong>, <strong>Pinecone</strong>, and <strong>FAISS</strong>.</p>
                        </div>
                        
                        <div class="code-box">
                            <h4>1. Chroma Vector Database - Complete Setup</h4>
                            <p><strong>What this does:</strong> Sets up Chroma, indexes documents with embeddings, and performs similarity search. Chroma is easy to use and good for self-hosted solutions.</p>
                            <pre><code class="language-python">import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import numpy as np

class ChromaRAG:
    """
    Complete RAG implementation using Chroma vector database.
    
    This class handles:
    1. Document indexing with embeddings
    2. Query processing and retrieval
    3. Metadata filtering
    """
    
    def __init__(self, collection_name="documents", persist_directory="./chroma_db"):
        """
        Initialize Chroma client and collection.
        
        Args:
            collection_name: Name of the collection to create/use
            persist_directory: Directory to persist data (for PersistentClient)
        """
        # Use PersistentClient for production (saves to disk)
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # Get or create collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Use cosine similarity
        )
        
        # Initialize embedding model
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        print(f"Initialized Chroma collection: {collection_name}")
    
    def add_documents(self, documents, ids=None, metadatas=None):
        """
        Add documents to the vector database.
        
        Args:
            documents: List of document text strings
            ids: Optional list of document IDs (auto-generated if None)
            metadatas: Optional list of metadata dictionaries
        """
        if not documents:
            raise ValueError("Documents list cannot be empty")
        
        # Generate IDs if not provided
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]
        
        # Generate embeddings
        print(f"Generating embeddings for {len(documents)} documents...")
        embeddings = self.embedder.encode(documents, show_progress_bar=True)
        embeddings_list = embeddings.tolist()
        
        # Prepare metadatas (ensure all have same keys)
        if metadatas is None:
            metadatas = [{}] * len(documents)
        
        # Add to collection
        self.collection.add(
            documents=documents,
            embeddings=embeddings_list,
            ids=ids,
            metadatas=metadatas
        )
        
        print(f"Added {len(documents)} documents to collection")
        print(f"Collection now has {self.collection.count()} total documents")
    
    def query(self, query_text, n_results=5, where_filter=None):
        """
        Query the vector database for similar documents.
        
        Args:
            query_text: User query string
            n_results: Number of results to return
            where_filter: Optional metadata filter (e.g., {"year": 2024})
            
        Returns:
            Dictionary with 'documents', 'ids', 'distances', 'metadatas'
        """
        # Generate query embedding
        query_embedding = self.embedder.encode([query_text])
        
        # Build query
        query_kwargs = {
            "query_embeddings": query_embedding.tolist(),
            "n_results": n_results
        }
        
        # Add metadata filter if provided
        if where_filter:
            query_kwargs["where"] = where_filter
        
        # Execute query
        results = self.collection.query(**query_kwargs)
        
        return {
            'documents': results['documents'][0],
            'ids': results['ids'][0],
            'distances': results['distances'][0],
            'metadatas': results['metadatas'][0]
        }
    
    def update_document(self, doc_id, new_text, new_metadata=None):
        """
        Update an existing document.
        
        Args:
            doc_id: ID of document to update
            new_text: New document text
            new_metadata: New metadata (optional)
        """
        # Generate new embedding
        new_embedding = self.embedder.encode([new_text])[0].tolist()
        
        # Update in collection
        self.collection.update(
            ids=[doc_id],
            documents=[new_text],
            embeddings=[new_embedding],
            metadatas=[new_metadata] if new_metadata else None
        )
        print(f"Updated document: {doc_id}")
    
    def delete_documents(self, doc_ids):
        """Delete documents by IDs."""
        self.collection.delete(ids=doc_ids)
        print(f"Deleted {len(doc_ids)} documents")

# Example usage
rag = ChromaRAG(collection_name="knowledge_base")

# Add documents with metadata
documents = [
    "Machine learning is a subset of AI that learns from data.",
    "Deep learning uses neural networks with multiple layers.",
    "Python is a popular programming language for data science."
]

metadatas = [
    {"topic": "machine_learning", "year": 2024, "category": "AI"},
    {"topic": "deep_learning", "year": 2024, "category": "AI"},
    {"topic": "programming", "year": 2024, "category": "languages"}
]

ids = ["doc_ml", "doc_dl", "doc_python"]

rag.add_documents(documents, ids=ids, metadatas=metadatas)

# Query without filter
results = rag.query("What is machine learning?", n_results=2)
print("\nQuery results:")
for i, (doc, distance) in enumerate(zip(results['documents'], results['distances']), 1):
    print(f"{i}. {doc} (distance: {distance:.3f})")

# Query with metadata filter
filtered_results = rag.query(
    "What is machine learning?",
    n_results=2,
    where_filter={"category": "AI"}  # Only search in AI category
)
print("\nFiltered results (AI category only):")
for i, doc in enumerate(filtered_results['documents'], 1):
    print(f"{i}. {doc}")</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Persistent storage:</strong> Uses PersistentClient to save data to disk (survives restarts)</li>
                                    <li><strong>Automatic indexing:</strong> Chroma automatically builds HNSW index for fast search</li>
                                    <li><strong>Metadata filtering:</strong> Can filter by metadata before similarity search</li>
                                    <li><strong>Update support:</strong> Can update documents without rebuilding entire index</li>
                                    <li><strong>Distance vs similarity:</strong> Chroma returns distances (lower = more similar), not similarity scores</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>2. Pinecone Vector Database - Cloud Deployment</h4>
                            <p><strong>What this does:</strong> Sets up Pinecone (managed cloud service), indexes documents, and performs queries. Ideal for production deployments without infrastructure management.</p>
                            <pre><code class="language-python">import pinecone
from sentence_transformers import SentenceTransformer
import os

class PineconeRAG:
    """
    RAG implementation using Pinecone (managed cloud vector database).
    
    Pinecone handles infrastructure, scaling, and optimization automatically.
    Good for production systems where you want minimal DevOps overhead.
    """
    
    def __init__(self, api_key, environment, index_name="rag-index"):
        """
        Initialize Pinecone connection.
        
        Args:
            api_key: Pinecone API key (get from pinecone.io)
            environment: Pinecone environment (e.g., "us-west1-gcp")
            index_name: Name of the index to create/use
        """
        # Initialize Pinecone
        pinecone.init(api_key=api_key, environment=environment)
        
        # Get or create index
        if index_name not in pinecone.list_indexes():
            # Create index with specifications
            pinecone.create_index(
                index_name,
                dimension=384,  # Match embedding dimension
                metric="cosine",  # Use cosine similarity
                metadata_config={"indexed": ["category", "year", "source"]}  # Indexed metadata for filtering
            )
            print(f"Created new index: {index_name}")
        else:
            print(f"Using existing index: {index_name}")
        
        self.index = pinecone.Index(index_name)
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        print(f"Connected to Pinecone index: {index_name}")
    
    def add_documents(self, documents, ids=None, metadatas=None):
        """
        Add documents to Pinecone index.
        
        Args:
            documents: List of document strings
            ids: Optional list of IDs
            metadatas: Optional list of metadata dicts
        """
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{}] * len(documents)
        
        # Generate embeddings
        embeddings = self.embedder.encode(documents, show_progress_bar=True)
        
        # Prepare vectors for upsert (Pinecone format)
        vectors = []
        for i, (doc_id, embedding, metadata) in enumerate(zip(ids, embeddings, metadatas)):
            vectors.append({
                "id": doc_id,
                "values": embedding.tolist(),
                "metadata": {**metadata, "text": documents[i]}  # Store text in metadata
            })
        
        # Upsert in batches (Pinecone supports batch operations)
        batch_size = 100
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            self.index.upsert(vectors=batch)
        
        print(f"Added {len(documents)} documents to Pinecone")
        print(f"Index stats: {self.index.describe_index_stats()}")
    
    def query(self, query_text, top_k=5, filter_dict=None):
        """
        Query Pinecone index.
        
        Args:
            query_text: User query
            top_k: Number of results
            filter_dict: Optional metadata filter (e.g., {"category": "AI"})
            
        Returns:
            Query results with documents, scores, and metadata
        """
        # Generate query embedding
        query_embedding = self.embedder.encode([query_text])[0].tolist()
        
        # Build query
        query_kwargs = {
            "vector": query_embedding,
            "top_k": top_k,
            "include_metadata": True
        }
        
        # Add filter if provided
        if filter_dict:
            query_kwargs["filter"] = filter_dict
        
        # Execute query
        results = self.index.query(**query_kwargs)
        
        # Format results
        formatted_results = []
        for match in results['matches']:
            formatted_results.append({
                'id': match['id'],
                'score': match['score'],  # Pinecone returns similarity scores (higher = more similar)
                'text': match['metadata'].get('text', ''),
                'metadata': {k: v for k, v in match['metadata'].items() if k != 'text'}
            })
        
        return formatted_results

# Example usage
# Initialize (requires Pinecone API key)
# rag = PineconeRAG(
#     api_key=os.getenv("PINECONE_API_KEY"),
#     environment="us-west1-gcp",
#     index_name="rag-tutorial"
# )
#
# # Add documents
# documents = ["Machine learning is...", "Deep learning uses...", ...]
# rag.add_documents(documents, metadatas=[{"category": "AI"}, ...])
#
# # Query
# results = rag.query("What is machine learning?", top_k=3)
# for result in results:
#     print(f"Score: {result['score']:.3f}")
#     print(f"Text: {result['text']}")
#     print(f"Metadata: {result['metadata']}\n")</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Managed service:</strong> No infrastructure to manage - Pinecone handles everything</li>
                                    <li><strong>Automatic scaling:</strong> Handles millions of vectors without configuration</li>
                                    <li><strong>Metadata indexing:</strong> Can index specific metadata fields for fast filtering</li>
                                    <li><strong>Batch operations:</strong> Efficient batch upsert for large document sets</li>
                                    <li><strong>Cost:</strong> Pay-per-use pricing, good for production but has ongoing costs</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>3. FAISS - High-Performance Self-Hosted Solution</h4>
                            <p><strong>What this does:</strong> Sets up FAISS (Facebook AI Similarity Search) for maximum performance and control. Best for research, self-hosted solutions, or when you need fine-grained control.</p>
                            <pre><code class="language-python">import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class FAISSRAG:
    """
    RAG implementation using FAISS for vector similarity search.
    
    FAISS provides maximum performance and flexibility, but requires
    more setup and infrastructure management than managed services.
    """
    
    def __init__(self, dimension=384, index_type="L2"):
        """
        Initialize FAISS index.
        
        Args:
            dimension: Embedding dimension (384 for all-MiniLM-L6-v2)
            index_type: "L2" (Euclidean) or "IP" (Inner Product/Cosine for normalized)
        """
        self.dimension = dimension
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Create FAISS index
        # IndexFlatL2: Exact search (slower but accurate)
        # IndexIVFFlat: Approximate search (faster, good for large datasets)
        # For production, use IndexIVFFlat or IndexHNSWFlat for speed
        
        # Simple exact search index (for small datasets)
        if index_type == "L2":
            self.index = faiss.IndexFlatL2(dimension)
        else:  # Inner product (for normalized embeddings = cosine similarity)
            self.index = faiss.IndexFlatIP(dimension)
        
        # Store document texts and metadata
        self.documents = []
        self.metadatas = []
        print(f"Initialized FAISS index: {index_type}, dimension: {dimension}")
    
    def add_documents(self, documents, metadatas=None):
        """
        Add documents to FAISS index.
        
        Args:
            documents: List of document strings
            metadatas: Optional list of metadata dictionaries
        """
        if not documents:
            raise ValueError("Documents list cannot be empty")
        
        # Generate embeddings
        embeddings = self.embedder.encode(documents, show_progress_bar=True)
        
        # Normalize embeddings for cosine similarity (if using IP index)
        if isinstance(self.index, faiss.IndexFlatIP):
            faiss.normalize_L2(embeddings)  # Normalize for cosine similarity
        
        # Convert to numpy array (float32 for FAISS)
        embeddings = np.array(embeddings).astype('float32')
        
        # Add to index
        self.index.add(embeddings)
        
        # Store documents and metadata
        self.documents.extend(documents)
        if metadatas:
            self.metadatas.extend(metadatas)
        else:
            self.metadatas.extend([{}] * len(documents))
        
        print(f"Added {len(documents)} documents. Index now has {self.index.ntotal} vectors")
    
    def search(self, query_text, top_k=5):
        """
        Search for similar documents.
        
        Args:
            query_text: User query
            top_k: Number of results
            
        Returns:
            List of (document, distance, metadata) tuples
        """
        if self.index.ntotal == 0:
            raise ValueError("Index is empty. Add documents first.")
        
        # Generate query embedding
        query_embedding = self.embedder.encode([query_text])
        
        # Normalize if using IP index
        if isinstance(self.index, faiss.IndexFlatIP):
            query_embedding = query_embedding.astype('float32')
            faiss.normalize_L2(query_embedding)
        else:
            query_embedding = query_embedding.astype('float32')
        
        # Search
        distances, indices = self.index.search(query_embedding, top_k)
        
        # Format results
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.documents):  # Valid index
                results.append({
                    'document': self.documents[idx],
                    'distance': float(distance),
                    'similarity': 1.0 - float(distance) if isinstance(self.index, faiss.IndexFlatL2) else float(distance),
                    'metadata': self.metadatas[idx] if idx < len(self.metadatas) else {}
                })
        
        return results
    
    def save_index(self, filepath):
        """Save FAISS index to disk."""
        faiss.write_index(self.index, filepath)
        print(f"Saved index to {filepath}")
    
    def load_index(self, filepath):
        """Load FAISS index from disk."""
        self.index = faiss.read_index(filepath)
        print(f"Loaded index from {filepath}")

# Example usage
rag = FAISSRAG(dimension=384, index_type="IP")  # IP = Inner Product (cosine for normalized)

# Add documents
documents = [
    "Machine learning algorithms learn from data.",
    "Deep learning uses neural networks.",
    "Python is a programming language."
]
rag.add_documents(documents, metadatas=[
    {"topic": "ML"}, {"topic": "DL"}, {"topic": "programming"}
])

# Search
results = rag.search("What is machine learning?", top_k=2)
for result in results:
    print(f"Similarity: {result['similarity']:.3f}")
    print(f"Document: {result['document']}\n")

# Save for later use
rag.save_index("faiss_index.bin")</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Maximum performance:</strong> FAISS is highly optimized C++ code, very fast</li>
                                    <li><strong>Full control:</strong> You control indexing, storage, and search parameters</li>
                                    <li><strong>Index types:</strong> Choose between exact (IndexFlat) or approximate (IndexIVF, IndexHNSW) search</li>
                                    <li><strong>Normalization:</strong> For cosine similarity with IP index, normalize embeddings first</li>
                                    <li><strong>Persistence:</strong> Can save/load indexes to disk</li>
                                    <li><strong>Use case:</strong> Best for research, maximum performance needs, or when you want full control</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>4. Metadata Filtering Implementation</h4>
                            <p><strong>What this does:</strong> Demonstrates how to combine vector similarity search with metadata filtering for precise retrieval.</p>
                            <pre><code class="language-python">import chromadb
from sentence_transformers import SentenceTransformer

class FilteredVectorSearch:
    """
    Vector search with metadata filtering capabilities.
    
    Filters documents by metadata before or after similarity search,
    enabling precise retrieval based on document properties.
    """
    
    def __init__(self):
        self.client = chromadb.Client()
        self.collection = self.client.create_collection(
            name="filtered_docs",
            metadata={"hnsw:space": "cosine"}
        )
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_documents_with_metadata(self, documents, metadatas):
        """
        Add documents with rich metadata for filtering.
        
        Args:
            documents: List of document texts
            metadatas: List of metadata dicts with filterable fields
        """
        embeddings = self.embedder.encode(documents).tolist()
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        self.collection.add(
            documents=documents,
            embeddings=embeddings,
            ids=ids,
            metadatas=metadatas
        )
    
    def search_with_filters(self, query, top_k=5, filters=None):
        """
        Search with metadata filters.
        
        Args:
            query: Search query
            top_k: Number of results
            filters: Metadata filter dict (e.g., {"year": 2024, "category": "AI"})
            
        Returns:
            Filtered search results
        """
        query_embedding = self.embedder.encode([query]).tolist()
        
        query_kwargs = {
            "query_embeddings": query_embedding,
            "n_results": top_k
        }
        
        # Add metadata filters
        if filters:
            # Chroma supports where clauses
            where_clause = {}
            for key, value in filters.items():
                if isinstance(value, list):
                    where_clause[key] = {"$in": value}  # IN operator
                elif isinstance(value, dict) and "$gte" in value:
                    where_clause[key] = value  # Range operators
                else:
                    where_clause[key] = value  # Equality
            
            query_kwargs["where"] = where_clause
        
        results = self.collection.query(**query_kwargs)
        return results

# Example: Filtered search
search = FilteredVectorSearch()

# Add documents with metadata
documents = [
    "Machine learning tutorial for beginners",
    "Advanced deep learning techniques",
    "Python programming guide",
    "Machine learning research paper 2024"
]

metadatas = [
    {"category": "tutorial", "year": 2023, "difficulty": "beginner"},
    {"category": "tutorial", "year": 2024, "difficulty": "advanced"},
    {"category": "programming", "year": 2023, "difficulty": "beginner"},
    {"category": "research", "year": 2024, "difficulty": "advanced"}
]

search.add_documents_with_metadata(documents, metadatas)

# Search with filters
results = search.search_with_filters(
    "machine learning",
    top_k=3,
    filters={"year": 2024, "category": "tutorial"}  # Only 2024 tutorials
)

print("Filtered results (2024 tutorials only):")
for doc in results['documents'][0]:
    print(f"- {doc}")</code></pre>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Installation Requirements</h3>
                            <p>Install the required packages based on your vector database choice:</p>
                            <pre><code class="language-bash"># For Chroma
pip install chromadb sentence-transformers

# For Pinecone
pip install pinecone-client sentence-transformers

# For FAISS
pip install faiss-cpu sentence-transformers  # CPU version
# or: pip install faiss-gpu sentence-transformers  # GPU version (faster)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Chunking in RAG Systems</h3>
                            <p><strong>Document processing:</strong> Split large documents into manageable chunks for embedding and retrieval</p>
                            <p><strong>Context management:</strong> Ensure chunks fit within LLM context windows</p>
                            <p><strong>Retrieval optimization:</strong> Smaller, focused chunks improve retrieval precision</p>
                            <p><strong>Storage efficiency:</strong> Balance between chunk size and storage costs</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Best Practices</h3>
                            <p><strong>Chunk size:</strong> 200-1000 tokens (depends on embedding model and LLM context)</p>
                            <p><strong>Overlap:</strong> 10-20% of chunk size</p>
                            <p><strong>Strategy:</strong> Use semantic chunking when possible, fallback to sentence-based, then fixed-size</p>
                            <p><strong>Testing:</strong> Evaluate retrieval quality with different chunk sizes</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                        <div class="quiz-question">
                                <h3>Question 1: What is a vector database?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) A regular SQL database</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) While vector databases do store data like traditional databases, they're specifically designed for high-dimensional vector similarity search using algorithms like HNSW and IVF, which is fundamentally different from SQL query-based retrieval</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Vector databases are caching layers that temporarily store frequently accessed data to improve response times in applications</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) A specialized database optimized for storing and efficiently searching high-dimensional vector embeddings using similarity search algorithms</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What are the key features of vector databases for RAG?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) A vector database is primarily a text indexing system that stores documents as plain text files with basic search capabilities</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A file system</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Although vector databases handle storage similar to file systems, their core functionality is optimized approximate nearest neighbor search for embeddings, not just file organization or basic text storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Fast approximate nearest neighbor (ANN) search, scalable to millions of vectors, metadata filtering, real-time updates, and support for similarity metrics (cosine, Euclidean, dot product)</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What are popular vector databases used in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) While vector databases do store data like traditional databases, they're specifically designed for high-dimensional vector similarity search using algorithms like HNSW and IVF, which is fundamentally different from SQL query-based retrieval</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Vector databases are essentially the same as traditional relational databases, just with a different storage format for data organization</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) Pinecone, Weaviate, Chroma, Qdrant, FAISS, Milvus, and pgvector (PostgreSQL extension)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A regular SQL database</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: Interview question: "What is HNSW indexing and why is it used in vector databases?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) A text storage system</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Hierarchical Navigable Small World - a graph-based ANN algorithm that provides fast approximate search with good accuracy, commonly used in production vector databases</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Although vector databases handle storage similar to file systems, their core functionality is optimized approximate nearest neighbor search for embeddings, not just file organization or basic text storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Vector databases are file systems optimized for storing large amounts of data, similar to cloud storage services but with faster access</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What is IVF (Inverted File Index) in vector databases?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Vector databases share some characteristics with caching systems in terms of fast access, but they're specialized for semantic similarity search using vector embeddings, which requires completely different indexing and query mechanisms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Vector databases are file systems optimized for storing large amounts of data, similar to cloud storage services but with faster access</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) An indexing method that partitions vector space into clusters (Voronoi cells), enabling faster search by only searching relevant clusters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A file system</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: Interview question: "How do you choose between different vector databases?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Vector databases are essentially the same as traditional relational databases, just with a different storage format for data organization</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Consider scale (millions vs billions), deployment (cloud vs self-hosted), features (metadata filtering, real-time updates), cost, ease of use, and integration with your stack</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) A file system</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Although vector databases handle storage similar to file systems, their core functionality is optimized approximate nearest neighbor search for embeddings, not just file organization or basic text storage</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is metadata filtering in vector databases?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Filtering search results by document metadata (date, author, category) before or after similarity search, enabling precise retrieval</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Vector databases share some characteristics with caching systems in terms of fast access, but they're specialized for semantic similarity search using vector embeddings, which requires completely different indexing and query mechanisms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Vector databases are essentially the same as traditional relational databases, just with a different storage format for data organization</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A caching system</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "What is the difference between exact and approximate nearest neighbor search?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) This is incorrect</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Exact search finds true nearest neighbors but is slow for large datasets. Approximate (ANN) is much faster with high accuracy, using indexing (HNSW, IVF) to trade some accuracy for speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) This comprehensive approach has been considered but doesn't work well in practice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) While this might seem reasonable, it's not the correct approach</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is FAISS and when would you use it?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Facebook AI Similarity Search - a library for efficient similarity search, good for self-hosted solutions, research, and when you need fine-grained control over indexing</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) While this might seem reasonable, it's not the correct approach</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) This comprehensive approach has been considered but doesn't work well in practice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not applicable</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: Interview question: "How do you handle vector database updates in a production RAG system?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Vector databases are file systems optimized for storing large amounts of data, similar to cloud storage services but with faster access</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A file system</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) While vector databases do store data like traditional databases, they're specifically designed for high-dimensional vector similarity search using algorithms like HNSW and IVF, which is fundamentally different from SQL query-based retrieval</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Implement incremental updates, batch processing for large changes, re-indexing strategies, versioning for document updates, and ensure consistency between embeddings and metadata</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the difference between Pinecone and self-hosted solutions like Chroma?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Vector databases are caching layers that temporarily store frequently accessed data to improve response times in applications</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Pinecone is managed cloud service (easy setup, scaling, but cost). Chroma is self-hosted (more control, lower cost, but requires infrastructure management)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) A regular SQL database</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) While vector databases do store data like traditional databases, they're specifically designed for high-dimensional vector similarity search using algorithms like HNSW and IVF, which is fundamentally different from SQL query-based retrieval</div>
                            </div>
                        <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "How do you scale a vector database for millions of documents?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Vector databases share some characteristics with caching systems in terms of fast access, but they're specialized for semantic similarity search using vector embeddings, which requires completely different indexing and query mechanisms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A regular SQL database</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Vector databases are file systems optimized for storing large amounts of data, similar to cloud storage services but with faster access</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Use distributed indexing, sharding by metadata or hash, horizontal scaling with multiple nodes, efficient indexing algorithms (HNSW), and consider approximate search for speed</div>
                            </div>
                        </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter3" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 3</a>
                <a href="/tutorials/rag/chapter5" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 5 →</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
    </script>
</body>
</html>