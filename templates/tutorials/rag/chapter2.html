<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Text Embeddings & Vector Representations - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 2: Text Embeddings & Vector Representations</h1>
                <p class="chapter-subtitle">Converting Text to Vectors</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="28"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn active">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand text embeddings & vector representations fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Text Embeddings & Vector Representations</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction: Why Convert Text to Vectors?</h3>
                            <p>In RAG systems, we need to find relevant documents quickly. But how do you search through thousands or millions of documents to find the ones most relevant to a user's question? Traditional keyword search (like Google) has limitations - it can't understand that "car" and "automobile" mean the same thing, or that "Paris is the capital of France" is similar to "What is France's capital city?"</p>
                            
                            <p><strong>Text embeddings solve this problem</strong> by converting text into numerical vectors (arrays of numbers) that capture semantic meaning. Similar meanings result in similar vectors, allowing us to find relevant documents even when they use different words.</p>
                            
                            <div class="example-box">
                                <h5>Real-World Analogy:</h5>
                                <p>Think of embeddings like GPS coordinates for meaning. Just as two places close together on a map have similar coordinates, two texts with similar meanings have similar embedding vectors. This allows us to find "nearby" documents in meaning-space, not just word-space.</p>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>The Core Problem Embeddings Solve</h3>
                            <p><strong>Challenge:</strong> How do we enable computers to understand that these sentences are similar?</p>
                            <ul>
                                <li>"What is the capital of France?"</li>
                                <li>"France's capital city"</li>
                                <li>"Where is the French capital located?"</li>
                            </ul>
                            
                            <p><strong>Solution:</strong> Embeddings convert all three into vectors that are mathematically similar (high cosine similarity), even though they use different words. This enables semantic search - finding documents based on meaning, not just exact word matches.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters for RAG</h4>
                            <p>In RAG systems, embeddings are the foundation of retrieval. Without good embeddings, you can't find relevant documents. With good embeddings, you can:</p>
                            <ul>
                                <li>Find documents even when they use synonyms or different phrasing</li>
                                <li>Search millions of documents in milliseconds</li>
                                <li>Understand semantic relationships (e.g., "machine learning" is similar to "ML" and "artificial intelligence")</li>
                                <li>Handle multilingual content if using multilingual embedding models</li>
                            </ul>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>What Are Vector Embeddings?</h3>
                            <p><strong>Vector embeddings are dense numerical representations of text that capture semantic meaning.</strong> They convert words, sentences, or documents into fixed-size arrays of numbers (vectors) where similar meanings result in similar vectors.</p>
                            
                            <h4>Understanding Embeddings with an Analogy</h4>
                            <p>Think of embeddings like coordinates on a map of meaning:</p>
                            <ul>
                                <li>Texts about "France" might be at coordinates [0.2, -0.5, 0.8, ...]</li>
                                <li>Texts about "Germany" might be at [0.3, -0.4, 0.7, ...] (close, since both are European countries)</li>
                                <li>Texts about "Python programming" might be at [-0.1, 0.9, -0.3, ...] (far away, different topic)</li>
                            </ul>
                            <p>Just as you can measure distance between GPS coordinates, you can measure similarity between embedding vectors using cosine similarity or Euclidean distance.</p>
                            
                            <h4>Key Properties of Embeddings</h4>
                            
                            <p><strong>1. Fixed Dimension:</strong></p>
                            <ul>
                                <li>Each embedding has a fixed number of dimensions (typically 384, 768, or 1536)</li>
                                <li>All texts are converted to vectors of the same size, enabling mathematical operations</li>
                                <li>Example: "Hello world" and "The entire history of human civilization" both become 384-dimensional vectors</li>
                            </ul>
                            
                            <p><strong>2. Semantic Similarity = Vector Similarity:</strong></p>
                            <ul>
                                <li>Texts with similar meanings have vectors that are close together in the high-dimensional space</li>
                                <li>We measure this using cosine similarity: similar texts have high cosine similarity (close to 1.0)</li>
                                <li>Example: "car" and "automobile" have high similarity, even though they're different words</li>
                            </ul>
                            
                            <p><strong>3. Vector Arithmetic:</strong></p>
                            <ul>
                                <li>Embeddings can capture relationships through vector arithmetic</li>
                                <li>Famous example: "king" - "man" + "woman" ‚âà "queen"</li>
                                <li>This shows embeddings capture semantic relationships, not just word similarity</li>
                            </ul>
                            
                            <p><strong>4. Enable Efficient Similarity Search:</strong></p>
                            <ul>
                                <li>Once text is in vector form, we can use mathematical operations to find similar texts</li>
                                <li>This enables fast semantic search across millions of documents</li>
                                <li>Vector databases can find similar vectors in milliseconds</li>
                            </ul>
                            
                            <h4>How Embeddings Capture Meaning</h4>
                            <p>Embeddings work because they're trained on massive amounts of text data. The model learns patterns like:</p>
                            <ul>
                                <li>Words that appear in similar contexts (e.g., "doctor" and "nurse" both appear near "hospital") should have similar vectors</li>
                                <li>Sentences with similar meanings should be close in vector space</li>
                                <li>Semantic relationships (synonyms, antonyms, related concepts) are encoded in the vector positions</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Example: How Embeddings Understand Similarity</h5>
                                <p>Consider these three sentences:</p>
                                <ol>
                                    <li>"The capital of France is Paris"</li>
                                    <li>"Paris is the capital city of France"</li>
                                    <li>"The weather in Tokyo is rainy today"</li>
                                </ol>
                                <p>After embedding:</p>
                                <ul>
                                    <li>Sentences 1 and 2 will have very similar vectors (high cosine similarity, e.g., 0.95)</li>
                                    <li>Sentence 3 will have a very different vector (low similarity, e.g., 0.15)</li>
                                </ul>
                                <p>This allows RAG systems to find sentence 1 or 2 when a user asks "What is France's capital?" even if the exact wording doesn't match.</p>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>What Are Embedding Models?</h3>
                            <p><strong>Embedding models are neural networks trained to convert text into dense vector representations.</strong> They learn to map semantically similar texts to nearby points in a high-dimensional vector space (typically 384-1536 dimensions).</p>
                            
                            <h4>How Embedding Models Work</h4>
                            <p>Embedding models are trained on massive text corpora (billions of sentences) to learn that:</p>
                            <ul>
                                <li>Words that appear in similar contexts should have similar vectors</li>
                                <li>Sentences with similar meanings should be close in vector space</li>
                                <li>Semantic relationships (like "king" - "man" + "woman" ‚âà "queen") should be preserved</li>
                            </ul>
                            
                            <p><strong>Training process:</strong> Models learn by predicting masked words, next sentences, or by contrasting similar vs. dissimilar sentence pairs. Through this training, they develop an internal "understanding" of language that gets encoded in the vector representations.</p>
                            
                            <h4>Types of Embedding Models</h4>
                            
                            <div class="example-box">
                                <h5>1. Sentence Transformers</h5>
                                <p><strong>What they are:</strong> Models specifically optimized for creating embeddings of entire sentences or paragraphs, not just individual words.</p>
                                <p><strong>Why we use them:</strong> They're designed for semantic similarity tasks and work excellently for RAG retrieval. They're fast, efficient, and produce high-quality embeddings.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li><strong>all-MiniLM-L6-v2:</strong> 384 dimensions, fast and efficient, good for most use cases</li>
                                    <li><strong>all-mpnet-base-v2:</strong> 768 dimensions, higher quality but slower</li>
                                    <li><strong>multi-qa-MiniLM-L6-cos-v1:</strong> Optimized for question-answering tasks</li>
                                </ul>
                                <p><strong>When to use:</strong> General-purpose RAG systems, when you need fast inference, or when working with sentence/paragraph-level documents.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. BERT-Based Embeddings</h5>
                                <p><strong>What they are:</strong> Embeddings derived from BERT (Bidirectional Encoder Representations from Transformers) models. These are contextual embeddings that consider the full sentence context.</p>
                                <p><strong>Why we use them:</strong> They capture rich contextual information and understand word meanings based on surrounding text.</p>
                                <p><strong>Examples:</strong> BERT-base, RoBERTa, DistilBERT</p>
                                <p><strong>When to use:</strong> When you need high-quality embeddings and can handle slower inference, or when working with domain-specific content.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>3. OpenAI Embeddings</h5>
                                <p><strong>What they are:</strong> Commercial embedding models provided by OpenAI via API.</p>
                                <p><strong>Why we use them:</strong> High quality, well-optimized, and easy to use via API. No need to host models yourself.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li><strong>text-embedding-ada-002:</strong> 1536 dimensions, OpenAI's current recommended model</li>
                                    <li><strong>text-embedding-3-small:</strong> 1536 dimensions, newer and more efficient</li>
                                    <li><strong>text-embedding-3-large:</strong> 3072 dimensions, highest quality</li>
                                </ul>
                                <p><strong>When to use:</strong> When you want high-quality embeddings without managing model infrastructure, or when building production systems where API costs are acceptable.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>4. Domain-Specific Embeddings</h5>
                                <p><strong>What they are:</strong> Embedding models fine-tuned on specific domains (medical, legal, scientific, etc.)</p>
                                <p><strong>Why we use them:</strong> They understand domain-specific terminology and relationships better than general models.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li>BioBERT for biomedical texts</li>
                                    <li>Legal-BERT for legal documents</li>
                                    <li>SciBERT for scientific papers</li>
                                </ul>
                                <p><strong>When to use:</strong> When working with specialized domains where general models struggle, or when domain terminology is critical for retrieval quality.</p>
                            </div>
                            
                            <h4>How to Choose an Embedding Model</h4>
                            <p><strong>Consider these factors:</strong></p>
                            <ul>
                                <li><strong>Quality vs. Speed:</strong> Larger models (768-1536 dims) are higher quality but slower. Smaller models (384 dims) are faster but may sacrifice some quality.</li>
                                <li><strong>Domain:</strong> Use domain-specific models if available for your use case.</li>
                                <li><strong>Language:</strong> For multilingual content, use multilingual models (e.g., multilingual-MiniLM).</li>
                                <li><strong>Infrastructure:</strong> API-based (OpenAI) vs. self-hosted (SentenceTransformers) - consider costs and latency.</li>
                                <li><strong>Evaluation:</strong> Test multiple models on your specific data and use cases to find the best fit.</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>What Are Vector Databases and Why Do We Need Them?</h3>
                            
                            <h4>The Problem Vector Databases Solve</h4>
                            <p>Imagine you have 1 million documents, each with a 384-dimensional embedding vector. When a user asks a question, you need to:</p>
                            <ol>
                                <li>Embed the query (get a 384-dim vector)</li>
                                <li>Compare this query vector with all 1 million document vectors</li>
                                <li>Find the top 5 most similar documents</li>
                            </ol>
                            
                            <p><strong>The challenge:</strong> Computing cosine similarity between the query and all 1 million documents would require 1 million vector operations. Even if each takes 0.001 seconds, that's 1000 seconds (16+ minutes) - way too slow for a real-time system!</p>
                            
                            <p><strong>Vector databases solve this</strong> by using specialized indexing algorithms (like HNSW - Hierarchical Navigable Small World) that can find similar vectors in milliseconds, even with millions of documents.</p>
                            
                            <h4>What Is a Vector Database?</h4>
                            <p><strong>A vector database is a specialized database designed to store and efficiently search high-dimensional vectors (embeddings).</strong> Unlike traditional databases that search by exact matches or keywords, vector databases search by similarity in vector space.</p>
                            
                            <div class="example-box">
                                <h5>Traditional Database vs. Vector Database</h5>
                                <p><strong>Traditional SQL Database:</strong></p>
                                <ul>
                                    <li>Stores: Structured data (names, dates, numbers)</li>
                                    <li>Searches: Exact matches, ranges, joins</li>
                                    <li>Query: "SELECT * WHERE name = 'John'"</li>
                                    <li><strong>Problem:</strong> Can't search by semantic similarity</li>
                                </ul>
                                
                                <p><strong>Vector Database:</strong></p>
                                <ul>
                                    <li>Stores: High-dimensional vectors (embeddings)</li>
                                    <li>Searches: Similarity search (find nearest neighbors)</li>
                                    <li>Query: "Find vectors most similar to [0.2, -0.5, 0.8, ...]"</li>
                                    <li><strong>Solution:</strong> Fast semantic similarity search</li>
                                </ul>
                            </div>
                            
                            <h4>Why Do We Need Vector Databases in RAG?</h4>
                            <p><strong>1. Speed:</strong> Vector databases use Approximate Nearest Neighbor (ANN) algorithms that can search millions of vectors in milliseconds, compared to minutes with brute-force search.</p>
                            
                            <p><strong>2. Scalability:</strong> As your knowledge base grows from thousands to millions of documents, vector databases maintain fast query times. Traditional methods would become prohibitively slow.</p>
                            
                            <p><strong>3. Efficiency:</strong> Vector databases are optimized for the specific task of similarity search. They use techniques like:</p>
                            <ul>
                                <li><strong>HNSW (Hierarchical Navigable Small World):</strong> Creates a graph structure where similar vectors are connected, enabling fast navigation to nearest neighbors</li>
                                <li><strong>IVF (Inverted File Index):</strong> Groups similar vectors into clusters, then searches only relevant clusters</li>
                                <li><strong>Product Quantization:</strong> Compresses vectors to reduce memory and speed up search</li>
                            </ul>
                            
                            <p><strong>4. Metadata Filtering:</strong> Vector databases allow you to combine similarity search with traditional filtering. For example: "Find documents similar to this query, but only from 2024, and only in the 'legal' category."</p>
                            
                            <h4>When Do We Use Vector Databases?</h4>
                            
                            <div class="example-box">
                                <h5>‚úÖ Use Vector Databases When:</h5>
                                <ul>
                                    <li><strong>Large-scale systems:</strong> You have thousands or millions of documents</li>
                                    <li><strong>Real-time requirements:</strong> You need sub-second query responses</li>
                                    <li><strong>Production systems:</strong> You need reliability, scalability, and managed infrastructure</li>
                                    <li><strong>Complex queries:</strong> You need metadata filtering combined with similarity search</li>
                                    <li><strong>Growing knowledge base:</strong> Your document collection will expand over time</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>‚ùå You Might Skip Vector Databases When:</h5>
                                <ul>
                                    <li><strong>Small datasets:</strong> You have fewer than 1,000 documents (NumPy arrays might be sufficient)</li>
                                    <li><strong>Prototyping:</strong> You're building a proof-of-concept and speed isn't critical</li>
                                    <li><strong>Simple use cases:</strong> You don't need advanced features like metadata filtering</li>
                                    <li><strong>Budget constraints:</strong> Managed vector databases have costs (though open-source options exist)</li>
                                </ul>
                            </div>
                            
                            <h4>Popular Vector Database Options</h4>
                            
                            <div class="example-box">
                                <h5>1. Pinecone</h5>
                                <p><strong>What it is:</strong> Fully managed, cloud-based vector database service</p>
                                <p><strong>Why use it:</strong> Zero infrastructure management, automatic scaling, high performance, built-in security</p>
                                <p><strong>Best for:</strong> Production systems, teams without DevOps resources, applications requiring high reliability</p>
                                <p><strong>Considerations:</strong> Paid service (though has free tier), requires internet connection</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. Weaviate</h5>
                                <p><strong>What it is:</strong> Open-source vector database with optional cloud hosting</p>
                                <p><strong>Why use it:</strong> Self-hostable, GraphQL API, built-in vectorization, good documentation</p>
                                <p><strong>Best for:</strong> Teams comfortable with self-hosting, need flexibility, want open-source solution</p>
                                <p><strong>Considerations:</strong> Requires infrastructure management if self-hosting</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>3. Chroma</h5>
                                <p><strong>What it is:</strong> Lightweight, open-source vector database designed for simplicity</p>
                                <p><strong>Why use it:</strong> Easy to use, Python-first, good for prototyping and small-to-medium scale</p>
                                <p><strong>Best for:</strong> Prototyping, Python-heavy projects, smaller datasets, getting started quickly</p>
                                <p><strong>Considerations:</strong> May not scale as well as others for very large datasets</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>4. FAISS (Facebook AI Similarity Search)</h5>
                                <p><strong>What it is:</strong> Library for efficient similarity search, not a full database</p>
                                <p><strong>Why use it:</strong> Extremely fast, open-source, used by Facebook at scale, in-memory or on-disk</p>
                                <p><strong>Best for:</strong> When you need maximum performance, have technical expertise, want to build custom solutions</p>
                                <p><strong>Considerations:</strong> Lower-level API, requires more setup, no built-in persistence (you handle it)</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>5. Qdrant</h5>
                                <p><strong>What it is:</strong> Open-source vector database with cloud option</p>
                                <p><strong>Why use it:</strong> High performance, good filtering capabilities, REST and gRPC APIs</p>
                                <p><strong>Best for:</strong> Production systems needing high performance, teams wanting open-source with cloud option</p>
                                <p><strong>Considerations:</strong> Requires infrastructure if self-hosting</p>
                            </div>
                            
                            <h4>How Vector Databases Work in RAG</h4>
                            <p><strong>Step 1 - Indexing (One-time):</strong></p>
                            <ol>
                                <li>Embed all documents using your embedding model</li>
                                <li>Store document embeddings in the vector database</li>
                                <li>Vector database builds an index (e.g., HNSW graph) for fast search</li>
                                <li>Store metadata (document ID, title, date, etc.) alongside embeddings</li>
                            </ol>
                            
                            <p><strong>Step 2 - Querying (Per Query):</strong></p>
                            <ol>
                                <li>Embed the user query</li>
                                <li>Query the vector database: "Find top-k vectors most similar to query embedding"</li>
                                <li>Vector database uses its index to quickly find similar vectors (milliseconds)</li>
                                <li>Return document IDs and metadata for the top-k matches</li>
                                <li>Retrieve actual document text using the IDs</li>
                            </ol>
                            
                            <p><strong>Performance comparison:</strong></p>
                            <ul>
                                <li><strong>Brute-force (NumPy):</strong> 1M documents = ~16 minutes</li>
                                <li><strong>Vector Database (HNSW):</strong> 1M documents = ~50-200 milliseconds</li>
                                <li><strong>Speedup:</strong> ~5,000-20,000x faster!</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Similarity and Distance Metrics Overview</h3>
                            <p>In RAG systems, we need to measure how similar two embedding vectors are. Different metrics serve different purposes and have different properties. Here are the most important ones used in production RAG systems:</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Embedding Function</h4>
                            <div class="formula-display">
                                \[E: \text{text} \rightarrow \mathbb{R}^d\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>This function maps any text input to a d-dimensional real-valued vector. The embedding model \(E\) learns this mapping during training.</p>
                                
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(E\): Embedding model (e.g., SentenceTransformer, BERT)</li>
                                    <li>\(\text{text}\): Input text string (word, sentence, or document)</li>
                                    <li>\(\mathbb{R}^d\): d-dimensional real vector space</li>
                                    <li>Typical d: 384 (fast), 768 (balanced), 1536 (high quality)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>\(E(\text{"machine learning"}) = [0.23, -0.45, 0.67, ..., 0.12] \in \mathbb{R}^{384}\)</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. Cosine Similarity (Most Common in RAG)</h4>
                            <div class="formula-display">
                                \[\text{similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|} = \cos(\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Cosine similarity measures the cosine of the angle between two vectors. It focuses on <strong>direction</strong> rather than magnitude, making it ideal for comparing embeddings where the length doesn't matter.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>\(v_1 \cdot v_2\): Dot product (sum of element-wise products)</li>
                                    <li>\(\|v_1\|\): Magnitude (norm) of vector \(v_1 = \sqrt{\sum_{i=1}^{d} v_{1i}^2}\)</li>
                                    <li>\(\|v_2\|\): Magnitude of vector \(v_2\)</li>
                                    <li>\(\theta\): Angle between the two vectors</li>
                                </ul>
                                
                                <h5>Properties:</h5>
                                <ul>
                                    <li><strong>Range:</strong> [-1, 1], typically [0, 1] for normalized embeddings</li>
                                    <li><strong>Scale-invariant:</strong> Only cares about direction, not magnitude</li>
                                    <li><strong>Interpretation:</strong> 1.0 = identical direction, 0 = orthogonal, -1 = opposite</li>
                                </ul>
                                
                                <h5>Why It's Preferred in RAG:</h5>
                                <ul>
                                    <li>Works well with normalized embeddings (most embedding models produce normalized vectors)</li>
                                    <li>Focuses on semantic similarity (direction) rather than vector magnitude</li>
                                    <li>Handles documents of different lengths well</li>
                                    <li>Fast to compute</li>
                                </ul>
                                
                                <h5>When to Use:</h5>
                                <p>‚úÖ <strong>Use cosine similarity when:</strong> Working with normalized embeddings, comparing semantic similarity, documents vary in length, or using most modern embedding models.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. Dot Product (Inner Product)</h4>
                            <div class="formula-display">
                                \[\text{dot\_product}(v_1, v_2) = v_1 \cdot v_2 = \sum_{i=1}^{d} v_{1i} \times v_{2i}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>The dot product is the sum of element-wise products of two vectors. It measures both direction AND magnitude, unlike cosine similarity which only measures direction.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>For each dimension \(i\), multiply \(v_{1i} \times v_{2i}\)</li>
                                    <li>Sum all these products: \(\sum_{i=1}^{d} v_{1i} \times v_{2i}\)</li>
                                    <li>Result is a scalar value (single number)</li>
                                </ul>
                                
                                <h5>Properties:</h5>
                                <ul>
                                    <li><strong>Range:</strong> Unbounded (can be any real number)</li>
                                    <li><strong>Magnitude-sensitive:</strong> Larger vectors produce larger dot products</li>
                                    <li><strong>Relationship to cosine:</strong> If vectors are normalized, dot product = cosine similarity</li>
                                </ul>
                                
                                <h5>When to Use:</h5>
                                <ul>
                                    <li>‚úÖ <strong>Use dot product when:</strong> Embeddings are already normalized (then it's equivalent to cosine), you need maximum speed (slightly faster than cosine), or using FAISS with inner product index</li>
                                    <li>‚ùå <strong>Avoid when:</strong> Embeddings aren't normalized, or you want magnitude-independent similarity</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>If \(v_1 = [0.5, 0.3, 0.8]\) and \(v_2 = [0.4, 0.6, 0.7]\), then:</p>
                                <p>\(v_1 \cdot v_2 = (0.5 \times 0.4) + (0.3 \times 0.6) + (0.8 \times 0.7) = 0.2 + 0.18 + 0.56 = 0.94\)</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Euclidean Distance (L2 Distance)</h4>
                            <div class="formula-display">
                                \[d_{\text{euclidean}}(v_1, v_2) = \|v_1 - v_2\|_2 = \sqrt{\sum_{i=1}^{d} (v_{1i} - v_{2i})^2}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Euclidean distance measures the straight-line distance between two points in vector space. It's the most intuitive distance measure - like measuring distance on a map.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>For each dimension \(i\), compute the difference: \(v_{1i} - v_{2i}\)</li>
                                    <li>Square each difference: \((v_{1i} - v_{2i})^2\)</li>
                                    <li>Sum all squared differences: \(\sum_{i=1}^{d} (v_{1i} - v_{2i})^2\)</li>
                                    <li>Take the square root: \(\sqrt{\sum_{i=1}^{d} (v_{1i} - v_{2i})^2}\)</li>
                                </ul>
                                
                                <h5>Properties:</h5>
                                <ul>
                                    <li><strong>Range:</strong> [0, ‚àû) - always non-negative</li>
                                    <li><strong>Interpretation:</strong> Lower distance = more similar vectors</li>
                                    <li><strong>Magnitude-sensitive:</strong> Affected by vector magnitudes</li>
                                    <li><strong>Metric:</strong> Satisfies triangle inequality</li>
                                </ul>
                                
                                <h5>Relationship to Cosine Similarity:</h5>
                                <p>For normalized vectors, Euclidean distance and cosine similarity are related:</p>
                                <p>If \(\|v_1\| = \|v_2\| = 1\), then: \(d_{\text{euclidean}}^2 = 2(1 - \cos(\theta))\)</p>
                                <p>This means: <strong>lower Euclidean distance = higher cosine similarity</strong> (for normalized vectors)</p>
                                
                                <h5>When to Use:</h5>
                                <ul>
                                    <li>‚úÖ <strong>Use Euclidean distance when:</strong> You want to consider both direction and magnitude, working with non-normalized embeddings, or using clustering algorithms that require distance metrics</li>
                                    <li>‚ùå <strong>Less common in RAG:</strong> Cosine similarity is usually preferred for semantic search</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>If \(v_1 = [1, 2, 3]\) and \(v_2 = [4, 5, 6]\), then:</p>
                                <p>\(d = \sqrt{(1-4)^2 + (2-5)^2 + (3-6)^2} = \sqrt{9 + 9 + 9} = \sqrt{27} \approx 5.20\)</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Manhattan Distance (L1 Distance)</h4>
                            <div class="formula-display">
                                \[d_{\text{manhattan}}(v_1, v_2) = \|v_1 - v_2\|_1 = \sum_{i=1}^{d} |v_{1i} - v_{2i}|\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Manhattan distance (also called L1 distance or taxicab distance) measures the sum of absolute differences along each dimension. It's like measuring distance in a city with a grid layout - you can only move along streets, not diagonally.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li>For each dimension \(i\), compute the absolute difference: \(|v_{1i} - v_{2i}|\)</li>
                                    <li>Sum all absolute differences: \(\sum_{i=1}^{d} |v_{1i} - v_{2i}|\)</li>
                                    <li>No squaring or square root needed</li>
                                </ul>
                                
                                <h5>Properties:</h5>
                                <ul>
                                    <li><strong>Range:</strong> [0, ‚àû) - always non-negative</li>
                                    <li><strong>Interpretation:</strong> Lower distance = more similar</li>
                                    <li><strong>Robust to outliers:</strong> Less sensitive to large differences in individual dimensions (compared to Euclidean)</li>
                                    <li><strong>Faster computation:</strong> No squaring or square root operations</li>
                                </ul>
                                
                                <h5>When to Use:</h5>
                                <ul>
                                    <li>‚úÖ <strong>Use Manhattan distance when:</strong> You have sparse embeddings, want robustness to outliers, need very fast computation, or working with high-dimensional data where Euclidean distance becomes less meaningful</li>
                                    <li>‚ùå <strong>Less common in RAG:</strong> Cosine similarity is typically preferred for semantic similarity</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>If \(v_1 = [1, 2, 3]\) and \(v_2 = [4, 5, 6]\), then:</p>
                                <p>\(d = |1-4| + |2-5| + |3-6| = 3 + 3 + 3 = 9\)</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Minkowski Distance (Generalization)</h4>
                            <div class="formula-display">
                                \[d_{\text{minkowski}}(v_1, v_2, p) = \left(\sum_{i=1}^{d} |v_{1i} - v_{2i}|^p\right)^{1/p}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>Minkowski distance is a generalization that includes both Euclidean and Manhattan distances as special cases. The parameter \(p\) controls the type of distance.</p>
                                
                                <h5>Special Cases:</h5>
                                <ul>
                                    <li><strong>\(p = 1\):</strong> Manhattan distance (L1): \(d = \sum |v_{1i} - v_{2i}|\)</li>
                                    <li><strong>\(p = 2\):</strong> Euclidean distance (L2): \(d = \sqrt{\sum (v_{1i} - v_{2i})^2}\)</li>
                                    <li><strong>\(p \to \infty\):</strong> Chebyshev distance (L‚àû): \(d = \max_i |v_{1i} - v_{2i}|\)</li>
                                </ul>
                                
                                <h5>Properties:</h5>
                                <ul>
                                    <li><strong>Flexibility:</strong> Can tune \(p\) to balance between Manhattan (p=1) and Euclidean (p=2) behaviors</li>
                                    <li><strong>Higher p:</strong> More emphasis on large differences in individual dimensions</li>
                                    <li><strong>Lower p:</strong> More robust to outliers, treats all dimensions more equally</li>
                                </ul>
                                
                                <h5>When to Use:</h5>
                                <ul>
                                    <li>‚úÖ <strong>Use Minkowski distance when:</strong> You need to experiment with different distance metrics, want to tune the sensitivity to outliers, or working on research/optimization problems</li>
                                    <li>‚ùå <strong>Less common in production RAG:</strong> Cosine similarity is the standard choice</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>6. Normalized Dot Product (For Non-Normalized Embeddings)</h4>
                            <div class="formula-display">
                                \[\text{normalized\_dot}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|} = \cos(\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Is:</h5>
                                <p>This is actually the same as cosine similarity! When you normalize the dot product by dividing by the vector magnitudes, you get cosine similarity. This formula shows the relationship explicitly.</p>
                                
                                <h5>Key Insight:</h5>
                                <p>If your embeddings are already normalized (most modern models produce normalized embeddings), then:</p>
                                <ul>
                                    <li>\(\|v_1\| = \|v_2\| = 1\)</li>
                                    <li>Normalized dot product = dot product = cosine similarity</li>
                                    <li>This is why many vector databases use dot product internally (it's faster) when embeddings are normalized</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Which Metric Should You Use in RAG?</h3>
                            
                            <div class="example-box">
                                <h5>ü•á Recommended: Cosine Similarity</h5>
                                <p><strong>Why:</strong> Most embedding models produce normalized vectors, and cosine similarity focuses on semantic direction rather than magnitude. It's the standard in production RAG systems.</p>
                                <p><strong>When:</strong> Default choice for most RAG applications, especially with SentenceTransformers, OpenAI embeddings, or BERT-based models.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>ü•à Alternative: Dot Product (for normalized embeddings)</h5>
                                <p><strong>Why:</strong> Mathematically equivalent to cosine similarity for normalized vectors, but slightly faster to compute (no division needed).</p>
                                <p><strong>When:</strong> When embeddings are guaranteed to be normalized and you need maximum speed, or when using FAISS with inner product index.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>ü•â Special Cases: Euclidean or Manhattan</h5>
                                <p><strong>Why:</strong> Useful when magnitude matters, or when working with non-normalized embeddings.</p>
                                <p><strong>When:</strong> Clustering applications, when vector magnitude carries important information, or when experimenting with different metrics.</p>
                            </div>
                            
                            <h4>Performance Comparison</h4>
                            <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                <thead>
                                    <tr style="background: var(--alpine-oat);">
                                        <th style="padding: 0.75rem; border: 1px solid var(--dill-green); text-align: left;">Metric</th>
                                        <th style="padding: 0.75rem; border: 1px solid var(--dill-green); text-align: left;">Speed</th>
                                        <th style="padding: 0.75rem; border: 1px solid var(--dill-green); text-align: left;">Magnitude-Sensitive</th>
                                        <th style="padding: 0.75rem; border: 1px solid var(--dill-green); text-align: left;">Common in RAG</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);"><strong>Cosine Similarity</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Fast</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">No</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">‚úÖ Most common</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);"><strong>Dot Product</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Fastest</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Yes (unless normalized)</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">‚úÖ Common (when normalized)</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);"><strong>Euclidean Distance</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Medium</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Yes</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">‚ö†Ô∏è Less common</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);"><strong>Manhattan Distance</strong></td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Fast</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">Yes</td>
                                        <td style="padding: 0.75rem; border: 1px solid var(--dill-green);">‚ùå Rare</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example 1: Converting Text to Embeddings - Step by Step</h4>
                            <p><strong>Scenario:</strong> You have a document about machine learning and want to convert it to an embedding vector for storage in a vector database.</p>
                            
                            <p><strong>Input text:</strong> "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming."</p>
                            
                            <p><strong>Step 1: Preprocessing</strong></p>
                            <ul>
                                <li>Text is cleaned and normalized (handling special characters, whitespace)</li>
                                <li>Result: Clean text ready for tokenization</li>
                            </ul>
                            
                            <p><strong>Step 2: Tokenization</strong></p>
                            <ul>
                                <li>The sentence is split into tokens (subwords or words depending on the model)</li>
                                <li>Example tokens: ["Machine", "learning", "is", "a", "subset", "of", "artificial", "intelligence", ...]</li>
                                <li>Each token is mapped to a token ID from the model's vocabulary</li>
                            </ul>
                            
                            <p><strong>Step 3: Model Processing</strong></p>
                            <ul>
                                <li>Token IDs are passed through the transformer model (e.g., SentenceTransformer)</li>
                                <li>The model processes the entire sentence, using attention mechanisms to understand relationships between words</li>
                                <li>Hidden states are generated for each token position</li>
                            </ul>
                            
                            <p><strong>Step 4: Pooling</strong></p>
                            <ul>
                                <li>Token-level embeddings are pooled (typically mean pooling) to create a single sentence-level embedding</li>
                                <li>This creates a fixed-size vector regardless of sentence length</li>
                            </ul>
                            
                            <p><strong>Step 5: Normalization</strong></p>
                            <ul>
                                <li>The embedding vector is normalized (L2 normalization) so its magnitude is 1</li>
                                <li>This makes cosine similarity equivalent to dot product</li>
                            </ul>
                            
                            <p><strong>Final Output:</strong></p>
                            <ul>
                                <li>384-dimensional vector: [0.23, -0.45, 0.67, 0.12, -0.34, ..., 0.89]</li>
                                <li>Each dimension captures some aspect of semantic meaning</li>
                                <li>This vector can now be stored in a vector database</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 2: Semantic Similarity in Action</h4>
                            <p><strong>Scenario:</strong> Demonstrating how embeddings capture semantic meaning, not just word matching.</p>
                            
                            <p><strong>Document 1:</strong> "The capital of France is Paris"<br>
                            <strong>Embedding:</strong> [0.24, -0.44, 0.66, 0.12, ...]</p>
                            
                            <p><strong>Document 2:</strong> "Paris is the capital city of France"<br>
                            <strong>Embedding:</strong> [0.25, -0.43, 0.65, 0.11, ...]</p>
                            
                            <p><strong>Document 3:</strong> "The weather in Paris is sunny today"<br>
                            <strong>Embedding:</strong> [0.15, 0.22, -0.18, 0.45, ...]</p>
                            
                            <p><strong>Query:</strong> "What is the capital of France?"<br>
                            <strong>Query Embedding:</strong> [0.24, -0.44, 0.66, 0.12, ...]</p>
                            
                            <p><strong>Similarity Calculations:</strong></p>
                            <ul>
                                <li><strong>Query vs Doc 1:</strong> Cosine similarity = 0.98 (almost identical - same meaning, different word order)</li>
                                <li><strong>Query vs Doc 2:</strong> Cosine similarity = 0.97 (very similar - paraphrased but same meaning)</li>
                                <li><strong>Query vs Doc 3:</strong> Cosine similarity = 0.42 (different topic - about weather, not capitals)</li>
                            </ul>
                            
                            <p><strong>Key Insight:</strong> Even though Doc 1 and Doc 2 use different word orders and slightly different phrasing, they have very similar embeddings because they convey the same semantic meaning. Doc 3 has a different embedding because it's about a different topic (weather vs. geography).</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 3: Complete Embedding and Retrieval Workflow</h4>
                            <p><strong>Scenario:</strong> Building a knowledge base and querying it using embeddings.</p>
                            
                            <p><strong>Step 1: Document Indexing</strong></p>
                            <p>You have 3 documents to index:</p>
                            <ul>
                                <li>Doc 1: "Machine learning uses algorithms to learn from data"</li>
                                <li>Doc 2: "Deep learning is a subset of machine learning using neural networks"</li>
                                <li>Doc 3: "Python is a popular programming language for data science"</li>
                            </ul>
                            
                            <p><strong>Step 2: Generate Embeddings</strong></p>
                            <ul>
                                <li>Doc 1 embedding: [0.45, -0.23, 0.67, ..., 0.12]</li>
                                <li>Doc 2 embedding: [0.48, -0.25, 0.65, ..., 0.14]</li>
                                <li>Doc 3 embedding: [0.12, 0.34, -0.21, ..., -0.45]</li>
                            </ul>
                            
                            <p><strong>Step 3: Store in Vector Database</strong></p>
                            <ul>
                                <li>All three embeddings are stored in the vector database with their document IDs</li>
                                <li>An index (e.g., HNSW) is built for fast similarity search</li>
                            </ul>
                            
                            <p><strong>Step 4: Query Processing</strong></p>
                            <p>User asks: "What is machine learning?"</p>
                            <ul>
                                <li>Query embedding: [0.46, -0.24, 0.66, ..., 0.13]</li>
                            </ul>
                            
                            <p><strong>Step 5: Similarity Search</strong></p>
                            <ul>
                                <li>Compare query embedding with all document embeddings:</li>
                                <li>Query vs Doc 1: similarity = 0.95 (very high - directly about machine learning)</li>
                                <li>Query vs Doc 2: similarity = 0.92 (high - related, mentions machine learning)</li>
                                <li>Query vs Doc 3: similarity = 0.35 (low - about Python, not machine learning)</li>
                            </ul>
                            
                            <p><strong>Step 6: Top-k Retrieval</strong></p>
                            <ul>
                                <li>Retrieve top-2: Doc 1 and Doc 2 (highest similarity scores)</li>
                                <li>These documents are passed to the LLM as context</li>
                            </ul>
                            
                            <p><strong>Result:</strong> The system successfully finds documents about machine learning, even though the query uses slightly different wording than the documents.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 4: Embedding Dimension Comparison</h4>
                            <p><strong>Scenario:</strong> Comparing embeddings of different dimensions to understand the trade-offs.</p>
                            
                            <p><strong>Same text:</strong> "Neural networks are used for deep learning"</p>
                            
                            <p><strong>384-dimensional embedding (all-MiniLM-L6-v2):</strong></p>
                            <ul>
                                <li>Size: 384 values</li>
                                <li>Storage: ~1.5 KB per embedding</li>
                                <li>Speed: Fast (quick to compute and compare)</li>
                                <li>Quality: Good for most use cases</li>
                                <li>Example: [0.23, -0.45, 0.67, ..., 0.12] (384 values)</li>
                            </ul>
                            
                            <p><strong>768-dimensional embedding (all-mpnet-base-v2):</strong></p>
                            <ul>
                                <li>Size: 768 values</li>
                                <li>Storage: ~3 KB per embedding (2x larger)</li>
                                <li>Speed: Slower (more computation)</li>
                                <li>Quality: Higher (better semantic understanding)</li>
                                <li>Example: [0.23, -0.45, 0.67, 0.12, ..., 0.89] (768 values)</li>
                            </ul>
                            
                            <p><strong>1536-dimensional embedding (OpenAI text-embedding-ada-002):</strong></p>
                            <ul>
                                <li>Size: 1536 values</li>
                                <li>Storage: ~6 KB per embedding (4x larger than 384-dim)</li>
                                <li>Speed: Slower (requires API call)</li>
                                <li>Quality: Highest (best semantic understanding)</li>
                                <li>Example: [0.23, -0.45, 0.67, ..., 0.12] (1536 values)</li>
                            </ul>
                            
                            <p><strong>Trade-off Analysis:</strong></p>
                            <p>For 1 million documents:</p>
                            <ul>
                                <li>384-dim: 1.5 GB storage, fast search, good quality</li>
                                <li>768-dim: 3 GB storage, slower search, better quality</li>
                                <li>1536-dim: 6 GB storage, slowest (API), best quality</li>
                            </ul>
                            
                            <p><strong>Recommendation:</strong> Start with 384-dim for speed, upgrade to 768-dim if quality is insufficient, use 1536-dim only if quality is critical and you can afford the cost/latency.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example 5: Handling Synonyms and Paraphrasing</h4>
                            <p><strong>Scenario:</strong> Demonstrating how embeddings handle synonyms and different phrasings.</p>
                            
                            <p><strong>Query:</strong> "How do I train a neural network?"</p>
                            
                            <p><strong>Document 1:</strong> "Training deep learning models requires adjusting hyperparameters"<br>
                            <strong>Note:</strong> Uses "deep learning models" instead of "neural network", and "training" instead of "train"</p>
                            
                            <p><strong>Document 2:</strong> "Neural networks are trained using backpropagation"<br>
                            <strong>Note:</strong> Uses exact phrase "neural network" and "trained"</p>
                            
                            <p><strong>Document 3:</strong> "The weather forecast predicts rain tomorrow"<br>
                            <strong>Note:</strong> Completely unrelated topic</p>
                            
                            <p><strong>Similarity Results:</strong></p>
                            <ul>
                                <li>Query vs Doc 1: 0.88 (high - understands "deep learning models" ‚âà "neural network")</li>
                                <li>Query vs Doc 2: 0.92 (very high - exact terminology match)</li>
                                <li>Query vs Doc 3: 0.15 (very low - completely different topic)</li>
                            </ul>
                            
                            <p><strong>Key Insight:</strong> Embeddings understand that "neural network" and "deep learning model" are semantically similar concepts, so Doc 1 is retrieved even though it doesn't contain the exact phrase "neural network". This is the power of semantic search - it finds relevant documents even when they use different terminology.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="explanation-box">
                            <h3>Implementation Overview</h3>
                            <p>This section provides practical Python code examples for creating embeddings and using vector databases in RAG systems. The examples use popular libraries: <strong>SentenceTransformers</strong> for embeddings and <strong>Chroma</strong> for vector storage.</p>
                        </div>
                        
                        <div class="code-box">
                            <h4>1. Creating Embeddings with SentenceTransformers</h4>
                            <p><strong>What this does:</strong> Converts text documents into dense vector embeddings that capture semantic meaning, enabling similarity search.</p>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np

# Load embedding model
# 'all-MiniLM-L6-v2' is a popular, fast model (384 dimensions)
# Other options: 'all-mpnet-base-v2' (768 dims, higher quality)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings for multiple documents
texts = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy is a country in Europe"
]

# Encode all texts at once (batch processing is more efficient)
embeddings = model.encode(texts, show_progress_bar=True)
print(f"Embedding shape: {embeddings.shape}")  # (3, 384)
# Output: 3 documents, each with 384-dimensional vector

# Create query embedding
query = "What is the capital of France?"
query_embedding = model.encode([query])  # Shape: (1, 384)

# Compute cosine similarities
# For normalized embeddings, dot product = cosine similarity
similarities = np.dot(embeddings, query_embedding.T).flatten()
print(f"Similarities: {similarities}")
# Output: [0.95, 0.45, 0.32] (example values)
# Higher values = more similar

# Get most similar document
most_similar_idx = np.argmax(similarities)
print(f"Most similar: {texts[most_similar_idx]}")
# Output: "The capital of France is Paris"

# Get top-k most similar
top_k = 2
top_k_indices = np.argsort(similarities)[-top_k:][::-1]  # Sort descending
print(f"Top {top_k} most similar:")
for idx in top_k_indices:
    print(f"  {texts[idx]} (similarity: {similarities[idx]:.3f})")</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Model selection:</strong> <code>all-MiniLM-L6-v2</code> is fast and good for most use cases. Use <code>all-mpnet-base-v2</code> for higher quality.</li>
                                    <li><strong>Batch encoding:</strong> Always encode multiple texts together for efficiency (faster than one-by-one).</li>
                                    <li><strong>Normalized embeddings:</strong> SentenceTransformers produces normalized embeddings, so dot product equals cosine similarity.</li>
                                    <li><strong>Similarity scores:</strong> Range from -1 to 1, typically 0 to 1 for normalized embeddings. Higher = more similar.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>2. Using Vector Database (Chroma)</h4>
                            <p><strong>What this does:</strong> Stores document embeddings in a vector database for efficient similarity search across large document collections.</p>
                            <pre><code class="language-python">import chromadb
from chromadb.config import Settings

# Initialize Chroma client
# For production, use PersistentClient to save data to disk
client = chromadb.PersistentClient(path="./chroma_db")

# Or use in-memory client for testing
# client = chromadb.Client(Settings())

# Create or get a collection
# Collections are like tables in traditional databases
collection = client.get_or_create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}  # Use cosine similarity
)

# Add documents with metadata
documents = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy's capital is Rome"
]

# Metadata for filtering (optional but recommended)
metadatas = [
    {"country": "France", "type": "capital"},
    {"country": "Germany", "type": "capital"},
    {"country": "Italy", "type": "capital"}
]

ids = ["doc1", "doc2", "doc3"]

# Add to collection
# Chroma automatically generates embeddings using default model
# Or you can provide your own embeddings
collection.add(
    documents=documents,
    metadatas=metadatas,
    ids=ids
)

# Query the collection
query = "What is the capital of France?"
results = collection.query(
    query_texts=[query],
    n_results=2,  # Return top 2 most similar
    # Optional: filter by metadata
    # where={"country": "France"}
)

# Access results
print("Retrieved documents:", results['documents'][0])
print("Similarity distances:", results['distances'][0])
print("Document IDs:", results['ids'][0])
print("Metadata:", results['metadatas'][0])

# Output example:
# Retrieved documents: ['The capital of France is Paris', 'Germany's capital is Berlin']
# Similarity distances: [0.12, 0.45]  # Lower = more similar (distance, not similarity)
# Document IDs: ['doc1', 'doc2']
# Metadata: [{'country': 'France', 'type': 'capital'}, {'country': 'Germany', 'type': 'capital'}]</code></pre>
                            
                            <div class="example-box">
                                <h5>Key Points:</h5>
                                <ul>
                                    <li><strong>Persistent storage:</strong> Use <code>PersistentClient</code> in production to save data to disk.</li>
                                    <li><strong>Collections:</strong> Organize documents into collections (like tables).</li>
                                    <li><strong>Metadata filtering:</strong> Store metadata (date, category, etc.) to enable filtering before similarity search.</li>
                                    <li><strong>Automatic embeddings:</strong> Chroma can generate embeddings automatically, or you can provide your own.</li>
                                    <li><strong>Distance vs similarity:</strong> Chroma returns distances (lower = more similar), not similarity scores.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="code-box">
                            <h4>3. Complete RAG Example: Embedding + Vector DB + Similarity Search</h4>
                            <p><strong>What this does:</strong> A complete example combining embedding generation, vector database storage, and similarity search for a RAG system.</p>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
import chromadb
import numpy as np

# Step 1: Initialize embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Step 2: Initialize vector database
client = chromadb.PersistentClient(path="./rag_db")
collection = client.get_or_create_collection(
    name="knowledge_base",
    metadata={"hnsw:space": "cosine"}
)

# Step 3: Prepare documents (in real RAG, these come from your knowledge base)
documents = [
    "Machine learning is a subset of artificial intelligence.",
    "Deep learning uses neural networks with multiple layers.",
    "Natural language processing enables computers to understand text.",
    "Computer vision allows machines to interpret visual information."
]

# Step 4: Generate embeddings
embeddings = embedding_model.encode(documents, show_progress_bar=True)

# Step 5: Add to vector database with custom embeddings
ids = [f"doc_{i}" for i in range(len(documents))]
collection.add(
    embeddings=embeddings.tolist(),  # Convert numpy array to list
    documents=documents,
    ids=ids
)

# Step 6: Query the knowledge base
user_query = "What is machine learning?"
query_embedding = embedding_model.encode([user_query])

# Retrieve top-k most similar documents
results = collection.query(
    query_embeddings=query_embedding.tolist(),
    n_results=2
)

# Step 7: Use retrieved context for RAG
retrieved_docs = results['documents'][0]
print("Retrieved context:")
for i, doc in enumerate(retrieved_docs, 1):
    print(f"{i}. {doc}")

# In a real RAG system, you would:
# 1. Combine retrieved_docs into a prompt
# 2. Send prompt + query to LLM (OpenAI, Anthropic, etc.)
# 3. Return LLM's generated answer

# Example prompt construction:
context = "\n\n".join(retrieved_docs)
prompt = f"""Context:
{context}

Question: {user_query}

Answer:"""
print("\nConstructed prompt:")
print(prompt)</code></pre>
                            
                            <div class="example-box">
                                <h5>Complete RAG Pipeline:</h5>
                                <ol>
                                    <li><strong>Document Processing:</strong> Load and chunk documents from your knowledge base</li>
                                    <li><strong>Embedding Generation:</strong> Convert chunks to embeddings (this example)</li>
                                    <li><strong>Vector Storage:</strong> Store embeddings in vector database (this example)</li>
                                    <li><strong>Query Processing:</strong> Embed user query and retrieve similar documents (this example)</li>
                                    <li><strong>Context Assembly:</strong> Combine retrieved documents into prompt (this example)</li>
                                    <li><strong>LLM Generation:</strong> Send prompt to LLM to generate answer (not shown - requires LLM API)</li>
                                </ol>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Installation Requirements</h3>
                            <p>To run these examples, install the required packages:</p>
                            <pre><code class="language-bash">pip install sentence-transformers chromadb numpy</code></pre>
                            
                            <h4>Alternative Vector Databases</h4>
                            <p>While this example uses Chroma, you can use other vector databases with similar APIs:</p>
                            <ul>
                                <li><strong>Pinecone:</strong> Managed cloud service, easy setup, good for production</li>
                                <li><strong>Weaviate:</strong> Self-hosted or cloud, supports GraphQL, good for complex queries</li>
                                <li><strong>Qdrant:</strong> Fast, supports filtering, good for high-performance applications</li>
                                <li><strong>FAISS:</strong> Facebook's library, good for research and self-hosted solutions</li>
                            </ul>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Embeddings in RAG</h3>
                            <p><strong>Document indexing:</strong></p>
                            <ul>
                                <li>Convert all documents to embeddings</li>
                                <li>Store in vector database</li>
                                <li>Enable fast semantic search</li>
                            </ul>
                            
                            <p><strong>Query processing:</strong></p>
                            <ul>
                                <li>Convert user query to embedding</li>
                                <li>Find similar document embeddings</li>
                                <li>Retrieve top-k most relevant documents</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong></p>
                            <ul>
                                <li>Semantic understanding (not just keyword matching)</li>
                                <li>Handles synonyms and paraphrasing</li>
                                <li>Fast similarity search</li>
                                <li>Scalable to millions of documents</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Choosing Embedding Models</h3>
                            <p><strong>Considerations:</strong></p>
                            <ul>
                                <li><strong>Dimension:</strong> Higher = more capacity but slower</li>
                                <li><strong>Domain:</strong> Use domain-specific models when available</li>
                                <li><strong>Language:</strong> Multilingual models for multiple languages</li>
                                <li><strong>Speed:</strong> Smaller models are faster</li>
                                <li><strong>Quality:</strong> Evaluate on your specific use case</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are text embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Text compression algorithms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Text formatting methods</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) Dense vector representations of text that capture semantic meaning, allowing similar texts to have similar vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Text storage formats</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What is the difference between dense and sparse embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Dense is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) Dense embeddings are learned vector representations (e.g., 384-dim) that capture semantic meaning. Sparse embeddings are high-dimensional with mostly zeros, based on word frequencies (e.g., TF-IDF, BM25)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Sparse is better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They are the same</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is cosine similarity used for in embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Training embedding models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Generating embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Storing embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Measuring semantic similarity between embedding vectors by computing the cosine of the angle between them (range: -1 to 1)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What are popular embedding models used in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Only GPT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only BERT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">C) OpenAI text-embedding-ada-002, Sentence-BERT (all-MiniLM-L6-v2), and other transformer-based models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only word2vec</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Interview question: "How do you choose the right embedding dimension?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Always use highest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use lowest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dimension doesn't matter</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Balance between quality (higher dim = better) and efficiency (lower dim = faster, less storage). Common: 384-1536 dimensions. Consider model capabilities, storage costs, and retrieval speed requirements</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What does the cosine similarity formula \(\cos(\theta) = \frac{q \cdot d}{\|q\| \|d\|}\) represent?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Euclidean distance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) The cosine of the angle between query vector q and document vector d, normalized by their magnitudes</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dot product</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Vector addition</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Why do similar texts get similar embedding vectors?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) They use the same words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They have the same length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Random chance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Embedding models are trained to map semantically similar texts to nearby points in vector space, capturing meaning rather than exact word matching</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "How would you evaluate embedding quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Only check embedding dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No evaluation needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Use semantic similarity benchmarks (STS, SICK), test on domain-specific tasks, measure retrieval performance (precision@k, recall@k), and evaluate on downstream RAG tasks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the advantage of using pre-trained embedding models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) They are always better than custom models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">B) They capture general semantic knowledge from large text corpora, work well out-of-the-box, and don't require training on your specific data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They are faster to train</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They use less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is the typical embedding dimension range used in production RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) 10-50 dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) 10000+ dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dimension doesn't matter</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) 384-1536 dimensions, with 384-768 being common for efficiency and 1536 for higher quality</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: Interview question: "How do you handle out-of-vocabulary words in embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Skip those words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Use random vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) OOV words don't exist</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Modern embedding models (subword tokenization) handle OOV words by breaking them into subwords. For truly unknown tokens, models use special UNK tokens or character-level embeddings</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the relationship between embedding quality and RAG performance?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">A) Embedding quality doesn't affect RAG</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only LLM quality matters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Embeddings are optional</div>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">D) Better embeddings lead to more accurate semantic retrieval, which improves RAG answer quality. Embedding quality directly impacts retrieval precision and recall</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/rag/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 1</a>
                <a href="/tutorials/rag/chapter3" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 3 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
