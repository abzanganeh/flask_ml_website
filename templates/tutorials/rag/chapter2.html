<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Text Embeddings & Vector Representations - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 2: Text Embeddings & Vector Representations</h1>
                <p class="chapter-subtitle">Converting Text to Vectors</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="28"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn active">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand text embeddings & vector representations fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Text Embeddings & Vector Representations</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Converting Text to Vectors</strong></p>
                            <p>This chapter provides comprehensive coverage of text embeddings & vector representations, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding text embeddings & vector representations is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Embeddings</h3>
                            <p><strong>What are embeddings:</strong> Dense vector representations of text that capture semantic meaning. Similar texts have similar vectors.</p>
                            
                            <p><strong>Properties:</strong></p>
                            <ul>
                                <li>Fixed dimension (e.g., 384, 768, 1536)</li>
                                <li>Semantic similarity = vector similarity</li>
                                <li>Can perform arithmetic (e.g., "king" - "man" + "woman" ‚âà "queen")</li>
                                <li>Enable efficient similarity search</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Embedding Models</h3>
                            <p><strong>Types of embedding models:</strong></p>
                            <ul>
                                <li><strong>Sentence transformers:</strong> Optimized for sentence-level embeddings (e.g., all-MiniLM-L6-v2)</li>
                                <li><strong>BERT-based:</strong> Contextual embeddings from transformer models</li>
                                <li><strong>OpenAI embeddings:</strong> text-embedding-ada-002 (1536 dimensions)</li>
                                <li><strong>Domain-specific:</strong> Trained on specific domains (medical, legal, etc.)</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Vector Databases</h3>
                            <p><strong>Purpose:</strong> Store and efficiently search large collections of embeddings.</p>
                            
                            <p><strong>Key features:</strong></p>
                            <ul>
                                <li>Fast similarity search (ANN - Approximate Nearest Neighbors)</li>
                                <li>Scalable to millions of vectors</li>
                                <li>Metadata filtering</li>
                                <li>Examples: Pinecone, Weaviate, Chroma, Qdrant, FAISS</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Embedding Function</h4>
                            <div class="formula-display">
                                \[E: \text{text} \rightarrow \mathbb{R}^d\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(E\): Embedding model (e.g., sentence transformer)</li>
                                    <li>\(\text{text}\): Input text string</li>
                                    <li>\(\mathbb{R}^d\): d-dimensional real vector</li>
                                    <li>Typical d: 384, 768, 1536</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Cosine Similarity</h4>
                            <div class="formula-display">
                                \[\text{similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|} = \cos(\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(v_1, v_2\): Embedding vectors</li>
                                    <li>\(\theta\): Angle between vectors</li>
                                    <li>Range: [-1, 1] (typically [0, 1] for normalized embeddings)</li>
                                    <li>Higher value = more similar</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Euclidean Distance</h4>
                            <div class="formula-display">
                                \[d(v_1, v_2) = \|v_1 - v_2\| = \sqrt{\sum_{i=1}^{d} (v_{1i} - v_{2i})^2}\]
                            </div>
                            <div class="formula-explanation">
                                <p>Alternative similarity measure. Lower distance = more similar. For normalized embeddings, cosine similarity is often preferred.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Text to Embedding</h4>
                            <p><strong>Input text:</strong> "The capital of France is Paris"</p>
                            
                            <p><strong>Embedding process:</strong></p>
                            <ul>
                                <li>Tokenize: ["The", "capital", "of", "France", "is", "Paris"]</li>
                                <li>Pass through transformer model</li>
                                <li>Output: [0.23, -0.45, 0.67, ..., 0.12] (384-dimensional vector)</li>
                            </ul>
                            
                            <p><strong>Similar texts get similar vectors:</strong></p>
                            <ul>
                                <li>"Paris is the capital of France" ‚Üí [0.25, -0.43, 0.65, ..., 0.11] (very similar!)</li>
                                <li>"The weather is nice today" ‚Üí [0.12, 0.34, -0.21, ..., -0.45] (different)</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Similarity Search</h4>
                            <p><strong>Query:</strong> "What is the capital of France?"</p>
                            
                            <p><strong>Step 1: Embed query</strong></p>
                            <ul>
                                <li>Query embedding: [0.24, -0.44, 0.66, ...]</li>
                            </ul>
                            
                            <p><strong>Step 2: Compare with document embeddings</strong></p>
                            <ul>
                                <li>Doc 1: "France's capital is Paris" ‚Üí similarity: 0.95</li>
                                <li>Doc 2: "Germany's capital is Berlin" ‚Üí similarity: 0.45</li>
                                <li>Doc 3: "Italy is a country" ‚Üí similarity: 0.32</li>
                            </ul>
                            
                            <p><strong>Step 3: Retrieve top-k</strong></p>
                            <ul>
                                <li>Top-1: Doc 1 (highest similarity)</li>
                            </ul>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Creating Embeddings</h4>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings
texts = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy is a country in Europe"
]

embeddings = model.encode(texts)
print(f"Embedding shape: {embeddings.shape}")  # (3, 384)

# Query embedding
query = "What is the capital of France?"
query_embedding = model.encode([query])

# Compute similarities
similarities = np.dot(embeddings, query_embedding.T).flatten()
print(f"Similarities: {similarities}")

# Get most similar
most_similar_idx = np.argmax(similarities)
print(f"Most similar: {texts[most_similar_idx]}")</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Using Vector Database (Chroma)</h4>
                            <pre><code class="language-python">import chromadb
from chromadb.config import Settings

# Initialize Chroma
client = chromadb.Client(Settings())

# Create collection
collection = client.create_collection(name="documents")

# Add documents
documents = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy's capital is Rome"
]

collection.add(
    documents=documents,
    ids=["doc1", "doc2", "doc3"]
)

# Query
query = "What is the capital of France?"
results = collection.query(
    query_texts=[query],
    n_results=2
)

print(results['documents'][0])  # Retrieved documents</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Embeddings in RAG</h3>
                            <p><strong>Document indexing:</strong></p>
                            <ul>
                                <li>Convert all documents to embeddings</li>
                                <li>Store in vector database</li>
                                <li>Enable fast semantic search</li>
                            </ul>
                            
                            <p><strong>Query processing:</strong></p>
                            <ul>
                                <li>Convert user query to embedding</li>
                                <li>Find similar document embeddings</li>
                                <li>Retrieve top-k most relevant documents</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong></p>
                            <ul>
                                <li>Semantic understanding (not just keyword matching)</li>
                                <li>Handles synonyms and paraphrasing</li>
                                <li>Fast similarity search</li>
                                <li>Scalable to millions of documents</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Choosing Embedding Models</h3>
                            <p><strong>Considerations:</strong></p>
                            <ul>
                                <li><strong>Dimension:</strong> Higher = more capacity but slower</li>
                                <li><strong>Domain:</strong> Use domain-specific models when available</li>
                                <li><strong>Language:</strong> Multilingual models for multiple languages</li>
                                <li><strong>Speed:</strong> Smaller models are faster</li>
                                <li><strong>Quality:</strong> Evaluate on your specific use case</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are text embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense vector representations of text that capture semantic meaning, allowing similar texts to have similar vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Text compression algorithms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Text formatting methods</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Text storage formats</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What is the difference between dense and sparse embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense embeddings are learned vector representations (e.g., 384-dim) that capture semantic meaning. Sparse embeddings are high-dimensional with mostly zeros, based on word frequencies (e.g., TF-IDF, BM25)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Dense is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Sparse is better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They are the same</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is cosine similarity used for in embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Measuring semantic similarity between embedding vectors by computing the cosine of the angle between them (range: -1 to 1)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Training embedding models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Generating embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Storing embeddings</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What are popular embedding models used in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) OpenAI text-embedding-ada-002, Sentence-BERT (all-MiniLM-L6-v2), and other transformer-based models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only GPT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only BERT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only word2vec</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Interview question: "How do you choose the right embedding dimension?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance between quality (higher dim = better) and efficiency (lower dim = faster, less storage). Common: 384-1536 dimensions. Consider model capabilities, storage costs, and retrieval speed requirements</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use highest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use lowest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Dimension doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What does the cosine similarity formula \(\cos(\theta) = \frac{q \cdot d}{\|q\| \|d\|}\) represent?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The cosine of the angle between query vector q and document vector d, normalized by their magnitudes</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Euclidean distance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dot product</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Vector addition</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Why do similar texts get similar embedding vectors?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Embedding models are trained to map semantically similar texts to nearby points in vector space, capturing meaning rather than exact word matching</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They use the same words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They have the same length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random chance</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "How would you evaluate embedding quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use semantic similarity benchmarks (STS, SICK), test on domain-specific tasks, measure retrieval performance (precision@k, recall@k), and evaluate on downstream RAG tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check embedding dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only check model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No evaluation needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the advantage of using pre-trained embedding models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) They capture general semantic knowledge from large text corpora, work well out-of-the-box, and don't require training on your specific data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are always better than custom models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They are faster to train</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They use less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is the typical embedding dimension range used in production RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) 384-1536 dimensions, with 384-768 being common for efficiency and 1536 for higher quality</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) 10-50 dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) 10000+ dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Dimension doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: Interview question: "How do you handle out-of-vocabulary words in embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Modern embedding models (subword tokenization) handle OOV words by breaking them into subwords. For truly unknown tokens, models use special UNK tokens or character-level embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Skip those words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use random vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) OOV words don't exist</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the relationship between embedding quality and RAG performance?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Better embeddings lead to more accurate semantic retrieval, which improves RAG answer quality. Embedding quality directly impacts retrieval precision and recall</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Embedding quality doesn't affect RAG</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only LLM quality matters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Embeddings are optional</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/rag/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 1</a>
                <a href="/tutorials/rag/chapter3" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 3 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}"></script>
    <script>
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
