<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Text Embeddings & Vector Representations - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 2: Text Embeddings & Vector Representations</h1>
                <p class="chapter-subtitle">Converting Text to Vectors</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="28"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn active">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand text embeddings & vector representations fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Text Embeddings & Vector Representations</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction: Why Convert Text to Vectors?</h3>
                            <p>In RAG systems, we need to find relevant documents quickly. But how do you search through thousands or millions of documents to find the ones most relevant to a user's question? Traditional keyword search (like Google) has limitations - it can't understand that "car" and "automobile" mean the same thing, or that "Paris is the capital of France" is similar to "What is France's capital city?"</p>
                            
                            <p><strong>Text embeddings solve this problem</strong> by converting text into numerical vectors (arrays of numbers) that capture semantic meaning. Similar meanings result in similar vectors, allowing us to find relevant documents even when they use different words.</p>
                            
                            <div class="example-box">
                                <h5>Real-World Analogy:</h5>
                                <p>Think of embeddings like GPS coordinates for meaning. Just as two places close together on a map have similar coordinates, two texts with similar meanings have similar embedding vectors. This allows us to find "nearby" documents in meaning-space, not just word-space.</p>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>The Core Problem Embeddings Solve</h3>
                            <p><strong>Challenge:</strong> How do we enable computers to understand that these sentences are similar?</p>
                            <ul>
                                <li>"What is the capital of France?"</li>
                                <li>"France's capital city"</li>
                                <li>"Where is the French capital located?"</li>
                            </ul>
                            
                            <p><strong>Solution:</strong> Embeddings convert all three into vectors that are mathematically similar (high cosine similarity), even though they use different words. This enables semantic search - finding documents based on meaning, not just exact word matches.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters for RAG</h4>
                            <p>In RAG systems, embeddings are the foundation of retrieval. Without good embeddings, you can't find relevant documents. With good embeddings, you can:</p>
                            <ul>
                                <li>Find documents even when they use synonyms or different phrasing</li>
                                <li>Search millions of documents in milliseconds</li>
                                <li>Understand semantic relationships (e.g., "machine learning" is similar to "ML" and "artificial intelligence")</li>
                                <li>Handle multilingual content if using multilingual embedding models</li>
                            </ul>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>What Are Vector Embeddings?</h3>
                            <p><strong>Vector embeddings are dense numerical representations of text that capture semantic meaning.</strong> They convert words, sentences, or documents into fixed-size arrays of numbers (vectors) where similar meanings result in similar vectors.</p>
                            
                            <h4>Understanding Embeddings with an Analogy</h4>
                            <p>Think of embeddings like coordinates on a map of meaning:</p>
                            <ul>
                                <li>Texts about "France" might be at coordinates [0.2, -0.5, 0.8, ...]</li>
                                <li>Texts about "Germany" might be at [0.3, -0.4, 0.7, ...] (close, since both are European countries)</li>
                                <li>Texts about "Python programming" might be at [-0.1, 0.9, -0.3, ...] (far away, different topic)</li>
                            </ul>
                            <p>Just as you can measure distance between GPS coordinates, you can measure similarity between embedding vectors using cosine similarity or Euclidean distance.</p>
                            
                            <h4>Key Properties of Embeddings</h4>
                            
                            <p><strong>1. Fixed Dimension:</strong></p>
                            <ul>
                                <li>Each embedding has a fixed number of dimensions (typically 384, 768, or 1536)</li>
                                <li>All texts are converted to vectors of the same size, enabling mathematical operations</li>
                                <li>Example: "Hello world" and "The entire history of human civilization" both become 384-dimensional vectors</li>
                            </ul>
                            
                            <p><strong>2. Semantic Similarity = Vector Similarity:</strong></p>
                            <ul>
                                <li>Texts with similar meanings have vectors that are close together in the high-dimensional space</li>
                                <li>We measure this using cosine similarity: similar texts have high cosine similarity (close to 1.0)</li>
                                <li>Example: "car" and "automobile" have high similarity, even though they're different words</li>
                            </ul>
                            
                            <p><strong>3. Vector Arithmetic:</strong></p>
                            <ul>
                                <li>Embeddings can capture relationships through vector arithmetic</li>
                                <li>Famous example: "king" - "man" + "woman" ‚âà "queen"</li>
                                <li>This shows embeddings capture semantic relationships, not just word similarity</li>
                            </ul>
                            
                            <p><strong>4. Enable Efficient Similarity Search:</strong></p>
                            <ul>
                                <li>Once text is in vector form, we can use mathematical operations to find similar texts</li>
                                <li>This enables fast semantic search across millions of documents</li>
                                <li>Vector databases can find similar vectors in milliseconds</li>
                            </ul>
                            
                            <h4>How Embeddings Capture Meaning</h4>
                            <p>Embeddings work because they're trained on massive amounts of text data. The model learns patterns like:</p>
                            <ul>
                                <li>Words that appear in similar contexts (e.g., "doctor" and "nurse" both appear near "hospital") should have similar vectors</li>
                                <li>Sentences with similar meanings should be close in vector space</li>
                                <li>Semantic relationships (synonyms, antonyms, related concepts) are encoded in the vector positions</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Example: How Embeddings Understand Similarity</h5>
                                <p>Consider these three sentences:</p>
                                <ol>
                                    <li>"The capital of France is Paris"</li>
                                    <li>"Paris is the capital city of France"</li>
                                    <li>"The weather in Tokyo is rainy today"</li>
                                </ol>
                                <p>After embedding:</p>
                                <ul>
                                    <li>Sentences 1 and 2 will have very similar vectors (high cosine similarity, e.g., 0.95)</li>
                                    <li>Sentence 3 will have a very different vector (low similarity, e.g., 0.15)</li>
                                </ul>
                                <p>This allows RAG systems to find sentence 1 or 2 when a user asks "What is France's capital?" even if the exact wording doesn't match.</p>
                            </div>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>What Are Embedding Models?</h3>
                            <p><strong>Embedding models are neural networks trained to convert text into dense vector representations.</strong> They learn to map semantically similar texts to nearby points in a high-dimensional vector space (typically 384-1536 dimensions).</p>
                            
                            <h4>How Embedding Models Work</h4>
                            <p>Embedding models are trained on massive text corpora (billions of sentences) to learn that:</p>
                            <ul>
                                <li>Words that appear in similar contexts should have similar vectors</li>
                                <li>Sentences with similar meanings should be close in vector space</li>
                                <li>Semantic relationships (like "king" - "man" + "woman" ‚âà "queen") should be preserved</li>
                            </ul>
                            
                            <p><strong>Training process:</strong> Models learn by predicting masked words, next sentences, or by contrasting similar vs. dissimilar sentence pairs. Through this training, they develop an internal "understanding" of language that gets encoded in the vector representations.</p>
                            
                            <h4>Types of Embedding Models</h4>
                            
                            <div class="example-box">
                                <h5>1. Sentence Transformers</h5>
                                <p><strong>What they are:</strong> Models specifically optimized for creating embeddings of entire sentences or paragraphs, not just individual words.</p>
                                <p><strong>Why we use them:</strong> They're designed for semantic similarity tasks and work excellently for RAG retrieval. They're fast, efficient, and produce high-quality embeddings.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li><strong>all-MiniLM-L6-v2:</strong> 384 dimensions, fast and efficient, good for most use cases</li>
                                    <li><strong>all-mpnet-base-v2:</strong> 768 dimensions, higher quality but slower</li>
                                    <li><strong>multi-qa-MiniLM-L6-cos-v1:</strong> Optimized for question-answering tasks</li>
                                </ul>
                                <p><strong>When to use:</strong> General-purpose RAG systems, when you need fast inference, or when working with sentence/paragraph-level documents.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. BERT-Based Embeddings</h5>
                                <p><strong>What they are:</strong> Embeddings derived from BERT (Bidirectional Encoder Representations from Transformers) models. These are contextual embeddings that consider the full sentence context.</p>
                                <p><strong>Why we use them:</strong> They capture rich contextual information and understand word meanings based on surrounding text.</p>
                                <p><strong>Examples:</strong> BERT-base, RoBERTa, DistilBERT</p>
                                <p><strong>When to use:</strong> When you need high-quality embeddings and can handle slower inference, or when working with domain-specific content.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>3. OpenAI Embeddings</h5>
                                <p><strong>What they are:</strong> Commercial embedding models provided by OpenAI via API.</p>
                                <p><strong>Why we use them:</strong> High quality, well-optimized, and easy to use via API. No need to host models yourself.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li><strong>text-embedding-ada-002:</strong> 1536 dimensions, OpenAI's current recommended model</li>
                                    <li><strong>text-embedding-3-small:</strong> 1536 dimensions, newer and more efficient</li>
                                    <li><strong>text-embedding-3-large:</strong> 3072 dimensions, highest quality</li>
                                </ul>
                                <p><strong>When to use:</strong> When you want high-quality embeddings without managing model infrastructure, or when building production systems where API costs are acceptable.</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>4. Domain-Specific Embeddings</h5>
                                <p><strong>What they are:</strong> Embedding models fine-tuned on specific domains (medical, legal, scientific, etc.)</p>
                                <p><strong>Why we use them:</strong> They understand domain-specific terminology and relationships better than general models.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li>BioBERT for biomedical texts</li>
                                    <li>Legal-BERT for legal documents</li>
                                    <li>SciBERT for scientific papers</li>
                                </ul>
                                <p><strong>When to use:</strong> When working with specialized domains where general models struggle, or when domain terminology is critical for retrieval quality.</p>
                            </div>
                            
                            <h4>How to Choose an Embedding Model</h4>
                            <p><strong>Consider these factors:</strong></p>
                            <ul>
                                <li><strong>Quality vs. Speed:</strong> Larger models (768-1536 dims) are higher quality but slower. Smaller models (384 dims) are faster but may sacrifice some quality.</li>
                                <li><strong>Domain:</strong> Use domain-specific models if available for your use case.</li>
                                <li><strong>Language:</strong> For multilingual content, use multilingual models (e.g., multilingual-MiniLM).</li>
                                <li><strong>Infrastructure:</strong> API-based (OpenAI) vs. self-hosted (SentenceTransformers) - consider costs and latency.</li>
                                <li><strong>Evaluation:</strong> Test multiple models on your specific data and use cases to find the best fit.</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>What Are Vector Databases and Why Do We Need Them?</h3>
                            
                            <h4>The Problem Vector Databases Solve</h4>
                            <p>Imagine you have 1 million documents, each with a 384-dimensional embedding vector. When a user asks a question, you need to:</p>
                            <ol>
                                <li>Embed the query (get a 384-dim vector)</li>
                                <li>Compare this query vector with all 1 million document vectors</li>
                                <li>Find the top 5 most similar documents</li>
                            </ol>
                            
                            <p><strong>The challenge:</strong> Computing cosine similarity between the query and all 1 million documents would require 1 million vector operations. Even if each takes 0.001 seconds, that's 1000 seconds (16+ minutes) - way too slow for a real-time system!</p>
                            
                            <p><strong>Vector databases solve this</strong> by using specialized indexing algorithms (like HNSW - Hierarchical Navigable Small World) that can find similar vectors in milliseconds, even with millions of documents.</p>
                            
                            <h4>What Is a Vector Database?</h4>
                            <p><strong>A vector database is a specialized database designed to store and efficiently search high-dimensional vectors (embeddings).</strong> Unlike traditional databases that search by exact matches or keywords, vector databases search by similarity in vector space.</p>
                            
                            <div class="example-box">
                                <h5>Traditional Database vs. Vector Database</h5>
                                <p><strong>Traditional SQL Database:</strong></p>
                                <ul>
                                    <li>Stores: Structured data (names, dates, numbers)</li>
                                    <li>Searches: Exact matches, ranges, joins</li>
                                    <li>Query: "SELECT * WHERE name = 'John'"</li>
                                    <li><strong>Problem:</strong> Can't search by semantic similarity</li>
                                </ul>
                                
                                <p><strong>Vector Database:</strong></p>
                                <ul>
                                    <li>Stores: High-dimensional vectors (embeddings)</li>
                                    <li>Searches: Similarity search (find nearest neighbors)</li>
                                    <li>Query: "Find vectors most similar to [0.2, -0.5, 0.8, ...]"</li>
                                    <li><strong>Solution:</strong> Fast semantic similarity search</li>
                                </ul>
                            </div>
                            
                            <h4>Why Do We Need Vector Databases in RAG?</h4>
                            <p><strong>1. Speed:</strong> Vector databases use Approximate Nearest Neighbor (ANN) algorithms that can search millions of vectors in milliseconds, compared to minutes with brute-force search.</p>
                            
                            <p><strong>2. Scalability:</strong> As your knowledge base grows from thousands to millions of documents, vector databases maintain fast query times. Traditional methods would become prohibitively slow.</p>
                            
                            <p><strong>3. Efficiency:</strong> Vector databases are optimized for the specific task of similarity search. They use techniques like:</p>
                            <ul>
                                <li><strong>HNSW (Hierarchical Navigable Small World):</strong> Creates a graph structure where similar vectors are connected, enabling fast navigation to nearest neighbors</li>
                                <li><strong>IVF (Inverted File Index):</strong> Groups similar vectors into clusters, then searches only relevant clusters</li>
                                <li><strong>Product Quantization:</strong> Compresses vectors to reduce memory and speed up search</li>
                            </ul>
                            
                            <p><strong>4. Metadata Filtering:</strong> Vector databases allow you to combine similarity search with traditional filtering. For example: "Find documents similar to this query, but only from 2024, and only in the 'legal' category."</p>
                            
                            <h4>When Do We Use Vector Databases?</h4>
                            
                            <div class="example-box">
                                <h5>‚úÖ Use Vector Databases When:</h5>
                                <ul>
                                    <li><strong>Large-scale systems:</strong> You have thousands or millions of documents</li>
                                    <li><strong>Real-time requirements:</strong> You need sub-second query responses</li>
                                    <li><strong>Production systems:</strong> You need reliability, scalability, and managed infrastructure</li>
                                    <li><strong>Complex queries:</strong> You need metadata filtering combined with similarity search</li>
                                    <li><strong>Growing knowledge base:</strong> Your document collection will expand over time</li>
                                </ul>
                            </div>
                            
                            <div class="example-box">
                                <h5>‚ùå You Might Skip Vector Databases When:</h5>
                                <ul>
                                    <li><strong>Small datasets:</strong> You have fewer than 1,000 documents (NumPy arrays might be sufficient)</li>
                                    <li><strong>Prototyping:</strong> You're building a proof-of-concept and speed isn't critical</li>
                                    <li><strong>Simple use cases:</strong> You don't need advanced features like metadata filtering</li>
                                    <li><strong>Budget constraints:</strong> Managed vector databases have costs (though open-source options exist)</li>
                                </ul>
                            </div>
                            
                            <h4>Popular Vector Database Options</h4>
                            
                            <div class="example-box">
                                <h5>1. Pinecone</h5>
                                <p><strong>What it is:</strong> Fully managed, cloud-based vector database service</p>
                                <p><strong>Why use it:</strong> Zero infrastructure management, automatic scaling, high performance, built-in security</p>
                                <p><strong>Best for:</strong> Production systems, teams without DevOps resources, applications requiring high reliability</p>
                                <p><strong>Considerations:</strong> Paid service (though has free tier), requires internet connection</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>2. Weaviate</h5>
                                <p><strong>What it is:</strong> Open-source vector database with optional cloud hosting</p>
                                <p><strong>Why use it:</strong> Self-hostable, GraphQL API, built-in vectorization, good documentation</p>
                                <p><strong>Best for:</strong> Teams comfortable with self-hosting, need flexibility, want open-source solution</p>
                                <p><strong>Considerations:</strong> Requires infrastructure management if self-hosting</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>3. Chroma</h5>
                                <p><strong>What it is:</strong> Lightweight, open-source vector database designed for simplicity</p>
                                <p><strong>Why use it:</strong> Easy to use, Python-first, good for prototyping and small-to-medium scale</p>
                                <p><strong>Best for:</strong> Prototyping, Python-heavy projects, smaller datasets, getting started quickly</p>
                                <p><strong>Considerations:</strong> May not scale as well as others for very large datasets</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>4. FAISS (Facebook AI Similarity Search)</h5>
                                <p><strong>What it is:</strong> Library for efficient similarity search, not a full database</p>
                                <p><strong>Why use it:</strong> Extremely fast, open-source, used by Facebook at scale, in-memory or on-disk</p>
                                <p><strong>Best for:</strong> When you need maximum performance, have technical expertise, want to build custom solutions</p>
                                <p><strong>Considerations:</strong> Lower-level API, requires more setup, no built-in persistence (you handle it)</p>
                            </div>
                            
                            <div class="example-box">
                                <h5>5. Qdrant</h5>
                                <p><strong>What it is:</strong> Open-source vector database with cloud option</p>
                                <p><strong>Why use it:</strong> High performance, good filtering capabilities, REST and gRPC APIs</p>
                                <p><strong>Best for:</strong> Production systems needing high performance, teams wanting open-source with cloud option</p>
                                <p><strong>Considerations:</strong> Requires infrastructure if self-hosting</p>
                            </div>
                            
                            <h4>How Vector Databases Work in RAG</h4>
                            <p><strong>Step 1 - Indexing (One-time):</strong></p>
                            <ol>
                                <li>Embed all documents using your embedding model</li>
                                <li>Store document embeddings in the vector database</li>
                                <li>Vector database builds an index (e.g., HNSW graph) for fast search</li>
                                <li>Store metadata (document ID, title, date, etc.) alongside embeddings</li>
                            </ol>
                            
                            <p><strong>Step 2 - Querying (Per Query):</strong></p>
                            <ol>
                                <li>Embed the user query</li>
                                <li>Query the vector database: "Find top-k vectors most similar to query embedding"</li>
                                <li>Vector database uses its index to quickly find similar vectors (milliseconds)</li>
                                <li>Return document IDs and metadata for the top-k matches</li>
                                <li>Retrieve actual document text using the IDs</li>
                            </ol>
                            
                            <p><strong>Performance comparison:</strong></p>
                            <ul>
                                <li><strong>Brute-force (NumPy):</strong> 1M documents = ~16 minutes</li>
                                <li><strong>Vector Database (HNSW):</strong> 1M documents = ~50-200 milliseconds</li>
                                <li><strong>Speedup:</strong> ~5,000-20,000x faster!</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Embedding Function</h4>
                            <div class="formula-display">
                                \[E: \text{text} \rightarrow \mathbb{R}^d\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(E\): Embedding model (e.g., sentence transformer)</li>
                                    <li>\(\text{text}\): Input text string</li>
                                    <li>\(\mathbb{R}^d\): d-dimensional real vector</li>
                                    <li>Typical d: 384, 768, 1536</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Cosine Similarity</h4>
                            <div class="formula-display">
                                \[\text{similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|} = \cos(\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(v_1, v_2\): Embedding vectors</li>
                                    <li>\(\theta\): Angle between vectors</li>
                                    <li>Range: [-1, 1] (typically [0, 1] for normalized embeddings)</li>
                                    <li>Higher value = more similar</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Euclidean Distance</h4>
                            <div class="formula-display">
                                \[d(v_1, v_2) = \|v_1 - v_2\| = \sqrt{\sum_{i=1}^{d} (v_{1i} - v_{2i})^2}\]
                            </div>
                            <div class="formula-explanation">
                                <p>Alternative similarity measure. Lower distance = more similar. For normalized embeddings, cosine similarity is often preferred.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Text to Embedding</h4>
                            <p><strong>Input text:</strong> "The capital of France is Paris"</p>
                            
                            <p><strong>Embedding process:</strong></p>
                            <ul>
                                <li>Tokenize: ["The", "capital", "of", "France", "is", "Paris"]</li>
                                <li>Pass through transformer model</li>
                                <li>Output: [0.23, -0.45, 0.67, ..., 0.12] (384-dimensional vector)</li>
                            </ul>
                            
                            <p><strong>Similar texts get similar vectors:</strong></p>
                            <ul>
                                <li>"Paris is the capital of France" ‚Üí [0.25, -0.43, 0.65, ..., 0.11] (very similar!)</li>
                                <li>"The weather is nice today" ‚Üí [0.12, 0.34, -0.21, ..., -0.45] (different)</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Similarity Search</h4>
                            <p><strong>Query:</strong> "What is the capital of France?"</p>
                            
                            <p><strong>Step 1: Embed query</strong></p>
                            <ul>
                                <li>Query embedding: [0.24, -0.44, 0.66, ...]</li>
                            </ul>
                            
                            <p><strong>Step 2: Compare with document embeddings</strong></p>
                            <ul>
                                <li>Doc 1: "France's capital is Paris" ‚Üí similarity: 0.95</li>
                                <li>Doc 2: "Germany's capital is Berlin" ‚Üí similarity: 0.45</li>
                                <li>Doc 3: "Italy is a country" ‚Üí similarity: 0.32</li>
                            </ul>
                            
                            <p><strong>Step 3: Retrieve top-k</strong></p>
                            <ul>
                                <li>Top-1: Doc 1 (highest similarity)</li>
                            </ul>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Creating Embeddings</h4>
                            <pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings
texts = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy is a country in Europe"
]

embeddings = model.encode(texts)
print(f"Embedding shape: {embeddings.shape}")  # (3, 384)

# Query embedding
query = "What is the capital of France?"
query_embedding = model.encode([query])

# Compute similarities
similarities = np.dot(embeddings, query_embedding.T).flatten()
print(f"Similarities: {similarities}")

# Get most similar
most_similar_idx = np.argmax(similarities)
print(f"Most similar: {texts[most_similar_idx]}")</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Using Vector Database (Chroma)</h4>
                            <pre><code class="language-python">import chromadb
from chromadb.config import Settings

# Initialize Chroma
client = chromadb.Client(Settings())

# Create collection
collection = client.create_collection(name="documents")

# Add documents
documents = [
    "The capital of France is Paris",
    "Germany's capital is Berlin",
    "Italy's capital is Rome"
]

collection.add(
    documents=documents,
    ids=["doc1", "doc2", "doc3"]
)

# Query
query = "What is the capital of France?"
results = collection.query(
    query_texts=[query],
    n_results=2
)

print(results['documents'][0])  # Retrieved documents</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Vector Embeddings in RAG</h3>
                            <p><strong>Document indexing:</strong></p>
                            <ul>
                                <li>Convert all documents to embeddings</li>
                                <li>Store in vector database</li>
                                <li>Enable fast semantic search</li>
                            </ul>
                            
                            <p><strong>Query processing:</strong></p>
                            <ul>
                                <li>Convert user query to embedding</li>
                                <li>Find similar document embeddings</li>
                                <li>Retrieve top-k most relevant documents</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong></p>
                            <ul>
                                <li>Semantic understanding (not just keyword matching)</li>
                                <li>Handles synonyms and paraphrasing</li>
                                <li>Fast similarity search</li>
                                <li>Scalable to millions of documents</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Choosing Embedding Models</h3>
                            <p><strong>Considerations:</strong></p>
                            <ul>
                                <li><strong>Dimension:</strong> Higher = more capacity but slower</li>
                                <li><strong>Domain:</strong> Use domain-specific models when available</li>
                                <li><strong>Language:</strong> Multilingual models for multiple languages</li>
                                <li><strong>Speed:</strong> Smaller models are faster</li>
                                <li><strong>Quality:</strong> Evaluate on your specific use case</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are text embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense vector representations of text that capture semantic meaning, allowing similar texts to have similar vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Text compression algorithms</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Text formatting methods</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Text storage formats</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What is the difference between dense and sparse embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense embeddings are learned vector representations (e.g., 384-dim) that capture semantic meaning. Sparse embeddings are high-dimensional with mostly zeros, based on word frequencies (e.g., TF-IDF, BM25)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Dense is faster</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Sparse is better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They are the same</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is cosine similarity used for in embeddings?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Measuring semantic similarity between embedding vectors by computing the cosine of the angle between them (range: -1 to 1)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Training embedding models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Generating embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Storing embeddings</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What are popular embedding models used in RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) OpenAI text-embedding-ada-002, Sentence-BERT (all-MiniLM-L6-v2), and other transformer-based models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only GPT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only BERT models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only word2vec</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Interview question: "How do you choose the right embedding dimension?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance between quality (higher dim = better) and efficiency (lower dim = faster, less storage). Common: 384-1536 dimensions. Consider model capabilities, storage costs, and retrieval speed requirements</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use highest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use lowest dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Dimension doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What does the cosine similarity formula \(\cos(\theta) = \frac{q \cdot d}{\|q\| \|d\|}\) represent?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The cosine of the angle between query vector q and document vector d, normalized by their magnitudes</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Euclidean distance</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dot product</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Vector addition</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Why do similar texts get similar embedding vectors?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Embedding models are trained to map semantically similar texts to nearby points in vector space, capturing meaning rather than exact word matching</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They use the same words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They have the same length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random chance</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: Interview question: "How would you evaluate embedding quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use semantic similarity benchmarks (STS, SICK), test on domain-specific tasks, measure retrieval performance (precision@k, recall@k), and evaluate on downstream RAG tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check embedding dimension</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only check model size</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No evaluation needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the advantage of using pre-trained embedding models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) They capture general semantic knowledge from large text corpora, work well out-of-the-box, and don't require training on your specific data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are always better than custom models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They are faster to train</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They use less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is the typical embedding dimension range used in production RAG systems?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) 384-1536 dimensions, with 384-768 being common for efficiency and 1536 for higher quality</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) 10-50 dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) 10000+ dimensions</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Dimension doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: Interview question: "How do you handle out-of-vocabulary words in embeddings?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Modern embedding models (subword tokenization) handle OOV words by breaking them into subwords. For truly unknown tokens, models use special UNK tokens or character-level embeddings</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Skip those words</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Use random vectors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) OOV words don't exist</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the relationship between embedding quality and RAG performance?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Better embeddings lead to more accurate semantic retrieval, which improves RAG answer quality. Embedding quality directly impacts retrieval precision and recall</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Embedding quality doesn't affect RAG</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only LLM quality matters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Embeddings are optional</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/rag/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 1</a>
                <a href="/tutorials/rag/chapter3" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 3 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
