<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 7: Production RAG Systems - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 7: Production RAG Systems</h1>
                <p class="chapter-subtitle">Deployment & Monitoring</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="100"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn active">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand production rag systems fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Production RAG Systems</h2>
                        
                        <div class="explanation-box">
                            <h3>From Prototype to Production: Building RAG Systems That Scale</h3>
                            <p>Building a working RAG prototype is one thing; deploying it to production where it handles millions of documents, thousands of queries per second, and requires 99.9%+ uptime is entirely different. <strong>Production RAG systems</strong> require careful attention to scalability, reliability, monitoring, performance optimization, and error handling.</p>
                            
                            <p><strong>The production challenge:</strong> A prototype that works with 1,000 documents might completely fail with 10 million. A system that responds in 2 seconds for 10 users might take minutes under load. A system that works perfectly in testing might fail in production due to edge cases, network issues, or resource constraints.</p>
                            
                            <h4>Critical Production Considerations</h4>
                            <ol>
                                <li><strong>Scalability:</strong> Handle millions of documents and high query throughput without performance degradation</li>
                                <li><strong>Latency:</strong> Sub-second response times for good user experience (retrieval + generation must be fast)</li>
                                <li><strong>Reliability:</strong> 99.9%+ uptime with graceful error handling and fallback mechanisms</li>
                                <li><strong>Monitoring:</strong> Track retrieval quality, answer quality, latency, errors, and costs in real-time</li>
                                <li><strong>Cost Management:</strong> Optimize API costs (embedding APIs, LLM APIs) while maintaining quality</li>
                                <li><strong>Error Handling:</strong> Graceful degradation when retrieval fails, no documents found, or LLM errors occur</li>
                                <li><strong>Performance Optimization:</strong> Caching, batch processing, async operations, and model selection for speed</li>
                            </ol>
                            
                            <div class="example-box">
                                <h5>Production vs Prototype Differences:</h5>
                                <p><strong>Prototype:</strong></p>
                                <ul>
                                    <li>✅ Works with 1,000 documents</li>
                                    <li>✅ 2-5 second response time acceptable</li>
                                    <li>✅ Manual testing, no monitoring</li>
                                    <li>✅ Crashes are okay, just restart</li>
                                    <li>✅ No error handling needed</li>
                                </ul>
                                <p><strong>Production:</strong></p>
                                <ul>
                                    <li>❌ Must handle 10+ million documents</li>
                                    <li>❌ Need sub-second latency for good UX</li>
                                    <li>❌ Real-time monitoring and alerting required</li>
                                    <li>❌ 99.9%+ uptime, graceful error handling</li>
                                    <li>❌ Comprehensive error handling and fallbacks</li>
                                </ul>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Scalability Strategies:</strong> Distributed vector databases, sharding, horizontal scaling, and efficient indexing for billion-scale systems</li>
                                <li><strong>Monitoring & Evaluation:</strong> Tracking retrieval metrics (precision@k, recall@k), answer quality (faithfulness, relevance), and operational metrics (latency, throughput, errors)</li>
                                <li><strong>Error Handling:</strong> Graceful fallbacks, retry mechanisms, validation, and circuit breakers for resilient systems</li>
                                <li><strong>Performance Optimization:</strong> Caching strategies, batch processing, async operations, and model selection for speed vs quality trade-offs</li>
                                <li><strong>Cost Optimization:</strong> Reducing API costs through caching, efficient batching, and smart model selection</li>
                                <li><strong>Quality Metrics:</strong> Measuring and improving retrieval quality, answer faithfulness, relevance, and completeness</li>
                                <li><strong>Production Best Practices:</strong> Deployment strategies, versioning, A/B testing, and continuous improvement</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> A RAG system that works in a prototype but fails in production is useless. Production deployment requires solving real-world challenges: handling scale, ensuring reliability, monitoring quality, optimizing performance, and managing costs. These considerations determine whether your RAG system succeeds or fails in real-world use.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Production RAG Considerations: Building Systems That Scale</h3>
                            
                            <p>Moving from a prototype RAG system to a production system requires addressing scalability, reliability, monitoring, and performance. This section covers the critical considerations for production deployment.</p>
                            
                            <h4>1. Scalability: Handling Growth</h4>
                            
                            <h5>Document Scale</h5>
                            <p><strong>The challenge:</strong> Production RAG systems often need to handle millions or billions of documents. A system that works with 10,000 documents might completely fail at 10 million.</p>
                            
                            <p><strong>Solutions:</strong></p>
                            <ul>
                                <li><strong>Efficient indexing:</strong> Use vector databases with scalable indexing (HNSW, IVF-PQ) that can handle billions of vectors</li>
                                <li><strong>Distributed storage:</strong> Partition documents across multiple nodes/servers</li>
                                <li><strong>Incremental updates:</strong> Support adding/updating documents without rebuilding entire index</li>
                                <li><strong>Metadata partitioning:</strong> Use metadata to partition documents (e.g., by date, category) for faster search</li>
                            </ul>
                            
                            <h5>Query Throughput</h5>
                            <p><strong>The challenge:</strong> Production systems need to handle hundreds or thousands of queries per second with consistent low latency.</p>
                            
                            <p><strong>Solutions:</strong></p>
                            <ul>
                                <li><strong>Horizontal scaling:</strong> Run multiple instances of your RAG service behind a load balancer</li>
                                <li><strong>Caching:</strong> Cache common queries and their results (see Performance Optimization below)</li>
                                <li><strong>Async processing:</strong> Use asynchronous operations to handle multiple queries concurrently</li>
                                <li><strong>Connection pooling:</strong> Reuse database connections instead of creating new ones for each query</li>
                            </ul>
                            
                            <h5>Fast Retrieval (Sub-Second Latency)</h5>
                            <p><strong>Target:</strong> End-to-end latency (query → retrieval → generation → response) should be under 1-2 seconds for good user experience.</p>
                            
                            <p><strong>How to achieve:</strong></p>
                            <ul>
                                <li><strong>Optimized indexes:</strong> Use HNSW or similar fast indexing algorithms</li>
                                <li><strong>Limit retrieval scope:</strong> Use metadata filtering to reduce search space</li>
                                <li><strong>Efficient reranking:</strong> Rerank only top-k candidates (50-200), not entire collection</li>
                                <li><strong>Fast embedding models:</strong> Use smaller, faster embedding models when possible (trade-off with quality)</li>
                                <li><strong>CDN for static assets:</strong> Serve embeddings and models from CDN for faster access</li>
                            </ul>
                            
                            <h5>Efficient Embedding Storage</h5>
                            <p><strong>The challenge:</strong> Storing embeddings for millions of documents requires significant storage. A 384-dimensional embedding is ~1.5KB, so 1M documents = ~1.5GB just for embeddings.</p>
                            
                            <p><strong>Solutions:</strong></p>
                            <ul>
                                <li><strong>Compression:</strong> Use product quantization (PQ) to compress embeddings (10-100x reduction)</li>
                                <li><strong>Deduplication:</strong> Store unique embeddings once, reference from multiple documents</li>
                                <li><strong>Tiered storage:</strong> Hot data (frequently accessed) in fast storage, cold data in cheaper storage</li>
                                <li><strong>Vector database optimization:</strong> Use databases that support efficient compression (FAISS, Qdrant)</li>
                            </ul>
                            
                            <h4>2. Monitoring and Evaluation: Ensuring Quality</h4>
                            
                            <h5>Retrieval Quality Metrics</h5>
                            <p><strong>Precision@k:</strong> Fraction of retrieved documents that are actually relevant. High precision = fewer irrelevant documents retrieved.</p>
                            
                            <p><strong>Recall@k:</strong> Fraction of relevant documents that were retrieved. High recall = fewer missed relevant documents.</p>
                            
                            <p><strong>How to measure:</strong></p>
                            <ul>
                                <li>Manually label a test set (queries with known relevant documents)</li>
                                <li>Run retrieval on test queries</li>
                                <li>Calculate precision and recall for each query</li>
                                <li>Track these metrics over time to detect degradation</li>
                            </ul>
                            
                            <h5>Answer Quality Metrics</h5>
                            <p><strong>Faithfulness:</strong> Fraction of answer claims that are supported by the retrieved context. Measures whether the answer is grounded in the documents (not hallucinated).</p>
                            
                            <p><strong>Relevance:</strong> How well the answer addresses the query. Measured by semantic similarity between query and answer, or by human evaluation.</p>
                            
                            <p><strong>Completeness:</strong> Whether the answer fully addresses all parts of the query. Particularly important for multi-part questions.</p>
                            
                            <p><strong>How to measure:</strong></p>
                            <ul>
                                <li><strong>Automated:</strong> Use LLMs to evaluate faithfulness, relevance, completeness (LLM-as-judge)</li>
                                <li><strong>Human evaluation:</strong> Have humans rate answers on these dimensions (gold standard but expensive)</li>
                                <li><strong>Hybrid:</strong> Use automated evaluation for monitoring, human evaluation for critical cases</li>
                            </ul>
                            
                            <h5>Operational Metrics</h5>
                            <p><strong>What to monitor:</strong></p>
                            <ul>
                                <li><strong>Latency:</strong> P50, P95, P99 latencies for retrieval and generation</li>
                                <li><strong>Throughput:</strong> Queries per second, successful vs failed requests</li>
                                <li><strong>Error rates:</strong> Percentage of queries that fail or timeout</li>
                                <li><strong>Cost:</strong> API costs (embedding API, LLM API), infrastructure costs</li>
                                <li><strong>Resource usage:</strong> CPU, memory, storage usage</li>
                            </ul>
                            
                            <h5>Logging and Alerting</h5>
                            <p><strong>What to log:</strong></p>
                            <ul>
                                <li>All queries and their responses (for debugging and improvement)</li>
                                <li>Retrieved documents and their similarity scores</li>
                                <li>Error messages and stack traces</li>
                                <li>Performance metrics (latency, token counts, costs)</li>
                            </ul>
                            
                            <p><strong>What to alert on:</strong></p>
                            <ul>
                                <li>Quality degradation (precision/recall drops below threshold)</li>
                                <li>High error rates (>1% failures)</li>
                                <li>Latency spikes (P95 > 2 seconds)</li>
                                <li>Cost anomalies (unexpected API cost increases)</li>
                            </ul>
                            
                            <h4>3. Error Handling: Building Resilient Systems</h4>
                            
                            <h5>Retrieval Failures</h5>
                            <p><strong>What can fail:</strong> Vector database connection, embedding API, timeout, index corruption</p>
                            
                            <p><strong>How to handle:</strong></p>
                            <ul>
                                <li><strong>Retry with exponential backoff:</strong> Transient failures often resolve on retry</li>
                                <li><strong>Fallback to cached results:</strong> If retrieval fails, use cached results for similar queries</li>
                                <li><strong>Graceful degradation:</strong> Return partial results or a helpful error message instead of crashing</li>
                                <li><strong>Circuit breakers:</strong> Stop calling failing services temporarily to prevent cascade failures</li>
                            </ul>
                            
                            <h5>No Relevant Documents Found</h5>
                            <p><strong>The problem:</strong> Sometimes retrieval returns documents with very low similarity scores, or no documents at all.</p>
                            
                            <p><strong>How to handle:</strong></p>
                            <ul>
                                <li><strong>Similarity threshold:</strong> Only use documents above a minimum similarity score (e.g., 0.7)</li>
                                <li><strong>Fallback response:</strong> If no good documents found, return: "I couldn't find relevant information. Please rephrase your question."</li>
                                <li><strong>Query rewriting:</strong> Try query expansion or rewriting to find more documents</li>
                                <li><strong>Log for improvement:</strong> Track queries with no results to identify knowledge gaps</li>
                            </ul>
                            
                            <h5>Context Quality Validation</h5>
                            <p><strong>What to validate:</strong></p>
                            <ul>
                                <li><strong>Similarity scores:</strong> Ensure retrieved documents have reasonable similarity (not all very low)</li>
                                <li><strong>Diversity:</strong> Check that retrieved documents aren't all duplicates or very similar</li>
                                <li><strong>Relevance:</strong> Use a quick relevance check (e.g., keyword matching) before sending to LLM</li>
                                <li><strong>Token limits:</strong> Ensure context fits within LLM's context window</li>
                            </ul>
                            
                            <h5>Retry Mechanisms</h5>
                            <p><strong>When to retry:</strong> Transient failures (network timeouts, rate limits, temporary service unavailability)</p>
                            
                            <p><strong>Retry strategy:</strong></p>
                            <ul>
                                <li><strong>Exponential backoff:</strong> Wait 1s, then 2s, then 4s before retries</li>
                                <li><strong>Max retries:</strong> Limit to 3-5 retries to avoid long delays</li>
                                <li><strong>Idempotency:</strong> Ensure retries don't cause duplicate operations</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Performance Optimization: Making RAG Fast and Efficient</h3>
                            
                            <h4>1. Caching: Reducing Redundant Computation</h4>
                            
                            <h5>Query Result Caching</h5>
                            <p><strong>What to cache:</strong> Cache the final answers for common queries. If the same query is asked multiple times, return the cached answer instead of re-running retrieval and generation.</p>
                            
                            <p><strong>Cache key:</strong> Use the exact query text (or normalized version) as the cache key</p>
                            
                            <p><strong>Cache TTL:</strong> Set appropriate time-to-live based on how often your documents change. Static documents: long TTL (hours/days). Frequently updated: short TTL (minutes).</p>
                            
                            <h5>Embedding Caching</h5>
                            <p><strong>What to cache:</strong> Cache document embeddings so you don't re-embed the same documents. Also cache query embeddings for common queries.</p>
                            
                            <p><strong>Benefits:</strong> Embedding generation is expensive (API costs, computation time). Caching can save 50-90% of embedding costs for repeated content.</p>
                            
                            <h5>Retrieval Result Caching</h5>
                            <p><strong>What to cache:</strong> Cache the top-k retrieved documents for common queries. Even if you regenerate the answer, you can reuse the same retrieved documents.</p>
                            
                            <p><strong>Benefits:</strong> Avoids expensive vector database queries for repeated queries.</p>
                            
                            <h4>2. Batch Processing: Processing Multiple Items Together</h4>
                            
                            <p><strong>Embedding batching:</strong> Instead of embedding documents one-by-one, batch them together (e.g., 32-128 documents at a time). Most embedding APIs support batching and it's much more efficient.</p>
                            
                            <p><strong>Query batching:</strong> If you have multiple queries to process, batch them and process in parallel. This improves throughput significantly.</p>
                            
                            <p><strong>Index updates:</strong> When adding many documents, batch the index updates rather than updating one-by-one.</p>
                            
                            <h4>3. Async Operations: Parallelizing Independent Work</h4>
                            
                            <p><strong>Parallel retrieval and generation:</strong> If you're using multiple retrieval strategies (dense + sparse), run them in parallel instead of sequentially.</p>
                            
                            <p><strong>Async LLM calls:</strong> If generating answers for multiple queries, use async/await to process them concurrently.</p>
                            
                            <p><strong>Pipeline parallelism:</strong> While one query is being processed by the LLM, start processing the next query's retrieval.</p>
                            
                            <h4>4. Model Selection: Balancing Quality and Latency</h4>
                            
                            <p><strong>Embedding models:</strong> Smaller models (e.g., all-MiniLM-L6-v2, 384 dims) are faster but may have lower quality. Larger models (e.g., all-mpnet-base-v2, 768 dims) are slower but higher quality. Choose based on your latency requirements.</p>
                            
                            <p><strong>LLM selection:</strong> For generation, smaller/faster models (GPT-3.5-turbo) are faster and cheaper but may have lower quality. Larger models (GPT-4) are slower and more expensive but higher quality. Consider using smaller models for simple queries, larger for complex ones.</p>
                            
                            <p><strong>Reranking models:</strong> Cross-encoder reranking is slower but more accurate. Consider skipping reranking for simple queries, using it only for complex ones.</p>
                            
                            <h4>5. Additional Optimizations</h4>
                            
                            <ul>
                                <li><strong>Connection pooling:</strong> Reuse database connections instead of creating new ones</li>
                                <li><strong>Pre-warming:</strong> Load models and indexes into memory at startup to avoid cold starts</li>
                                <li><strong>CDN for static assets:</strong> Serve embedding models and other static files from CDN</li>
                                <li><strong>Database query optimization:</strong> Use appropriate indexes, limit result sets, use pagination</li>
                                <li><strong>Monitoring and profiling:</strong> Identify bottlenecks through profiling and optimize the slowest parts</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Production RAG Evaluation Metrics</h3>
                            <p>Measuring RAG system performance requires quantitative metrics for both retrieval quality and answer quality. These formulas provide standardized ways to evaluate, monitor, and improve production RAG systems. Understanding these metrics is essential for ensuring your system meets quality standards.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. Retrieval Precision@k</h4>
                            <div class="formula-display">
                                \[\text{Precision@k} = \frac{|\{\text{relevant docs}\} \cap \{\text{retrieved top-k}\}|}{k}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Precision@k measures the <strong>quality</strong> of retrieval - what fraction of the top-k retrieved documents are actually relevant to the query. High precision means you're retrieving mostly relevant documents (few false positives).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\{\text{relevant docs}\}\):</strong> Set of all documents in the knowledge base that are actually relevant to the query (ground truth, typically labeled by humans)</li>
                                    <li><strong>\(\{\text{retrieved top-k}\}\):</strong> Set of the top-k documents returned by the retrieval system</li>
                                    <li><strong>\(\cap\):</strong> Set intersection - documents that are both relevant AND retrieved</li>
                                    <li><strong>\(|\ldots|\):</strong> Cardinality (size) of the set</li>
                                    <li><strong>\(k\):</strong> Number of documents retrieved (e.g., k=5 means top-5 documents)</li>
                                </ul>
                                
                                <h5>Interpretation:</h5>
                                <ul>
                                    <li><strong>Precision@k = 1.0:</strong> All retrieved documents are relevant (perfect precision, no false positives)</li>
                                    <li><strong>Precision@k = 0.8:</strong> 80% of retrieved documents are relevant (good precision)</li>
                                    <li><strong>Precision@k = 0.5:</strong> Only 50% of retrieved documents are relevant (poor precision, many false positives)</li>
                                    <li><strong>Precision@k = 0.0:</strong> None of the retrieved documents are relevant (worst case)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is Python?"</p>
                                <p>Relevant documents (ground truth): {doc1, doc2, doc3, doc4, doc5}</p>
                                <p>Retrieved top-5: {doc1, doc6, doc2, doc7, doc3}</p>
                                <p>Intersection: {doc1, doc2, doc3} (3 documents are both relevant and retrieved)</p>
                                <p>Precision@5: \(\frac{3}{5} = 0.6\) (60% of retrieved docs are relevant)</p>
                                
                                <h5>Why Precision Matters:</h5>
                                <p>High precision means the LLM receives mostly relevant context, leading to better answers. Low precision means the LLM gets irrelevant context, which can cause confusion or hallucination.</p>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>Production systems:</strong> Precision@5 = 0.7-0.9 (70-90% of top-5 are relevant)</li>
                                    <li><strong>Good systems:</strong> Precision@5 = 0.8-0.95</li>
                                    <li><strong>Excellent systems:</strong> Precision@5 > 0.9</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. Retrieval Recall@k</h4>
                            <div class="formula-display">
                                \[\text{Recall@k} = \frac{|\{\text{relevant docs}\} \cap \{\text{retrieved top-k}\}|}{|\{\text{relevant docs}\}|}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Recall@k measures the <strong>coverage</strong> of retrieval - what fraction of all relevant documents were actually retrieved. High recall means you're finding most relevant documents (few false negatives).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\{\text{relevant docs}\}\):</strong> Set of all documents that are actually relevant (ground truth)</li>
                                    <li><strong>\(\{\text{retrieved top-k}\}\):</strong> Set of top-k documents retrieved</li>
                                    <li><strong>\(\{\text{relevant docs}\} \cap \{\text{retrieved top-k}\}\):</strong> Relevant documents that were successfully retrieved</li>
                                    <li><strong>\(|\{\text{relevant docs}\}|\):</strong> Total number of relevant documents (denominator)</li>
                                </ul>
                                
                                <h5>Interpretation:</h5>
                                <ul>
                                    <li><strong>Recall@k = 1.0:</strong> All relevant documents were retrieved (perfect recall, no false negatives)</li>
                                    <li><strong>Recall@k = 0.8:</strong> 80% of relevant documents were retrieved (good recall)</li>
                                    <li><strong>Recall@k = 0.5:</strong> Only 50% of relevant documents were retrieved (poor recall, many missed)</li>
                                    <li><strong>Recall@k = 0.0:</strong> No relevant documents were retrieved (worst case)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is Python?"</p>
                                <p>Relevant documents: {doc1, doc2, doc3, doc4, doc5} (5 total relevant)</p>
                                <p>Retrieved top-5: {doc1, doc6, doc2, doc7, doc3}</p>
                                <p>Intersection: {doc1, doc2, doc3} (3 relevant docs retrieved)</p>
                                <p>Recall@5: \(\frac{3}{5} = 0.6\) (60% of relevant docs were found)</p>
                                <p>❌ <strong>Problem:</strong> doc4 and doc5 are relevant but weren't retrieved (missed 40% of relevant docs)</p>
                                
                                <h5>Precision vs Recall Trade-off:</h5>
                                <ul>
                                    <li><strong>High precision, low recall:</strong> Retrieved docs are very relevant, but you miss many relevant docs</li>
                                    <li><strong>Low precision, high recall:</strong> You find most relevant docs, but also retrieve many irrelevant ones</li>
                                    <li><strong>Ideal:</strong> High precision AND high recall (retrieve mostly relevant docs AND find most relevant docs)</li>
                                </ul>
                                
                                <h5>Why Recall Matters:</h5>
                                <p>Low recall means you're missing relevant information. If the answer requires information from doc4 and doc5, but they weren't retrieved, the LLM can't generate a complete answer.</p>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>Production systems:</strong> Recall@10 = 0.6-0.8 (60-80% of relevant docs found in top-10)</li>
                                    <li><strong>Good systems:</strong> Recall@10 = 0.7-0.9</li>
                                    <li><strong>Note:</strong> Recall typically increases with k (Recall@10 > Recall@5)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. F1 Score (Harmonic Mean of Precision and Recall)</h4>
                            <div class="formula-display">
                                \[\text{F1@k} = 2 \times \frac{\text{Precision@k} \times \text{Recall@k}}{\text{Precision@k} + \text{Recall@k}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. It's useful when you need one number to summarize retrieval quality.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>Harmonic mean:</strong> More conservative than arithmetic mean - penalizes systems that are good at one metric but poor at the other</li>
                                    <li><strong>Range:</strong> [0, 1], where 1.0 is perfect (both precision and recall are 1.0)</li>
                                    <li>F1 is high only when BOTH precision and recall are high</li>
                                </ul>
                                
                                <h5>Why Harmonic Mean?</h5>
                                <p>Arithmetic mean can be misleading. A system with Precision=0.9, Recall=0.1 has arithmetic mean 0.5, but F1=0.18 (correctly identifies it as poor). Harmonic mean penalizes imbalance.</p>
                                
                                <h5>Example:</h5>
                                <p>System A: Precision@5=0.9, Recall@5=0.5<br>
                                F1@5: \(2 \times \frac{0.9 \times 0.5}{0.9 + 0.5} = 2 \times \frac{0.45}{1.4} = 0.64\)</p>
                                
                                <p>System B: Precision@5=0.7, Recall@5=0.7<br>
                                F1@5: \(2 \times \frac{0.7 \times 0.7}{0.7 + 0.7} = 2 \times \frac{0.49}{1.4} = 0.70\)</p>
                                
                                <p>✅ System B has higher F1 despite lower precision, because it's more balanced.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Answer Faithfulness</h4>
                            <div class="formula-display">
                                \[\text{Faithfulness} = \frac{|\{\text{claims in answer}\} \cap \{\text{claims in context}\}|}{|\{\text{claims in answer}\}|}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Faithfulness measures how well the answer is <strong>grounded</strong> in the retrieved context. It's the fraction of claims/facts in the answer that are supported by the context. High faithfulness means the answer is based on the documents (not hallucinated).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\{\text{claims in answer}\}\):</strong> Set of factual claims made in the generated answer (e.g., "Paris is the capital", "France is in Europe")</li>
                                    <li><strong>\(\{\text{claims in context}\}\):</strong> Set of factual claims present in the retrieved context</li>
                                    <li><strong>\(\{\text{claims in answer}\} \cap \{\text{claims in context}\}\):</strong> Claims that appear in both answer and context (supported claims)</li>
                                    <li><strong>\(|\{\text{claims in answer}\}|\):</strong> Total number of claims in the answer (denominator)</li>
                                </ul>
                                
                                <h5>Interpretation:</h5>
                                <ul>
                                    <li><strong>Faithfulness = 1.0:</strong> All answer claims are supported by context (perfect grounding, no hallucinations)</li>
                                    <li><strong>Faithfulness = 0.8:</strong> 80% of claims are supported (good, minor hallucinations)</li>
                                    <li><strong>Faithfulness = 0.5:</strong> Only 50% of claims are supported (poor, significant hallucinations)</li>
                                    <li><strong>Faithfulness = 0.0:</strong> No claims are supported (worst case, answer is completely hallucinated)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is the capital of France?"</p>
                                <p>Context: "France is a country in Europe. Its capital city is Paris."</p>
                                <p>Answer: "The capital of France is Paris. France is located in Europe."</p>
                                
                                <p>Claims in answer: {"capital of France is Paris", "France is in Europe"}</p>
                                <p>Claims in context: {"France is a country", "France is in Europe", "capital is Paris"}</p>
                                <p>Supported claims: {"France is in Europe", "capital is Paris"} (2 out of 2)</p>
                                <p>Faithfulness: \(\frac{2}{2} = 1.0\) (perfect - all claims supported)</p>
                                
                                <p><strong>Bad example:</strong></p>
                                <p>Answer: "The capital of France is Paris. France has a population of 70 million."</p>
                                <p>Claims: {"capital is Paris", "population is 70 million"}</p>
                                <p>Supported: {"capital is Paris"} (1 out of 2)</p>
                                <p>Faithfulness: \(\frac{1}{2} = 0.5\) (50% - population claim is hallucinated, not in context)</p>
                                
                                <h5>Why Faithfulness is Critical:</h5>
                                <p>Low faithfulness means the LLM is making up information not in the retrieved documents. This defeats the purpose of RAG (grounding answers in documents). High faithfulness ensures answers are factual and verifiable.</p>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>Production systems:</strong> Faithfulness = 0.85-0.95 (85-95% of claims supported)</li>
                                    <li><strong>Good systems:</strong> Faithfulness > 0.9</li>
                                    <li><strong>Critical systems:</strong> Faithfulness > 0.95 (medical, legal, financial domains)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Answer Relevance</h4>
                            <div class="formula-display">
                                \[\text{Relevance} = \text{cosine}(E(\text{query}), E(\text{answer}))\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Relevance measures how well the answer addresses the query using semantic similarity. It's calculated as the cosine similarity between the query embedding and answer embedding. High relevance means the answer is semantically similar to what was asked.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(E(\text{query})\):</strong> Embedding vector of the user's query</li>
                                    <li><strong>\(E(\text{answer})\):</strong> Embedding vector of the generated answer</li>
                                    <li><strong>\(\text{cosine}(\ldots)\):</strong> Cosine similarity between the two embeddings (range: [-1, 1], typically [0, 1] for normalized embeddings)</li>
                                </ul>
                                
                                <h5>Intuition:</h5>
                                <p>If the answer is relevant to the query, their embeddings should point in similar directions in vector space, resulting in high cosine similarity. If the answer is off-topic, embeddings point in different directions, resulting in low similarity.</p>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is machine learning?"</p>
                                <p><strong>Relevant answer:</strong> "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming."<br>
                                Cosine similarity: 0.92 (very high - answer directly addresses the query)</p>
                                
                                <p><strong>Irrelevant answer:</strong> "The weather today is sunny with a high of 75 degrees."<br>
                                Cosine similarity: 0.15 (very low - answer is completely off-topic)</p>
                                
                                <p><strong>Partially relevant answer:</strong> "Artificial intelligence includes various techniques."<br>
                                Cosine similarity: 0.65 (moderate - somewhat related but not directly answering)</p>
                                
                                <h5>Why This Matters:</h5>
                                <p>Even if an answer is faithful (grounded in context), it might not be relevant to the query. For example, if someone asks "What is Python?" and the system answers "Python is a snake species," the answer is faithful to some context but not relevant to the programming question.</p>
                                
                                <h5>Typical Values:</h5>
                                <ul>
                                    <li><strong>Highly relevant:</strong> Relevance > 0.85</li>
                                    <li><strong>Moderately relevant:</strong> Relevance = 0.7-0.85</li>
                                    <li><strong>Low relevance:</strong> Relevance < 0.7</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>6. Answer Completeness</h4>
                            <div class="formula-display">
                                \[\text{Completeness} = \frac{|\{\text{query aspects addressed}\}|}{|\{\text{all query aspects}\}|}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>Completeness measures whether the answer addresses all parts of a multi-part or complex query. A query might have multiple aspects, and completeness checks if all aspects were covered.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\{\text{all query aspects}\}\):</strong> Set of all distinct questions or topics in the query</li>
                                    <li><strong>\(\{\text{query aspects addressed}\}\):</strong> Set of aspects that the answer actually covers</li>
                                    <li><strong>Completeness:</strong> Fraction of query aspects that were addressed</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "What is the capital of France and what is its population?"</p>
                                <p>Query aspects: {"capital of France", "population of France"}</p>
                                
                                <p><strong>Complete answer:</strong> "The capital of France is Paris. France has a population of approximately 67 million people."<br>
                                Aspects addressed: {"capital of France", "population of France"} (2 out of 2)<br>
                                Completeness: \(\frac{2}{2} = 1.0\) (100% complete)</p>
                                
                                <p><strong>Incomplete answer:</strong> "The capital of France is Paris."<br>
                                Aspects addressed: {"capital of France"} (1 out of 2)<br>
                                Completeness: \(\frac{1}{2} = 0.5\) (50% complete - missing population)</p>
                                
                                <h5>Why Completeness Matters:</h5>
                                <p>Users ask questions expecting complete answers. If a query has multiple parts and only some are answered, the user experience is poor. Completeness is especially important for complex, multi-hop questions.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Step-by-Step Examples</h4>
                            <h4>Example: Evaluating Retrieval</h4>
                            <p><strong>Query:</strong> "What is Python?"</p>
                            
                            <p><strong>Relevant documents:</strong> ["Python is a programming language", "Python tutorial"]</p>
                            
                            <p><strong>Retrieved top-3:</strong> ["Python is a programming language", "Java tutorial", "Python tutorial"]</p>
                            
                            <p><strong>Precision@3:</strong> 2/3 = 0.67 (2 relevant out of 3 retrieved)</p>
                            <p><strong>Recall@3:</strong> 2/2 = 1.0 (all relevant docs retrieved)</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Evaluating Generation</h4>
                            <p><strong>Query:</strong> "What is the capital of France?"</p>
                            <p><strong>Context:</strong> "France is a country. Its capital is Paris."</p>
                            <p><strong>Generated answer:</strong> "The capital of France is Paris."</p>
                            
                            <p><strong>Faithfulness:</strong> 1.0 (answer fully supported by context)</p>
                            <p><strong>Relevance:</strong> 0.95 (high semantic similarity to query)</p>
                            <p><strong>Completeness:</strong> 1.0 (answer is complete)</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>RAG Evaluation Metrics</h4>
                            <pre><code class="language-python">def evaluate_retrieval(relevant_docs, retrieved_docs, k=10):
    """
    Evaluate retrieval quality
    """
    # Precision@k
    relevant_retrieved = len(set(relevant_docs) & set(retrieved_docs[:k]))
    precision = relevant_retrieved / k
    
    # Recall@k
    recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0
    
    # F1 score
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'precision@k': precision,
        'recall@k': recall,
        'f1@k': f1
    }

def evaluate_answer_faithfulness(answer, context):
    """
    Check if answer claims are supported by context
    """
    # Extract claims from answer (simplified)
    answer_claims = extract_claims(answer)
    context_claims = extract_claims(context)
    
    # Check how many answer claims are in context
    supported = len(set(answer_claims) & set(context_claims))
    faithfulness = supported / len(answer_claims) if answer_claims else 0
    
    return faithfulness

# Example usage
relevant = ['doc1', 'doc2', 'doc3']
retrieved = ['doc1', 'doc4', 'doc2', 'doc5']
metrics = evaluate_retrieval(relevant, retrieved, k=3)
print(metrics)</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Production RAG with Error Handling</h4>
                            <pre><code class="language-python">class ProductionRAG:
    """Production-ready RAG with error handling"""
    
    def __init__(self, embedder, vector_db, llm):
        self.embedder = embedder
        self.vector_db = vector_db
        self.llm = llm
        self.cache = {}
    
    def query(self, question, top_k=5, use_cache=True):
        """Query with error handling and caching"""
        try:
            # Check cache
            if use_cache and question in self.cache:
                return self.cache[question]
            
            # Retrieve
            contexts = self.retrieve(question, top_k)
            
            if not contexts:
                return "I couldn't find relevant information to answer your question."
            
            # Generate
            answer = self.generate(question, contexts)
            
            # Cache result
            if use_cache:
                self.cache[question] = answer
            
            return answer
            
        except Exception as e:
            # Log error
            print(f"Error in RAG query: {e}")
            return "I encountered an error processing your question. Please try again."
    
    def retrieve(self, question, top_k):
        """Retrieve with fallback"""
        try:
            query_embedding = self.embedder.encode([question])
            results = self.vector_db.search(query_embedding, top_k=top_k)
            return [r['text'] for r in results if r['score'] > 0.7]  # Threshold
        except:
            return []
    
    def generate(self, question, contexts):
        """Generate with context validation"""
        if not contexts:
            return "No relevant context found."
        
        prompt = f"Context: {' '.join(contexts)}\n\nQuestion: {question}\nAnswer:"
        return self.llm.generate(prompt)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Production RAG Systems</h3>
                            <p><strong>Enterprise knowledge bases:</strong></p>
                            <ul>
                                <li>Internal documentation search (Confluence, Notion)</li>
                                <li>Company policy Q&A systems</li>
                                <li>Technical support knowledge bases</li>
                            </ul>
                            
                            <p><strong>Customer-facing applications:</strong></p>
                            <ul>
                                <li>E-commerce product Q&A</li>
                                <li>FAQ chatbots</li>
                                <li>Help center assistants</li>
                            </ul>
                            
                            <p><strong>Research and analysis:</strong></p>
                            <ul>
                                <li>Legal document analysis systems</li>
                                <li>Medical literature Q&A</li>
                                <li>Academic paper search and summarization</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Best Practices</h3>
                            <p><strong>Deployment:</strong> Use managed vector databases, implement caching, monitor performance</p>
                            <p><strong>Quality:</strong> Regular evaluation, A/B testing, continuous improvement</p>
                            <p><strong>Reliability:</strong> Error handling, fallbacks, retries, graceful degradation</p>
                            <p><strong>Security:</strong> Access control, data privacy, input validation</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: Interview question: "What are the key considerations for production RAG systems?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Scalability (millions of docs, high traffic), monitoring (retrieval/answer quality), error handling (fallbacks, retries), performance optimization (caching, async), and reliability (99.9%+ uptime)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only cost</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No special considerations</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is retrieval precision@k and how is it calculated?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(\frac{|\{\text{relevant docs}\} \cap \{\text{retrieved top-k}\}|}{k}\) - fraction of retrieved top-k documents that are actually relevant</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Total number of documents</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Average similarity score</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Retrieval speed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: Interview question: "How do you monitor RAG system quality in production?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Track retrieval metrics (precision@k, recall@k), answer quality (faithfulness, relevance), log queries/responses, set up alerts for quality degradation, and use A/B testing</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check errors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No monitoring needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only check once</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is answer faithfulness and why is it important?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Fraction of answer claims supported by retrieved context. Measures grounding quality - ensures answers are based on documents, not hallucinated</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Answer length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Answer speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Answer cost</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Interview question: "How would you scale a RAG system to handle millions of documents?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use distributed vector databases, implement sharding, use efficient indexing (HNSW), implement caching, use approximate search (ANN), and optimize embedding storage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Use single server</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No scaling needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only vertical scaling</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is the difference between precision@k and recall@k?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Precision@k: fraction of retrieved docs that are relevant. Recall@k: fraction of relevant docs that were retrieved. Precision measures quality, recall measures coverage</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Precision is speed, recall is accuracy</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Interview question: "How do you handle errors in production RAG systems?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Implement graceful fallbacks (return "no relevant info found"), retry mechanisms for transient failures, validate retrieved context quality, log errors for debugging, and use circuit breakers for downstream services</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Stop the system</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Ignore errors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Return empty response</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is answer relevance and how is it measured?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Semantic similarity between query and answer embeddings. Measures how well the answer addresses the query. Higher similarity = more relevant</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Answer length</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Number of documents used</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Retrieval speed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: Interview question: "How do you optimize RAG system performance?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Cache embeddings and retrieval results, use batch processing, implement async operations, optimize model selection (balance quality/latency), use approximate search, and implement connection pooling</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only use faster models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only use more servers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No optimization possible</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is the formula for retrieval recall@k?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(\frac{|\{\text{relevant docs}\} \cap \{\text{retrieved top-k}\}|}{|\{\text{relevant docs}\}|}\) - fraction of all relevant documents that were retrieved in top-k</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Total documents retrieved</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Average similarity</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Retrieval time</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: Interview question: "How do you ensure RAG system reliability in production?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Implement comprehensive error handling, fallback mechanisms, retry logic with exponential backoff, health checks, monitoring/alerting, graceful degradation, and redundancy for critical components</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No error handling needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only check once</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Manual monitoring only</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: Interview question: "What metrics would you track for a production RAG system?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Retrieval metrics (precision@k, recall@k, MRR), answer quality (faithfulness, relevance, completeness), latency (retrieval time, generation time), cost (API calls, tokens), error rates, and user satisfaction</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only cost</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No metrics needed</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter6" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 6</a>
                
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
