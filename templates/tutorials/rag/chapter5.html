<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: Retrieval Strategies - RAG & Retrieval Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}?v=2">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/rag/rag.css') }}?v=3">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/rag" class="course-link">
                    <span>RAG & Retrieval Systems</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 5: Retrieval Strategies</h1>
                <p class="chapter-subtitle">Finding Relevant Information</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="71"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/rag/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/rag/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/rag/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/rag/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/rag/chapter5" class="chapter-nav-btn active">Chapter 5</a>
                    <a href="/tutorials/rag/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/rag/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand retrieval strategies fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Retrieval Strategies</h2>
                        
                        <div class="explanation-box">
                            <h3>The Heart of RAG: Finding the Right Documents</h3>
                            <p>Retrieval is arguably the most critical component of a RAG system. If you retrieve irrelevant documents, even the best LLM can't generate a good answer. <strong>Retrieval strategies</strong> determine how you find the most relevant documents (or chunks) for a user's query from your knowledge base.</p>
                            
                            <p><strong>The retrieval challenge:</strong> Users ask questions in natural language, but your knowledge base contains documents written in different styles, using different terminology, and covering various topics. A query like "How do I train a neural network?" needs to find documents about "neural network training," "model training," "deep learning training," or "backpropagation" - even if those exact phrases don't appear in the documents.</p>
                            
                            <h4>Two Fundamental Approaches</h4>
                            <p>There are two fundamentally different ways to find relevant documents:</p>
                            
                            <ol>
                                <li><strong>Dense Retrieval (Semantic Search):</strong> Uses vector embeddings to find documents with similar <em>meaning</em>, even if they use different words. Handles synonyms, paraphrasing, and conceptual queries well.</li>
                                <li><strong>Sparse Retrieval (Keyword Search):</strong> Uses keyword matching (BM25, TF-IDF) to find documents containing specific terms. Fast and good for exact term matching, but misses semantic relationships.</li>
                            </ol>
                            
                            <p><strong>Hybrid Retrieval</strong> combines both approaches, often achieving better results than either method alone by leveraging the strengths of both semantic understanding and exact keyword matching.</p>
                            
                            <div class="example-box">
                                <h5>Example: Why Retrieval Strategy Matters</h5>
                                <p><strong>Query:</strong> "Python machine learning library"</p>
                                <p><strong>Dense retrieval finds:</strong> Documents about "scikit-learn," "TensorFlow," "PyTorch" (semantic similarity - these are ML libraries for Python)</p>
                                <p><strong>Sparse retrieval finds:</strong> Documents containing exact phrase "Python machine learning library"</p>
                                <p><strong>Hybrid retrieval finds:</strong> Both semantically similar libraries AND documents with exact phrase matches, providing comprehensive coverage</p>
                                <p>✅ <strong>Result:</strong> Hybrid retrieval gives you the best of both worlds - you don't miss relevant documents.</p>
                            </div>
                        </div>

                        <div class="explanation-box">
                            <h3>Key Concepts You'll Learn</h3>
                            <ul>
                                <li><strong>Dense vs Sparse Retrieval:</strong> Understanding when to use semantic search vs keyword search, and the trade-offs between them</li>
                                <li><strong>Hybrid Retrieval:</strong> Combining dense and sparse methods with weighted scoring for optimal results</li>
                                <li><strong>BM25 Algorithm:</strong> The most popular sparse retrieval algorithm - how it works and why it's better than simple TF-IDF</li>
                                <li><strong>Reranking:</strong> Using more sophisticated models (cross-encoders, LLMs) to refine initial retrieval results for better accuracy</li>
                                <li><strong>Top-k Selection:</strong> Choosing how many documents to retrieve - balancing context richness with computational cost</li>
                                <li><strong>Similarity Thresholds:</strong> Filtering out low-quality matches to ensure only relevant documents are used</li>
                                <li><strong>Retrieval Evaluation:</strong> Measuring retrieval quality using precision@k, recall@k, and other metrics</li>
                            </ul>
                            
                            <p><strong>Why this matters:</strong> Retrieval quality directly determines RAG system performance. Poor retrieval means the LLM receives irrelevant context, leading to inaccurate or hallucinated answers. Good retrieval ensures the LLM has the right information to generate accurate, grounded responses. Most RAG system improvements come from better retrieval, not just better LLMs.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Retrieval Strategies: Dense, Sparse, and Hybrid Approaches</h3>
                            
                            <p>Retrieval is the heart of RAG systems—it's the process of finding the most relevant documents for a user's query. There are three main approaches, each with distinct characteristics and use cases.</p>
                            
                            <h4>1. Dense Retrieval (Semantic Search)</h4>
                            <p><strong>What it is:</strong> Dense retrieval uses <strong>vector embeddings</strong> to represent both queries and documents in a high-dimensional space, then finds documents whose embeddings are "close" to the query embedding using similarity metrics like cosine similarity.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li>Convert the query into a dense vector using an embedding model (e.g., SentenceTransformer, OpenAI embeddings)</li>
                                <li>Compare this query vector with all document vectors in the vector database</li>
                                <li>Calculate similarity scores (cosine similarity, dot product, or Euclidean distance)</li>
                                <li>Return the top-k documents with highest similarity scores</li>
                            </ol>
                            
                            <p><strong>Key Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Semantic understanding:</strong> Understands meaning, not just keywords. A query about "automobile" will find documents about "car," "vehicle," or "auto" even if those exact words aren't present.</li>
                                <li>✅ <strong>Handles synonyms and paraphrasing:</strong> "Machine learning" and "ML" are treated as similar concepts. "How do I train a model?" finds documents about "model training" even if they use different wording.</li>
                                <li>✅ <strong>Context-aware:</strong> Understands that "Python programming" is different from "python snake" based on context.</li>
                                <li>✅ <strong>Language-agnostic:</strong> Works across languages if using multilingual embeddings (e.g., multilingual-MiniLM)</li>
                                <li>✅ <strong>Most common in modern RAG:</strong> This is the default approach for most production RAG systems</li>
                            </ul>
                            
                            <p><strong>Limitations:</strong></p>
                            <ul>
                                <li>❌ <strong>More computationally expensive:</strong> Requires embedding models and vector similarity calculations</li>
                                <li>❌ <strong>May miss exact keyword matches:</strong> If a document uses very specific terminology not well-represented in the embedding model's training data, it might be missed</li>
                                <li>❌ <strong>Requires good embeddings:</strong> Quality depends heavily on the embedding model used</li>
                                <li>❌ <strong>Storage overhead:</strong> Need to store vector embeddings for all documents</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Example: Dense Retrieval in Action</h5>
                                <p><strong>Query:</strong> "How do I train a neural network?"</p>
                                <p><strong>Document 1:</strong> "Training deep learning models requires adjusting hyperparameters like learning rate..." (No exact match for "neural network")</p>
                                <p><strong>Document 2:</strong> "Neural networks are trained using backpropagation..." (Exact match)</p>
                                <p><strong>Result:</strong> Dense retrieval finds BOTH documents because it understands that "neural network" and "deep learning model" are semantically similar concepts, even though Document 1 doesn't contain the exact phrase.</p>
                            </div>
                            
                            <h4>2. Sparse Retrieval (Keyword Search)</h4>
                            <p><strong>What it is:</strong> Sparse retrieval uses <strong>keyword matching</strong> based on term frequency and document frequency. The most common algorithms are BM25 (Best Matching 25) and TF-IDF (Term Frequency-Inverse Document Frequency).</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li>Extract keywords from the query (tokenization, stemming, stop word removal)</li>
                                <li>Count how often each keyword appears in each document (term frequency)</li>
                                <li>Weight keywords by their rarity across all documents (inverse document frequency - rare words are more important)</li>
                                <li>Calculate a relevance score for each document based on keyword matches</li>
                                <li>Return the top-k documents with highest scores</li>
                            </ol>
                            
                            <p><strong>Key Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Fast:</strong> Very efficient, especially with inverted indexes (traditional search engine technology used by Google, Elasticsearch)</li>
                                <li>✅ <strong>Good for exact matches:</strong> Excellent when you need to find documents containing specific terms or phrases</li>
                                <li>✅ <strong>Interpretable:</strong> You can see exactly which keywords matched and why a document was retrieved</li>
                                <li>✅ <strong>No embedding model needed:</strong> Works with raw text, no neural networks required</li>
                                <li>✅ <strong>Low storage:</strong> Only stores keyword indexes, not full vectors</li>
                            </ul>
                            
                            <p><strong>Limitations:</strong></p>
                            <ul>
                                <li>❌ <strong>No semantic understanding:</strong> "automobile" won't find "car" unless both terms are present in the document</li>
                                <li>❌ <strong>Requires exact term matches:</strong> Misses synonyms, paraphrases, and related concepts</li>
                                <li>❌ <strong>Language-specific:</strong> Needs language-specific tokenization and stemming</li>
                                <li>❌ <strong>Poor for conceptual queries:</strong> Struggles with queries like "things that help you learn" (no specific keywords)</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Example: Sparse Retrieval Limitation</h5>
                                <p><strong>Query:</strong> "automobile safety features"</p>
                                <p><strong>Document:</strong> "Modern cars have advanced safety systems including airbags and ABS brakes."</p>
                                <p><strong>Result:</strong> ❌ Sparse retrieval might MISS this document because it doesn't contain "automobile" or "safety features" as exact terms, even though the document is highly relevant (mentions "cars" and "safety systems").</p>
                            </div>
                            
                            <h4>3. Hybrid Retrieval: Best of Both Worlds</h4>
                            <p><strong>What it is:</strong> Hybrid retrieval <strong>combines</strong> dense and sparse retrieval, taking advantage of both approaches' strengths. This is often the best approach for production RAG systems.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ol>
                                <li>Run both dense retrieval and sparse retrieval separately</li>
                                <li>Get top-k results from each method (e.g., top 50 from each)</li>
                                <li>Combine the results (union or intersection of result sets)</li>
                                <li>Rerank the combined results using a weighted score: <code>final_score = α × dense_score + (1-α) × sparse_score</code> where α is typically 0.3-0.7</li>
                                <li>Return the top-k documents from the reranked list</li>
                            </ol>
                            
                            <p><strong>Why Hybrid Works Better:</strong></p>
                            <ul>
                                <li>✅ <strong>Catches both semantic and exact matches:</strong> Dense finds semantically similar docs, sparse finds exact keyword matches</li>
                                <li>✅ <strong>More comprehensive:</strong> Less likely to miss relevant documents</li>
                                <li>✅ <strong>Better for diverse queries:</strong> Some queries benefit from semantic search (conceptual questions), others from keyword search (specific term lookups)</li>
                                <li>✅ <strong>Production-proven:</strong> Used by many production systems (e.g., Google Search uses hybrid approaches)</li>
                            </ul>
                            
                            <div class="example-box">
                                <h5>Example: Hybrid Retrieval</h5>
                                <p><strong>Query:</strong> "Python machine learning library"</p>
                                <p><strong>Dense retrieval finds:</strong> Documents about "scikit-learn," "TensorFlow," "PyTorch" (semantic similarity - these are ML libraries for Python)</p>
                                <p><strong>Sparse retrieval finds:</strong> Documents containing exact phrase "Python machine learning library"</p>
                                <p><strong>Hybrid result:</strong> Combines both, ensuring you get both semantically similar libraries AND documents that explicitly mention the exact phrase. This gives you comprehensive coverage.</p>
                            </div>
                            
                            <p><strong>When to use:</strong></p>
                            <ul>
                                <li>✅ When you have diverse query types (some need semantic understanding, others need exact matches)</li>
                                <li>✅ When retrieval quality is critical and you can afford the computational cost</li>
                                <li>✅ For production systems where you want maximum coverage and accuracy</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Reranking: Refining Initial Retrieval Results</h3>
                            
                            <h4>Why Reranking is Necessary</h4>
                            <p>Initial retrieval (whether dense, sparse, or hybrid) is fast but not perfect. It uses relatively simple similarity calculations that may not capture the nuanced relationship between a query and a document. Reranking is a <strong>second-stage refinement</strong> that improves relevance by using more sophisticated (but slower) models.</p>
                            
                            <p><strong>The Problem with Initial Retrieval:</strong></p>
                            <ul>
                                <li>❌ <strong>Embedding similarity is approximate:</strong> Two documents might have similar embeddings but different relevance to a specific query. For example, two documents about "Python" might be similar (both about programming), but one might be about "Python for data science" and the other about "Python web development" - only one is relevant to a query about "data science."</li>
                                <li>❌ <strong>Keyword matching is surface-level:</strong> A document might contain all query keywords but not actually answer the question. For example, a document might mention "machine learning" and "tutorial" but be about a different topic entirely.</li>
                                <li>❌ <strong>Context matters:</strong> The same words in different contexts have different meanings. "Bank" could mean financial institution or river bank.</li>
                                <li>❌ <strong>Query intent:</strong> Initial retrieval doesn't deeply understand what the user is really asking. A query "how to install" might retrieve documents about installation in general, but the user might specifically want installation on Windows.</li>
                            </ul>
                            
                            <p><strong>The Solution: Reranking</strong></p>
                            <p>Reranking takes the top-k candidates from initial retrieval (e.g., top 100) and uses a more sophisticated model to re-score and reorder them, producing a final top-k list (e.g., top 5-10) that better matches query intent.</p>
                            
                            <h4>Reranking Methods</h4>
                            
                            <h5>1. Cross-Encoder Reranking</h5>
                            <p><strong>What it is:</strong> A neural network model (typically BERT-based) that processes the query and document <strong>together</strong> in a single forward pass, allowing the model to see the full interaction between query and document through attention mechanisms.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Input: <code>[CLS] query [SEP] document [SEP]</code> (concatenated query and document)</li>
                                <li>The model processes both simultaneously, allowing attention mechanisms to identify which parts of the document are most relevant to the query</li>
                                <li>Output: A relevance score (typically 0-1 or a similarity score)</li>
                                <li>Documents are reranked by these scores</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>More accurate:</strong> Significantly better than embedding-based similarity because it sees the full query-document interaction</li>
                                <li>✅ <strong>Context-aware:</strong> Understands how query and document relate in context</li>
                                <li>✅ <strong>Better for nuanced queries:</strong> Handles complex questions that require deep understanding</li>
                                <li>✅ <strong>Production-ready models:</strong> Pre-trained models available (e.g., ms-marco-MiniLM, cross-encoder models from SentenceTransformers)</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Slower:</strong> Must process each query-document pair separately (can't batch efficiently for different queries)</li>
                                <li>❌ <strong>More expensive:</strong> Requires running a neural network for each candidate</li>
                                <li>❌ <strong>Limited to top-k:</strong> Too slow to use on entire document collection (typically used on top 50-200 candidates)</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you have a small candidate set (50-200 documents) from initial retrieval and want maximum accuracy. This is the most common reranking approach in production RAG systems.</p>
                            
                            <h5>2. LLM-Based Reranking</h5>
                            <p><strong>What it is:</strong> Uses a large language model (like GPT-4, Claude) to score or rank documents based on their relevance to the query.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>For each candidate document, create a prompt asking the LLM to score relevance</li>
                                <li>Example prompt: "Rate how relevant this document is to the query 'X' on a scale of 1-10. Document: [document text]"</li>
                                <li>Use LLM's output to rerank documents</li>
                                <li>Alternatively, ask LLM to directly rank documents in order of relevance</li>
                            </ul>
                            
                            <p><strong>Advantages:</strong></p>
                            <ul>
                                <li>✅ <strong>Most accurate:</strong> LLMs have deep understanding of language and context</li>
                                <li>✅ <strong>Handles complex queries:</strong> Can understand nuanced questions and multi-part queries</li>
                                <li>✅ <strong>Flexible:</strong> Can be prompted to consider specific factors (relevance, completeness, accuracy)</li>
                            </ul>
                            
                            <p><strong>Disadvantages:</strong></p>
                            <ul>
                                <li>❌ <strong>Very slow:</strong> LLM inference is much slower than specialized reranking models</li>
                                <li>❌ <strong>Expensive:</strong> API costs can be high if reranking many documents</li>
                                <li>❌ <strong>Inconsistent:</strong> LLM outputs can be non-deterministic</li>
                                <li>❌ <strong>Limited scalability:</strong> Not practical for high-volume systems</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> For high-value queries where accuracy is critical and you can afford the latency and cost. Often used in research or low-volume production systems.</p>
                            
                            <h4>Reranking Best Practices</h4>
                            <ul>
                                <li><strong>Use two-stage retrieval:</strong> Fast initial retrieval (dense/sparse) → slower reranking on top-k candidates</li>
                                <li><strong>Typical pipeline:</strong> Retrieve top 100-200 → Rerank to top 5-10</li>
                                <li><strong>Balance accuracy vs speed:</strong> More accurate reranking is slower; choose based on your latency requirements</li>
                                <li><strong>Cache reranking results:</strong> For common queries, cache reranked results to avoid recomputation</li>
                                <li><strong>Monitor performance:</strong> Track reranking accuracy, latency, and cost</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="explanation-box">
                            <h3>Retrieval Scoring Formulas Overview</h3>
                            <p>Retrieval strategies use mathematical formulas to score and rank documents. Understanding these formulas helps you choose the right retrieval method, tune parameters, and optimize performance. This section covers the key formulas used in dense, sparse, hybrid, and reranking approaches.</p>
                        </div>
                        
                        <div class="formula-box">
                            <h4>1. Hybrid Retrieval Score</h4>
                            <div class="formula-display">
                                \[\text{hybrid\_score}(q, d) = \alpha \cdot \text{cosine}(E(q), E(d)) + (1 - \alpha) \cdot \text{BM25}(q, d)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Combines:</h5>
                                <p>Hybrid retrieval combines dense (semantic) and sparse (keyword) retrieval scores using a weighted average. This leverages both semantic understanding and exact keyword matching for better results than either method alone.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\text{cosine}(E(q), E(d))\):</strong> Dense retrieval score - cosine similarity between query embedding \(E(q)\) and document embedding \(E(d)\). Range: [-1, 1], typically [0, 1] for normalized embeddings.</li>
                                    <li><strong>\(\text{BM25}(q, d)\):</strong> Sparse retrieval score - BM25 keyword matching score. Range: [0, ∞), but typically normalized to [0, 1] before combination.</li>
                                    <li><strong>\(\alpha\):</strong> Weighting factor controlling the balance between dense and sparse scores. Range: [0, 1]</li>
                                    <li><strong>\((1 - \alpha)\):</strong> Weight for sparse score (ensures weights sum to 1)</li>
                                </ul>
                                
                                <h5>Choosing \(\alpha\):</h5>
                                <ul>
                                    <li><strong>\(\alpha = 0.7\) (70% dense, 30% sparse):</strong> Emphasizes semantic similarity. Good when synonyms and paraphrasing are important.</li>
                                    <li><strong>\(\alpha = 0.5\) (50/50):</strong> Balanced approach, most common in production systems</li>
                                    <li><strong>\(\alpha = 0.3\) (30% dense, 70% sparse):</strong> Emphasizes exact keyword matching. Good when specific terminology is critical.</li>
                                </ul>
                                
                                <h5>Why Hybrid Works Better:</h5>
                                <p>Dense retrieval finds semantically similar documents (handles synonyms), while sparse retrieval finds documents with exact keyword matches. Combining both ensures you don't miss relevant documents that use different terminology OR exact phrases.</p>
                                
                                <h5>Example:</h5>
                                <p>Query: "Python machine learning library"</p>
                                <p>Document 1: "scikit-learn is a Python ML library" (semantic match, no exact phrase)<br>
                                Dense score: 0.85, BM25 score: 0.60<br>
                                Hybrid score (α=0.7): \(0.7 \times 0.85 + 0.3 \times 0.60 = 0.775\)</p>
                                
                                <p>Document 2: "Python machine learning library tutorial" (exact phrase match)<br>
                                Dense score: 0.80, BM25 score: 0.95<br>
                                Hybrid score (α=0.7): \(0.7 \times 0.80 + 0.3 \times 0.95 = 0.845\)</p>
                                
                                <p>✅ Both documents are retrieved, with Document 2 ranked higher due to exact phrase match.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>2. BM25 (Best Matching 25) Score</h4>
                            <div class="formula-display">
                                \[\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \times \frac{f(t, d) \times (k_1 + 1)}{f(t, d) + k_1 \times (1 - b + b \times \frac{|d|}{\text{avgdl}})}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Formula Measures:</h5>
                                <p>BM25 is a probabilistic ranking function that scores documents based on how well they match query keywords. It's an improvement over TF-IDF that handles document length normalization better and is widely used in search engines (including Elasticsearch, Solr).</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(\sum_{t \in q}\):</strong> Sum over all terms \(t\) in the query \(q\)</li>
                                    <li><strong>\(\text{IDF}(t)\):</strong> Inverse Document Frequency - measures how rare/important a term is. Formula: \(\text{IDF}(t) = \log \frac{N - n(t) + 0.5}{n(t) + 0.5}\) where \(N\) is total documents and \(n(t)\) is documents containing term \(t\)</li>
                                    <li><strong>\(f(t, d)\):</strong> Term frequency - how many times term \(t\) appears in document \(d\)</li>
                                    <li><strong>\(\frac{f(t, d) \times (k_1 + 1)}{f(t, d) + k_1 \times (1 - b + b \times \frac{|d|}{\text{avgdl}})}\):</strong> Term frequency saturation function - prevents very frequent terms from dominating</li>
                                    <li><strong>\(k_1\):</strong> Term frequency saturation parameter (typically 1.2-2.0). Controls how quickly term frequency saturates</li>
                                    <li><strong>\(b\):</strong> Length normalization parameter (typically 0.75). Controls how much document length affects the score</li>
                                    <li><strong>\(|d|\):</strong> Document length (number of words/tokens)</li>
                                    <li><strong>\(\text{avgdl}\):</strong> Average document length in the collection</li>
                                </ul>
                                
                                <h5>Key Components Explained:</h5>
                                
                                <p><strong>IDF Component:</strong> Rare terms (low document frequency) get higher IDF scores. This means documents containing rare, specific terms score higher than documents with common terms.</p>
                                
                                <p><strong>Term Frequency Saturation:</strong> The fraction \(\frac{f(t, d) \times (k_1 + 1)}{f(t, d) + k_1 \times \ldots}\) ensures that:
                                <ul>
                                    <li>First occurrence of a term: high contribution</li>
                                    <li>Additional occurrences: diminishing returns (saturation)</li>
                                    <li>Prevents documents with excessive term repetition from scoring too high</li>
                                </ul>
                                </p>
                                
                                <p><strong>Length Normalization:</strong> The term \(1 - b + b \times \frac{|d|}{\text{avgdl}}\) penalizes very long documents. Longer documents naturally have more term matches, so this normalization prevents them from always ranking highest.</p>
                                
                                <h5>Typical Parameter Values:</h5>
                                <ul>
                                    <li><strong>\(k_1 = 1.2\):</strong> Standard value, good for most use cases</li>
                                    <li><strong>\(b = 0.75\):</strong> Standard value, moderate length normalization</li>
                                    <li><strong>\(b = 0\):</strong> No length normalization (rarely used)</li>
                                    <li><strong>\(b = 1\):</strong> Full length normalization (strong penalty for long docs)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "machine learning"<br>
                                Document 1: "Machine learning is a subset of AI. Machine learning uses algorithms." (2 occurrences, short doc)<br>
                                Document 2: "Machine learning machine learning machine learning..." (10 occurrences, very long doc)</p>
                                
                                <p>BM25 gives Document 1 a higher score because:
                                <ul>
                                    <li>Length normalization penalizes Document 2's excessive length</li>
                                    <li>Term frequency saturation means the extra occurrences in Document 2 don't help much</li>
                                    <li>Document 1 has a better balance of term frequency and document length</li>
                                </ul>
                                </p>
                                
                                <h5>Why BM25 is Better Than TF-IDF:</h5>
                                <ul>
                                    <li>✅ <strong>Better length normalization:</strong> Handles document length more intelligently</li>
                                    <li>✅ <strong>Term frequency saturation:</strong> Prevents excessive term repetition from dominating scores</li>
                                    <li>✅ <strong>Proven in practice:</strong> Used by major search engines and information retrieval systems</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>3. Inverse Document Frequency (IDF)</h4>
                            <div class="formula-display">
                                \[\text{IDF}(t) = \log \frac{N - n(t) + 0.5}{n(t) + 0.5}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Measures:</h5>
                                <p>IDF measures how "rare" or "important" a term is across the entire document collection. Rare terms (appearing in few documents) get higher IDF scores, making them more important for ranking.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(N\):</strong> Total number of documents in the collection</li>
                                    <li><strong>\(n(t)\):</strong> Number of documents containing term \(t\)</li>
                                    <li><strong>\(N - n(t)\):</strong> Number of documents NOT containing term \(t\)</li>
                                    <li><strong>\(+ 0.5\):</strong> Smoothing factor to avoid division by zero and handle edge cases</li>
                                    <li><strong>\(\log\):</strong> Logarithm (typically natural log or base 2) - compresses the range</li>
                                </ul>
                                
                                <h5>Intuition:</h5>
                                <p>If a term appears in many documents (common word like "the", "is"), it's not very informative - low IDF. If a term appears in few documents (rare word like "quantum", "neural"), it's very informative - high IDF.</p>
                                
                                <h5>Example:</h5>
                                <p>Collection: 10,000 documents</p>
                                <p>Term "the": appears in 9,500 documents<br>
                                IDF: \(\log \frac{10000 - 9500 + 0.5}{9500 + 0.5} = \log \frac{500.5}{9500.5} \approx -2.95\) (very low, common word)</p>
                                
                                <p>Term "quantum": appears in 50 documents<br>
                                IDF: \(\log \frac{10000 - 50 + 0.5}{50 + 0.5} = \log \frac{9950.5}{50.5} \approx 5.3\) (very high, rare and informative)</p>
                                
                                <p>When query contains "quantum computing", documents with "quantum" score much higher because it's a rare, informative term.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>4. Cross-Encoder Reranking Score</h4>
                            <div class="formula-display">
                                \[\text{rerank\_score}(q, d) = \text{CrossEncoder}([q; d])\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Represents:</h5>
                                <p>A cross-encoder is a neural network model (typically BERT-based) that processes the query and document together in a single forward pass. The notation \([q; d]\) means the query and document are concatenated: <code>[CLS] query [SEP] document [SEP]</code>.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>\(q\):</strong> Query text</li>
                                    <li><strong>\(d\):</strong> Document text</li>
                                    <li><strong>\([q; d]\):</strong> Concatenated input: query and document together</li>
                                    <li><strong>\(\text{CrossEncoder}\):</strong> Neural network model that processes the concatenated input</li>
                                    <li><strong>Output:</strong> Relevance score (typically 0-1 or similarity score)</li>
                                </ul>
                                
                                <h5>Why Cross-Encoder is More Accurate:</h5>
                                <p><strong>Bi-encoder (embedding-based):</strong> Processes query and document separately, then compares embeddings. Fast but loses interaction information.</p>
                                <p><strong>Cross-encoder:</strong> Processes query and document together, allowing attention mechanisms to identify which parts of the document are most relevant to the query. More accurate but slower.</p>
                                
                                <h5>Attention Mechanism:</h5>
                                <p>The cross-encoder's attention mechanism allows the model to "look" at specific parts of the document when processing the query. For example, when the query is "capital of France", the model can focus attention on the part of the document that mentions "Paris" or "capital".</p>
                                
                                <h5>Performance Trade-off:</h5>
                                <ul>
                                    <li><strong>Accuracy:</strong> Cross-encoder typically achieves 10-30% better accuracy than bi-encoder</li>
                                    <li><strong>Speed:</strong> Cross-encoder is 10-100x slower because it must process each (query, document) pair separately</li>
                                    <li><strong>Use case:</strong> Rerank top-50 to top-200 candidates from initial retrieval (too slow for full collection)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <p>Query: "How to train a neural network?"<br>
                                Document: "Training deep learning models requires adjusting hyperparameters..."</p>
                                <p><strong>Bi-encoder:</strong> Embed query and document separately, compare embeddings → score: 0.75</p>
                                <p><strong>Cross-encoder:</strong> Process "[CLS] How to train a neural network? [SEP] Training deep learning models requires adjusting hyperparameters... [SEP]" → score: 0.92</p>
                                <p>✅ Cross-encoder sees the full interaction and understands that "neural network" and "deep learning models" are related in this context.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>5. Normalized Hybrid Score</h4>
                            <div class="formula-display">
                                \[\text{hybrid\_score}(q, d) = \alpha \cdot \frac{\text{cosine}(E(q), E(d)) - \min_{\text{cosine}}}{\max_{\text{cosine}} - \min_{\text{cosine}}} + (1 - \alpha) \cdot \frac{\text{BM25}(q, d) - \min_{\text{BM25}}}{\max_{\text{BM25}} - \min_{\text{BM25}}}\]
                            </div>
                            <div class="formula-explanation">
                                <h5>What This Does:</h5>
                                <p>Before combining dense and sparse scores, they should be normalized to the same scale (typically [0, 1]). This ensures that one method doesn't dominate simply because its scores are larger.</p>
                                
                                <h5>Breaking It Down:</h5>
                                <ul>
                                    <li><strong>Min-max normalization:</strong> \(\frac{x - \min}{\max - \min}\) scales values to [0, 1] range</li>
                                    <li><strong>\(\min_{\text{cosine}}, \max_{\text{cosine}}\):</strong> Minimum and maximum cosine similarity scores across all documents for this query</li>
                                    <li><strong>\(\min_{\text{BM25}}, \max_{\text{BM25}}\):</strong> Minimum and maximum BM25 scores across all documents for this query</li>
                                    <li>After normalization, both scores are in [0, 1] range, making them comparable</li>
                                </ul>
                                
                                <h5>Why Normalization is Important:</h5>
                                <p>Without normalization, if BM25 scores range from 0-100 and cosine scores range from 0-1, the BM25 component would dominate the hybrid score regardless of the \(\alpha\) value. Normalization ensures both components contribute proportionally.</p>
                                
                                <h5>Example:</h5>
                                <p>For a query, you get:</p>
                                <ul>
                                    <li>Cosine scores: [0.65, 0.72, 0.58, 0.80] → normalized: [0.27, 0.64, 0.0, 1.0]</li>
                                    <li>BM25 scores: [12.5, 45.3, 8.2, 67.8] → normalized: [0.07, 0.62, 0.0, 1.0]</li>
                                </ul>
                                <p>Now both are on the same [0, 1] scale and can be meaningfully combined with \(\alpha = 0.7\).</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Step-by-Step Examples</h4>
                            <h4>Example: Hybrid Search</h4>
                            <p><strong>Query:</strong> "Python machine learning library"</p>
                            
                            <p><strong>Vector search:</strong> Finds documents semantically similar (e.g., "scikit-learn", "TensorFlow")</p>
                            <p><strong>Keyword search:</strong> Finds documents with exact terms ("Python", "machine learning", "library")</p>
                            <p><strong>Hybrid:</strong> Combines both, retrieves documents that are both semantically relevant AND contain keywords</p>
                            
                            <p><strong>Result:</strong> Better retrieval quality than either method alone.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Reranking</h4>
                            <p><strong>Initial retrieval:</strong> Top-100 documents from vector search</p>
                            
                            <p><strong>Reranking:</strong> Use cross-encoder to score each (query, document) pair</p>
                            
                            <p><strong>Result:</strong> Top-5 most relevant documents after reranking</p>
                            
                            <p><strong>Benefit:</strong> Cross-encoder sees full query and document, more accurate than embedding similarity alone.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Code Implementation</h4>
                            <pre><code class="language-python">import numpy as np

from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
import numpy as np

# Initialize models
embedder = SentenceTransformer('all-MiniLM-L6-v2')
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# Documents
documents = ["Python machine learning library", "scikit-learn tutorial", "TensorFlow guide"]

# Hybrid search
def hybrid_search(query, documents, alpha=0.7, top_k=5):
    # Vector search
    query_emb = embedder.encode([query])
    doc_embs = embedder.encode(documents)
    vector_scores = np.dot(query_emb, doc_embs.T)[0]
    
    # Keyword search (BM25)
    tokenized_docs = [doc.split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)
    keyword_scores = bm25.get_scores(query.split())
    
    # Normalize scores
    vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min() + 1e-8)
    keyword_scores = (keyword_scores - keyword_scores.min()) / (keyword_scores.max() - keyword_scores.min() + 1e-8)
    
    # Combine
    hybrid_scores = alpha * vector_scores + (1 - alpha) * keyword_scores
    
    # Get top-k
    top_indices = np.argsort(hybrid_scores)[-top_k:][::-1]
    return [documents[i] for i in top_indices]

# Reranking
def rerank(query, documents, top_k=3):
    # Initial retrieval (e.g., top-10 from vector search)
    initial_docs = documents[:10]
    
    # Rerank with cross-encoder
    pairs = [[query, doc] for doc in initial_docs]
    scores = reranker.predict(pairs)
    
    # Get top-k
    top_indices = np.argsort(scores)[-top_k:][::-1]
    return [initial_docs[i] for i in top_indices]</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Where This Is Used</h3>
                            <h3>Retrieval Strategies in Practice</h3>
                            <p><strong>When to use vector search:</strong> Semantic similarity is important, synonyms matter, multilingual content</p>
                            <p><strong>When to use keyword search:</strong> Exact term matching needed, domain-specific terminology</p>
                            <p><strong>When to use hybrid:</strong> Best of both worlds, production systems often use this</p>
                            <p><strong>When to use reranking:</strong> Need highest accuracy, can afford extra latency</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Performance Trade-offs</h3>
                            <p><strong>Vector search:</strong> Fast, good semantic understanding, but may miss exact matches</p>
                            <p><strong>Keyword search:</strong> Very fast, exact matches, but misses synonyms</p>
                            <p><strong>Hybrid:</strong> Balanced, combines strengths, slightly slower</p>
                            <p><strong>Reranking:</strong> Most accurate, but slowest (processes each candidate)</p>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What are the main retrieval strategies in RAG?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense retrieval (semantic search), sparse retrieval (keyword/BM25), and hybrid retrieval (combining both)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only keyword search</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only semantic search</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random selection</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: Interview question: "What is the difference between dense and sparse retrieval?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense uses embeddings for semantic similarity (handles synonyms, paraphrasing). Sparse uses keyword matching (BM25, TF-IDF) for exact term matching. Dense is better for meaning, sparse for keywords</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Dense is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Sparse is always better</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is hybrid retrieval and why is it effective?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Combining dense and sparse retrieval scores, leveraging semantic understanding and keyword matching together for better results than either alone</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Using two different models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Searching twice</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Hybrid is not effective</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: In the hybrid score formula \(\text{score}(q, d) = \alpha \times \text{BM25}(q, d) + (1 - \alpha) \times \text{cosine}(E(q), E(d))\), what does \(\alpha\) control?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The weight between sparse (BM25) and dense (cosine similarity) scores. Higher α emphasizes keywords, lower α emphasizes semantics</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The number of results</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) The similarity threshold</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The embedding dimension</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Interview question: "What is BM25 and how does it work?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Best Matching 25 - a probabilistic ranking function that scores documents based on term frequency, inverse document frequency, and document length normalization. Better than TF-IDF for retrieval</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A neural network</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) An embedding model</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A database</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is reranking in retrieval?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Re-scoring initial retrieval results using more accurate models (cross-encoders) to improve relevance ordering before passing to LLM</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Removing results</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Combining results</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Reranking is not used</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: Interview question: "When would you use dense vs sparse retrieval?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Dense for semantic queries, synonyms, paraphrasing. Sparse for exact keyword matching, technical terms, names. Hybrid for best of both</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use dense</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use sparse</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random choice</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is a cross-encoder and how is it used in reranking?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A model that processes query and document together (not separately like bi-encoder), providing more accurate relevance scores but slower. Used for reranking top-k results</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A database</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) An embedding model</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A search algorithm</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: Interview question: "How do you choose top-k for retrieval?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Balance context quality (more docs = more info) vs cost/latency (fewer = faster, cheaper). Typical: 3-10. Test on your data. Use reranking if retrieving more initially</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always use k=1</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always use k=100</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) k doesn't matter</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What is the typical value of α in hybrid retrieval?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) 0.3-0.7, often around 0.5. Tune based on your data - more semantic queries need lower α, more keyword queries need higher α</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always 0.0</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always 1.0</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) α is not used</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: Interview question: "How do you evaluate retrieval quality?"</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Use metrics like precision@k, recall@k, MRR (Mean Reciprocal Rank), NDCG. Test on labeled query-document pairs. Measure downstream RAG answer quality</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only check speed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only check cost</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No evaluation needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the trade-off between retrieval speed and accuracy?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) More accurate methods (reranking, exact search) are slower. Approximate search (ANN) is faster but less accurate. Balance based on latency requirements and quality needs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Speed and accuracy are the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Faster is always more accurate</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Slower is always more accurate</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ↑ Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/rag" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Back to Tutorial</a>
                <a href="/tutorials/rag/chapter4" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">← Chapter 4</a>
                <a href="/tutorials/rag/chapter6" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 6 →</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/shared-quiz.js') }}?v=2"></script>
<script src="{{ url_for('static', filename='js/tutorials/rag/shared-tutorial.js') }}?v=2"></script>
    <script>
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
