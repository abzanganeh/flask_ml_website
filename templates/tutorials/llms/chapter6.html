<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 6: LoRA & Parameter-Efficient Fine-tuning - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 6: LoRA & Parameter-Efficient Fine-tuning</h1>
                <p class="chapter-subtitle">Efficient Adaptation</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="75"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn active">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand lora & parameter-efficient fine-tuning fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>LoRA & Parameter-Efficient Fine-tuning</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Efficient Adaptation</strong></p>
                            <p>This chapter provides comprehensive coverage of lora & parameter-efficient fine-tuning, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding lora & parameter-efficient fine-tuning is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Why Parameter-Efficient Fine-tuning?</h3>
                            <p><strong>Problem with full fine-tuning:</strong></p>
                            <ul>
                                <li>Large models (billions of parameters) require massive memory</li>
                                <li>Training all parameters is expensive</li>
                                <li>Storing multiple fine-tuned models requires huge storage</li>
                                <li>Not feasible for many users or edge devices</li>
                            </ul>
                            
                            <p><strong>Solution: Parameter-efficient methods</strong></p>
                            <ul>
                                <li>Only train small subset of parameters</li>
                                <li>Dramatically reduce memory and compute</li>
                                <li>Can fine-tune on single GPU</li>
                                <li>Store only small adapter weights</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>LoRA (Low-Rank Adaptation)</h3>
                            <p><strong>Key insight:</strong> Weight updates during fine-tuning have low intrinsic rank. We can approximate updates with low-rank matrices.</p>
                            
                            <p><strong>How it works:</strong></p>
                            <ul>
                                <li>Original weights W are frozen</li>
                                <li>Add trainable low-rank matrices B and A</li>
                                <li>W' = W + BA (where BA is low-rank)</li>
                                <li>Only train B and A (much fewer parameters)</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong> 10-100x reduction in trainable parameters, minimal performance loss.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Other Parameter-Efficient Methods</h3>
                            <p><strong>Adapter layers:</strong> Insert small trainable layers between transformer layers. Only adapters are trained.</p>
                            
                            <p><strong>Prompt tuning:</strong> Learn soft prompts (continuous embeddings) instead of model weights.</p>
                            
                            <p><strong>Prefix tuning:</strong> Similar to prompt tuning but applied to all layers.</p>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>LoRA Weight Update</h4>
                            <div class="formula-display">
                                \[W' = W_0 + \Delta W = W_0 + BA\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(W_0 \in \mathbb{R}^{d \times d}\): Original weight matrix (frozen)</li>
                                    <li>\(B \in \mathbb{R}^{d \times r}\): Trainable matrix (rank r)</li>
                                    <li>\(A \in \mathbb{R}^{r \times d}\): Trainable matrix</li>
                                    <li>\(r \ll d\): Rank is much smaller than dimension</li>
                                    <li>Parameters: \(2dr\) instead of \(d^2\)</li>
                                </ul>
                                
                                <h5>Example:</h5>
                                <ul>
                                    <li>d = 768, r = 8</li>
                                    <li>Full: 768¬≤ = 589,824 parameters</li>
                                    <li>LoRA: 2√ó768√ó8 = 12,288 parameters (2% of original!)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Forward Pass with LoRA</h4>
                            <div class="formula-display">
                                \[h = W'x = (W_0 + BA)x = W_0x + B(Ax)\]
                            </div>
                            <div class="formula-explanation">
                                <p>During forward pass, compute both terms. \(W_0x\) can be computed once and cached. Only \(B(Ax)\) needs recomputation during training.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Rank Selection</h4>
                            <div class="formula-display">
                                \[\text{Compression ratio} = \frac{2dr}{d^2} = \frac{2r}{d}\]
                            </div>
                            <div class="formula-explanation">
                                <p>For r=8 and d=768, compression ratio is ~2%. Lower rank = more compression but potentially lower performance. Typical ranks: 4-16.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: LoRA Parameter Reduction</h4>
                            <p><strong>GPT-2 base model:</strong> 124M parameters</p>
                            
                            <p><strong>Full fine-tuning:</strong></p>
                            <ul>
                                <li>Trainable: 124M parameters</li>
                                <li>Memory: ~2GB per batch</li>
                                <li>Storage: 500MB per fine-tuned model</li>
                            </ul>
                            
                            <p><strong>LoRA fine-tuning (r=8):</strong></p>
                            <ul>
                                <li>Trainable: ~1M parameters (LoRA matrices)</li>
                                <li>Memory: ~500MB per batch</li>
                                <li>Storage: 4MB per fine-tuned model (125x smaller!)</li>
                            </ul>
                            
                            <p><strong>Result:</strong> Can store 125 LoRA models in space of 1 full model, train on single GPU.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: LoRA Application</h4>
                            <p><strong>Scenario:</strong> Fine-tune GPT-2 for code generation</p>
                            
                            <p><strong>Step 1: Identify target layers</strong></p>
                            <ul>
                                <li>Apply LoRA to attention layers (c_attn, c_proj)</li>
                                <li>These layers capture most task-specific patterns</li>
                            </ul>
                            
                            <p><strong>Step 2: Initialize LoRA matrices</strong></p>
                            <ul>
                                <li>B initialized to zeros (so W' = W initially)</li>
                                <li>A initialized with small random values</li>
                                <li>Ensures training starts from pre-trained weights</li>
                            </ul>
                            
                            <p><strong>Step 3: Train only LoRA parameters</strong></p>
                            <ul>
                                <li>Freeze all original weights</li>
                                <li>Only update B and A matrices</li>
                                <li>Much faster and memory-efficient</li>
                            </ul>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>LoRA with PEFT Library</h4>
                            <pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# Load model
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # Rank (low-rank dimension)
    lora_alpha=16,  # Scaling factor
    lora_dropout=0.1,  # Dropout for LoRA layers
    target_modules=["c_attn", "c_proj"],  # Which layers to apply LoRA to
    bias="none"  # Don't train bias
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Print parameter counts
trainable = model.num_parameters(only_trainable=True)
total = model.num_parameters()
print(f"Trainable: {trainable:,}")
print(f"Total: {total:,}")
print(f"Trainable %: {100 * trainable / total:.2f}%")

# Now fine-tune as usual (but only LoRA params update)
# model.train()
# ... training loop ...</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Manual LoRA Implementation</h4>
                            <pre><code class="language-python">import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    """LoRA layer implementation"""
    
    def __init__(self, original_layer, rank=8, alpha=16):
        super().__init__()
        self.original_layer = original_layer  # Frozen
        self.rank = rank
        self.alpha = alpha
        
        # Get dimensions
        if isinstance(original_layer, nn.Linear):
            in_features = original_layer.in_features
            out_features = original_layer.out_features
        else:
            raise ValueError("Original layer must be nn.Linear")
        
        # LoRA matrices
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.02)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.scaling = alpha / rank
    
    def forward(self, x):
        # Original output
        original_output = self.original_layer(x)
        
        # LoRA output
        lora_output = self.lora_B @ (self.lora_A @ x.T).T
        lora_output = lora_output * self.scaling
        
        return original_output + lora_output

# Example usage
# original = nn.Linear(768, 768)
# lora_layer = LoRALayer(original, rank=8)
# output = lora_layer(input_tensor)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>LoRA Use Cases</h3>
                            <p><strong>Personalized models:</strong></p>
                            <ul>
                                <li>Fine-tune large models for individual users</li>
                                <li>Store only small LoRA weights per user</li>
                                <li>Enable personalization without massive storage</li>
                            </ul>
                            
                            <p><strong>Multi-task deployment:</strong></p>
                            <ul>
                                <li>One base model, multiple LoRA adapters</li>
                                <li>Switch between tasks by loading different LoRA weights</li>
                                <li>Much more efficient than multiple full models</li>
                            </ul>
                            
                            <p><strong>Edge deployment:</strong></p>
                            <ul>
                                <li>Fine-tune on edge devices with limited memory</li>
                                <li>LoRA enables fine-tuning on consumer GPUs</li>
                                <li>Makes large models accessible to more users</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>When to Use LoRA</h3>
                            <p><strong>Use LoRA when:</strong></p>
                            <ul>
                                <li>Memory/compute is limited</li>
                                <li>You need to fine-tune many variants</li>
                                <li>Storage is a concern</li>
                                <li>Performance trade-off is acceptable</li>
                            </ul>
                            
                            <p><strong>Use full fine-tuning when:</strong></p>
                            <ul>
                                <li>You need maximum performance</li>
                                <li>Resources are abundant</li>
                                <li>Only fine-tuning one or few models</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main problem that parameter-efficient fine-tuning methods like LoRA solve?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Large models require massive memory and compute for full fine-tuning, making it infeasible for many users. Parameter-efficient methods only train a small subset of parameters, dramatically reducing requirements</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Models are too small</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Training is too fast</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Models don't need fine-tuning</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is the key insight behind LoRA (Low-Rank Adaptation)?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Weight updates during fine-tuning have low intrinsic rank, so we can approximate updates with low-rank matrices B and A, where W' = W + BA, training only B and A instead of the full weight matrix W</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All weights must be trained</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) High-rank matrices are needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) LoRA increases model size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is the mathematical formulation for LoRA weight update?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(W' = W_0 + \Delta W = W_0 + BA\) where \(W_0\) is frozen, and only low-rank matrices B and A are trainable</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(W' = W_0 \times BA\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(W' = W_0 - BA\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(W' = BA\) only</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: For a weight matrix of size 768√ó768, how many parameters does LoRA train if rank r=8?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) 12,288 parameters (2√ó768√ó8), which is only about 2% of the original 589,824 parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) 589,824 parameters (same as full fine-tuning)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) 768 parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) 8 parameters</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What are other parameter-efficient fine-tuning methods besides LoRA?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Adapter layers (small trainable layers inserted between transformer layers), prompt tuning (learn soft prompts), and prefix tuning (similar to prompt tuning but applied to all layers)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only LoRA exists</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Full fine-tuning is the only method</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only freezing layers</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is the forward pass computation with LoRA?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(h = W'x = (W_0 + BA)x = W_0x + B(Ax)\), where \(W_0x\) can be cached and only \(B(Ax)\) needs recomputation during training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(h = W_0x\) only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(h = BAx\) only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(h = W_0 \times BA \times x\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is the compression ratio formula for LoRA?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(\text{Compression ratio} = \frac{2dr}{d^2} = \frac{2r}{d}\), which for r=8 and d=768 is approximately 2%</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(\text{Compression ratio} = \frac{d}{r}\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(\text{Compression ratio} = d^2\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(\text{Compression ratio} = 2r\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What are typical rank values (r) used in LoRA?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Typically 4-16, where lower rank means more compression but potentially lower performance, and higher rank means better performance but more parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Always 1</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Always equal to the dimension d</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Random values</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What are the benefits of LoRA fine-tuning compared to full fine-tuning?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) 10-100x reduction in trainable parameters, much less memory usage, faster training, ability to fine-tune on single GPU, and can store many LoRA adapters in the space of one full model</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) LoRA requires more memory</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) LoRA is slower</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) LoRA requires multiple GPUs</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: How are LoRA matrices typically initialized?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) B is initialized to zeros (so W' = W initially) and A is initialized with small random values, ensuring training starts from pre-trained weights</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Both are initialized randomly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Both are initialized to zeros</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Both are initialized to ones</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What are some use cases for LoRA?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Personalized models (one base model with multiple user-specific LoRA adapters), multi-task deployment (switch between tasks by loading different LoRA weights), and edge deployment (fine-tune on devices with limited memory)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only for large-scale training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only for image tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only for speech recognition</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What is the typical performance trade-off when using LoRA compared to full fine-tuning?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) LoRA typically achieves 90-95% of full fine-tuning performance while using only 1-10% of the trainable parameters, making it an excellent efficiency-performance trade-off</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) LoRA always outperforms full fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) LoRA performs significantly worse (less than 50% of full fine-tuning)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) LoRA and full fine-tuning have identical performance</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter5" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 5</a>
                <a href="/tutorials/llms/chapter7" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 7 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
