<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: Fine-tuning Strategies - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 5: Fine-tuning Strategies</h1>
                <p class="chapter-subtitle">Adapting Pre-trained Models</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="62"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn active">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand fine-tuning strategies fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Fine-tuning Strategies</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Adapting Pre-trained Models</strong></p>
                            <p>This chapter provides comprehensive coverage of fine-tuning strategies, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding fine-tuning strategies is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Fine-tuning Strategies</h3>
                            <p><strong>Full fine-tuning:</strong> Update all model parameters. Most powerful but requires most memory and compute.</p>
                            
                            <p><strong>Partial fine-tuning:</strong> Freeze early layers, only train later layers. Reduces memory requirements while maintaining most performance.</p>
                            
                            <p><strong>Parameter-efficient fine-tuning:</strong> Only train small subset of parameters (LoRA, adapters, prompt tuning). Very efficient, can run on single GPU.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Instruction Tuning</h3>
                            <p><strong>What it is:</strong> Fine-tuning on diverse tasks formatted as instructions. Teaches model to follow instructions and generalize to new tasks.</p>
                            
                            <p><strong>Example format:</strong></p>
                            <ul>
                                <li>Input: "Translate to French: Hello"</li>
                                <li>Output: "Bonjour"</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong> Model becomes better at following prompts, can handle diverse tasks, shows improved few-shot performance.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Multi-task Fine-tuning</h3>
                            <p><strong>Training on multiple tasks simultaneously:</strong></p>
                            <ul>
                                <li>Combines data from different tasks</li>
                                <li>Model learns to handle diverse scenarios</li>
                                <li>Better generalization than single-task fine-tuning</li>
                                <li>Requires careful task balancing</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Fine-tuning Objective</h4>
                            <div class="formula-display">
                                \[L_{\text{ft}} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | x_i, \theta_0 + \Delta\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(\theta_0\): Pre-trained parameters (frozen or partially frozen)</li>
                                    <li>\(\Delta\theta\): Parameter updates (small compared to \(\theta_0\))</li>
                                    <li>\(x_i, y_i\): Task-specific input-output pairs</li>
                                    <li>Much smaller dataset than pre-training</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>LoRA Decomposition</h4>
                            <div class="formula-display">
                                \[W' = W_0 + \Delta W = W_0 + BA\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(W_0\): Original weight matrix (frozen, d√ód)</li>
                                    <li>\(B\): Trainable matrix (d√ór, rank r)</li>
                                    <li>\(A\): Trainable matrix (r√ód)</li>
                                    <li>Only \(2dr\) parameters trained instead of \(d^2\)</li>
                                    <li>Typical: r = 4-16, much smaller than d</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Multi-task Loss</h4>
                            <div class="formula-display">
                                \[L = \sum_{t=1}^{T} \alpha_t L_t\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(T\): Number of tasks</li>
                                    <li>\(L_t\): Loss for task t</li>
                                    <li>\(\alpha_t\): Task weight (balances importance)</li>
                                    <li>Allows training on multiple tasks simultaneously</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Full vs LoRA Fine-tuning</h4>
                            <p><strong>Task:</strong> Fine-tune GPT-2 for sentiment analysis</p>
                            
                            <p><strong>Full fine-tuning:</strong></p>
                            <ul>
                                <li>Trainable parameters: 124M (all GPT-2 parameters)</li>
                                <li>Memory: ~2GB per batch</li>
                                <li>Training time: ~2 hours on GPU</li>
                                <li>Performance: Best possible</li>
                            </ul>
                            
                            <p><strong>LoRA fine-tuning (r=8):</strong></p>
                            <ul>
                                <li>Trainable parameters: ~1M (LoRA matrices only)</li>
                                <li>Memory: ~500MB per batch</li>
                                <li>Training time: ~30 minutes on GPU</li>
                                <li>Performance: ~95% of full fine-tuning</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Instruction Tuning</h4>
                            <p><strong>Training examples:</strong></p>
                            <ul>
                                <li>"Translate to French: Hello" ‚Üí "Bonjour"</li>
                                <li>"Summarize: [long text]" ‚Üí "[summary]"</li>
                                <li>"Classify sentiment: Great movie!" ‚Üí "Positive"</li>
                                <li>"Answer: What is AI?" ‚Üí "AI is..."</li>
                            </ul>
                            
                            <p><strong>Result:</strong> Model learns to follow instructions and can generalize to new instruction-formatted tasks.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Partial Fine-tuning (Freeze Early Layers)</h4>
                            <pre><code class="language-python">from transformers import GPT2ForSequenceClassification, GPT2Tokenizer

# Load model
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=2)

# Freeze early layers (first 6 out of 12)
for i in range(6):
    for param in model.transformer.h[i].parameters():
        param.requires_grad = False

# Later layers remain trainable
# Only train: layers 6-11 + classification head

# Count trainable parameters
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable}, Total: {total}, Ratio: {trainable/total:.2%}")</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Instruction Tuning Setup</h4>
                            <pre><code class="language-python"># Instruction tuning data format
instruction_data = [
    {
        "instruction": "Translate to French",
        "input": "Hello",
        "output": "Bonjour"
    },
    {
        "instruction": "Summarize",
        "input": "Long article text...",
        "output": "Summary text..."
    },
    {
        "instruction": "Classify sentiment",
        "input": "Great movie!",
        "output": "Positive"
    }
]

def format_instruction(example):
    """Format instruction for training"""
    prompt = f"{example['instruction']}: {example['input']}"
    target = example['output']
    return prompt, target

# Use with standard language modeling loss
# Model learns to generate target given instruction+input</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Fine-tuning Applications</h3>
                            <p><strong>Domain-specific models:</strong></p>
                            <ul>
                                <li>Medical LLMs: Fine-tuned on medical literature</li>
                                <li>Legal LLMs: Fine-tuned on legal documents</li>
                                <li>Code models: Fine-tuned on code repositories</li>
                                <li>Customer service: Fine-tuned on support tickets</li>
                            </ul>
                            
                            <p><strong>Task-specific models:</strong></p>
                            <ul>
                                <li>Sentiment analysis for product reviews</li>
                                <li>Named entity recognition for information extraction</li>
                                <li>Question answering for knowledge bases</li>
                                <li>Text classification for content moderation</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Instruction-tuned Models</h3>
                            <p><strong>Models like ChatGPT, Claude use instruction tuning:</strong></p>
                            <ul>
                                <li>Better at following user instructions</li>
                                <li>More helpful and aligned with human intent</li>
                                <li>Can handle diverse tasks without task-specific fine-tuning</li>
                                <li>Show improved safety and reduced harmful outputs</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main concept covered in this chapter?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Fine-tuning Strategies</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Related concept</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Different topic</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Unrelated topic</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter4" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 4</a>
                <a href="/tutorials/llms/chapter6" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 6 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>