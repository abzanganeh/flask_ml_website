<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: BERT Architecture - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 3: BERT Architecture</h1>
                <p class="chapter-subtitle">Understanding Encoder-Only Models</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="37"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn active">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand bert architecture fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>BERT Architecture</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Understanding Encoder-Only Models</strong></p>
                            <p>This chapter provides comprehensive coverage of bert architecture, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding bert architecture is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Fine-tuning Strategies</h3>
                            <p><strong>Full fine-tuning:</strong> Update all model parameters. Most powerful but requires most resources.</p>
                            
                            <p><strong>Partial fine-tuning:</strong> Freeze early layers, only train later layers. Reduces memory and compute.</p>
                            
                            <p><strong>Parameter-efficient methods:</strong> Only train small subset of parameters (LoRA, adapters). Very efficient but may have slight performance trade-off.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Learning Rate Considerations</h3>
                            <p><strong>Why lower learning rates:</strong></p>
                            <ul>
                                <li>Pre-trained weights are already good</li>
                                <li>High learning rates can destroy pre-trained knowledge</li>
                                <li>Typical: 1e-5 to 1e-3 (vs 1e-3 to 1e-2 for training from scratch)</li>
                                <li>Often use learning rate schedule with warmup</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Catastrophic Forgetting</h3>
                            <p><strong>The problem:</strong> Fine-tuning on new task can cause model to forget what it learned during pre-training.</p>
                            
                            <p><strong>Solutions:</strong></p>
                            <ul>
                                <li>Lower learning rates</li>
                                <li>Freeze early layers</li>
                                <li>Use regularization</li>
                                <li>Continual learning techniques</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Fine-tuning Loss</h4>
                            <div class="formula-display">
                                \[L_{\text{fine-tune}} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | x_i, \theta_{\text{pre-trained}} + \Delta\theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(\theta_{\text{pre-trained}}\): Pre-trained model parameters</li>
                                    <li>\(\Delta\theta\): Parameter updates during fine-tuning</li>
                                    <li>\(y_i, x_i\): Task-specific labeled examples</li>
                                    <li>Updates are typically small (\(\Delta\theta\) is small)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>LoRA (Low-Rank Adaptation)</h4>
                            <div class="formula-display">
                                \[W' = W + \Delta W = W + BA\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(W\): Original weight matrix (frozen)</li>
                                    <li>\(B\): Low-rank matrix (trainable, rank r)</li>
                                    <li>\(A\): Low-rank matrix (trainable, rank r)</li>
                                    <li>Only \(B\) and \(A\) are trained, not \(W\)</li>
                                    <li>Reduces trainable parameters significantly</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Learning Rate Schedule</h4>
                            <div class="formula-display">
                                \[\text{lr}(t) = \begin{cases} 
                                \text{lr}_{\text{max}} \times \frac{t}{T_{\text{warmup}}} & \text{if } t < T_{\text{warmup}} \\
                                \text{lr}_{\text{max}} \times \left(1 - \frac{t - T_{\text{warmup}}}{T_{\text{total}} - T_{\text{warmup}}}\right) & \text{if } t \geq T_{\text{warmup}}
                                \end{cases}\]
                            </div>
                            <div class="formula-explanation">
                                <p>Warmup phase gradually increases learning rate, then linear decay. Prevents large gradient updates early in fine-tuning.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Fine-tuning for Sentiment Analysis</h4>
                            <p><strong>Pre-trained model:</strong> GPT-2 (general language understanding)</p>
                            
                            <p><strong>Task:</strong> Classify movie reviews as positive or negative</p>
                            
                            <p><strong>Step 1: Prepare data</strong></p>
                            <ul>
                                <li>Training examples: ("This movie was amazing!", "positive")</li>
                                <li>Format: Review text ‚Üí sentiment label</li>
                            </ul>
                            
                            <p><strong>Step 2: Add classification head</strong></p>
                            <ul>
                                <li>Add linear layer on top of pre-trained model</li>
                                <li>Output: 2 classes (positive, negative)</li>
                            </ul>
                            
                            <p><strong>Step 3: Fine-tune</strong></p>
                            <ul>
                                <li>Learning rate: 2e-5 (much lower than pre-training)</li>
                                <li>Freeze early layers, train later layers + classification head</li>
                                <li>Train for 3-5 epochs</li>
                            </ul>
                            
                            <p><strong>Result:</strong> Model adapts general language knowledge to sentiment classification task.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: LoRA Fine-tuning</h4>
                            <p><strong>Original weight matrix:</strong> W (768√ó768) = 589,824 parameters</p>
                            
                            <p><strong>LoRA approach:</strong></p>
                            <ul>
                                <li>W (frozen): 589,824 parameters</li>
                                <li>B (768√ó8): 6,144 parameters (trainable)</li>
                                <li>A (8√ó768): 6,144 parameters (trainable)</li>
                                <li>Total trainable: 12,288 (2% of original!)</li>
                            </ul>
                            
                            <p><strong>Benefits:</strong> Much less memory, faster training, can fine-tune on single GPU.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Fine-tuning with HuggingFace</h4>
                            <pre><code class="language-python">from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import Dataset

# Load pre-trained model
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=2)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Prepare data
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

# Example data
data = {
    "text": ["This movie was amazing!", "Terrible acting.", "It was okay."],
    "label": [1, 0, 0]  # 1=positive, 0=negative
}
dataset = Dataset.from_dict(data)
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,  # Low learning rate for fine-tuning
    warmup_steps=100,
    logging_dir="./logs",
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)
trainer.train()</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>LoRA Fine-tuning with PEFT</h4>
                            <pre><code class="language-python">from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType

# Load model
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # Rank (low-rank dimension)
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["c_attn", "c_proj"]  # Which layers to apply LoRA to
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Now only LoRA parameters are trainable
print(f"Trainable parameters: {model.num_parameters(only_trainable=True)}")
print(f"Total parameters: {model.num_parameters()}")

# Fine-tune as usual (but with much fewer parameters)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Fine-tuning Use Cases</h3>
                            <p><strong>Domain-specific applications:</strong></p>
                            <ul>
                                <li><strong>Medical:</strong> Fine-tune on medical literature for clinical applications</li>
                                <li><strong>Legal:</strong> Fine-tune on legal documents for contract analysis</li>
                                <li><strong>Code:</strong> Fine-tune on code repositories for programming assistance</li>
                                <li><strong>Customer service:</strong> Fine-tune on support tickets for automated responses</li>
                            </ul>
                            
                            <p><strong>Task-specific fine-tuning:</strong></p>
                            <ul>
                                <li>Sentiment analysis for product reviews</li>
                                <li>Named entity recognition for information extraction</li>
                                <li>Question answering for knowledge bases</li>
                                <li>Text classification for content moderation</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>When to Use Fine-tuning vs Prompting</h3>
                            <p><strong>Use fine-tuning when:</strong></p>
                            <ul>
                                <li>You have task-specific labeled data</li>
                                <li>You need high performance on specific domain</li>
                                <li>Prompting doesn't achieve desired accuracy</li>
                                <li>You can afford training time and resources</li>
                            </ul>
                            
                            <p><strong>Use prompting when:</strong></p>
                            <ul>
                                <li>You have limited or no labeled data</li>
                                <li>You need quick iteration</li>
                                <li>Task is simple enough for few-shot learning</li>
                                <li>You want to avoid training overhead</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the key architectural difference between BERT and GPT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) BERT is encoder-only with bidirectional attention, while GPT is decoder-only with unidirectional (causal) attention</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) BERT is decoder-only while GPT is encoder-only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They have identical architectures</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) BERT uses only feedforward layers while GPT uses only attention</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is Masked Language Modeling (MLM) in BERT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A pre-training objective where random tokens are masked and the model predicts them using bidirectional context</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A method to hide model parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) A technique for generating text</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A way to reduce model size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is Next Sentence Prediction (NSP) used for in BERT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) To help the model understand relationships between sentences, improving performance on tasks like question answering and natural language inference</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To generate the next sentence in a sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To translate sentences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) To classify individual sentences</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What are some popular BERT variants?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) RoBERTa, ALBERT, and DistilBERT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) GPT-2, GPT-3, and GPT-4</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) LSTM and GRU</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) ResNet and VGG</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Why is BERT particularly well-suited for understanding tasks rather than generation tasks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Because its bidirectional attention allows it to see context from both directions, making it better at understanding relationships and meaning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Because it has fewer parameters than GPT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Because it uses a different activation function</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Because it trains faster</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is the main component of BERT's architecture?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Transformer encoder layers with self-attention and feedforward networks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Convolutional layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Recurrent layers (LSTM/GRU)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only feedforward layers</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: How does BERT handle input sequences?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It processes the entire sequence simultaneously using bidirectional attention, allowing each token to attend to all other tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It processes tokens sequentially from left to right only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It processes tokens sequentially from right to left only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It randomly processes tokens</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What special tokens does BERT use?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) [CLS] for classification, [SEP] for separation, and [MASK] for masked tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only [START] and [END] tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) No special tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only punctuation marks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What is the purpose of the [CLS] token in BERT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It aggregates sequence-level information and is often used as the representation for classification tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It marks the end of a sentence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It indicates the start of generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It is used for masking tokens</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: How is BERT typically fine-tuned for downstream tasks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A task-specific head (like a classification layer) is added on top of the pre-trained BERT model, and the entire model is fine-tuned on task-specific data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only the new head is trained while BERT is frozen</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) BERT is retrained from scratch for each task</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) BERT cannot be fine-tuned</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What makes DistilBERT different from BERT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) DistilBERT is a smaller, faster, and lighter version of BERT that uses knowledge distillation to achieve similar performance with fewer parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) DistilBERT is larger than BERT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) DistilBERT uses a different architecture (CNN instead of Transformer)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) DistilBERT is only for generation tasks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What types of tasks is BERT particularly good at?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Text classification, named entity recognition, question answering, sentiment analysis, and natural language inference</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only text generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only image classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only speech recognition</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 2</a>
                <a href="/tutorials/llms/chapter4" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 4 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
