<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: GPT Architecture - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 4: GPT Architecture</h1>
                <p class="chapter-subtitle">Understanding Decoder-Only Models</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="50"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn active">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand gpt architecture fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>GPT Architecture</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Understanding Decoder-Only Models</strong></p>
                            <p>This chapter provides comprehensive coverage of gpt architecture, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding gpt architecture is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>GPT Architecture Overview</h3>
                            <p><strong>GPT (Generative Pre-trained Transformer) is decoder-only:</strong></p>
                            <ul>
                                <li>Uses only decoder layers from transformer architecture</li>
                                <li>Autoregressive: generates text one token at a time</li>
                                <li>Causal masking: can only attend to previous tokens</li>
                                <li>Pre-trained on next token prediction</li>
                            </ul>
                            
                            <p><strong>Key difference from BERT:</strong></p>
                            <ul>
                                <li>BERT: Bidirectional, understands context from both directions</li>
                                <li>GPT: Unidirectional, generates left-to-right</li>
                                <li>BERT: Better for understanding tasks</li>
                                <li>GPT: Better for generation tasks</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Scaling in GPT Models</h3>
                            <p><strong>GPT models have scaled dramatically:</strong></p>
                            <ul>
                                <li>GPT-1 (2018): 117M parameters</li>
                                <li>GPT-2 (2019): 1.5B parameters</li>
                                <li>GPT-3 (2020): 175B parameters</li>
                                <li>GPT-4 (2023): Estimated 1T+ parameters</li>
                            </ul>
                            
                            <p><strong>With scale comes:</strong></p>
                            <ul>
                                <li>Better language understanding</li>
                                <li>Emergent capabilities (reasoning, few-shot learning)</li>
                                <li>More coherent and contextually appropriate generation</li>
                                <li>Ability to follow complex instructions</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Generation Process</h3>
                            <p><strong>How GPT generates text:</strong></p>
                            <ul>
                                <li>Start with prompt/context</li>
                                <li>Process through transformer layers</li>
                                <li>Output probability distribution over vocabulary</li>
                                <li>Sample next token (greedy or with temperature)</li>
                                <li>Append to sequence, repeat until stop condition</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Autoregressive Generation</h4>
                            <div class="formula-display">
                                \[P(x_{t+1} | x_1, \ldots, x_t) = \text{softmax}(W \cdot h_t)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(x_1, \ldots, x_t\): Previous tokens</li>
                                    <li>\(h_t\): Hidden state at position t (from transformer)</li>
                                    <li>\(W\): Output projection matrix</li>
                                    <li>Output: Probability distribution over vocabulary</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Complete Sequence Probability</h4>
                            <div class="formula-display">
                                \[P(x_1, \ldots, x_n) = \prod_{t=1}^{n} P(x_t | x_1, \ldots, x_{t-1})\]
                            </div>
                            <div class="formula-explanation">
                                <p>The probability of a complete sequence is the product of conditional probabilities. Each token's probability depends on all previous tokens.</p>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Causal Attention Mask</h4>
                            <div class="formula-display">
                                \[\text{Mask}[i, j] = \begin{cases} 1 & \text{if } j \leq i \\ 0 & \text{if } j > i \end{cases}\]
                            </div>
                            <div class="formula-explanation">
                                <p>Position i can only attend to positions j where j ‚â§ i. This ensures the model cannot see future tokens during training or generation.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: GPT Generation Process</h4>
                            <p><strong>Prompt:</strong> "The capital of France is"</p>
                            
                            <p><strong>Step 1: Tokenization</strong></p>
                            <ul>
                                <li>Input ‚Üí ["The", "capital", "of", "France", "is"]</li>
                                <li>Each token converted to embedding</li>
                            </ul>
                            
                            <p><strong>Step 2: Forward Pass</strong></p>
                            <ul>
                                <li>Process through 12-96 transformer layers (depending on model)</li>
                                <li>Each layer refines the representation</li>
                                <li>Final hidden state for "is" position</li>
                            </ul>
                            
                            <p><strong>Step 3: Prediction</strong></p>
                            <ul>
                                <li>Output projection ‚Üí vocabulary probabilities</li>
                                <li>P("Paris") = 0.85, P("London") = 0.05, ...</li>
                            </ul>
                            
                            <p><strong>Step 4: Sampling</strong></p>
                            <ul>
                                <li>Sample "Paris"</li>
                                <li>New sequence: "The capital of France is Paris"</li>
                                <li>Continue generating if needed</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Causal Masking</h4>
                            <p><strong>Sequence:</strong> "The cat sat"</p>
                            
                            <p><strong>Attention patterns:</strong></p>
                            <ul>
                                <li>"The" can only attend to itself</li>
                                <li>"cat" can attend to "The" and itself</li>
                                <li>"sat" can attend to "The", "cat", and itself</li>
                                <li>Cannot see future tokens</li>
                            </ul>
                            
                            <p><strong>This matches inference:</strong> During generation, model only sees previous tokens, so training with causal masking ensures consistency.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>GPT Text Generation</h4>
                            <pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load model
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def generate_text(prompt, max_length=50, temperature=0.7):
    """
    Generate text using GPT model
    """
    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# Example
prompt = "The capital of France is"
result = generate_text(prompt)
print(result)  # "The capital of France is Paris"</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Implementing Causal Mask</h4>
                            <pre><code class="language-python">import torch
import torch.nn.functional as F

def create_causal_mask(seq_len):
    """
    Create causal attention mask
    """
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask

def masked_attention(Q, K, V, mask):
    """
    Apply causal masking to attention
    """
    # Compute attention scores
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)
    
    # Apply mask (set masked positions to -inf)
    mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions
    scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # Softmax
    attention_weights = F.softmax(scores, dim=-1)
    
    # Apply to values
    output = torch.matmul(attention_weights, V)
    return output

# Example
seq_len = 5
Q = torch.randn(1, 1, seq_len, 64)  # (batch, heads, seq, dim)
K = torch.randn(1, 1, seq_len, 64)
V = torch.randn(1, 1, seq_len, 64)
mask = create_causal_mask(seq_len)
output = masked_attention(Q, K, V, mask)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>GPT Applications</h3>
                            <p><strong>Text Generation:</strong></p>
                            <ul>
                                <li>Chatbots and conversational AI (ChatGPT)</li>
                                <li>Content creation (articles, stories, marketing)</li>
                                <li>Creative writing assistance</li>
                                <li>Email and document drafting</li>
                            </ul>
                            
                            <p><strong>Code Generation:</strong></p>
                            <ul>
                                <li>GitHub Copilot (code completion)</li>
                                <li>Code generation from natural language</li>
                                <li>Code explanation and documentation</li>
                                <li>Bug fixing and refactoring suggestions</li>
                            </ul>
                            
                            <p><strong>Specialized Tasks:</strong></p>
                            <ul>
                                <li>Text summarization</li>
                                <li>Translation</li>
                                <li>Question answering</li>
                                <li>Data extraction and formatting</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>GPT vs BERT Use Cases</h3>
                            <p><strong>Use GPT when:</strong></p>
                            <ul>
                                <li>You need to generate new text</li>
                                <li>Task requires creativity or continuation</li>
                                <li>You want conversational responses</li>
                                <li>Task benefits from autoregressive generation</li>
                            </ul>
                            
                            <p><strong>Use BERT when:</strong></p>
                            <ul>
                                <li>You need to understand/classify existing text</li>
                                <li>Task requires bidirectional context</li>
                                <li>You're extracting information, not generating</li>
                                <li>Task is classification or understanding</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the key architectural characteristic of GPT models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) GPT uses decoder-only transformer layers with causal (unidirectional) attention, enabling autoregressive text generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) GPT uses encoder-only layers with bidirectional attention</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) GPT uses both encoder and decoder layers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) GPT uses only feedforward layers</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What is causal masking in GPT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A mechanism that prevents tokens from attending to future tokens, ensuring each position can only see previous positions (j ‚â§ i)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A method to hide model parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) A technique to mask input tokens randomly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) A way to reduce model size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: How does GPT generate text?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Autoregressively, one token at a time, where each new token is predicted based on all previous tokens, and the process continues until a stop condition</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All tokens are generated simultaneously</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Tokens are generated from right to left</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Tokens are generated randomly</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is the mathematical formulation for autoregressive generation in GPT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(P(x_1, \ldots, x_n) = \prod_{t=1}^{n} P(x_t | x_1, \ldots, x_{t-1})\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(P(x_1, \ldots, x_n) = \sum_{t=1}^{n} P(x_t)\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(P(x_1, \ldots, x_n) = \max_t P(x_t)\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(P(x_1, \ldots, x_n) = \frac{1}{n} \sum_{t=1}^{n} P(x_t)\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: How has GPT scaled from GPT-1 to GPT-4?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) From 117M parameters (GPT-1) to an estimated 1T+ parameters (GPT-4), with dramatic improvements in capabilities and emergent abilities</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) The model size has remained constant</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) GPT models have gotten smaller over time</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only the training data has changed, not the model size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is the primary advantage of GPT's autoregressive architecture?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It enables natural text generation where each token is coherently built upon previous context, making it ideal for creative writing, code generation, and conversational AI</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It allows bidirectional understanding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It processes all tokens simultaneously</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It requires less memory</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What happens during GPT's forward pass for text generation?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The prompt is processed through transformer layers, producing a probability distribution over the vocabulary for the next token, which is then sampled and appended to the sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All possible sequences are generated simultaneously</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only the first token is generated</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Tokens are generated in reverse order</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is the key difference between GPT and BERT in terms of their training objectives?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) GPT is trained on next token prediction (autoregressive), while BERT is trained on masked token prediction (bidirectional)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They use identical training objectives</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) GPT is trained on classification while BERT is trained on generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) GPT uses masked language modeling while BERT uses next token prediction</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What are some key applications of GPT models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Chatbots and conversational AI, content creation, code generation (like GitHub Copilot), text summarization, and creative writing</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only image classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only speech recognition</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only data analysis</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: What does the causal attention mask ensure during GPT training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) That the model cannot see future tokens during training, which matches the inference scenario where only previous tokens are available</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) That all tokens can see all other tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) That tokens are randomly masked</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) That only the first token is visible</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the mathematical expression for the probability of the next token in GPT?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(P(x_{t+1} | x_1, \ldots, x_t) = \text{softmax}(W \cdot h_t)\) where \(h_t\) is the hidden state from the transformer</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(P(x_{t+1}) = \text{random()}\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(P(x_{t+1} | x_1, \ldots, x_t) = \text{constant}\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(P(x_{t+1}) = \frac{1}{\text{vocab\_size}}\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What are some emergent capabilities that appear in larger GPT models like GPT-3 and GPT-4?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Few-shot learning, chain-of-thought reasoning, instruction following, code understanding, and improved generalization to new tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only text generation</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only classification tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No new capabilities emerge with scale</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter3" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 3</a>
                <a href="/tutorials/llms/chapter5" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 5 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
