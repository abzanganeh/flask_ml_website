<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Introduction to Large Language Models - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 1: Introduction to Large Language Models</h1>
                <p class="chapter-subtitle">The Era of Pre-trained Models</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="12"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn active">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn ">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand introduction to large language models fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Introduction to Large Language Models</h2>
                        
                        <div class="explanation-box">
                            <h3>What are Large Language Models?</h3>
                            <p><strong>Large Language Models (LLMs) are neural networks trained on massive amounts of text data to understand and generate human-like text.</strong> They represent a paradigm shift in NLP: instead of training models from scratch for each task, we pre-train on vast corpora and then fine-tune for specific applications.</p>
                            
                            <p><strong>Think of LLMs like a student who has read everything:</strong></p>
                            <ul>
                                <li><strong>Pre-training:</strong> Like reading millions of books, articles, and websites - learning language patterns, facts, reasoning</li>
                                <li><strong>Fine-tuning:</strong> Like taking a specialized course - adapting general knowledge to a specific task</li>
                                <li><strong>Result:</strong> A model that understands language deeply and can be adapted to many tasks</li>
                            </ul>
                        </div>

                        <div class="explanation-box">
                            <h4>The Pre-training Revolution</h4>
                            <p><strong>Before LLMs (Pre-2018):</strong></p>
                            <ul>
                                <li>Each task required a separate model</li>
                                <li>Training from scratch for every application</li>
                                <li>Limited by available labeled data</li>
                                <li>Like learning to drive separately for each car model</li>
                            </ul>
                            
                            <p><strong>With LLMs (2018+):</strong></p>
                            <ul>
                                <li>One pre-trained model for many tasks</li>
                                <li>Fine-tune or prompt for specific needs</li>
                                <li>Leverage vast unlabeled text data</li>
                                <li>Like learning to drive once, then adapting to different vehicles</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <h4>üìö Evolution Timeline</h4>
                            <ul>
                                <li><strong>2013 - Word2Vec:</strong> Word embeddings (300 dimensions per word)</li>
                                <li><strong>2018 - BERT:</strong> Bidirectional encoder, 110M-340M parameters</li>
                                <li><strong>2019 - GPT-2:</strong> Decoder-only, 1.5B parameters</li>
                                <li><strong>2020 - GPT-3:</strong> 175B parameters, few-shot learning</li>
                                <li><strong>2022 - ChatGPT:</strong> Instruction-tuned GPT-3.5</li>
                                <li><strong>2023 - GPT-4:</strong> Multimodal, improved reasoning</li>
                            </ul>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Scale and Emergence</h3>
                            <p><strong>The scaling hypothesis:</strong> As models get larger (more parameters), trained on more data, with more compute, they show predictable improvements and emergent capabilities not present in smaller models.</p>
                            
                            <p><strong>Emergent abilities include:</strong></p>
                            <ul>
                                <li>Few-shot learning (performing tasks with just a few examples)</li>
                                <li>Chain-of-thought reasoning (step-by-step problem solving)</li>
                                <li>Instruction following (understanding and following complex instructions)</li>
                                <li>Code generation and understanding</li>
                                <li>Mathematical problem solving</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Pre-training vs Fine-tuning</h3>
                            <p><strong>Pre-training:</strong> Learning general language patterns from massive unlabeled text. This is expensive (weeks/months, many GPUs) but done once.</p>
                            
                            <p><strong>Fine-tuning:</strong> Adapting the pre-trained model to specific tasks using labeled data. Much faster and cheaper, can be done for many tasks.</p>
                            
                            <p><strong>Prompt engineering:</strong> Using carefully crafted prompts to guide model behavior without any training. Fastest approach but less powerful than fine-tuning.</p>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Architecture Types</h3>
                            <p><strong>Encoder-only (BERT):</strong> Bidirectional understanding, best for classification, QA, NER</p>
                            <p><strong>Decoder-only (GPT):</strong> Autoregressive generation, best for text generation, completion</p>
                            <p><strong>Encoder-decoder (T5):</strong> Both understanding and generation, best for translation, summarization</p>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Language Modeling Objective</h4>
                            <div class="formula-display">
                                \[P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} P(x_i | x_1, \ldots, x_{i-1})\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(x_i\): Token at position i</li>
                                    <li>\(P(x_i | x_1, \ldots, x_{i-1})\): Probability of token \(x_i\) given previous tokens</li>
                                    <li>Model learns to predict next token given context</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Training Loss (Cross-Entropy)</h4>
                            <div class="formula-display">
                                \[L = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | x_{<i}, \theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(N\): Number of training examples</li>
                                    <li>\(y_i\): Target token</li>
                                    <li>\(x_{<i}\): Context (previous tokens)</li>
                                    <li>\(\theta\): Model parameters</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Perplexity</h4>
                            <div class="formula-display">
                                \[\text{Perplexity} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_{<i})\right)\]
                            </div>
                            <div class="formula-explanation">
                                <p>Perplexity measures how well the model predicts a sequence. Lower perplexity means better prediction. It's the exponentiated average negative log-likelihood.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: How LLMs Generate Text</h4>
                            <p><strong>Input prompt:</strong> "The capital of France is"</p>
                            
                            <p><strong>Step 1: Tokenization</strong></p>
                            <ul>
                                <li>Input ‚Üí ["The", "capital", "of", "France", "is"]</li>
                                <li>Each token converted to embedding vector</li>
                            </ul>
                            
                            <p><strong>Step 2: Forward Pass</strong></p>
                            <ul>
                                <li>Model processes sequence through transformer layers</li>
                                <li>Creates contextualized representation for "is"</li>
                            </ul>
                            
                            <p><strong>Step 3: Prediction</strong></p>
                            <ul>
                                <li>Model outputs probability distribution over vocabulary</li>
                                <li>P("Paris") = 0.85, P("London") = 0.05, P("Berlin") = 0.03, ...</li>
                            </ul>
                            
                            <p><strong>Step 4: Sampling</strong></p>
                            <ul>
                                <li>Sample "Paris" (highest probability)</li>
                                <li>Output: "The capital of France is Paris"</li>
                            </ul>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Few-Shot Learning</h4>
                            <p><strong>Task:</strong> Classify sentiment without training</p>
                            
                            <p><strong>Prompt with examples:</strong></p>
                            <pre style="background: #f5f5f5; padding: 1rem; border-radius: 5px;">
Review: "This movie was amazing!"
Sentiment: Positive

Review: "Terrible acting and plot."
Sentiment: Negative

Review: "It was okay, nothing special."
Sentiment: Neutral</pre>
                            
                            <p><strong>Model learns the pattern</strong> from examples and can classify new reviews without fine-tuning!</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Using Pre-trained LLM with HuggingFace</h4>
                            <pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load pre-trained model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set pad token
tokenizer.pad_token = tokenizer.eos_token

# Input text
text = "The capital of France is"

# Tokenize
inputs = tokenizer(text, return_tensors="pt")

# Generate
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        max_length=20,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True
    )

# Decode
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
# Output: "The capital of France is Paris"</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Few-Shot Prompting Example</h4>
                            <pre><code class="language-python">def few_shot_classification(review, model, tokenizer):
    """
    Classify sentiment using few-shot prompting
    """
    prompt = f"""Review: "This movie was amazing!"
Sentiment: Positive

Review: "Terrible acting and plot."
Sentiment: Negative

Review: "{review}"
Sentiment:"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=inputs.input_ids.shape[1] + 10,
            temperature=0.3,
            do_sample=True
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sentiment = result.split("Sentiment:")[-1].strip()
    return sentiment

# Example usage
review = "It was okay, nothing special."
sentiment = few_shot_classification(review, model, tokenizer)
print(f"Sentiment: {sentiment}")  # Output: "Neutral"</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Major LLM Applications</h3>
                            <p><strong>Text Generation:</strong></p>
                            <ul>
                                <li>Chatbots and conversational AI (ChatGPT, Claude)</li>
                                <li>Content creation (articles, stories, marketing copy)</li>
                                <li>Code generation (GitHub Copilot, Codex)</li>
                                <li>Creative writing assistance</li>
                            </ul>
                            
                            <p><strong>Understanding Tasks:</strong></p>
                            <ul>
                                <li>Question answering systems</li>
                                <li>Text classification and sentiment analysis</li>
                                <li>Named entity recognition</li>
                                <li>Document summarization</li>
                            </ul>
                            
                            <p><strong>Specialized Applications:</strong></p>
                            <ul>
                                <li>Translation services</li>
                                <li>Educational tutoring systems</li>
                                <li>Customer service automation</li>
                                <li>Research assistance and information retrieval</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Impact on Industry</h3>
                            <p><strong>LLMs are transforming:</strong></p>
                            <ul>
                                <li><strong>Software Development:</strong> Code completion, debugging, documentation</li>
                                <li><strong>Content Creation:</strong> Writing, editing, translation</li>
                                <li><strong>Education:</strong> Personalized tutoring, content generation</li>
                                <li><strong>Healthcare:</strong> Medical documentation, research assistance</li>
                                <li><strong>Business:</strong> Customer service, data analysis, report generation</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is a Large Language Model (LLM)?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) A neural network with billions of parameters trained on massive text corpora to understand and generate human-like text, capable of performing various language tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) A small model with few parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only for classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not trained on text</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: What are the key characteristics of LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Large scale (billions of parameters), trained on massive datasets, transformer architecture, capable of few-shot learning, emergent abilities at scale, and general-purpose language understanding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Small scale only</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only for one task</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Limited capabilities</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: How do LLMs differ from traditional NLP models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) LLMs are pre-trained on general text and can be adapted to many tasks, while traditional models are task-specific and require labeled data for each task. LLMs show emergent abilities and can perform tasks they weren't explicitly trained on</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Traditional models are larger</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No difference</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is the scaling hypothesis in LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The idea that increasing model size, data size, and compute leads to predictable improvements in performance, with larger models showing emergent capabilities not present in smaller ones</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Smaller is always better</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Size doesn't matter</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only data matters</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: What are some examples of prominent LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) GPT series (GPT-3, GPT-4), BERT, T5, PaLM, LLaMA, Claude, and many others, each with different architectures and training approaches</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only GPT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only BERT</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No examples</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What is few-shot learning in LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) The ability to perform a task after seeing just a few examples in the prompt, without fine-tuning, demonstrating the model's learned understanding of language patterns</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Requires many examples</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Requires fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not possible</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: What is the difference between pre-training and fine-tuning in LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Pre-training learns general language patterns from large unlabeled text, while fine-tuning adapts the model to specific tasks using labeled data, typically with smaller learning rates</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're the same</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Fine-tuning comes first</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) No pre-training needed</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What are emergent abilities in LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Capabilities that appear only in larger models, such as reasoning, following instructions, and performing complex tasks, which smaller models cannot do despite similar training</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Present in all models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only in small models</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Not real</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What challenges come with training LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Massive computational requirements, need for large high-quality datasets, long training times, high costs, managing model size and memory, and ensuring data quality and diversity</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) No challenges</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only small datasets needed</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Very fast training</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: How do LLMs generate text?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Autoregressively, predicting the next token based on previous tokens, using probability distributions over vocabulary, often with sampling strategies like temperature and top-k sampling</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) All at once</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Randomly</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only first token</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the relationship between LLMs and transformers?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Most modern LLMs use transformer architecture as their backbone, leveraging attention mechanisms and the scalability of transformers to process and generate text effectively</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They're unrelated</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) LLMs don't use transformers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Transformers are LLMs</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: What are some applications of LLMs?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Text generation, chatbots, code generation, translation, summarization, question answering, content creation, language understanding, and many other NLP tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only classification</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Limited applications</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only generation</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter2" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 2 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
    <script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
        // Initialize KaTeX rendering
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\[", right: "\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\(", right: "\)", display: false}
                    ],
                    throwOnError: false
                });
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>