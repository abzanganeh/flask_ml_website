<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Pre-training Strategies - Large Language Models (LLMs)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/llms/llms.css') }}">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <!-- Prism.js for code syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>


</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/llms" class="course-link">
                    <span>Large Language Models (LLMs)</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <div class="tutorial-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 2: Pre-training Strategies</h1>
                <p class="chapter-subtitle">Learning from Unlabeled Data</p>
                
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" data-progress="25"></div>
                </div>
                
                <div class="chapter-navigation">
                    <a href="/tutorials/llms/chapter1" class="chapter-nav-btn ">Chapter 1</a>
                    <a href="/tutorials/llms/chapter2" class="chapter-nav-btn active">Chapter 2</a>
                    <a href="/tutorials/llms/chapter3" class="chapter-nav-btn ">Chapter 3</a>
                    <a href="/tutorials/llms/chapter4" class="chapter-nav-btn ">Chapter 4</a>
                    <a href="/tutorials/llms/chapter5" class="chapter-nav-btn ">Chapter 5</a>
                    <a href="/tutorials/llms/chapter6" class="chapter-nav-btn ">Chapter 6</a>
                    <a href="/tutorials/llms/chapter7" class="chapter-nav-btn ">Chapter 7</a>
                    <a href="/tutorials/llms/chapter8" class="chapter-nav-btn ">Chapter 8</a>
                </div>
                
                <div class="section-progress">
                    <div class="section-progress-fill" data-progress="14.3"></div>
                </div>
                
                <div class="section-nav">
                    <button class="section-nav-btn azbn-btn active" data-section="overview">Overview</button>
                    <button class="section-nav-btn azbn-btn" data-section="concepts">Key Concepts</button>
                    <button class="section-nav-btn azbn-btn" data-section="formulas">Formulas</button>
                    <button class="section-nav-btn azbn-btn" data-section="examples">Examples</button>
                    <button class="section-nav-btn azbn-btn" data-section="implementation">Implementation</button>
                    <button class="section-nav-btn azbn-btn" data-section="applications">Applications</button>
                    <button class="section-nav-btn azbn-btn" data-section="quiz">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-box">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand pre-training strategies fundamentals</li>
                        <li>Master the mathematical foundations</li>
                        <li>Learn practical implementation</li>
                        <li>Apply knowledge through examples</li>
                        <li>Recognize real-world applications</li>
                    </ul>
                </div>

                <main class="chapter-main-content">
                    <div id="overview" class="content-section active">
                        <h2>Pre-training Strategies</h2>
                        
                        <div class="explanation-box">
                            <h3>Introduction</h3>
                            <p><strong>Learning from Unlabeled Data</strong></p>
                            <p>This chapter provides comprehensive coverage of pre-training strategies, including detailed explanations, mathematical formulations, code implementations, and real-world examples.</p>
                        </div>

                        <div class="example-box">
                            <h4>üìö Why This Matters</h4>
                            <p>Understanding pre-training strategies is crucial for mastering modern AI systems. This chapter breaks down complex concepts into digestible explanations with step-by-step examples.</p>
                        </div>
                    </div>

                    <div id="concepts" class="content-section">
                        <h2>Key Concepts</h2>
                        
                        <div class="explanation-box">
                            <h3>Pre-training Objectives</h3>
                            <p><strong>Autoregressive Language Modeling (GPT-style):</strong></p>
                            <ul>
                                <li>Predict next token given previous tokens</li>
                                <li>Unidirectional (left-to-right)</li>
                                <li>Enables text generation</li>
                                <li>Training: "The cat sat" ‚Üí predict "on"</li>
                            </ul>
                            
                            <p><strong>Masked Language Modeling (BERT-style):</strong></p>
                            <ul>
                                <li>Predict masked tokens using bidirectional context</li>
                                <li>Bidirectional understanding</li>
                                <li>Better for understanding tasks</li>
                                <li>Training: "The [MASK] sat" ‚Üí predict "cat"</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Data Requirements</h3>
                            <p><strong>Scale matters:</strong></p>
                            <ul>
                                <li>GPT-3: Trained on ~500B tokens</li>
                                <li>LLaMA: Trained on ~1.4T tokens</li>
                                <li>More data generally leads to better performance</li>
                                <li>Quality is as important as quantity</li>
                            </ul>
                            
                            <p><strong>Data sources:</strong></p>
                            <ul>
                                <li>Web text (Common Crawl)</li>
                                <li>Books and literature</li>
                                <li>Wikipedia and encyclopedias</li>
                                <li>Code repositories</li>
                                <li>Scientific papers</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Training Challenges</h3>
                            <p><strong>Computational requirements:</strong></p>
                            <ul>
                                <li>GPT-3: Months on thousands of GPUs</li>
                                <li>Memory: Models require 100s of GB</li>
                                <li>Cost: Millions of dollars in compute</li>
                                <li>Infrastructure: Distributed training across data centers</li>
                            </ul>
                        </div>
                    </div>

                    <div id="formulas" class="content-section">
                        <h2>Mathematical Formulations</h2>
                        
                        <div class="formula-box">
                            <h4>Autoregressive Language Modeling</h4>
                            <div class="formula-display">
                                \[P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} P(x_i | x_1, \ldots, x_{i-1}, \theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(x_i\): Token at position i</li>
                                    <li>\(\theta\): Model parameters</li>
                                    <li>Model predicts probability of each token given previous context</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Masked Language Modeling</h4>
                            <div class="formula-display">
                                \[L = -\sum_{i \in M} \log P(x_i | x_{\backslash M}, \theta)\]
                            </div>
                            <div class="formula-explanation">
                                <h5>Where:</h5>
                                <ul>
                                    <li>\(M\): Set of masked token positions</li>
                                    <li>\(x_{\backslash M}\): All tokens except masked ones</li>
                                    <li>Model predicts masked tokens using bidirectional context</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="formula-box">
                            <h4>Next Sentence Prediction (BERT)</h4>
                            <div class="formula-display">
                                \[P(\text{IsNext} | \text{Sentence}_A, \text{Sentence}_B)\]
                            </div>
                            <div class="formula-explanation">
                                <p>Binary classification task: predict if Sentence_B follows Sentence_A. Helps model understand sentence relationships.</p>
                            </div>
                        </div>
                    </div>

                    <div id="examples" class="content-section">
                        <h2>Detailed Examples</h2>
                        
                        <div class="example-box">
                            <h4>Example: Autoregressive Pre-training</h4>
                            <p><strong>Training sequence:</strong> "The cat sat on the mat"</p>
                            
                            <p><strong>Training examples created:</strong></p>
                            <ul>
                                <li>Context: "The" ‚Üí Target: "cat"</li>
                                <li>Context: "The cat" ‚Üí Target: "sat"</li>
                                <li>Context: "The cat sat" ‚Üí Target: "on"</li>
                                <li>Context: "The cat sat on" ‚Üí Target: "the"</li>
                                <li>Context: "The cat sat on the" ‚Üí Target: "mat"</li>
                            </ul>
                            
                            <p><strong>Model learns:</strong> Given any context, predict the most likely next token. This builds understanding of language patterns, grammar, and semantics.</p>
                        </div>
                        
                        <div class="example-box">
                            <h4>Example: Masked Language Modeling</h4>
                            <p><strong>Original:</strong> "The cat sat on the mat"</p>
                            
                            <p><strong>Masked version:</strong> "The [MASK] sat on the mat"</p>
                            
                            <p><strong>Model task:</strong> Predict what [MASK] should be</p>
                            
                            <p><strong>Model sees:</strong> All tokens except the masked one (bidirectional context)</p>
                            
                            <p><strong>Prediction:</strong> P("cat") = 0.9, P("dog") = 0.05, P("bird") = 0.03, ...</p>
                            
                            <p><strong>Learning:</strong> Model learns to use context from both directions to understand word meaning and relationships.</p>
                        </div>
                    </div>

                    <div id="implementation" class="content-section">
                        <h2>Implementation</h2>
                        
                        <div class="code-box">
                            <h4>Pre-training Data Preparation</h4>
                            <pre><code class="language-python">import torch
from torch.utils.data import Dataset

class LanguageModelingDataset(Dataset):
    """Dataset for autoregressive language modeling"""
    
    def __init__(self, texts, tokenizer, max_length=512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # Tokenize
        tokens = self.tokenizer.encode(text, max_length=self.max_length, 
                                      truncation=True, padding='max_length')
        
        # Create input and target (shifted by 1)
        input_ids = torch.tensor(tokens[:-1])
        labels = torch.tensor(tokens[1:])
        
        return input_ids, labels

# Example usage
texts = [
    "The cat sat on the mat.",
    "Machine learning is fascinating.",
    "Transformers revolutionized NLP."
]

# tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# dataset = LanguageModelingDataset(texts, tokenizer)
# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)</code></pre>
                        </div>
                        
                        <div class="code-box">
                            <h4>Masked Language Modeling Data Preparation</h4>
                            <pre><code class="language-python">import random
import torch

def create_masked_lm_example(tokens, tokenizer, mask_prob=0.15):
    """
    Create masked language modeling example
    """
    labels = tokens.copy()
    masked_indices = []
    
    for i in range(len(tokens)):
        if tokens[i] in [tokenizer.cls_token_id, tokenizer.sep_token_id]:
            continue
        
        prob = random.random()
        if prob < mask_prob:
            masked_indices.append(i)
            # 80% of the time, replace with [MASK]
            if prob < mask_prob * 0.8:
                tokens[i] = tokenizer.mask_token_id
            # 10% of the time, replace with random token
            elif prob < mask_prob * 0.9:
                tokens[i] = random.randint(0, tokenizer.vocab_size - 1)
            # 10% of the time, keep original (but still predict it)
    
    return torch.tensor(tokens), torch.tensor(labels), masked_indices

# Example
# tokens = tokenizer.encode("The cat sat on the mat")
# input_ids, labels, masked = create_masked_lm_example(tokens, tokenizer)</code></pre>
                        </div>
                    </div>

                    <div id="applications" class="content-section">
                        <h2>Real-World Applications</h2>
                        
                        <div class="explanation-box">
                            <h3>Pre-training Enables Transfer Learning</h3>
                            <p><strong>Pre-trained models serve as foundation for:</strong></p>
                            <ul>
                                <li><strong>Fine-tuning:</strong> Adapt to specific tasks (classification, QA, NER)</li>
                                <li><strong>Few-shot learning:</strong> Perform tasks with minimal examples</li>
                                <li><strong>Zero-shot learning:</strong> Perform tasks without training examples</li>
                                <li><strong>Domain adaptation:</strong> Transfer to new domains with less data</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Pre-training Strategies in Practice</h3>
                            <p><strong>Different objectives for different goals:</strong></p>
                            <ul>
                                <li><strong>Autoregressive (GPT):</strong> Best for generation tasks</li>
                                <li><strong>Masked (BERT):</strong> Best for understanding tasks</li>
                                <li><strong>Hybrid approaches:</strong> Combine multiple objectives</li>
                                <li><strong>Instruction tuning:</strong> Fine-tune on instruction-following data</li>
                            </ul>
                        </div>
                        
                        <div class="explanation-box">
                            <h3>Scaling Laws</h3>
                            <p><strong>Research shows predictable relationships:</strong></p>
                            <ul>
                                <li>Performance improves with model size (parameters)</li>
                                <li>Performance improves with training data size</li>
                                <li>Performance improves with compute budget</li>
                                <li>Optimal ratios exist between these factors</li>
                            </ul>
                        </div>
                    </div>

                    <div id="quiz" class="content-section">
                        <h2>Test Your Understanding</h2>
                        
                        <div class="quiz-container">
                            <div class="quiz-question">
                                <h3>Question 1: What is the main difference between Autoregressive Language Modeling (GPT-style) and Masked Language Modeling (BERT-style)?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Autoregressive is unidirectional (left-to-right) and enables generation, while Masked is bidirectional and better for understanding tasks</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Autoregressive is bidirectional while Masked is unidirectional</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They are identical approaches with different names</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Autoregressive is for classification while Masked is for generation</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 2: In Masked Language Modeling, what percentage of tokens are typically masked during training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Approximately 15% of tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) 50% of tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) All tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only 5% of tokens</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 3: What is Next Sentence Prediction (NSP) used for in BERT pre-training?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) To help the model understand sentence relationships and improve performance on tasks requiring sentence pair understanding</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) To generate new sentences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) To classify individual sentences</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) To translate between languages</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 4: What is the mathematical formulation for Autoregressive Language Modeling?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) \(P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} P(x_i | x_1, \ldots, x_{i-1}, \theta)\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) \(P(x_1, x_2, \ldots, x_n) = \sum_{i=1}^{n} P(x_i | x_1, \ldots, x_{i-1})\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) \(P(x_1, x_2, \ldots, x_n) = \max_{i} P(x_i | x_1, \ldots, x_{i-1})\)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) \(P(x_1, x_2, \ldots, x_n) = \frac{1}{n} \sum_{i=1}^{n} P(x_i)\)</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 5: Approximately how many tokens was GPT-3 trained on?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) ~500 billion tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) ~50 billion tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) ~5 trillion tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) ~1 million tokens</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 6: What are the main data sources used for pre-training large language models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Web text (Common Crawl), books, Wikipedia, code repositories, and scientific papers</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only Wikipedia</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only social media posts</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only news articles</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 7: In the Masked Language Modeling loss function, what does \(x_{\backslash M}\) represent?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) All tokens except the masked ones</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only the masked tokens</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) The model parameters</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) The vocabulary size</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 8: What is the primary computational challenge in pre-training large language models?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Massive memory requirements (100s of GB), months of training on thousands of GPUs, and millions of dollars in compute costs</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Finding enough training data</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Choosing the right learning rate</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Implementing the loss function</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 9: What does the scaling law research show about LLM performance?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Performance improves predictably with model size, training data size, and compute budget, with optimal ratios existing between these factors</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Performance is random regardless of scale</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) Only model size matters, not data or compute</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Smaller models always perform better</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 10: In autoregressive pre-training, how are training examples created from a sequence like "The cat sat on the mat"?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) Multiple examples are created where each token is predicted given all previous tokens (e.g., "The" ‚Üí "cat", "The cat" ‚Üí "sat", etc.)</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) Only the last token is predicted</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) All tokens are predicted simultaneously</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) Only the first token is used</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 11: What is the primary advantage of pre-training for downstream tasks?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) It enables transfer learning, allowing models to be fine-tuned on specific tasks with less data, and supports few-shot and zero-shot learning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) It eliminates the need for any fine-tuning</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) It makes models smaller</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) It reduces training time for all tasks</div>
                            </div>
                            
                            <div class="quiz-question" style="margin-top: 2rem;">
                                <h3>Question 12: When creating masked language modeling examples, what happens to masked tokens 80% of the time?</h3>
                                <div class="quiz-option" onclick="checkAnswer(this, true)">A) They are replaced with the [MASK] token</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">B) They are left unchanged</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">C) They are deleted from the sequence</div>
                                <div class="quiz-option" onclick="checkAnswer(this, false)">D) They are replaced with random tokens</div>
                            </div>
                        </div>
                    </div>
                </main>
            </div>
        </section>
    </main>

    <footer style="background: #f8f9fa; padding: 2rem 0; margin-top: 3rem; border-top: 2px solid #5B7553;">
        <div class="azbn-container" style="text-align: center;">
            <button onclick="scrollToSectionNav()" class="azbn-btn" style="margin: 0.5rem;">
                ‚Üë Back to Section Navigation
            </button>
            <div style="margin-top: 1.5rem;">
                <a href="/tutorials/llms" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Back to Tutorial</a>
                <a href="/tutorials/llms/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none; margin: 0.5rem;">‚Üê Chapter 1</a>
                <a href="/tutorials/llms/chapter3" class="azbn-btn" style="text-decoration: none; margin: 0.5rem;">Chapter 3 ‚Üí</a>
            </div>
        </div>
    </footer>
    
        <!-- KaTeX for math rendering -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script src="{{ url_for('static', filename='js/tutorials/llms/shared-tutorial.js') }}"></script>
    <script>
        function scrollToSectionNav() {
            const sectionNav = document.querySelector('.section-nav');
            if (sectionNav) {
                sectionNav.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
        
        function checkAnswer(element, isCorrect) {
            element.parentNode.querySelectorAll('.quiz-option').forEach(opt => {
                opt.classList.remove('correct', 'incorrect');
            });
            if (isCorrect) {
                element.classList.add('correct');
                element.textContent += ' ‚úì Correct!';
            } else {
                element.classList.add('incorrect');
                element.textContent += ' ‚úó Incorrect';
                const correctOption = Array.from(element.parentNode.querySelectorAll('.quiz-option'))
                    .find(opt => {
                        const onclick = opt.getAttribute('onclick') || '';
                        return onclick.includes('true');
                    });
                if (correctOption && !correctOption.classList.contains('incorrect')) {
                    correctOption.classList.add('correct');
                    correctOption.textContent += ' ‚úì Correct Answer';
                }
            }
        }
            
// Initialize KaTeX rendering - ensure KaTeX is loaded first
        function initKaTeX() {
            if (typeof katex !== 'undefined' && typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: "$$", right: "$$", display: true},
                        {left: "\\[", right: "\\]", display: true},
                        {left: "$", right: "$", display: false},
                        {left: "\\(", right: "\\)", display: false}
                    ],
                    throwOnError: false
                });
            } else {
                // Retry if KaTeX not loaded yet
                setTimeout(initKaTeX, 100);
                return;
            }
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        
        // Wait for DOM and KaTeX to be ready
        if (document.readyState === 'loading') {
            document.addEventListener("DOMContentLoaded", initKaTeX);
        } else {
            // DOM already loaded, wait for KaTeX
            initKaTeX();
