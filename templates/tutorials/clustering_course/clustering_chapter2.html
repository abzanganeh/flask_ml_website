<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Distance Metrics Fundamentals - ML Fundamentals</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/ml_fundamentals/chapter2.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <style>
        .visualization-placeholder {
            background: linear-gradient(45deg, #f0f0f0, #e0e0e0);
            border: 2px dashed #999;
            padding: 2rem;
            text-align: center;
            border-radius: 8px;
            margin: 1rem 0;
            font-style: italic;
            color: #666;
        }
        .interactive-demo {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }
        .formula-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .proof-box {
            background: #fff8e1;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .property-box {
            background: #e8f5e8;
            border-left: 4px solid #4caf50;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f5f5f5;
            font-weight: bold;
        }
        .demo-controls {
            background: #e8f5e8;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .distance-calculator {
            background: white;
            border: 1px solid #ddd;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .metric-visualization {
            border: 1px solid #ccc;
            background: white;
            padding: 1rem;
            margin: 1rem 0;
            min-height: 250px;
            position: relative;
        }
        .point-input {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav" style="top: 50px;">
            <div class="azbn-container" style="display: flex; justify-content: space-between; align-items: center;">
                <a href="/tutorials/ml-fundamentals" style="text-decoration: none; color: #4f46e5; display: flex; align-items: center; gap: 0.5rem;">
                    <img src="/static/images/logo.png" alt="Logo" style="height: 40px;">
                    <span>ML Fundamentals - Clustering Course</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main style="padding-top: 100px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <div class="navigation-buttons">
                    <a href="/tutorials/ml-fundamentals/clustering/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none;">‚Üê Chapter 1: Introduction</a>
                    <a href="/tutorials/ml-fundamentals/clustering/chapter3" class="azbn-btn" style="text-decoration: none;">Chapter 3: Minkowski Distance ‚Üí</a>
                </div>

                <h1>Chapter 2: Distance Metrics Fundamentals</h1>
                <p style="font-size: 1.1rem; color: #666; margin-bottom: 2rem;">
                    Master the mathematical foundations of Euclidean and Manhattan distance metrics, the building blocks of clustering algorithms.
                </p>

                <div class="learning-objectives-card">
                    <h2>üéØ Learning Objectives</h2>
                    <ul>
                        <li>Understand the mathematical definitions and properties of Euclidean distance</li>
                        <li>Master Manhattan distance theory and geometric interpretations</li>
                        <li>Learn formal proofs of metric space properties</li>
                        <li>Analyze computational complexity and efficiency considerations</li>
                        <li>Apply distance metrics to real-world clustering problems</li>
                        <li>Compare and contrast different distance measures through interactive demos</li>
                        <li>Understand when to choose each metric for specific data types</li>
                    </ul>
                </div>

                <div class="section-nav">
                    <button class="active" onclick="showSection('introduction')">Metric Space Theory</button>
                    <button onclick="showSection('euclidean')">Euclidean Distance</button>
                    <button onclick="showSection('manhattan')">Manhattan Distance</button>
                    <button onclick="showSection('properties')">Mathematical Properties</button>
                    <button onclick="showSection('geometric')">Geometric Interpretation</button>
                    <button onclick="showSection('computational')">Computational Aspects</button>
                    <button onclick="showSection('applications')">Applications & Examples</button>
                    <button onclick="showSection('interactive')">Interactive Calculator</button>
                    <button onclick="showSection('quiz')">Quiz</button>
                </div>

                <!-- Metric Space Theory Section -->
                <div id="introduction" class="content-section active">
                    <h2>Metric Space Theory: The Mathematical Foundation</h2>
                    
                    <p>Before diving into specific distance metrics, we must understand the mathematical framework that underlies all distance measures in clustering. A metric space provides the formal foundation for measuring similarity and dissimilarity between data points.</p>

                    <div class="formula-box">
                        <h3>üìê Definition of a Metric Space</h3>
                        <p>A metric space is an ordered pair (X, d) where X is a set and d is a metric on X. A metric d: X √ó X ‚Üí ‚Ñù is a function that satisfies four fundamental properties for all x, y, z ‚àà X:</p>
                        
                        <div style="margin: 1.5rem 0;">
                            <h4>1Ô∏è‚É£ Non-negativity (Positivity)</h4>
                            <div style="text-align: center; font-size: 1.2rem; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                <strong>d(x, y) ‚â• 0</strong>
                            </div>
                            <p>The distance between any two points is always non-negative.</p>
                        </div>

                        <div style="margin: 1.5rem 0;">
                            <h4>2Ô∏è‚É£ Identity of Indiscernibles</h4>
                            <div style="text-align: center; font-size: 1.2rem; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                <strong>d(x, y) = 0 ‚ü∫ x = y</strong>
                            </div>
                            <p>The distance is zero if and only if the two points are identical.</p>
                        </div>

                        <div style="margin: 1.5rem 0;">
                            <h4>3Ô∏è‚É£ Symmetry</h4>
                            <div style="text-align: center; font-size: 1.2rem; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                <strong>d(x, y) = d(y, x)</strong>
                            </div>
                            <p>The distance from x to y equals the distance from y to x.</p>
                        </div>

                        <div style="margin: 1.5rem 0;">
                            <h4>4Ô∏è‚É£ Triangle Inequality</h4>
                            <div style="text-align: center; font-size: 1.2rem; margin: 0.5rem 0; background: white; padding: 0.5rem; border-radius: 4px;">
                                <strong>d(x, z) ‚â§ d(x, y) + d(y, z)</strong>
                            </div>
                            <p>The direct distance between two points is always less than or equal to any indirect path through a third point.</p>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìä Visualization: Metric Space Properties</h4>
                        <p><strong>Image Description:</strong> Four separate 2D diagrams illustrating each metric property. (1) Non-negativity: Two points A and B with distance arrow labeled d ‚â• 0. (2) Identity: Point A with d(A,A) = 0. (3) Symmetry: Points A and B with bidirectional arrows showing d(A,B) = d(B,A). (4) Triangle inequality: Triangle ABC with sides labeled showing d(A,C) ‚â§ d(A,B) + d(B,C).</p>
                        <p><em>This demonstrates the four fundamental properties that any valid distance metric must satisfy</em></p>
                    </div>

                    <h3>üî¨ Why These Properties Matter</h3>
                    <p>These four properties are not arbitrary mathematical abstractions‚Äîthey encode our intuitive understanding of distance and ensure that clustering algorithms behave predictably and meaningfully.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üéØ Non-negativity Impact</h4>
                            <p>Ensures that similarity measures have a consistent interpretation. Negative distances would create ambiguity in clustering algorithms.</p>
                            <p><strong>Example:</strong> K-means centroids are always meaningful since all distances are positive.</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üîç Identity Importance</h4>
                            <p>Guarantees that identical data points are recognized as such. Critical for handling duplicate data in clustering.</p>
                            <p><strong>Example:</strong> Prevents artificial cluster fragmentation due to identical points.</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>‚öñÔ∏è Symmetry Significance</h4>
                            <p>Ensures that similarity is mutual. Clustering algorithms rely on this for consistent neighbor relationships.</p>
                            <p><strong>Example:</strong> Hierarchical clustering linkage calculations require symmetric distances.</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üìê Triangle Inequality Utility</h4>
                            <p>Enables computational optimizations and ensures geometric consistency. Many algorithms use this for pruning.</p>
                            <p><strong>Example:</strong> DBSCAN uses triangle inequality to efficiently find neighbors.</p>
                        </div>
                    </div>

                    <h3>üßÆ Mathematical Notation and Conventions</h3>
                    <p>Throughout this course, we'll use consistent mathematical notation. Understanding this notation is crucial for following the theoretical developments.</p>

                    <div class="formula-box">
                        <h4>üìù Standard Notation</h4>
                        <ul>
                            <li><strong>‚Ñù‚Åø:</strong> n-dimensional real vector space</li>
                            <li><strong>x, y, z:</strong> Points/vectors in the space (typically column vectors)</li>
                            <li><strong>x·µ¢:</strong> The i-th component of vector x</li>
                            <li><strong>‚Äñx‚Äñ:</strong> Norm of vector x</li>
                            <li><strong>‚ü®x, y‚ü©:</strong> Inner product (dot product) of vectors x and y</li>
                            <li><strong>d(x, y):</strong> Distance between points x and y</li>
                            <li><strong>‚àÄ:</strong> "For all" (universal quantifier)</li>
                            <li><strong>‚àÉ:</strong> "There exists" (existential quantifier)</li>
                            <li><strong>‚ü∫:</strong> "If and only if" (bidirectional implication)</li>
                        </ul>
                    </div>

                    <h3>üé™ Common Distance Families</h3>
                    <p>Distance metrics can be classified into several major families, each with distinct mathematical properties and optimal use cases.</p>

                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Distance Family</th>
                                <th>Examples</th>
                                <th>Mathematical Form</th>
                                <th>Best For</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Lp Norms</strong></td>
                                <td>Euclidean, Manhattan, Chebyshev</td>
                                <td>(Œ£·µ¢ |x·µ¢ - y·µ¢|·µñ)^(1/p)</td>
                                <td>Continuous data, geometric problems</td>
                            </tr>
                            <tr>
                                <td><strong>Angular Metrics</strong></td>
                                <td>Cosine, Angular distance</td>
                                <td>Based on vector angles</td>
                                <td>High-dimensional, sparse data</td>
                            </tr>
                            <tr>
                                <td><strong>Edit Distances</strong></td>
                                <td>Hamming, Levenshtein</td>
                                <td>Character/element operations</td>
                                <td>Strings, sequences, categorical data</td>
                            </tr>
                            <tr>
                                <td><strong>Statistical Distances</strong></td>
                                <td>Mahalanobis, Chi-squared</td>
                                <td>Based on distributions</td>
                                <td>Correlated features, statistical data</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="property-box">
                        <h4>üß† Cache-Aware Computing</h4>
                        
                        <h5>üìÅ Memory Hierarchy Considerations:</h5>
                        <ul>
                            <li><strong>Cache Lines:</strong> Modern CPUs load 64-byte cache lines</li>
                            <li><strong>Spatial Locality:</strong> Adjacent memory accesses are faster</li>
                            <li><strong>Temporal Locality:</strong> Recently accessed data is faster</li>
                            <li><strong>Cache Misses:</strong> Can be 100x slower than cache hits</li>
                        </ul>
                        
                        <h5>üéØ Optimization Strategies:</h5>
                        <ul>
                            <li><strong>Row-major layout:</strong> Store points contiguously for better cache performance</li>
                            <li><strong>Blocking:</strong> Process data in cache-sized chunks</li>
                            <li><strong>Prefetching:</strong> Load next data while computing current</li>
                            <li><strong>Memory alignment:</strong> Align data structures to cache line boundaries</li>
                        </ul>
                        
                        <h5>üìä Practical Impact:</h5>
                        <p>Well-optimized distance calculations can be 5-10x faster than naive implementations, with Manhattan distance typically showing greater improvement due to simpler operations.</p>
                    </div>

                    <h3>üîß Algorithm-Specific Optimizations</h3>
                    <p>Different clustering algorithms can leverage specific properties of distance metrics for significant performance improvements.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üéØ K-means Optimizations</h4>
                            <h5>üîµ Euclidean Distance:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Squared distances:</strong> Avoid square root in comparison</li>
                                <li><strong>Triangle inequality:</strong> Skip calculations when possible</li>
                                <li><strong>Precompute centroids:</strong> Cache ‚Äñcentroid‚Äñ¬≤ values</li>
                                <li><strong>BLAS libraries:</strong> Use optimized linear algebra</li>
                            </ul>
                            
                            <h5>üî∑ Manhattan Distance:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Early termination:</strong> Stop when distance exceeds threshold</li>
                                <li><strong>Median updates:</strong> Use median instead of mean for centroids</li>
                                <li><strong>Sparse optimization:</strong> Skip zero components</li>
                                <li><strong>Integer arithmetic:</strong> Use when data allows</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üå≥ Hierarchical Clustering</h4>
                            <h5>üîß Distance Matrix Optimization:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Symmetry:</strong> Compute only upper triangle</li>
                                <li><strong>Sparse storage:</strong> Use compressed formats for large matrices</li>
                                <li><strong>Incremental updates:</strong> Update only affected distances</li>
                                <li><strong>Parallel computation:</strong> Distribute matrix calculations</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üîç Approximation Methods</h3>
                    <p>For very large datasets, exact distance calculations may be too expensive. Various approximation methods can provide significant speedups with controlled accuracy loss.</p>

                    <div class="formula-box">
                        <h4>‚ö° Fast Approximation Techniques</h4>
                        
                        <h5>üé≤ Random Sampling:</h5>
                        <ul>
                            <li><strong>Landmark points:</strong> Compute distances to representative subset</li>
                            <li><strong>Monte Carlo estimation:</strong> Sample point pairs for average distance</li>
                            <li><strong>Sketching algorithms:</strong> Use random projections for approximation</li>
                        </ul>
                        
                        <h5>üåê Spatial Data Structures:</h5>
                        <ul>
                            <li><strong>KD-trees:</strong> Effective for low-dimensional Euclidean spaces</li>
                            <li><strong>Ball trees:</strong> Work with any metric, better for high dimensions</li>
                            <li><strong>LSH (Locality Sensitive Hashing):</strong> Approximate nearest neighbors</li>
                            <li><strong>VP-trees:</strong> Vantage point trees for metric spaces</li>
                        </ul>
                        
                        <h5>üìä Dimensionality Reduction:</h5>
                        <ul>
                            <li><strong>PCA:</strong> Preserve Euclidean distances in lower dimensions</li>
                            <li><strong>Random projections:</strong> Johnson-Lindenstrauss lemma</li>
                            <li><strong>Feature selection:</strong> Remove irrelevant dimensions</li>
                        </ul>
                    </div>

                    <h3>üñ•Ô∏è Parallel and Distributed Computing</h3>
                    <p>Large-scale clustering requires parallel computation strategies that can efficiently distribute distance calculations across multiple processors or machines.</p>

                    <div class="property-box">
                        <h4>üîÑ Parallelization Strategies</h4>
                        
                        <h5>üßµ Thread-Level Parallelism:</h5>
                        <ul>
                            <li><strong>Data parallelism:</strong> Distribute points across threads</li>
                            <li><strong>Task parallelism:</strong> Parallel distance matrix computation</li>
                            <li><strong>Pipeline parallelism:</strong> Overlap computation and data movement</li>
                            <li><strong>NUMA awareness:</strong> Consider memory topology</li>
                        </ul>
                        
                        <h5>üåê Distributed Computing:</h5>
                        <ul>
                            <li><strong>MapReduce:</strong> Distribute pairwise distance calculations</li>
                            <li><strong>Spark:</strong> In-memory distributed distance matrices</li>
                            <li><strong>MPI:</strong> Message passing for HPC environments</li>
                            <li><strong>GPU acceleration:</strong> CUDA/OpenCL for massively parallel computation</li>
                        </ul>
                        
                        <h5>üìà Scalability Considerations:</h5>
                        <ul>
                            <li><strong>Communication overhead:</strong> Minimize data transfer</li>
                            <li><strong>Load balancing:</strong> Ensure even work distribution</li>
                            <li><strong>Fault tolerance:</strong> Handle node failures gracefully</li>
                            <li><strong>Memory constraints:</strong> Stream processing for large datasets</li>
                        </ul>
                    </div>
                </div>

                <!-- Applications Section -->
                <div id="applications" class="content-section">
                    <h2>Applications and Real-World Examples</h2>
                    
                    <p>Understanding when and how to apply Euclidean versus Manhattan distance requires examining real-world scenarios where each metric's properties align with problem characteristics. This section explores diverse applications across multiple domains, providing practical guidance for metric selection.</p>

                    <h3>üè™ E-commerce and Recommendation Systems</h3>
                    <p>Distance metrics play a crucial role in recommendation systems, where the choice between Euclidean and Manhattan distance can significantly impact recommendation quality and user experience.</p>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üõí Product Similarity (Euclidean)</h4>
                            <p><strong>Use Case:</strong> Finding similar products based on numerical features</p>
                            <p><strong>Features:</strong> Price, rating, dimensions, weight</p>
                            
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Example:</strong> Camera similarity<br>
                                Product A: [price: 500, rating: 4.2, megapixels: 24, weight: 600g]<br>
                                Product B: [price: 520, rating: 4.1, megapixels: 26, weight: 580g]<br>
                                <em>Euclidean distance captures overall similarity well</em>
                            </div>
                            
                            <p><strong>Why Euclidean:</strong> Features are continuous and correlations matter</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üë§ User Behavior (Manhattan)</h4>
                            <p><strong>Use Case:</strong> Finding similar users based on categorical preferences</p>
                            <p><strong>Features:</strong> Category purchases, brand preferences, activity counts</p>
                            
                            <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                                <strong>Example:</strong> User similarity<br>
                                User A: [books: 5, electronics: 2, clothing: 8, sports: 0]<br>
                                User B: [books: 7, electronics: 1, clothing: 6, sports: 1]<br>
                                <em>Manhattan distance better handles discrete counts</em>
                            </div>
                            
                            <p><strong>Why Manhattan:</strong> Features are counts/frequencies, independent categories</p>
                        </div>
                    </div>

                    <h3>üè• Healthcare and Medical Diagnosis</h3>
                    <p>Medical applications require careful consideration of distance metrics, as the choice can impact diagnostic accuracy and patient outcomes.</p>

                    <div class="formula-box">
                        <h4>üî¨ Medical Data Types and Metric Selection</h4>
                        
                        <h5>üìä Continuous Medical Measurements (Euclidean):</h5>
                        <ul>
                            <li><strong>Vital signs:</strong> Blood pressure, heart rate, temperature</li>
                            <li><strong>Lab values:</strong> Blood glucose, cholesterol, protein levels</li>
                            <li><strong>Physical measurements:</strong> Height, weight, BMI</li>
                            <li><strong>Imaging features:</strong> Tumor dimensions, organ volumes</li>
                        </ul>
                        
                        <h5>üè• Discrete Medical Data (Manhattan):</h5>
                        <ul>
                            <li><strong>Symptom counts:</strong> Number of symptoms present</li>
                            <li><strong>Medication dosages:</strong> Discrete pill counts</li>
                            <li><strong>Frequency data:</strong> Episodes per month, visits per year</li>
                            <li><strong>Severity scales:</strong> Pain scales (1-10), functional scores</li>
                        </ul>
                        
                        <h5>üéØ Case Study: Patient Similarity for Treatment Recommendation</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Scenario:</strong> Finding similar patients for personalized treatment</p>
                            <p><strong>Data:</strong> Mixed continuous (age, BMI, lab values) and discrete (symptom counts, severity scores)</p>
                            <p><strong>Solution:</strong> Combine normalized Euclidean for continuous features with Manhattan for discrete features</p>
                            <p><strong>Formula:</strong> d_total = w‚ÇÅ √ó d_E(continuous) + w‚ÇÇ √ó d_M(discrete)</p>
                        </div>
                    </div>

                    <h3>üåç Geographic and Location-Based Services</h3>
                    <p>Geographic applications provide clear intuitive examples of when each distance metric is appropriate.</p>

                    <div class="visualization-placeholder">
                        <h4>üó∫Ô∏è Visualization: Geographic Distance Comparison</h4>
                        <p><strong>Image Description:</strong> A city map showing two points A and B. Three different paths are highlighted: (1) Straight-line distance (red line) representing Euclidean distance "as the crow flies", (2) Driving route (blue path) following roads that approximates Euclidean in open areas, (3) Walking route in Manhattan grid (green step-like path) representing Manhattan distance in downtown grid. Distance measurements shown: Euclidean = 2.3 km, Driving = 3.1 km, Manhattan grid = 3.8 km.</p>
                        <p><em>This demonstrates how geographic constraints determine appropriate distance metrics</em></p>
                    </div>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>‚úàÔ∏è Air Travel (Euclidean)</h4>
                            <p><strong>Application:</strong> Flight routing, airport clustering</p>
                            <p><strong>Why Euclidean:</strong> Aircraft can travel in straight lines (great circle distances)</p>
                            <p><strong>Example:</strong> Grouping airports by geographic proximity for hub-and-spoke networks</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üöó Ground Transportation (Manhattan)</h4>
                            <p><strong>Application:</strong> Urban delivery, taxi routing</p>
                            <p><strong>Why Manhattan:</strong> Roads constrain movement to grid-like patterns</p>
                            <p><strong>Example:</strong> Optimizing delivery routes in downtown areas with grid street layouts</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üè¢ Service Area Planning</h4>
                            <p><strong>Application:</strong> Emergency services, retail locations</p>
                            <p><strong>Metric Choice:</strong> Depends on service type and terrain</p>
                            <p><strong>Example:</strong> Helicopter emergency services (Euclidean) vs ambulance services (Manhattan/road network)</p>
                        </div>
                    </div>

                    <h3>üí∞ Financial Services and Risk Analysis</h3>
                    <p>Financial applications require careful metric selection based on the nature of financial relationships and risk characteristics.</p>

                    <div class="property-box">
                        <h4>üìà Financial Distance Applications</h4>
                        
                        <h5>üìä Portfolio Analysis (Euclidean):</h5>
                        <ul>
                            <li><strong>Asset correlation clustering:</strong> Group correlated stocks</li>
                            <li><strong>Risk profiling:</strong> Cluster investments by risk-return characteristics</li>
                            <li><strong>Market segment analysis:</strong> Identify market sectors based on performance metrics</li>
                        </ul>
                        
                        <h5>üîç Fraud Detection (Manhattan):</h5>
                        <ul>
                            <li><strong>Transaction pattern analysis:</strong> Compare discrete transaction counts</li>
                            <li><strong>Behavioral anomaly detection:</strong> Identify unusual activity patterns</li>
                            <li><strong>Risk scoring:</strong> Combine multiple independent risk factors</li>
                        </ul>
                        
                        <h5>üí≥ Case Study: Credit Card Fraud Detection</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Challenge:</strong> Identify fraudulent transactions in real-time</p>
                            <p><strong>Features:</strong> Transaction amount, merchant category, time since last transaction, geographic distance from home</p>
                            <p><strong>Manhattan Distance Advantages:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Robust to outliers (extremely large transactions)</li>
                                <li>Better handles discrete categories</li>
                                <li>Computationally faster for real-time processing</li>
                                <li>More interpretable for explaining decisions</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üéÆ Gaming and Entertainment</h3>
                    <p>Game development and entertainment applications showcase practical distance metric selection based on gameplay mechanics and user experience considerations.</p>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üé≤ Turn-Based Strategy Games</h4>
                            <p><strong>Movement System:</strong> Grid-based with orthogonal movement</p>
                            <p><strong>Distance Metric:</strong> Manhattan distance</p>
                            <p><strong>Examples:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Chess piece movement (rook moves)</li>
                                <li>Tactical RPGs with grid movement</li>
                                <li>Board game simulations</li>
                                <li>Tile-based strategy games</li>
                            </ul>
                            <p><strong>Advantage:</strong> Natural mapping to game mechanics</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üèÉ Real-Time Action Games</h4>
                            <p><strong>Movement System:</strong> Free movement in continuous space</p>
                            <p><strong>Distance Metric:</strong> Euclidean distance</p>
                            <p><strong>Examples:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>First-person shooters</li>
                                <li>Racing games</li>
                                <li>Flight simulators</li>
                                <li>Physics-based games</li>
                            </ul>
                            <p><strong>Advantage:</strong> Matches player intuition for "real" distance</p>
                        </div>
                    </div>

                    <h3>üî¨ Text Mining and Natural Language Processing</h3>
                    <p>Text analysis applications demonstrate how data characteristics determine optimal distance metric selection.</p>

                    <div class="formula-box">
                        <h4>üìö Text Distance Applications</h4>
                        
                        <h5>üìñ Document Similarity (Often Neither Euclidean nor Manhattan):</h5>
                        <p>While text applications often use cosine similarity, understanding when Euclidean or Manhattan might apply:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Manhattan Distance for Text:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Word count comparisons</li>
                                <li>Frequency-based features</li>
                                <li>Sparse high-dimensional representations</li>
                                <li>Categorical text features (topic presence/absence)</li>
                            </ul>
                            
                            <p><strong>Euclidean Distance for Text:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Dense word embeddings (Word2Vec, BERT)</li>
                                <li>Continuous semantic features</li>
                                <li>Style analysis with continuous metrics</li>
                                <li>Sentiment scores and continuous ratings</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üè≠ Manufacturing and Quality Control</h3>
                    <p>Industrial applications often require robust distance metrics that can handle measurement noise and discrete quality indicators.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>‚öôÔ∏è Product Quality Clustering</h4>
                            <p><strong>Euclidean Applications:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Dimensional measurements</li>
                                <li>Temperature/pressure sensors</li>
                                <li>Continuous process variables</li>
                                <li>Physical property measurements</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üîß Defect Pattern Analysis</h4>
                            <p><strong>Manhattan Applications:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Defect counts by type</li>
                                <li>Binary quality indicators</li>
                                <li>Discrete severity levels</li>
                                <li>Pass/fail test results</li>
                            </ul>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìä Visualization: Manufacturing Quality Clustering</h4>
                        <p><strong>Image Description:</strong> Two side-by-side scatter plots showing the same manufacturing data clustered differently. Left plot: "Euclidean Distance" shows smooth, circular clusters of products based on continuous measurements (temperature, pressure). Right plot: "Manhattan Distance" shows angular, diamond-shaped clusters better separating products based on discrete defect counts and quality indicators. Different cluster boundaries highlight how metric choice affects quality categorization.</p>
                        <p><em>This demonstrates how distance metric choice impacts quality control clustering results</em></p>
                    </div>

                    <h3>üéØ Practical Guidelines for Metric Selection</h3>
                    <p>Based on the applications explored, here are practical guidelines for choosing between Euclidean and Manhattan distance:</p>

                    <div class="property-box">
                        <h4>‚úÖ Decision Framework</h4>
                        
                        <h5>üîµ Choose Euclidean Distance When:</h5>
                        <ul>
                            <li><strong>Continuous data:</strong> Features represent continuous measurements</li>
                            <li><strong>Correlated features:</strong> Feature relationships matter</li>
                            <li><strong>Geometric problems:</strong> Physical space or geometric intuition applies</li>
                            <li><strong>Gaussian assumptions:</strong> Data follows normal distributions</li>
                            <li><strong>Dense representations:</strong> Most feature values are non-zero</li>
                            <li><strong>Optimization friendly:</strong> Need smooth, differentiable objective functions</li>
                        </ul>
                        
                        <h5>üî∑ Choose Manhattan Distance When:</h5>
                        <ul>
                            <li><strong>Discrete/count data:</strong> Features represent counts or frequencies</li>
                            <li><strong>Independent features:</strong> Features represent separate, unrelated quantities</li>
                            <li><strong>Outlier robustness:</strong> Need resistance to extreme values</li>
                            <li><strong>Grid-constrained problems:</strong> Movement restricted to orthogonal directions</li>
                            <li><strong>Sparse data:</strong> Many features are zero</li>
                            <li><strong>High dimensions:</strong> Better discrimination in high-dimensional spaces</li>
                            <li><strong>Computational efficiency:</strong> Speed is critical</li>
                        </ul>
                        
                        <h5>ü§î Consider Hybrid Approaches When:</h5>
                        <ul>
                            <li><strong>Mixed data types:</strong> Combination of continuous and discrete features</li>
                            <li><strong>Domain-specific needs:</strong> Different feature groups require different treatments</li>
                            <li><strong>Uncertain assumptions:</strong> Try both and compare results</li>
                        </ul>
                    </div>
                </div>

                <!-- Interactive Calculator Section -->
                <div id="interactive" class="content-section">
                    <h2>Interactive Distance Calculator</h2>
                    
                    <p>Explore Euclidean and Manhattan distances hands-on with this interactive calculator. Input your own points, compare different metrics, and visualize the results to build intuition about distance behavior.</p>

                    <div class="interactive-demo">
                        <h3>üßÆ Two-Point Distance Calculator</h3>
                        <p>Calculate and compare Euclidean and Manhattan distances between two points in up to 5 dimensions.</p>
                        
                        <div class="distance-calculator">
                            <h4>üìç Point Coordinates</h4>
                            <div class="point-input">
                                <div>
                                    <label for="dim-count">Number of Dimensions:</label>
                                    <select id="dim-count" onchange="updateDimensions()">
                                        <option value="2">2D</option>
                                        <option value="3">3D</option>
                                        <option value="4">4D</option>
                                        <option value="5">5D</option>
                                    </select>
                                </div>
                            </div>
                            
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 1.5rem 0;">
                                <div>
                                    <h5>Point A:</h5>
                                    <div id="point-a-inputs">
                                        <input type="number" id="a-x" placeholder="X" value="1" step="0.1">
                                        <input type="number" id="a-y" placeholder="Y" value="2" step="0.1">
                                    </div>
                                </div>
                                <div>
                                    <h5>Point B:</h5>
                                    <div id="point-b-inputs">
                                        <input type="number" id="b-x" placeholder="X" value="4" step="0.1">
                                        <input type="number" id="b-y" placeholder="Y" value="6" step="0.1">
                                    </div>
                                </div>
                            </div>
                            
                            <button onclick="calculateDistances()" class="azbn-btn">Calculate Distances</button>
                            
                            <div id="distance-results" style="display: none; margin-top: 1.5rem;">
                                <h4>üìä Results</h4>
                                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                                    <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px;">
                                        <h5>üîµ Euclidean Distance</h5>
                                        <div id="euclidean-result" style="font-size: 1.2rem; font-weight: bold; color: #2196f3;"></div>
                                        <div id="euclidean-formula" style="font-size: 0.9rem; margin-top: 0.5rem;"></div>
                                    </div>
                                    <div style="background: #fff8e1; padding: 1rem; border-radius: 6px;">
                                        <h5>üî∑ Manhattan Distance</h5>
                                        <div id="manhattan-result" style="font-size: 1.2rem; font-weight: bold; color: #ff9800;"></div>
                                        <div id="manhattan-formula" style="font-size: 0.9rem; margin-top: 0.5rem;"></div>
                                    </div>
                                    <div style="background: #f3e5f5; padding: 1rem; border-radius: 6px;">
                                        <h5>üìê Ratio Analysis</h5>
                                        <div id="ratio-result" style="font-size: 1.2rem; font-weight: bold; color: #9c27b0;"></div>
                                        <div style="font-size: 0.9rem; margin-top: 0.5rem;">Manhattan / Euclidean</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3>üìà Batch Distance Analysis</h3>
                        <p>Generate random points and analyze distance distributions to understand metric behavior.</p>
                        
                        <div class="demo-controls">
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin: 1rem 0;">
                                <div>
                                    <label for="num-random-points">Number of Points:</label>
                                    <input type="range" id="num-random-points" min="10" max="100" value="50">
                                    <span id="points-count">50</span>
                                </div>
                                <div>
                                    <label for="random-dimensions">Dimensions:</label>
                                    <select id="random-dimensions">
                                        <option value="2">2D</option>
                                        <option value="3">3D</option>
                                        <option value="5">5D</option>
                                        <option value="10">10D</option>
                                    </select>
                                </div>
                                <div>
                                    <label for="distribution-type">Distribution:</label>
                                    <select id="distribution-type">
                                        <option value="uniform">Uniform</option>
                                        <option value="normal">Normal</option>
                                        <option value="exponential">Exponential</option>
                                    </select>
                                </div>
                            </div>
                            
                            <button onclick="generateBatchAnalysis()" class="azbn-btn">Generate & Analyze</button>
                        </div>

                        <div id="batch-results" style="display: none; margin-top: 1.5rem;">
                            <h4>üìä Statistical Analysis</h4>
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem;">
                                <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px;">
                                    <h5>üîµ Euclidean Statistics</h5>
                                    <div id="euclidean-stats"></div>
                                </div>
                                <div style="background: #fff8e1; padding: 1rem; border-radius: 6px;">
                                    <h5>üî∑ Manhattan Statistics</h5>
                                    <div id="manhattan-stats"></div>
                                </div>
                                <div style="background: #e8f5e8; padding: 1rem; border-radius: 6px;">
                                    <h5>üìà Comparison</h5>
                                    <div id="comparison-stats"></div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3>üéØ Clustering Comparison Demo</h3>
                        <p>See how Euclidean and Manhattan distances create different clustering results on the same data.</p>
                        
                        <div class="demo-controls">
                            <button onclick="generateClusteringDemo()" class="azbn-btn">Generate Clustering Demo</button>
                            <button onclick="stepClusteringDemo()" class="azbn-btn azbn-secondary">Step Through Iterations</button>
                            <button onclick="resetClusteringDemo()" class="azbn-btn azbn-secondary">Reset</button>
                        </div>

                        <div id="clustering-comparison" style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                            <div>
                                <h4>üîµ Euclidean K-means</h4>
                                <div class="metric-visualization" id="euclidean-clustering">
                                    <p style="text-align: center; margin-top: 100px; color: #666;">Click "Generate Clustering Demo" to start</p>
                                </div>
                                <div id="euclidean-cluster-info" style="margin-top: 0.5rem; font-size: 0.9rem;"></div>
                            </div>
                            <div>
                                <h4>üî∑ Manhattan K-means</h4>
                                <div class="metric-visualization" id="manhattan-clustering">
                                    <p style="text-align: center; margin-top: 100px; color: #666;">Click "Generate Clustering Demo" to start</p>
                                </div>
                                <div id="manhattan-cluster-info" style="margin-top: 0.5rem; font-size: 0.9rem;"></div>
                            </div>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3>üß™ Custom Experiment Designer</h3>
                        <p>Design your own experiments to test specific hypotheses about distance metric behavior.</p>
                        
                        <div class="demo-controls">
                            <h4>Experiment Setup</h4>
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin: 1rem 0;">
                                <div>
                                    <label for="experiment-type">Experiment Type:</label>
                                    <select id="experiment-type">
                                        <option value="outliers">Outlier Sensitivity</option>
                                        <option value="scaling">Feature Scaling Impact</option>
                                        <option value="correlation">Feature Correlation Effects</option>
                                        <option value="dimensions">High-Dimensional Behavior</option>
                                    </select>
                                </div>
                                <div>
                                    <label for="experiment-intensity">Intensity:</label>
                                    <input type="range" id="experiment-intensity" min="1" max="10" value="5">
                                    <span id="intensity-value">5</span>
                                </div>
                            </div>
                            
                            <button onclick="runCustomExperiment()" class="azbn-btn">Run Experiment</button>
                        </div>

                        <div id="experiment-results" style="display: none; margin-top: 1.5rem;">
                            <h4>üî¨ Experiment Results</h4>
                            <div id="experiment-analysis"></div>
                        </div>
                    </div>
                </div>

                <!-- Quiz Section -->
                <div id="quiz" class="content-section">
                    <h2>Chapter 2 Quiz: Distance Metrics Mastery</h2>
                    <p>Test your understanding of Euclidean and Manhattan distance theory, properties, and applications.</p>

                    <div class="quiz-question">
                        <h4>Question 1: Which property is required for a function to be a valid metric?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q1" value="a" id="q1a">
                            <label for="q1a">Differentiability everywhere</label><br>
                            <input type="radio" name="q1" value="b" id="q1b">
                            <label for="q1b">Triangle inequality</label><br>
                            <input type="radio" name="q1" value="c" id="q1c">
                            <label for="q1c">Rotation invariance</label><br>
                            <input type="radio" name="q1" value="d" id="q1d">
                            <label for="q1d">Computational efficiency</label>
                        </div>
                        <button onclick="checkQuizAnswer(1, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q1-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 2: What is the Manhattan distance between points (2, 3, 1) and (5, 1, 4)?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q2" value="a" id="q2a">
                            <label for="q2a">5</label><br>
                            <input type="radio" name="q2" value="b" id="q2b">
                            <label for="q2b">8</label><br>
                            <input type="radio" name="q2" value="c" id="q2c">
                            <label for="q2c">‚àö22</label><br>
                            <input type="radio" name="q2" value="d" id="q2d">
                            <label for="q2d">22</label>
                        </div>
                        <button onclick="checkQuizAnswer(2, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q2-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 3: Which statement about high-dimensional behavior is correct?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q3" value="a" id="q3a">
                            <label for="q3a">Euclidean distance becomes more discriminative in high dimensions</label><br>
                            <input type="radio" name="q3" value="b" id="q3b">
                            <label for="q3b">Manhattan distance maintains better discrimination than Euclidean in high dimensions</label><br>
                            <input type="radio" name="q3" value="c" id="q3c">
                            <label for="q3c">Both metrics perform equally well in high dimensions</label><br>
                            <input type="radio" name="q3" value="d" id="q3d">
                            <label for="q3d">Distance metrics become meaningless in high dimensions</label>
                        </div>
                        <button onclick="checkQuizAnswer(3, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q3-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 4: What is the shape of the L‚ÇÅ (Manhattan) unit ball in 2D?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q4" value="a" id="q4a">
                            <label for="q4a">Circle</label><br>
                            <input type="radio" name="q4" value="b" id="q4b">
                            <label for="q4b">Square</label><br>
                            <input type="radio" name="q4" value="c" id="q4c">
                            <label for="q4c">Diamond (rotated square)</label><br>
                            <input type="radio" name="q4" value="d" id="q4d">
                            <label for="q4d">Ellipse</label>
                        </div>
                        <button onclick="checkQuizAnswer(4, 'c')" class="azbn-btn">Check Answer</button>
                        <div id="q4-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 5: When is Manhattan distance typically preferred over Euclidean distance?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q5" value="a" id="q5a">
                            <label for="q5a">When features are highly correlated</label><br>
                            <input type="radio" name="q5" value="b" id="q5b">
                            <label for="q5a">When working with count data and need outlier robustness</label><br>
                            <input type="radio" name="q5" value="c" id="q5c">
                            <label for="q5c">When rotation invariance is important</label><br>
                            <input type="radio" name="q5" value="d" id="q5d">
                            <label for="q5d">When all features have the same scale</label>
                        </div>
                        <button onclick="checkQuizAnswer(5, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q5-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div style="text-align: center; margin: 2rem 0;">
                        <button onclick="submitChapter2Quiz()" class="azbn-btn">Submit Quiz</button>
                        <div id="chapter2-quiz-score" style="margin-top: 1rem; font-size: 1.2rem; font-weight: bold;"></div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <a href="/tutorials/ml-fundamentals/clustering/chapter1" class="azbn-btn azbn-secondary" style="text-decoration: none;">‚Üê Chapter 1: Introduction</a>
                    <a href="/tutorials/ml-fundamentals/clustering/chapter3" class="azbn-btn" style="text-decoration: none;">Chapter 3: Minkowski Distance ‚Üí</a>
                </div>
            </div>
        </section>
    </main>

    <script>
        let quizAnswers = {};
        let currentPoints = [];
        let currentCentroids = [];
        let experimentData = {};

        function showSection(sectionName) {
            // Hide all sections
            document.querySelectorAll('.content-section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            event.target.classList.add('active');
        }

        // Distance Calculator Functions
        function updateDimensions() {
            const dimCount = parseInt(document.getElementById('dim-count').value);
            const pointADiv = document.getElementById('point-a-inputs');
            const pointBDiv = document.getElementById('point-b-inputs');
            
            pointADiv.innerHTML = '';
            pointBDiv.innerHTML = '';
            
            const labels = ['X', 'Y', 'Z', 'W', 'V'];
            const defaultA = [1, 2, 3, 4, 5];
            const defaultB = [4, 6, 1, 2, 8];
            
            for (let i = 0; i < dimCount; i++) {
                const inputA = document.createElement('input');
                inputA.type = 'number';
                inputA.id = `a-${i}`;
                inputA.placeholder = labels[i];
                inputA.value = defaultA[i];
                inputA.step = '0.1';
                inputA.style.margin = '0.2rem';
                inputA.style.width = '80px';
                pointADiv.appendChild(inputA);
                
                const inputB = document.createElement('input');
                inputB.type = 'number';
                inputB.id = `b-${i}`;
                inputB.placeholder = labels[i];
                inputB.value = defaultB[i];
                inputB.step = '0.1';
                inputB.style.margin = '0.2rem';
                inputB.style.width = '80px';
                pointBDiv.appendChild(inputB);
            }
        }

        function calculateDistances() {
            const dimCount = parseInt(document.getElementById('dim-count').value);
            const pointA = [];
            const pointB = [];
            
            for (let i = 0; i < dimCount; i++) {
                pointA.push(parseFloat(document.getElementById(`a-${i}`).value) || 0);
                pointB.push(parseFloat(document.getElementById(`b-${i}`).value) || 0);
            }
            
            // Calculate Euclidean distance
            let euclideanSum = 0;
            let euclideanFormula = '‚àö(';
            for (let i = 0; i < dimCount; i++) {
                const diff = pointA[i] - pointB[i];
                euclideanSum += diff * diff;
                euclideanFormula += `(${pointA[i]} - ${pointB[i]})¬≤`;
                if (i < dimCount - 1) euclideanFormula += ' + ';
            }
            euclideanFormula += ')';
            const euclideanDistance = Math.sqrt(euclideanSum);
            
            // Calculate Manhattan distance
            let manhattanSum = 0;
            let manhattanFormula = '';
            for (let i = 0; i < dimCount; i++) {
                const diff = Math.abs(pointA[i] - pointB[i]);
                manhattanSum += diff;
                manhattanFormula += `|${pointA[i]} - ${pointB[i]}|`;
                if (i < dimCount - 1) manhattanFormula += ' + ';
            }
            const manhattanDistance = manhattanSum;
            
            // Display results
            document.getElementById('distance-results').style.display = 'block';
            document.getElementById('euclidean-result').textContent = euclideanDistance.toFixed(3);
            document.getElementById('euclidean-formula').textContent = euclideanFormula;
            document.getElementById('manhattan-result').textContent = manhattanDistance.toFixed(3);
            document.getElementById('manhattan-formula').textContent = manhattanFormula;
            
            const ratio = manhattanDistance / euclideanDistance;
            document.getElementById('ratio-result').textContent = ratio.toFixed(3);
        }

        function generateBatchAnalysis() {
            const numPoints = parseInt(document.getElementById('num-random-points').value);
            const dimensions = parseInt(document.getElementById('random-dimensions').value);
            const distribution = document.getElementById('distribution-type').value;
            
            // Generate random points
            const points = [];
            for (let i = 0; i < numPoints; i++) {
                const point = [];
                for (let d = 0; d < dimensions; d++) {
                    let value;
                    switch (distribution) {
                        case 'uniform':
                            value = Math.random() * 10;
                            break;
                        case 'normal':
                            value = normalRandom() * 3 + 5;
                            break;
                        case 'exponential':
                            value = -Math.log(Math.random()) * 2;
                            break;
                    }
                    point.push(value);
                }
                points.push(point);
            }
            
            // Calculate all pairwise distances
            const euclideanDistances = [];
            const manhattanDistances = [];
            
            for (let i = 0; i < numPoints; i++) {
                for (let j = i + 1; j < numPoints; j++) {
                    let euclideanSum = 0;
                    let manhattanSum = 0;
                    
                    for (let d = 0; d < dimensions; d++) {
                        const diff = points[i][d] - points[j][d];
                        euclideanSum += diff * diff;
                        manhattanSum += Math.abs(diff);
                    }
                    
                    euclideanDistances.push(Math.sqrt(euclideanSum));
                    manhattanDistances.push(manhattanSum);
                }
            }
            
            // Calculate statistics
            const euclideanStats = calculateStats(euclideanDistances);
            const manhattanStats = calculateStats(manhattanDistances);
            
            // Display results
            document.getElementById('batch-results').style.display = 'block';
            document.getElementById('euclidean-stats').innerHTML = formatStats(euclideanStats);
            document.getElementById('manhattan-stats').innerHTML = formatStats(manhattanStats);
            
            const correlation = calculateCorrelation(euclideanDistances, manhattanDistances);
            const meanRatio = manhattanDistances.reduce((sum, dist, i) => sum + dist / euclideanDistances[i], 0) / manhattanDistances.length;
            
            document.getElementById('comparison-stats').innerHTML = `
                <strong>Correlation:</strong> ${correlation.toFixed(3)}<br>
                <strong>Mean Ratio:</strong> ${meanRatio.toFixed(3)}<br>
                <strong>Max Ratio:</strong> ${Math.max(...manhattanDistances.map((dist, i) => dist / euclideanDistances[i])).toFixed(3)}<br>
                <strong>Min Ratio:</strong> ${Math.min(...manhattanDistances.map((dist, i) => dist / euclideanDistances[i])).toFixed(3)}
            `;
        }

        function normalRandom() {
            // Box-Muller transform for normal distribution
            let u = 0, v = 0;
            while(u === 0) u = Math.random();
            while(v === 0) v = Math.random();
            return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
        }

        function calculateStats(data) {
            const sorted = [...data].sort((a, b) => a - b);
            const mean = data.reduce((sum, val) => sum + val, 0) / data.length;
            const variance = data.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / data.length;
            const std = Math.sqrt(variance);
            
            return {
                mean: mean,
                std: std,
                min: Math.min(...data),
                max: Math.max(...data),
                median: sorted[Math.floor(sorted.length / 2)],
                q1: sorted[Math.floor(sorted.length * 0.25)],
                q3: sorted[Math.floor(sorted.length * 0.75)]
            };
        }

        function formatStats(stats) {
            return `
                <strong>Mean:</strong> ${stats.mean.toFixed(3)}<br>
                <strong>Std Dev:</strong> ${stats.std.toFixed(3)}<br>
                <strong>Min:</strong> ${stats.min.toFixed(3)}<br>
                <strong>Max:</strong> ${stats.max.toFixed(3)}<br>
                <strong>Median:</strong> ${stats.median.toFixed(3)}
            `;
        }

        function calculateCorrelation(x, y) {
            const n = x.length;
            const meanX = x.reduce((sum, val) => sum + val, 0) / n;
            const meanY = y.reduce((sum, val) => sum + val, 0) / n;
            
            let numerator = 0;
            let denomX = 0;
            let denomY = 0;
            
            for (let i = 0; i < n; i++) {
                const diffX = x[i] - meanX;
                const diffY = y[i] - meanY;
                numerator += diffX * diffY;
                denomX += diffX * diffX;
                denomY += diffY * diffY;
            }
            
            return numerator / Math.sqrt(denomX * denomY);
        }

        function generateClusteringDemo() {
            // Generate sample data for clustering
            const points = [];
            const numClusters = 3;
            const pointsPerCluster = 15;
            
            const centers = [[2, 2], [6, 2], [4, 5]];
            
            for (let cluster = 0; cluster < numClusters; cluster++) {
                for (let i = 0; i < pointsPerCluster; i++) {
                    const point = [
                        centers[cluster][0] + (Math.random() - 0.5) * 3,
                        centers[cluster][1] + (Math.random() - 0.5) * 3
                    ];
                    points.push(point);
                }
            }
            
            currentPoints = points;
            
            // Run K-means with both metrics
            const euclideanResult = kmeans(points, 3, 'euclidean');
            const manhattanResult = kmeans(points, 3, 'manhattan');
            
            // Visualize results
            drawClustering('euclidean-clustering', points, euclideanResult.centroids, euclideanResult.assignments, 'euclidean');
            drawClustering('manhattan-clustering', points, manhattanResult.centroids, manhattanResult.assignments, 'manhattan');
            
            document.getElementById('euclidean-cluster-info').textContent = `Iterations: ${euclideanResult.iterations}, WCSS: ${euclideanResult.wcss.toFixed(2)}`;
            document.getElementById('manhattan-cluster-info').textContent = `Iterations: ${manhattanResult.iterations}, WCSS: ${manhattanResult.wcss.toFixed(2)}`;
        }

        function kmeans(points, k, metric) {
            // Simple K-means implementation
            let centroids = [];
            for (let i = 0; i < k; i++) {
                centroids.push([Math.random() * 8, Math.random() * 6]);
            }
            
            let assignments = new Array(points.length);
            let iterations = 0;
            const maxIterations = 50;
            
            while (iterations < maxIterations) {
                let changed = false;
                
                // Assign points to nearest centroid
                for (let i = 0; i < points.length; i++) {
                    let minDistance = Infinity;
                    let bestCluster = 0;
                    
                    for (let j = 0; j < k; j++) {
                        const distance = metric === 'euclidean' ? 
                            euclideanDistance(points[i], centroids[j]) :
                            manhattanDistance(points[i], centroids[j]);
                        
                        if (distance < minDistance) {
                            minDistance = distance;
                            bestCluster = j;
                        }
                    }
                    
                    if (assignments[i] !== bestCluster) {
                        changed = true;
                        assignments[i] = bestCluster;
                    }
                }
                
                if (!changed) break;
                
                // Update centroids
                for (let j = 0; j < k; j++) {
                    const clusterPoints = points.filter((_, i) => assignments[i] === j);
                    if (clusterPoints.length > 0) {
                        if (metric === 'euclidean') {
                            centroids[j] = [
                                clusterPoints.reduce((sum, p) => sum + p[0], 0) / clusterPoints.length,
                                clusterPoints.reduce((sum, p) => sum + p[1], 0) / clusterPoints.length
                            ];
                        } else {
                            // For Manhattan distance, use median
                            const xValues = clusterPoints.map(p => p[0]).sort((a, b) => a - b);
                            const yValues = clusterPoints.map(p => p[1]).sort((a, b) => a - b);
                            centroids[j] = [
                                xValues[Math.floor(xValues.length / 2)],
                                yValues[Math.floor(yValues.length / 2)]
                            ];
                        }
                    }
                }
                
                iterations++;
            }
            
            // Calculate WCSS
            let wcss = 0;
            for (let i = 0; i < points.length; i++) {
                const distance = metric === 'euclidean' ? 
                    euclideanDistance(points[i], centroids[assignments[i]]) :
                    manhattanDistance(points[i], centroids[assignments[i]]);
                wcss += distance * distance;
            }
            
            return { centroids, assignments, iterations, wcss };
        }

        function euclideanDistance(p1, p2) {
            return Math.sqrt(Math.pow(p1[0] - p2[0], 2) + Math.pow(p1[1] - p2[1], 2));
        }

        function manhattanDistance(p1, p2) {
            return Math.abs(p1[0] - p2[0]) + Math.abs(p1[1] - p2[1]);
        }

        function drawClustering(canvasId, points, centroids, assignments, metric) {
            const canvas = document.getElementById(canvasId);
            canvas.innerHTML = '';
            
            const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
            svg.setAttribute('width', '100%');
            svg.setAttribute('height', '250');
            svg.setAttribute('viewBox', '0 0 400 300');
            
            const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#f9ca24'];
            const scale = 40;
            const offsetX = 20;
            const offsetY = 20;
            
            // Draw points
            points.forEach((point, index) => {
                const circle = document.createElementNS('http://www.w3.org/2000/svg', 'circle');
                circle.setAttribute('cx', point[0] * scale + offsetX);
                circle.setAttribute('cy', point[1] * scale + offsetY);
                circle.setAttribute('r', 4);
                circle.setAttribute('fill', colors[assignments[index] % colors.length]);
                circle.setAttribute('stroke', '#333');
                circle.setAttribute('stroke-width', 1);
                svg.appendChild(circle);
            });
            
            // Draw centroids
            centroids.forEach((centroid, index) => {
                const rect = document.createElementNS('http://www.w3.org/2000/svg', 'rect');
                rect.setAttribute('x', centroid[0] * scale + offsetX - 6);
                rect.setAttribute('y', centroid[1] * scale + offsetY - 6);
                rect.setAttribute('width', 12);
                rect.setAttribute('height', 12);
                rect.setAttribute('fill', colors[index % colors.length]);
                rect.setAttribute('stroke', '#000');
                rect.setAttribute('stroke-width', 2);
                svg.appendChild(rect);
            });
            
            canvas.appendChild(svg);
        }

        function runCustomExperiment() {
            const experimentType = document.getElementById('experiment-type').value;
            const intensity = parseInt(document.getElementById('experiment-intensity').value);
            
            let results = '';
            
            switch (experimentType) {
                case 'outliers':
                    results = runOutlierExperiment(intensity);
                    break;
                case 'scaling':
                    results = runScalingExperiment(intensity);
                    break;
                case 'correlation':
                    results = runCorrelationExperiment(intensity);
                    break;
                case 'dimensions':
                    results = runDimensionExperiment(intensity);
                    break;
            }
            
            document.getElementById('experiment-results').style.display = 'block';
            document.getElementById('experiment-analysis').innerHTML = results;
        }

        function runOutlierExperiment(intensity) {
            // Generate data with outliers
            const basePoints = [];
            const outlierPoints = [];
            
            // Generate normal points
            for (let i = 0; i < 50; i++) {
                basePoints.push([normalRandom() * 2 + 5, normalRandom() * 2 + 5]);
            }
            
            // Add outliers based on intensity
            const numOutliers = intensity;
            for (let i = 0; i < numOutliers; i++) {
                outlierPoints.push([Math.random() * 20, Math.random() * 20]);
            }
            
            const allPoints = [...basePoints, ...outlierPoints];
            
            // Calculate distances with and without outliers
            const baseDistances = calculatePairwiseDistances(basePoints);
            const allDistances = calculatePairwiseDistances(allPoints);
            
            return `
                <h5>Outlier Sensitivity Analysis</h5>
                <p><strong>Base data (no outliers):</strong></p>
                <p>Euclidean mean: ${baseDistances.euclidean.mean.toFixed(3)}, std: ${baseDistances.euclidean.std.toFixed(3)}</p>
                <p>Manhattan mean: ${baseDistances.manhattan.mean.toFixed(3)}, std: ${baseDistances.manhattan.std.toFixed(3)}</p>
                
                <p><strong>With ${numOutliers} outliers:</strong></p>
                <p>Euclidean mean: ${allDistances.euclidean.mean.toFixed(3)}, std: ${allDistances.euclidean.std.toFixed(3)}</p>
                <p>Manhattan mean: ${allDistances.manhattan.mean.toFixed(3)}, std: ${allDistances.manhattan.std.toFixed(3)}</p>
                
                <p><strong>Impact:</strong></p>
                <p>Euclidean std change: ${((allDistances.euclidean.std - baseDistances.euclidean.std) / baseDistances.euclidean.std * 100).toFixed(1)}%</p>
                <p>Manhattan std change: ${((allDistances.manhattan.std - baseDistances.manhattan.std) / baseDistances.manhattan.std * 100).toFixed(1)}%</p>
                
                <p><strong>Conclusion:</strong> Manhattan distance is more robust to outliers, showing less variance increase.</p>
            `;
        }

        function calculatePairwiseDistances(points) {
            const euclideanDists = [];
            const manhattanDists = [];
            
            for (let i = 0; i < points.length; i++) {
                for (let j = i + 1; j < points.length; j++) {
                    euclideanDists.push(euclideanDistance(points[i], points[j]));
                    manhattanDists.push(manhattanDistance(points[i], points[j]));
                }
            }
            
            return {
                euclidean: calculateStats(euclideanDists),
                manhattan: calculateStats(manhattanDists)
            };
        }

        // Quiz Functions
        function checkQuizAnswer(questionNum, correctAnswer) {
            const selectedAnswer = document.querySelector(`input[name="q${questionNum}"]:checked`);
            const resultDiv = document.getElementById(`q${questionNum}-result`);
            
            if (!selectedAnswer) {
                resultDiv.innerHTML = '<p style="color: orange;">Please select an answer first.</p>';
                return;
            }
            
            const isCorrect = selectedAnswer.value === correctAnswer;
            quizAnswers[questionNum] = isCorrect;
            
            if (isCorrect) {
                resultDiv.innerHTML = '<p style="color: green;">‚úÖ Correct!</p>';
            } else {
                let explanation = '';
                if (questionNum === 1) explanation = 'Triangle inequality d(x,z) ‚â§ d(x,y) + d(y,z) is a fundamental metric property.';
                if (questionNum === 2) explanation = 'Manhattan distance: |2-5| + |3-1| + |1-4| = 3 + 2 + 3 = 8';
                if (questionNum === 3) explanation = 'Manhattan distance maintains better discrimination in high dimensions due to slower concentration.';
                if (questionNum === 4) explanation = 'L‚ÇÅ unit ball {x: |x‚ÇÅ| + |x‚ÇÇ| ‚â§ 1} forms a diamond shape in 2D.';
                if (questionNum === 5) explanation = 'Manhattan distance excels with count data and provides outlier robustness.';
                
                resultDiv.innerHTML = `<p style="color: red;">‚ùå Incorrect. ${explanation}</p>`;
            }
        }

        function submitChapter2Quiz() {
            const totalQuestions = 5;
            const correctAnswers = Object.values(quizAnswers).filter(answer => answer).length;
            const score = Math.round((correctAnswers / totalQuestions) * 100);
            
            const scoreDiv = document.getElementById('chapter2-quiz-score');
            scoreDiv.innerHTML = `
                <h3>Chapter 2 Quiz Results</h3>
                <p>Score: ${correctAnswers}/${totalQuestions} (${score}%)</p>
                ${score >= 80 ? '<p style="color: green;">üéâ Excellent! You\'ve mastered distance metrics fundamentals!</p>' : 
                               '<p style="color: orange;">üìö Review the material and try again for full mastery!</p>'}
            `;
        }

        // Initialize sliders and default values
        document.getElementById('num-random-points').addEventListener('input', function() {
            document.getElementById('points-count').textContent = this.value;
        });

        document.getElementById('experiment-intensity').addEventListener('input', function() {
            document.getElementById('intensity-value').textContent = this.value;
        });

        // Initialize with default 2D points
        window.addEventListener('load', function() {
            updateDimensions();
            calculateDistances();
        });
    </script>
</body>
</html>üèóÔ∏è Building Intuition: Why Distance Matters in Clustering</h4>
                        <p>The choice of distance metric fundamentally determines what patterns your clustering algorithm will discover. Consider these scenarios:</p>
                        <ul>
                            <li><strong>Customer Segmentation:</strong> Euclidean distance treats all features equally, while Mahalanobis accounts for feature correlations</li>
                            <li><strong>Document Clustering:</strong> Cosine similarity focuses on topic proportions rather than document length</li>
                            <li><strong>Gene Expression:</strong> Correlation-based distances capture co-expression patterns</li>
                            <li><strong>Image Analysis:</strong> Perceptual distances might be more relevant than pixel-wise distances</li>
                        </ul>
                        <p><strong>Key Insight:</strong> There is no universally "best" distance metric‚Äîthe optimal choice depends on your data characteristics and domain knowledge.</p>
                    </div>
                </div>

                <!-- Euclidean Distance Section -->
                <div id="euclidean" class="content-section">
                    <h2>Euclidean Distance: The Geometric Standard</h2>
                    
                    <p>Euclidean distance, also known as the L‚ÇÇ norm or 2-norm, is perhaps the most intuitive and widely used distance metric. It represents the "straight-line" distance between two points, directly extending our everyday understanding of distance into higher dimensions.</p>

                    <div class="formula-box">
                        <h3>üìê Mathematical Definition</h3>
                        <p>For two points x, y ‚àà ‚Ñù‚Åø, the Euclidean distance is defined as:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>d_E(x, y) = ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤)</strong>
                        </div>
                        
                        <p>This can also be expressed using vector notation:</p>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>d_E(x, y) = ‚Äñx - y‚Äñ‚ÇÇ = ‚àö‚ü®x - y, x - y‚ü©</strong>
                        </div>
                        
                        <p>Where ‚ü®¬∑, ¬∑‚ü© denotes the inner product (dot product) in ‚Ñù‚Åø.</p>
                    </div>

                    <h3>üéØ Dimensional Examples</h3>
                    <p>Let's explore how Euclidean distance works across different dimensions, building intuition from simple cases to complex scenarios.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üìè 1D Case (Number Line)</h4>
                            <p>For points x, y ‚àà ‚Ñù:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_E(x, y) = |x - y|</strong>
                            </div>
                            <p><strong>Example:</strong> d_E(3, 7) = |3 - 7| = 4</p>
                            <p>This is simply the absolute difference on the number line.</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üìê 2D Case (Plane)</h4>
                            <p>For points (x‚ÇÅ, x‚ÇÇ), (y‚ÇÅ, y‚ÇÇ) ‚àà ‚Ñù¬≤:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_E = ‚àö((x‚ÇÅ - y‚ÇÅ)¬≤ + (x‚ÇÇ - y‚ÇÇ)¬≤)</strong>
                            </div>
                            <p><strong>Example:</strong> d_E((1, 2), (4, 6)) = ‚àö((1-4)¬≤ + (2-6)¬≤) = ‚àö(9 + 16) = 5</p>
                            <p>This is the Pythagorean theorem in 2D.</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üé≤ 3D Case (Space)</h4>
                            <p>For points (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ), (y‚ÇÅ, y‚ÇÇ, y‚ÇÉ) ‚àà ‚Ñù¬≥:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_E = ‚àö((x‚ÇÅ - y‚ÇÅ)¬≤ + (x‚ÇÇ - y‚ÇÇ)¬≤ + (x‚ÇÉ - y‚ÇÉ)¬≤)</strong>
                            </div>
                            <p><strong>Example:</strong> d_E((1, 2, 3), (4, 6, 8)) = ‚àö(9 + 16 + 25) = ‚àö50 ‚âà 7.07</p>
                            <p>Extension of Pythagorean theorem to 3D.</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üåê n-D Case (High Dimensions)</h4>
                            <p>For points x, y ‚àà ‚Ñù‚Åø:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_E = ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤)</strong>
                            </div>
                            <p>This generalizes naturally to any number of dimensions, though high-dimensional behavior has special considerations.</p>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìä Visualization: Euclidean Distance in 2D and 3D</h4>
                        <p><strong>Image Description:</strong> Two side-by-side diagrams. Left: 2D coordinate plane showing points A(1,2) and B(4,6) connected by a straight line labeled "Euclidean distance = 5". A right triangle is drawn with legs of length 3 and 4, illustrating the Pythagorean theorem. Right: 3D coordinate system showing two points connected by a straight line in 3D space, with projections on each plane shown as dashed lines.</p>
                        <p><em>This shows how Euclidean distance extends the Pythagorean theorem to higher dimensions</em></p>
                    </div>

                    <h3>üî¨ Mathematical Properties of Euclidean Distance</h3>
                    <p>Understanding the mathematical properties of Euclidean distance helps us appreciate why it's so fundamental and when it's appropriate to use.</p>

                    <div class="proof-box">
                        <h4>üìã Verification of Metric Properties</h4>
                        <p>Let's prove that Euclidean distance satisfies all four metric space axioms:</p>
                        
                        <h5>1Ô∏è‚É£ Non-negativity Proof:</h5>
                        <p>Since (x·µ¢ - y·µ¢)¬≤ ‚â• 0 for all i, we have:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤ ‚â• 0 ‚üπ ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤) ‚â• 0
                        </div>
                        <p>Therefore, d_E(x, y) ‚â• 0. ‚úì</p>
                        
                        <h5>2Ô∏è‚É£ Identity of Indiscernibles Proof:</h5>
                        <p>d_E(x, y) = 0 ‚ü∫ ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤) = 0 ‚ü∫ Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤ = 0</p>
                        <p>Since each (x·µ¢ - y·µ¢)¬≤ ‚â• 0, the sum equals zero iff each term equals zero:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            (x·µ¢ - y·µ¢)¬≤ = 0 ‚àÄi ‚ü∫ x·µ¢ = y·µ¢ ‚àÄi ‚ü∫ x = y
                        </div>
                        <p>Therefore, d_E(x, y) = 0 ‚ü∫ x = y. ‚úì</p>
                        
                        <h5>3Ô∏è‚É£ Symmetry Proof:</h5>
                        <p>d_E(x, y) = ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤) = ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - x·µ¢)¬≤) = d_E(y, x)</p>
                        <p>Since (x·µ¢ - y·µ¢)¬≤ = (y·µ¢ - x·µ¢)¬≤ for all i. ‚úì</p>
                        
                        <h5>4Ô∏è‚É£ Triangle Inequality Proof:</h5>
                        <p>We need to show d_E(x, z) ‚â§ d_E(x, y) + d_E(y, z). This follows from the Cauchy-Schwarz inequality:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            ‚Äñx - z‚Äñ‚ÇÇ = ‚Äñ(x - y) + (y - z)‚Äñ‚ÇÇ ‚â§ ‚Äñx - y‚Äñ‚ÇÇ + ‚Äñy - z‚Äñ‚ÇÇ
                        </div>
                        <p>This is a fundamental result in linear algebra. ‚úì</p>
                    </div>

                    <h3>‚ö° Computational Considerations</h3>
                    <p>While conceptually simple, computing Euclidean distance efficiently requires careful consideration of numerical stability and computational complexity.</p>

                    <div class="formula-box">
                        <h4>üñ•Ô∏è Computational Complexity</h4>
                        <ul>
                            <li><strong>Time Complexity:</strong> O(n) for n-dimensional vectors</li>
                            <li><strong>Space Complexity:</strong> O(1) additional space</li>
                            <li><strong>Numerical Stability:</strong> Susceptible to overflow for large values</li>
                            <li><strong>Vectorization:</strong> Highly amenable to SIMD operations</li>
                        </ul>
                        
                        <h5>üíª Efficient Implementation Strategies:</h5>
                        <ol>
                            <li><strong>Avoid Square Root:</strong> For comparisons, use squared Euclidean distance</li>
                            <li><strong>Vectorization:</strong> Use numpy/BLAS for large arrays</li>
                            <li><strong>Memory Layout:</strong> Ensure contiguous memory for cache efficiency</li>
                            <li><strong>Numerical Precision:</strong> Consider double precision for high-dimensional data</li>
                        </ol>
                    </div>

                    <h3>üéØ When to Use Euclidean Distance</h3>
                    <p>Euclidean distance is not universally optimal. Understanding its strengths and limitations guides appropriate application.</p>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>‚úÖ Euclidean Distance is Good For:</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Continuous numerical features</li>
                                <li>Features with similar scales</li>
                                <li>Geometric/spatial data</li>
                                <li>Physical measurements</li>
                                <li>When direction matters less than magnitude</li>
                                <li>Low to moderate dimensionality</li>
                                <li>Isotropic (spherical) cluster shapes</li>
                            </ul>
                        </div>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 8px;">
                            <h4>‚ùå Euclidean Distance Struggles With:</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Mixed data types (categorical + numerical)</li>
                                <li>Different feature scales/units</li>
                                <li>High-dimensional sparse data</li>
                                <li>Correlated features</li>
                                <li>Non-spherical cluster shapes</li>
                                <li>Outliers (sensitive to extreme values)</li>
                                <li>Text/document similarity</li>
                            </ul>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Euclidean Distance Sensitivity to Scale</h4>
                        <p><strong>Image Description:</strong> Two 2D scatter plots side by side. Left plot: "Unscaled Data" showing customer data with age (20-60) on x-axis and income ($20K-$200K) on y-axis. Income dominates the distance calculation, creating horizontal clusters. Right plot: "Scaled Data" showing the same data after standardization, with both features now contributing equally, revealing more natural circular clusters.</p>
                        <p><em>This demonstrates why feature scaling is crucial when using Euclidean distance</em></p>
                    </div>

                    <h3>üßÆ Alternative Formulations</h3>
                    <p>Euclidean distance can be expressed in several mathematically equivalent ways, each offering different computational or theoretical insights.</p>

                    <div class="formula-box">
                        <h4>üîÑ Equivalent Formulations</h4>
                        
                        <h5>1. Standard Form:</h5>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            d_E(x, y) = ‚àö(Œ£·µ¢‚Çå‚ÇÅ‚Åø (x·µ¢ - y·µ¢)¬≤)
                        </div>
                        
                        <h5>2. Vector Norm Form:</h5>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            d_E(x, y) = ‚Äñx - y‚Äñ‚ÇÇ
                        </div>
                        
                        <h5>3. Inner Product Form:</h5>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            d_E(x, y) = ‚àö‚ü®x - y, x - y‚ü©
                        </div>
                        
                        <h5>4. Expanded Form:</h5>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            d_E¬≤(x, y) = ‚ü®x, x‚ü© - 2‚ü®x, y‚ü© + ‚ü®y, y‚ü© = ‚Äñx‚Äñ¬≤ - 2‚ü®x, y‚ü© + ‚Äñy‚Äñ¬≤
                        </div>
                        
                        <h5>5. Matrix Form (for multiple points):</h5>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            D¬≤·µ¢‚±º = (x·µ¢ - x‚±º)·µÄ(x·µ¢ - x‚±º)
                        </div>
                        
                        <p>The expanded form (4) is particularly useful in machine learning algorithms like K-means, where ‚Äñx‚Äñ¬≤ and ‚Äñy‚Äñ¬≤ can be precomputed.</p>
                    </div>
                </div>

                <!-- Manhattan Distance Section -->
                <div id="manhattan" class="content-section">
                    <h2>Manhattan Distance: The City Block Metric</h2>
                    
                    <p>Manhattan distance, also known as L‚ÇÅ norm, city block distance, or taxicab distance, measures distance as the sum of absolute differences across all dimensions. The name comes from the grid-like street pattern of Manhattan, where travel distance is constrained to horizontal and vertical movements.</p>

                    <div class="formula-box">
                        <h3>üèôÔ∏è Mathematical Definition</h3>
                        <p>For two points x, y ‚àà ‚Ñù‚Åø, the Manhattan distance is defined as:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>d_M(x, y) = Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢|</strong>
                        </div>
                        
                        <p>This can also be expressed using vector notation:</p>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>d_M(x, y) = ‚Äñx - y‚Äñ‚ÇÅ</strong>
                        </div>
                        
                        <p>Where ‚Äñ¬∑‚Äñ‚ÇÅ denotes the L‚ÇÅ norm or 1-norm of a vector.</p>
                    </div>

                    <h3>üó∫Ô∏è Geometric Intuition</h3>
                    <p>The key insight behind Manhattan distance is that it measures distance along axis-aligned paths rather than straight-line distance. This makes it particularly suitable when movement is constrained or when features represent independent quantities.</p>

                    <div class="visualization-placeholder">
                        <h4>üöï Visualization: Manhattan vs Euclidean Paths</h4>
                        <p><strong>Image Description:</strong> A city grid showing the path from point A to point B. Multiple step-like paths are shown in blue (all representing Manhattan distance = 7), while a single red diagonal line shows Euclidean distance = 5. Street grid lines emphasize the constraint to orthogonal movement. Small taxi icons along the blue paths reinforce the "taxicab" metaphor.</p>
                        <p><em>This illustrates why Manhattan distance is called "taxicab distance"</em></p>
                    </div>

                    <h3>üìê Dimensional Analysis</h3>
                    <p>Like Euclidean distance, Manhattan distance extends naturally across dimensions, but with different geometric properties.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üìè 1D Case</h4>
                            <p>For points x, y ‚àà ‚Ñù:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_M(x, y) = |x - y|</strong>
                            </div>
                            <p><strong>Example:</strong> d_M(3, 7) = |3 - 7| = 4</p>
                            <p><em>Note: In 1D, Manhattan distance equals Euclidean distance!</em></p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üè¢ 2D Case</h4>
                            <p>For points (x‚ÇÅ, x‚ÇÇ), (y‚ÇÅ, y‚ÇÇ) ‚àà ‚Ñù¬≤:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_M = |x‚ÇÅ - y‚ÇÅ| + |x‚ÇÇ - y‚ÇÇ|</strong>
                            </div>
                            <p><strong>Example:</strong> d_M((1, 2), (4, 6)) = |1-4| + |2-6| = 3 + 4 = 7</p>
                            <p>This creates diamond-shaped "circles" instead of circular ones.</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üé≤ 3D Case</h4>
                            <p>For points (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ), (y‚ÇÅ, y‚ÇÇ, y‚ÇÉ) ‚àà ‚Ñù¬≥:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_M = |x‚ÇÅ - y‚ÇÅ| + |x‚ÇÇ - y‚ÇÇ| + |x‚ÇÉ - y‚ÇÉ|</strong>
                            </div>
                            <p><strong>Example:</strong> d_M((1, 2, 3), (4, 6, 8)) = 3 + 4 + 5 = 12</p>
                            <p>Creates octahedral "spheres" in 3D space.</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üåê n-D Case</h4>
                            <p>For points x, y ‚àà ‚Ñù‚Åø:</p>
                            <div style="background: white; padding: 0.5rem; border-radius: 4px; margin: 0.5rem 0; text-align: center;">
                                <strong>d_M = Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢|</strong>
                            </div>
                            <p>Creates hyperoctahedral "spheres" - the L‚ÇÅ unit ball becomes increasingly sparse in high dimensions.</p>
                        </div>
                    </div>

                    <h3>üî¨ Mathematical Properties</h3>
                    <p>Manhattan distance satisfies all metric space axioms and has additional properties that make it useful in specific scenarios.</p>

                    <div class="proof-box">
                        <h4>üìã Metric Space Verification</h4>
                        
                        <h5>1Ô∏è‚É£ Non-negativity:</h5>
                        <p>Since |x·µ¢ - y·µ¢| ‚â• 0 for all i:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            d_M(x, y) = Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢| ‚â• 0
                        </div>
                        
                        <h5>2Ô∏è‚É£ Identity of Indiscernibles:</h5>
                        <p>d_M(x, y) = 0 ‚ü∫ Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢| = 0</p>
                        <p>Since each |x·µ¢ - y·µ¢| ‚â• 0, the sum is zero iff each term is zero:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            |x·µ¢ - y·µ¢| = 0 ‚àÄi ‚ü∫ x·µ¢ = y·µ¢ ‚àÄi ‚ü∫ x = y
                        </div>
                        
                        <h5>3Ô∏è‚É£ Symmetry:</h5>
                        <p>d_M(x, y) = Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢| = Œ£·µ¢‚Çå‚ÇÅ‚Åø |y·µ¢ - x·µ¢| = d_M(y, x)</p>
                        
                        <h5>4Ô∏è‚É£ Triangle Inequality:</h5>
                        <p>For each dimension i: |x·µ¢ - z·µ¢| ‚â§ |x·µ¢ - y·µ¢| + |y·µ¢ - z·µ¢| (triangle inequality for absolute values)</p>
                        <p>Summing over all dimensions:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            Œ£·µ¢ |x·µ¢ - z·µ¢| ‚â§ Œ£·µ¢ |x·µ¢ - y·µ¢| + Œ£·µ¢ |y·µ¢ - z·µ¢|
                        </div>
                        <p>Therefore: d_M(x, z) ‚â§ d_M(x, y) + d_M(y, z)</p>
                    </div>

                    <h3>‚ö° Computational Advantages</h3>
                    <p>Manhattan distance offers several computational benefits over Euclidean distance, particularly in specific contexts.</p>

                    <div class="formula-box">
                        <h4>üíª Computational Properties</h4>
                        <ul>
                            <li><strong>Time Complexity:</strong> O(n) - same as Euclidean</li>
                            <li><strong>No Square Root:</strong> Avoids expensive sqrt() operation</li>
                            <li><strong>No Squaring:</strong> Reduces risk of numerical overflow</li>
                            <li><strong>Sparse Data Friendly:</strong> Many terms may be zero</li>
                            <li><strong>Numerically Stable:</strong> Less sensitive to extreme values</li>
                            <li><strong>Gradient Properties:</strong> Subdifferentiable (useful in optimization)</li>
                        </ul>
                        
                        <h5>üéØ Performance Considerations:</h5>
                        <ul>
                            <li><strong>Cache Efficiency:</strong> Same memory access pattern as Euclidean</li>
                            <li><strong>Vectorization:</strong> Can use SIMD instructions effectively</li>
                            <li><strong>Early Termination:</strong> Can bound distance during computation</li>
                            <li><strong>Sparse Optimization:</strong> Skip zero components in sparse vectors</li>
                        </ul>
                    </div>

                    <h3>üîç Manhattan Distance in High Dimensions</h3>
                    <p>Manhattan distance exhibits different behavior from Euclidean distance in high-dimensional spaces, which can be advantageous for certain applications.</p>

                    <div class="property-box">
                        <h4>üìä High-Dimensional Behavior</h4>
                        <p>Unlike Euclidean distance, Manhattan distance doesn't suffer as severely from the curse of dimensionality:</p>
                        
                        <ul>
                            <li><strong>Concentration Effect:</strong> Less pronounced than with Euclidean distance</li>
                            <li><strong>Relative Distances:</strong> Maintains better discriminative power</li>
                            <li><strong>Sparse Data:</strong> Natural handling of zero components</li>
                            <li><strong>Robust to Outliers:</strong> Less sensitive to extreme values in any single dimension</li>
                        </ul>
                        
                        <h5>üßÆ Mathematical Analysis:</h5>
                        <p>For random vectors in high dimensions, the coefficient of variation (CV) for Manhattan distance:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            CV_Manhattan ‚àù 1/‚àön
                        </div>
                        <p>Compared to Euclidean distance:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            CV_Euclidean ‚àù 1/‚àö(2n)
                        </div>
                        <p>This means Manhattan distance maintains better relative discrimination in high dimensions.</p>
                    </div>

                    <h3>üéØ When to Use Manhattan Distance</h3>
                    <p>Manhattan distance is particularly well-suited for specific types of data and problem contexts.</p>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>‚úÖ Manhattan Distance Excels With:</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Discrete or ordinal features</li>
                                <li>Count data (frequencies, occurrences)</li>
                                <li>Grid-based movement problems</li>
                                <li>Robust clustering (outlier resistance)</li>
                                <li>High-dimensional sparse data</li>
                                <li>Independent feature dimensions</li>
                                <li>L‚ÇÅ regularization contexts</li>
                                <li>City/network distance problems</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>‚ö†Ô∏è Manhattan Distance Limitations:</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Doesn't capture diagonal relationships well</li>
                                <li>Less intuitive for continuous geometric data</li>
                                <li>May create "blocky" cluster boundaries</li>
                                <li>Can be overly simplistic for complex patterns</li>
                                <li>Direction information is lost</li>
                                <li>May not respect feature correlations</li>
                            </ul>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üîÑ Visualization: Manhattan vs Euclidean Distance Contours</h4>
                        <p><strong>Image Description:</strong> A 2D plot centered at origin showing distance contours. Euclidean distance creates concentric circles in red (distances 1, 2, 3). Manhattan distance creates concentric diamonds in blue (same distances). The diamonds are rotated 45¬∞ squares. Points at equal distance from center form different shapes: circles vs diamonds, illustrating fundamental geometric differences.</p>
                        <p><em>This shows how the choice of distance metric shapes the geometry of "equal distance" regions</em></p>
                    </div>

                    <h3>üèóÔ∏è Real-World Applications</h3>
                    <p>Manhattan distance finds natural applications in domains where its geometric properties align with problem constraints.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üó∫Ô∏è Urban Planning</h4>
                            <p>Calculating actual travel distances in grid-based cities, delivery route optimization, emergency response planning.</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üìä Data Mining</h4>
                            <p>Market basket analysis, recommendation systems, outlier detection in transaction data.</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üéÆ Game Development</h4>
                            <p>Grid-based movement, pathfinding in tile-based games, turn-based strategy games.</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üß¨ Bioinformatics</h4>
                            <p>Gene expression analysis, phylogenetic distance, sequence comparison with independent features.</p>
                        </div>
                    </div>
                </div>

                <!-- Properties Section -->
                <div id="properties" class="content-section">
                    <h2>Mathematical Properties and Relationships</h2>
                    
                    <p>Understanding the deeper mathematical relationships between Euclidean and Manhattan distances reveals fundamental insights about metric spaces and helps guide practical applications. These relationships form the foundation for more advanced distance metrics and clustering algorithms.</p>

                    <h3>‚öñÔ∏è Comparative Analysis: Euclidean vs Manhattan</h3>
                    <p>Both metrics belong to the family of Lp norms, but they exhibit fundamentally different geometric and analytical properties.</p>

                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Property</th>
                                <th>Euclidean Distance (L‚ÇÇ)</th>
                                <th>Manhattan Distance (L‚ÇÅ)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Formula</strong></td>
                                <td>‚àö(Œ£·µ¢ (x·µ¢ - y·µ¢)¬≤)</td>
                                <td>Œ£·µ¢ |x·µ¢ - y·µ¢|</td>
                            </tr>
                            <tr>
                                <td><strong>Unit Ball Shape</strong></td>
                                <td>Circle/Sphere</td>
                                <td>Diamond/Octahedron</td>
                            </tr>
                            <tr>
                                <td><strong>Smoothness</strong></td>
                                <td>Differentiable everywhere</td>
                                <td>Non-differentiable at origin</td>
                            </tr>
                            <tr>
                                <td><strong>Computational Cost</strong></td>
                                <td>Higher (sqrt, squares)</td>
                                <td>Lower (absolute values only)</td>
                            </tr>
                            <tr>
                                <td><strong>Outlier Sensitivity</strong></td>
                                <td>High (quadratic penalty)</td>
                                <td>Lower (linear penalty)</td>
                            </tr>
                            <tr>
                                <td><strong>High-Dim Behavior</strong></td>
                                <td>Distance concentration</td>
                                <td>Better discrimination</td>
                            </tr>
                            <tr>
                                <td><strong>Rotation Invariance</strong></td>
                                <td>Yes</td>
                                <td>No</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>üìê Norm Relationships and Bounds</h3>
                    <p>The relationship between different Lp norms provides important theoretical bounds and practical insights.</p>

                    <div class="formula-box">
                        <h4>üîó Fundamental Norm Inequalities</h4>
                        <p>For any vector v ‚àà ‚Ñù‚Åø, the following relationships always hold:</p>
                        
                        <h5>1Ô∏è‚É£ Basic Ordering:</h5>
                        <div style="text-align: center; font-size: 1.1rem; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>‚Äñv‚Äñ‚àû ‚â§ ‚Äñv‚Äñ‚ÇÇ ‚â§ ‚Äñv‚Äñ‚ÇÅ</strong>
                        </div>
                        
                        <h5>2Ô∏è‚É£ Tight Bounds with Dimension:</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>‚Äñv‚Äñ‚ÇÇ ‚â§ ‚Äñv‚Äñ‚ÇÅ ‚â§ ‚àön ‚Äñv‚Äñ‚ÇÇ</strong><br>
                            <strong>‚Äñv‚Äñ‚àû ‚â§ ‚Äñv‚Äñ‚ÇÇ ‚â§ ‚àön ‚Äñv‚Äñ‚àû</strong><br>
                            <strong>‚Äñv‚Äñ‚àû ‚â§ ‚Äñv‚Äñ‚ÇÅ ‚â§ n ‚Äñv‚Äñ‚àû</strong>
                        </div>
                        
                        <h5>3Ô∏è‚É£ Distance Implications:</h5>
                        <p>For distance between points x and y:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>d_E(x, y) ‚â§ d_M(x, y) ‚â§ ‚àön ¬∑ d_E(x, y)</strong>
                        </div>
                        
                        <p>These bounds are tight and achieved by specific vector configurations.</p>
                    </div>

                    <div class="proof-box">
                        <h4>üìã Proof of Key Inequality: ‚Äñv‚Äñ‚ÇÇ ‚â§ ‚Äñv‚Äñ‚ÇÅ</h4>
                        <p>We'll prove that Euclidean norm is always ‚â§ Manhattan norm using the Cauchy-Schwarz inequality.</p>
                        
                        <p><strong>Given:</strong> Vector v = [v‚ÇÅ, v‚ÇÇ, ..., v‚Çô]·µÄ ‚àà ‚Ñù‚Åø</p>
                        
                        <p><strong>Proof:</strong></p>
                        <p>Apply Cauchy-Schwarz inequality with vectors (|v‚ÇÅ|, |v‚ÇÇ|, ..., |v‚Çô|) and (1, 1, ..., 1):</p>
                        
                        <div style="margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 4px;">
                            ‚ü®(|v‚ÇÅ|, ..., |v‚Çô|), (1, ..., 1)‚ü© ‚â§ ‚Äñ(|v‚ÇÅ|, ..., |v‚Çô|)‚Äñ‚ÇÇ ¬∑ ‚Äñ(1, ..., 1)‚Äñ‚ÇÇ
                        </div>
                        
                        <p>This gives us:</p>
                        <div style="margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 4px;">
                            Œ£·µ¢ |v·µ¢| ‚â§ ‚àö(Œ£·µ¢ |v·µ¢|¬≤) ¬∑ ‚àön = ‚àö(Œ£·µ¢ v·µ¢¬≤) ¬∑ ‚àön
                        </div>
                        
                        <p>Therefore:</p>
                        <div style="margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 4px;">
                            ‚Äñv‚Äñ‚ÇÅ ‚â§ ‚àön ¬∑ ‚Äñv‚Äñ‚ÇÇ ‚üπ ‚Äñv‚Äñ‚ÇÇ ‚â§ ‚Äñv‚Äñ‚ÇÅ
                        </div>
                        
                        <p><strong>Equality cases:</strong> The bound is achieved when all |v·µ¢| are equal. ‚àé</p>
                    </div>

                    <h3>üîÑ Transformation Properties</h3>
                    <p>Understanding how distance metrics behave under common transformations is crucial for preprocessing and algorithm design.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üîÑ Scaling Invariance</h4>
                            <p><strong>Euclidean:</strong> d_E(Œ±x, Œ±y) = |Œ±| ¬∑ d_E(x, y)</p>
                            <p><strong>Manhattan:</strong> d_M(Œ±x, Œ±y) = |Œ±| ¬∑ d_M(x, y)</p>
                            <p>Both scale linearly with uniform scaling.</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üåÄ Rotation Invariance</h4>
                            <p><strong>Euclidean:</strong> Rotationally invariant</p>
                            <p>d_E(Rx, Ry) = d_E(x, y) for orthogonal R</p>
                            <p><strong>Manhattan:</strong> Not rotationally invariant</p>
                            <p>d_M(Rx, Ry) ‚â† d_M(x, y) in general</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üìê Translation Invariance</h4>
                            <p><strong>Both metrics:</strong> Translation invariant</p>
                            <p>d(x + c, y + c) = d(x, y) for any constant c</p>
                            <p>This is essential for clustering algorithms.</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üîß Affine Transformations</h4>
                            <p>Under general affine transformation Ax + b:</p>
                            <p><strong>Euclidean:</strong> d_E(Ax, Ay) = ‚ÄñA(x-y)‚Äñ‚ÇÇ</p>
                            <p><strong>Manhattan:</strong> d_M(Ax, Ay) = ‚ÄñA(x-y)‚Äñ‚ÇÅ</p>
                            <p>Behavior depends on matrix A properties.</p>
                        </div>
                    </div>

                    <h3>üìä Statistical Properties</h3>
                    <p>The statistical behavior of distance metrics under random data provides insights for algorithm design and analysis.</p>

                    <div class="formula-box">
                        <h4>üé≤ Random Vector Analysis</h4>
                        <p>For random vectors X, Y ‚àà ‚Ñù‚Åø with i.i.d. components from standard normal distribution:</p>
                        
                        <h5>üìà Expected Values:</h5>
                        <ul>
                            <li><strong>E[d_E¬≤(X, Y)]</strong> = 2n (squared Euclidean distance)</li>
                            <li><strong>E[d_E(X, Y)]</strong> ‚âà ‚àö(2n) (for large n)</li>
                            <li><strong>E[d_M(X, Y)]</strong> = 2‚àö(2n/œÄ) ‚âà 1.596‚àön</li>
                        </ul>
                        
                        <h5>üìä Variance Behavior:</h5>
                        <ul>
                            <li><strong>Var[d_E¬≤(X, Y)]</strong> = 4n</li>
                            <li><strong>Var[d_M(X, Y)]</strong> ‚âà (4/œÄ - 8/œÄ¬≤)n</li>
                        </ul>
                        
                        <h5>üéØ Concentration Properties:</h5>
                        <p>Coefficient of variation (CV = œÉ/Œº) behavior:</p>
                        <ul>
                            <li><strong>CV[d_E(X, Y)]</strong> ‚àù 1/‚àön ‚Üí 0 as n ‚Üí ‚àû</li>
                            <li><strong>CV[d_M(X, Y)]</strong> ‚àù 1/‚àön ‚Üí 0 as n ‚Üí ‚àû</li>
                        </ul>
                        <p>Both metrics suffer from concentration of measure in high dimensions, but at different rates.</p>
                    </div>

                    <h3>üîç Optimization Properties</h3>
                    <p>The mathematical properties of distance functions significantly impact optimization algorithms and convergence guarantees.</p>

                    <div class="property-box">
                        <h4>‚ö° Smoothness and Differentiability</h4>
                        
                        <h5>üåä Euclidean Distance:</h5>
                        <ul>
                            <li><strong>Smoothness:</strong> C‚àû (infinitely differentiable) everywhere except at x = y</li>
                            <li><strong>Gradient:</strong> ‚àá_x d_E(x, y) = (x - y)/‚Äñx - y‚Äñ‚ÇÇ</li>
                            <li><strong>Hessian:</strong> Positive definite away from x = y</li>
                            <li><strong>Convexity:</strong> Convex in x for fixed y</li>
                        </ul>
                        
                        <h5>üèôÔ∏è Manhattan Distance:</h5>
                        <ul>
                            <li><strong>Smoothness:</strong> Not differentiable when x·µ¢ = y·µ¢ for any i</li>
                            <li><strong>Subgradient:</strong> ‚àÇd_M(x, y) = sign(x - y) (when well-defined)</li>
                            <li><strong>Convexity:</strong> Convex in x for fixed y</li>
                            <li><strong>L‚ÇÅ Regularization:</strong> Promotes sparsity in optimization</li>
                        </ul>
                        
                        <h5>üéØ Implications for Algorithms:</h5>
                        <ul>
                            <li><strong>Gradient Descent:</strong> Works well with Euclidean, needs subgradient methods for Manhattan</li>
                            <li><strong>Newton's Method:</strong> Natural for Euclidean, challenging for Manhattan</li>
                            <li><strong>Proximal Methods:</strong> Particularly suited for Manhattan distance optimization</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Distance Function Landscapes</h4>
                        <p><strong>Image Description:</strong> Two 3D surface plots side by side. Left: Euclidean distance function d_E(x, (0,0)) showing a smooth cone shape with the tip at origin, smooth everywhere except the peak. Right: Manhattan distance function d_M(x, (0,0)) showing a pyramid shape with sharp ridges along the axes, non-differentiable along the coordinate axes. Color gradients from blue (low) to red (high) show distance values.</p>
                        <p><em>This illustrates the fundamental difference in smoothness between the two metrics</em></p>
                    </div>

                    <h3>üî¨ Advanced Theoretical Results</h3>
                    <p>Several deep theoretical results connect Euclidean and Manhattan distances to broader mathematical frameworks.</p>

                    <div class="formula-box">
                        <h4>üéì Johnson-Lindenstrauss Lemma</h4>
                        <p>For Euclidean distances in high-dimensional spaces:</p>
                        <blockquote style="background: white; padding: 1rem; border-left: 4px solid #2196f3; margin: 1rem 0; font-style: italic;">
                            Any set of n points in ‚Ñù·µà can be embedded into ‚Ñù·µè with k = O(log n/Œµ¬≤) such that all pairwise Euclidean distances are preserved within factor (1 ¬± Œµ).
                        </blockquote>
                        <p>This result is fundamental to dimensionality reduction and doesn't directly apply to Manhattan distance.</p>
                        
                        <h4>üìä Dvoretzky's Theorem</h4>
                        <p>In high-dimensional spaces, most sections of convex bodies are approximately spherical. This explains why Euclidean distance becomes natural in high dimensions despite other metrics being more appropriate in low dimensions.</p>
                        
                        <h4>üéØ Concentration of Measure</h4>
                        <p>For both metrics, in high dimensions, random distances concentrate around their mean. However, the rate and practical implications differ:</p>
                        <ul>
                            <li><strong>Euclidean:</strong> Faster concentration, more severe in practice</li>
                            <li><strong>Manhattan:</strong> Slower concentration, better discrimination retained</li>
                        </ul>
                    </div>
                </div>

                <!-- Geometric Interpretation Section -->
                <div id="geometric" class="content-section">
                    <h2>Geometric Interpretation and Unit Balls</h2>
                    
                    <p>The geometric interpretation of distance metrics provides crucial intuition for understanding clustering behavior. The shape of unit balls (sets of points at unit distance from the origin) reveals fundamental properties about how each metric organizes space and influences clustering algorithms.</p>

                    <h3>üîÆ Unit Ball Geometry</h3>
                    <p>The unit ball B_p = {x ‚àà ‚Ñù‚Åø : ‚Äñx‚Äñ_p ‚â§ 1} characterizes the geometric structure of each metric. Understanding these shapes helps predict clustering behavior and algorithm performance.</p>

                    <div class="visualization-placeholder">
                        <h4>üéØ Visualization: Unit Balls in 2D</h4>
                        <p><strong>Image Description:</strong> A coordinate plane showing three overlaid unit balls: (1) L‚ÇÅ (Manhattan) unit ball as a diamond shape rotated 45¬∞, vertices at (¬±1,0) and (0,¬±1), (2) L‚ÇÇ (Euclidean) unit ball as a perfect circle centered at origin with radius 1, (3) L‚àû (Chebyshev) unit ball as a square with vertices at (¬±1,¬±1). All shapes contain the same "unit distance" from origin but with different geometries. Grid lines help show the coordinate system.</p>
                        <p><em>This demonstrates how different norms create different notions of "unit distance"</em></p>
                    </div>

                    <div class="formula-box">
                        <h4>üìê Mathematical Definitions of Unit Balls</h4>
                        
                        <h5>üîµ Euclidean Unit Ball (L‚ÇÇ):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>B‚ÇÇ = {x ‚àà ‚Ñù‚Åø : Œ£·µ¢‚Çå‚ÇÅ‚Åø x·µ¢¬≤ ‚â§ 1}</strong>
                        </div>
                        <p><strong>Shape:</strong> Circle (2D), Sphere (3D), Hypersphere (n-D)</p>
                        <p><strong>Properties:</strong> Smooth boundary, rotationally symmetric, maximum volume among all Lp unit balls</p>
                        
                        <h5>üî∑ Manhattan Unit Ball (L‚ÇÅ):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>B‚ÇÅ = {x ‚àà ‚Ñù‚Åø : Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢| ‚â§ 1}</strong>
                        </div>
                        <p><strong>Shape:</strong> Diamond (2D), Octahedron (3D), Cross-polytope (n-D)</p>
                        <p><strong>Properties:</strong> Sharp corners, axis-aligned symmetry, vertices on coordinate axes</p>
                        
                        <h5>‚¨ú Chebyshev Unit Ball (L‚àû):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>B‚àû = {x ‚àà ‚Ñù‚Åø : max_i |x·µ¢| ‚â§ 1}</strong>
                        </div>
                        <p><strong>Shape:</strong> Square (2D), Cube (3D), Hypercube (n-D)</p>
                        <p><strong>Properties:</strong> Flat faces, maximum "reach" along axes</p>
                    </div>

                    <h3>üìè Volume Relationships</h3>
                    <p>The volumes of unit balls provide quantitative measures of how different metrics fill space, with important implications for high-dimensional analysis.</p>

                    <div class="formula-box">
                        <h4>üìä Volume Calculations</h4>
                        
                        <h5>üîµ Euclidean Unit Ball Volume (V‚ÇÇ):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>V‚ÇÇ(n) = œÄ^(n/2) / Œì(n/2 + 1)</strong>
                        </div>
                        <p>Where Œì is the gamma function. For specific dimensions:</p>
                        <ul>
                            <li><strong>2D:</strong> V‚ÇÇ(2) = œÄ ‚âà 3.14159</li>
                            <li><strong>3D:</strong> V‚ÇÇ(3) = 4œÄ/3 ‚âà 4.1888</li>
                            <li><strong>4D:</strong> V‚ÇÇ(4) = œÄ¬≤/2 ‚âà 4.9348</li>
                        </ul>
                        
                        <h5>üî∑ Manhattan Unit Ball Volume (V‚ÇÅ):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>V‚ÇÅ(n) = 2‚Åø / n!</strong>
                        </div>
                        <p>For specific dimensions:</p>
                        <ul>
                            <li><strong>2D:</strong> V‚ÇÅ(2) = 4/2! = 2</li>
                            <li><strong>3D:</strong> V‚ÇÅ(3) = 8/3! ‚âà 1.333</li>
                            <li><strong>4D:</strong> V‚ÇÅ(4) = 16/4! ‚âà 0.667</li>
                        </ul>
                        
                        <h5>‚¨ú Chebyshev Unit Ball Volume (V‚àû):</h5>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>V‚àû(n) = 2‚Åø</strong>
                        </div>
                        <p>This grows exponentially and dominates in high dimensions.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Unit Ball Volumes vs Dimension</h4>
                        <p><strong>Image Description:</strong> A log-scale plot showing volume vs dimension (1-20) for the three unit balls. Three curves: (1) Blue line for Euclidean volume, starts high, peaks around dimension 5-6, then decreases rapidly toward zero. (2) Red line for Manhattan volume, decreases monotonically as factorial grows in denominator. (3) Green line for Chebyshev volume, increases exponentially as 2‚Åø. Crossover points show when different metrics dominate. Y-axis is logarithmic scale.</p>
                        <p><em>This reveals the dramatic dimensional dependence of unit ball volumes</em></p>
                    </div>

                    <h3>üéØ Clustering Implications of Unit Ball Geometry</h3>
                    <p>The shape of unit balls directly influences clustering behavior, determining natural cluster shapes and algorithm performance.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üîµ Euclidean Distance Clustering</h4>
                            <h5>Natural Cluster Shapes:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Spherical/circular clusters</li>
                                <li>Isotropic (equal spread in all directions)</li>
                                <li>Optimal for globular data</li>
                            </ul>
                            <h5>Algorithm Behavior:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>K-means creates Voronoi cells</li>
                                <li>Sensitive to feature scaling</li>
                                <li>Assumes similar cluster sizes</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üî∑ Manhattan Distance Clustering</h4>
                            <h5>Natural Cluster Shapes:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Diamond/octahedral clusters</li>
                                <li>Axis-aligned orientations</li>
                                <li>Sharp boundaries along axes</li>
                            </ul>
                            <h5>Algorithm Behavior:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Creates "blocky" partitions</li>
                                <li>More robust to outliers</li>
                                <li>Natural for grid-based data</li>
                            </ul>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üé® Visualization: Clustering Shapes by Distance Metric</h4>
                        <p><strong>Image Description:</strong> Two 2D scatter plots comparing clustering results. Left plot: "Euclidean K-means" showing three circular clusters with smooth, round boundaries. Points are colored by cluster (red, blue, green). Cluster centroids marked with X's. Right plot: "Manhattan K-means" showing the same data points but with diamond-shaped cluster boundaries that are more angular and oriented along coordinate axes. Decision boundaries are visibly different, creating different cluster assignments for some boundary points.</p>
                        <p><em>This shows how distance metric choice fundamentally shapes clustering results</em></p>
                    </div>

                    <h3>üîç Nearest Neighbor Behavior</h3>
                    <p>The geometry of unit balls determines which points are considered "nearest neighbors," fundamentally affecting clustering algorithms and data analysis.</p>

                    <div class="property-box">
                        <h4>üéØ Nearest Neighbor Analysis</h4>
                        
                        <h5>üîµ Euclidean Nearest Neighbors:</h5>
                        <ul>
                            <li><strong>Symmetry:</strong> Rotationally invariant neighborhood structure</li>
                            <li><strong>Boundary:</strong> Smooth, curved decision boundaries</li>
                            <li><strong>Sensitivity:</strong> Highly sensitive to feature scaling</li>
                            <li><strong>High Dimensions:</strong> Neighbors become increasingly equidistant</li>
                        </ul>
                        
                        <h5>üî∑ Manhattan Nearest Neighbors:</h5>
                        <ul>
                            <li><strong>Symmetry:</strong> Axis-aligned structure, not rotationally invariant</li>
                            <li><strong>Boundary:</strong> Piecewise linear decision boundaries</li>
                            <li><strong>Robustness:</strong> More robust to outliers in individual dimensions</li>
                            <li><strong>High Dimensions:</strong> Better discrimination than Euclidean</li>
                        </ul>
                        
                        <h5>‚ö†Ô∏è Practical Implications:</h5>
                        <ul>
                            <li><strong>Feature Engineering:</strong> Manhattan distance less sensitive to irrelevant features</li>
                            <li><strong>Preprocessing:</strong> Euclidean requires careful scaling, Manhattan more forgiving</li>
                            <li><strong>Dimensionality:</strong> Manhattan often superior in high dimensions</li>
                            <li><strong>Interpretability:</strong> Manhattan provides clearer feature-wise contributions</li>
                        </ul>
                    </div>

                    <h3>üìê High-Dimensional Geometry</h3>
                    <p>As dimensionality increases, the geometry of unit balls reveals profound insights about distance behavior and clustering performance.</p>

                    <div class="formula-box">
                        <h4>üåê High-Dimensional Phenomena</h4>
                        
                        <h5>üìä Volume Concentration:</h5>
                        <p>In high dimensions, most of the volume of a unit ball concentrates near the boundary:</p>
                        <ul>
                            <li><strong>Euclidean:</strong> Volume ratio of shell (1-Œµ, 1) to full ball approaches 1 as n ‚Üí ‚àû</li>
                            <li><strong>Manhattan:</strong> Similar concentration but with different geometric structure</li>
                        </ul>
                        
                        <h5>üéØ Distance Concentration:</h5>
                        <p>Random points become approximately equidistant:</p>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                            <strong>lim_{n‚Üí‚àû} (d_max - d_min) / d_min ‚Üí 0</strong>
                        </div>
                        <p>But Manhattan distance maintains better relative discrimination.</p>
                        
                        <h5>üîç Nearest Neighbor Behavior:</h5>
                        <ul>
                            <li><strong>k-NN becomes unstable:</strong> Many points are approximately equidistant</li>
                            <li><strong>Hub phenomenon:</strong> Some points become neighbors to many others</li>
                            <li><strong>Metric choice matters more:</strong> Small differences in metric can lead to large differences in results</li>
                        </ul>
                    </div>

                    <h3>üé≤ Practical Geometric Insights</h3>
                    <p>Understanding unit ball geometry provides actionable insights for practical clustering applications.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üéØ Algorithm Selection</h4>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Spherical clusters:</strong> Use Euclidean distance</li>
                                <li><strong>Axis-aligned patterns:</strong> Use Manhattan distance</li>
                                <li><strong>Mixed orientations:</strong> Consider both and compare</li>
                                <li><strong>High dimensions:</strong> Lean toward Manhattan</li>
                            </ul>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üîß Preprocessing Strategy</h4>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Euclidean:</strong> Standardize features carefully</li>
                                <li><strong>Manhattan:</strong> Consider median-based scaling</li>
                                <li><strong>Rotation:</strong> PCA before Euclidean clustering</li>
                                <li><strong>Outliers:</strong> Remove before Euclidean, keep for Manhattan</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üìä Evaluation Metrics</h4>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Silhouette:</strong> Natural for spherical clusters (Euclidean)</li>
                                <li><strong>Calinski-Harabasz:</strong> Assumes spherical clusters</li>
                                <li><strong>Custom metrics:</strong> Match evaluation to distance choice</li>
                                <li><strong>Visual inspection:</strong> Always validate geometrically</li>
                            </ul>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üî¨ Visualization: High-Dimensional Unit Ball Cross-Sections</h4>
                        <p><strong>Image Description:</strong> Four panels showing 2D cross-sections of unit balls in different dimensions. Panel 1: "2D" showing full unit circles/diamonds. Panel 2: "5D cross-section" showing slightly thinner shapes. Panel 3: "10D cross-section" showing much thinner shapes concentrated near boundary. Panel 4: "20D cross-section" showing extremely thin rings/boundaries. Demonstrates how volume concentrates at the boundary in high dimensions.</p>
                        <p><em>This illustrates the concentration of measure phenomenon in high-dimensional spaces</em></p>
                    </div>
                </div>

                <!-- Computational Aspects Section -->
                <div id="computational" class="content-section">
                    <h2>Computational Aspects and Optimization</h2>
                    
                    <p>Understanding the computational characteristics of distance metrics is crucial for implementing efficient clustering algorithms. This section covers algorithmic complexity, numerical considerations, optimization techniques, and practical implementation strategies for both Euclidean and Manhattan distances.</p>

                    <h3>‚ö° Computational Complexity Analysis</h3>
                    <p>While both metrics have similar theoretical complexity, their practical performance characteristics differ significantly across various contexts.</p>

                    <div class="formula-box">
                        <h4>üñ•Ô∏è Time Complexity Breakdown</h4>
                        
                        <h5>üìä Per-Distance Calculation:</h5>
                        <table class="comparison-table" style="margin: 1rem 0;">
                            <thead>
                                <tr>
                                    <th>Operation</th>
                                    <th>Euclidean</th>
                                    <th>Manhattan</th>
                                    <th>Notes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Basic Operations</strong></td>
                                    <td>n multiplications<br>n additions<br>n subtractions<br>1 square root</td>
                                    <td>n subtractions<br>n absolute values<br>n-1 additions</td>
                                    <td>Manhattan avoids expensive operations</td>
                                </tr>
                                <tr>
                                    <td><strong>Time Complexity</strong></td>
                                    <td>O(n)</td>
                                    <td>O(n)</td>
                                    <td>Same asymptotic complexity</td>
                                </tr>
                                <tr>
                                    <td><strong>Constant Factors</strong></td>
                                    <td>Higher</td>
                                    <td>Lower</td>
                                    <td>Manhattan typically 2-3x faster</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <h5>‚öôÔ∏è Pairwise Distance Matrix:</h5>
                        <p>For n points in d dimensions:</p>
                        <ul>
                            <li><strong>Naive approach:</strong> O(n¬≤d) for both metrics</li>
                            <li><strong>Memory complexity:</strong> O(n¬≤) to store distance matrix</li>
                            <li><strong>Practical consideration:</strong> Manhattan distance matrix computation is 2-3x faster</li>
                        </ul>
                    </div>

                    <h3>üî¢ Numerical Stability and Precision</h3>
                    <p>Different distance metrics exhibit varying levels of numerical stability, particularly important when dealing with extreme values or high-precision requirements.</p>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üîµ Euclidean Distance Numerical Issues</h4>
                            <h5>‚ö†Ô∏è Common Problems:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Overflow:</strong> Squaring large values can exceed float limits</li>
                                <li><strong>Underflow:</strong> Very small differences can vanish</li>
                                <li><strong>Catastrophic cancellation:</strong> (a¬≤ + b¬≤) - (c¬≤ + d¬≤) when values are similar</li>
                                <li><strong>Square root precision:</strong> Loss of precision in final step</li>
                            </ul>
                            
                            <h5>‚úÖ Solutions:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Use squared distances when possible</li>
                                <li>Apply scaling to prevent overflow</li>
                                <li>Use double precision for critical applications</li>
                                <li>Implement numerically stable algorithms</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üî∑ Manhattan Distance Numerical Advantages</h4>
                            <h5>‚úÖ Stability Benefits:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>No squaring:</strong> Eliminates overflow risk from x¬≤</li>
                                <li><strong>No square root:</strong> Avoids precision loss</li>
                                <li><strong>Linear operations:</strong> Addition/subtraction only</li>
                                <li><strong>Bounded growth:</strong> Distance grows linearly with dimension</li>
                            </ul>
                            
                            <h5>‚ö†Ô∏è Potential Issues:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Absolute value discontinuity at zero</li>
                                <li>Less numerical precision in edge cases</li>
                                <li>Platform-dependent absolute value implementation</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üöÄ Optimization Techniques</h3>
                    <p>Modern computing architectures offer various optimization opportunities for distance calculations, from vectorization to GPU acceleration.</p>

                    <div class="formula-box">
                        <h4>‚ö° Vectorization and SIMD</h4>
                        
                        <h5>üîß CPU Optimization:</h5>
                        <ul>
                            <li><strong>SIMD Instructions:</strong> Process multiple elements simultaneously</li>
                            <li><strong>Memory Layout:</strong> Ensure contiguous storage for cache efficiency</li>
                            <li><strong>Loop Unrolling:</strong> Reduce loop overhead for small dimensions</li>
                            <li><strong>Compiler Optimizations:</strong> Enable auto-vectorization flags</li>
                        </ul>
                        
                        <h5>üéØ Implementation Example (Conceptual):</h5>
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<span style="color: #75715e;"># Euclidean distance with NumPy (vectorized)</span>
<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">euclidean_distance_optimized</span>(<span style="color: #f8f8f2;">X, Y</span>):
    <span style="color: #75715e;"># X, Y are n x d matrices</span>
    <span style="color: #f8f8f2;">diff</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">Y</span>  <span style="color: #75715e;"># Vectorized subtraction</span>
    <span style="color: #f8f8f2;">squared_diff</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">diff</span> <span style="color: #f92672;">**</span> <span style="color: #ae81ff;">2</span>  <span style="color: #75715e;"># Vectorized squaring</span>
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">np.sqrt(np.sum(squared_diff, axis</span><span style="color: #f92672;">=</span><span style="color: #ae81ff;">1</span><span style="color: #f8f8f2;">))</span>

<span style="color: #75715e;"># Manhattan distance with NumPy</span>
<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">manhattan_distance_optimized</span>(<span style="color: #f8f8f2;">X, Y</span>):
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">np.sum(np.abs(X</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">Y), axis</span><span style="color: #f92672;">=</span><span style="color: #ae81ff;">1</span><span style="color: #f8f8f2;">)</span>
                        </div>
                    </div>

                    <h3>üìä Memory Access Patterns</h3>
                    <p>Understanding memory hierarchy and access patterns is crucial for high-performance distance calculations.</p>

                    <div class="property-box">
                        <h4>