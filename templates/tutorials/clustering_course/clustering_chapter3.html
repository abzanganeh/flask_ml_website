<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Minkowski Distance and Generalized Formulas - ML Fundamentals</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/ml_fundamentals/chapter3.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <style>
        .visualization-placeholder {
            background: linear-gradient(45deg, #f0f0f0, #e0e0e0);
            border: 2px dashed #999;
            padding: 2rem;
            text-align: center;
            border-radius: 8px;
            margin: 1rem 0;
            font-style: italic;
            color: #666;
        }
        .interactive-demo {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }
        .formula-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .theorem-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .proof-box {
            background: #fff8e1;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .property-box {
            background: #e8f5e8;
            border-left: 4px solid #4caf50;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f5f5f5;
            font-weight: bold;
        }
        .demo-controls {
            background: #e8f5e8;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .p-value-demo {
            background: white;
            border: 1px solid #ddd;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .metric-visualization {
            border: 1px solid #ccc;
            background: white;
            padding: 1rem;
            margin: 1rem 0;
            min-height: 300px;
            position: relative;
        }
        .parameter-slider {
            width: 100%;
            margin: 1rem 0;
        }
        .convergence-analysis {
            background: #fafafa;
            border: 1px solid #e0e0e0;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav" style="top: 50px;">
            <div class="azbn-container" style="display: flex; justify-content: space-between; align-items: center;">
                <a href="/tutorials/ml-fundamentals" style="text-decoration: none; color: #4f46e5; display: flex; align-items: center; gap: 0.5rem;">
                    <img src="/static/images/logo.png" alt="Logo" style="height: 40px;">
                    <span>ML Fundamentals - Clustering Course</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main style="padding-top: 100px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <div class="navigation-buttons">
                    <a href="/tutorials/ml-fundamentals/clustering/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none;">‚Üê Chapter 2: Distance Metrics</a>
                    <a href="/tutorials/ml-fundamentals/clustering/chapter4" class="azbn-btn" style="text-decoration: none;">Chapter 4: Advanced Distance Metrics ‚Üí</a>
                </div>

                <h1>Chapter 3: Minkowski Distance and Generalized Mathematical Formulas</h1>
                <p style="font-size: 1.1rem; color: #666; margin-bottom: 2rem;">
                    Master the mathematical framework that unifies all Lp distance metrics through comprehensive analysis of Minkowski distance and its parameter space.
                </p>

                <div class="learning-objectives-card">
                    <h2>üéØ Learning Objectives</h2>
                    <ul>
                        <li>Understand the complete mathematical theory of Minkowski distance and Lp norms</li>
                        <li>Master the generalized distance formula and its parameter space</li>
                        <li>Analyze the behavior of distance metrics as p varies from 1 to infinity</li>
                        <li>Learn rigorous proofs of metric properties for the Minkowski family</li>
                        <li>Explore limit cases and their geometric interpretations</li>
                        <li>Apply parameter selection strategies for real-world clustering problems</li>
                        <li>Understand convergence behavior and mathematical continuity</li>
                        <li>Implement efficient algorithms for computing arbitrary Lp distances</li>
                    </ul>
                </div>

                <div class="section-nav">
                    <button class="active" onclick="showSection('introduction')">Minkowski Distance Theory</button>
                    <button onclick="showSection('mathematical')">Mathematical Framework</button>
                    <button onclick="showSection('parameter')">Parameter Analysis</button>
                    <button onclick="showSection('special-cases')">Special Cases</button>
                    <button onclick="showSection('geometric')">Geometric Properties</button>
                    <button onclick="showSection('convergence')">Convergence & Limits</button>
                    <button onclick="showSection('computational')">Computational Aspects</button>
                    <button onclick="showSection('interactive')">Interactive Explorer</button>
                    <button onclick="showSection('quiz')">Quiz</button>
                </div>

                <!-- Introduction Section -->
                <div id="introduction" class="content-section active">
                    <h2>Minkowski Distance: The Unifying Framework</h2>
                    
                    <p>Minkowski distance, named after German mathematician Hermann Minkowski, provides a unified mathematical framework that encompasses virtually all commonly used distance metrics in clustering and machine learning. This powerful generalization allows us to understand the entire spectrum of distance behavior through a single parameterized formula.</p>

                    <div class="formula-box">
                        <h3>üìê The Minkowski Distance Formula</h3>
                        <p>For two points x, y ‚àà ‚Ñù‚Åø, the Minkowski distance of order p (where p ‚â• 1) is defined as:</p>
                        
                        <div style="text-align: center; font-size: 1.4rem; margin: 1.5rem 0; background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <strong>d_p(x, y) = (Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢ - y·µ¢|·µñ)^(1/p)</strong>
                        </div>
                        
                        <p>This can also be expressed using vector notation:</p>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>d_p(x, y) = ‚Äñx - y‚Äñ_p</strong>
                        </div>
                        
                        <p>Where ‚Äñ¬∑‚Äñ_p denotes the Lp norm of a vector.</p>
                    </div>

                    <h3>üîç Understanding the Components</h3>
                    <p>Each element of the Minkowski distance formula carries specific mathematical significance:</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üìä The Parameter p</h4>
                            <p><strong>Domain:</strong> p ‚àà [1, ‚àû)</p>
                            <p><strong>Role:</strong> Controls the "shape" of distance measurement</p>
                            <p><strong>Effect:</strong> Higher p values emphasize larger differences</p>
                            <p><strong>Constraint:</strong> Must be ‚â• 1 for triangle inequality</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üìè Absolute Differences</h4>
                            <p><strong>Expression:</strong> |x·µ¢ - y·µ¢|</p>
                            <p><strong>Purpose:</strong> Ensures non-negativity</p>
                            <p><strong>Property:</strong> Distance is symmetric</p>
                            <p><strong>Interpretation:</strong> Component-wise difference magnitude</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>‚ö° Power Operation</h4>
                            <p><strong>Expression:</strong> |x·µ¢ - y·µ¢|·µñ</p>
                            <p><strong>Effect:</strong> Amplifies larger differences when p > 1</p>
                            <p><strong>Behavior:</strong> Linear when p = 1, quadratic when p = 2</p>
                            <p><strong>Limit:</strong> Approaches max operation as p ‚Üí ‚àû</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üìê Root Operation</h4>
                            <p><strong>Expression:</strong> (...)^(1/p)</p>
                            <p><strong>Purpose:</strong> Maintains proper scaling</p>
                            <p><strong>Property:</strong> Ensures homogeneity</p>
                            <p><strong>Effect:</strong> Balances the power amplification</p>
                        </div>
                    </div>

                    <h3>üéØ The Parameter Space Landscape</h3>
                    <p>The parameter p creates a continuous family of distance metrics, each with distinct geometric and analytical properties. Understanding this parameter space is crucial for selecting appropriate metrics for specific applications.</p>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Parameter p Effect on Distance Behavior</h4>
                        <p><strong>Image Description:</strong> A 3D surface plot showing how Minkowski distance varies with parameter p (x-axis, range 1-10) and component difference magnitude (y-axis, range 0-5). The z-axis shows the contribution of each component to total distance. Multiple colored curves show: p=1 (linear, yellow), p=2 (quadratic, blue), p=4 (quartic, red), p=‚àû (max function, green). The surface demonstrates how larger p values increasingly emphasize the largest differences.</p>
                        <p><em>This shows how parameter p controls the relative importance of large vs small component differences</em></p>
                    </div>

                    <h3>üî¨ Mathematical Properties Overview</h3>
                    <p>The Minkowski distance family inherits and extends the fundamental properties of metric spaces, with additional structure that varies smoothly with parameter p.</p>

                    <div class="theorem-box">
                        <h4>üìã Fundamental Theorem: Minkowski Distances Form a Metric Space</h4>
                        <p><strong>Theorem:</strong> For any p ‚â• 1, the function d_p(x, y) = (Œ£·µ¢ |x·µ¢ - y·µ¢|·µñ)^(1/p) defines a metric on ‚Ñù‚Åø.</p>
                        
                        <p><strong>Proof outline:</strong></p>
                        <ol>
                            <li><strong>Non-negativity:</strong> Follows from absolute values and positive powers</li>
                            <li><strong>Identity:</strong> d_p(x, y) = 0 ‚ü∫ |x·µ¢ - y·µ¢| = 0 ‚àÄi ‚ü∫ x = y</li>
                            <li><strong>Symmetry:</strong> |x·µ¢ - y·µ¢| = |y·µ¢ - x·µ¢| for all i</li>
                            <li><strong>Triangle inequality:</strong> Follows from H√∂lder's inequality (detailed proof in Mathematical Framework section)</li>
                        </ol>
                        
                        <p><strong>Significance:</strong> This theorem guarantees that all Lp distances are valid metrics, enabling consistent clustering algorithms across the entire parameter space.</p>
                    </div>

                    <h3>üåü Historical Context and Importance</h3>
                    <p>Hermann Minkowski introduced this distance concept in the early 20th century as part of his work on the geometry of numbers and later in developing the mathematical foundation for Einstein's special relativity. Today, Minkowski distances are fundamental to:</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>ü§ñ Machine Learning</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>K-nearest neighbors algorithms</li>
                                <li>Clustering optimization</li>
                                <li>Anomaly detection systems</li>
                                <li>Feature space analysis</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üìä Data Science</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Similarity measures</li>
                                <li>Dimensionality reduction</li>
                                <li>Recommender systems</li>
                                <li>Information retrieval</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4>üî¨ Scientific Computing</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Numerical optimization</li>
                                <li>Approximation theory</li>
                                <li>Signal processing</li>
                                <li>Image analysis</li>
                            </ul>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4>üéØ Operations Research</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Location optimization</li>
                                <li>Facility layout</li>
                                <li>Resource allocation</li>
                                <li>Network design</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üé™ Preview: The Journey Ahead</h3>
                    <p>This chapter will take you on a comprehensive mathematical journey through the Minkowski distance landscape. You'll discover:</p>

                    <div class="property-box">
                        <h4>üó∫Ô∏è Chapter Roadmap</h4>
                        <ul>
                            <li><strong>Mathematical Framework:</strong> Rigorous derivations and proofs of all metric properties</li>
                            <li><strong>Parameter Analysis:</strong> Deep dive into how p affects distance behavior and clustering results</li>
                            <li><strong>Special Cases:</strong> Detailed analysis of p = 1, 2, ‚àû and their unique properties</li>
                            <li><strong>Geometric Properties:</strong> Unit ball evolution and shape transformations across parameter space</li>
                            <li><strong>Convergence Theory:</strong> Limit behavior as p approaches boundary values</li>
                            <li><strong>Computational Methods:</strong> Efficient algorithms and numerical stability considerations</li>
                            <li><strong>Interactive Tools:</strong> Hands-on exploration of parameter effects on real data</li>
                        </ul>
                    </div>
                </div>

                <!-- Mathematical Framework Section -->
                <div id="mathematical" class="content-section">
                    <h2>Mathematical Framework and Rigorous Analysis</h2>
                    
                    <p>The mathematical foundation of Minkowski distance rests on deep results from functional analysis, particularly the theory of Banach spaces and H√∂lder's inequality. This section provides complete mathematical rigor for understanding why and how Minkowski distances work.</p>

                    <h3>üìê The Lp Norm Space Foundation</h3>
                    <p>Minkowski distances are intrinsically connected to Lp norm spaces, which form a fundamental structure in functional analysis.</p>

                    <div class="formula-box">
                        <h3>üìä Lp Norm Definition and Properties</h3>
                        <p>For a vector x = (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) ‚àà ‚Ñù‚Åø and parameter p ‚â• 1, the Lp norm is:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>‚Äñx‚Äñ_p = (Œ£·µ¢‚Çå‚ÇÅ‚Åø |x·µ¢|·µñ)^(1/p)</strong>
                        </div>
                        
                        <h4>üîß Fundamental Norm Properties:</h4>
                        <p>Any function ‚Äñ¬∑‚Äñ: ‚Ñù‚Åø ‚Üí ‚Ñù‚Çä is a norm if it satisfies:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <ol>
                                <li><strong>Positive Definiteness:</strong> ‚Äñx‚Äñ ‚â• 0, and ‚Äñx‚Äñ = 0 ‚ü∫ x = 0</li>
                                <li><strong>Homogeneity:</strong> ‚ÄñŒ±x‚Äñ = |Œ±| ‚Äñx‚Äñ for all scalars Œ±</li>
                                <li><strong>Triangle Inequality:</strong> ‚Äñx + y‚Äñ ‚â§ ‚Äñx‚Äñ + ‚Äñy‚Äñ</li>
                            </ol>
                        </div>
                        
                        <p><strong>Connection to Distance:</strong> The Minkowski distance is derived from the Lp norm via:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            <strong>d_p(x, y) = ‚Äñx - y‚Äñ_p</strong>
                        </div>
                    </div>

                    <h3>üéØ H√∂lder's Inequality: The Foundation of Triangle Inequality</h3>
                    <p>The triangle inequality for Minkowski distances relies on one of the most important inequalities in analysis: H√∂lder's inequality.</p>

                    <div class="theorem-box">
                        <h4>üìã H√∂lder's Inequality</h4>
                        <p><strong>Statement:</strong> For p, q > 1 with 1/p + 1/q = 1 (conjugate exponents), and vectors u, v ‚àà ‚Ñù‚Åø:</p>
                        
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>Œ£·µ¢‚Çå‚ÇÅ‚Åø |u·µ¢v·µ¢| ‚â§ ‚Äñu‚Äñ_p ‚Äñv‚Äñ_q</strong>
                        </div>
                        
                        <p><strong>Special Case (Cauchy-Schwarz):</strong> When p = q = 2:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            <strong>Œ£·µ¢‚Çå‚ÇÅ‚Åø |u·µ¢v·µ¢| ‚â§ ‚Äñu‚Äñ‚ÇÇ ‚Äñv‚Äñ‚ÇÇ</strong>
                        </div>
                    </div>

                    <div class="proof-box">
                        <h4>üìù Proof of Triangle Inequality for Minkowski Distance</h4>
                        <p><strong>Theorem:</strong> For p ‚â• 1, ‚Äñx + y‚Äñ_p ‚â§ ‚Äñx‚Äñ_p + ‚Äñy‚Äñ_p</p>
                        
                        <p><strong>Proof:</strong></p>
                        <p><strong>Case 1:</strong> p = 1 (trivial case)</p>
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 0.5rem 0;">
                            ‚Äñx + y‚Äñ‚ÇÅ = Œ£·µ¢ |x·µ¢ + y·µ¢| ‚â§ Œ£·µ¢ (|x·µ¢| + |y·µ¢|) = ‚Äñx‚Äñ‚ÇÅ + ‚Äñy‚Äñ‚ÇÅ
                        </div>
                        
                        <p><strong>Case 2:</strong> p > 1</p>
                        <p>We need to prove: (Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ)^(1/p) ‚â§ (Œ£·µ¢ |x·µ¢|·µñ)^(1/p) + (Œ£·µ¢ |y·µ¢|·µñ)^(1/p)</p>
                        
                        <p>Let q = p/(p-1) be the conjugate exponent (so 1/p + 1/q = 1). Note that (p-1)q = p.</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 0.5rem 0;">
                            <p>Start with: Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ = Œ£·µ¢ |x·µ¢ + y·µ¢| ¬∑ |x·µ¢ + y·µ¢|^(p-1)</p>
                            <p>‚â§ Œ£·µ¢ |x·µ¢| ¬∑ |x·µ¢ + y·µ¢|^(p-1) + Œ£·µ¢ |y·µ¢| ¬∑ |x·µ¢ + y·µ¢|^(p-1)</p>
                        </div>
                        
                        <p>Apply H√∂lder's inequality to each term:</p>
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 0.5rem 0;">
                            <p>Œ£·µ¢ |x·µ¢| ¬∑ |x·µ¢ + y·µ¢|^(p-1) ‚â§ (Œ£·µ¢ |x·µ¢|·µñ)^(1/p) (Œ£·µ¢ |x·µ¢ + y·µ¢|^((p-1)q))^(1/q)</p>
                            <p>= (Œ£·µ¢ |x·µ¢|·µñ)^(1/p) (Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ)^(1/q)</p>
                        </div>
                        
                        <p>Similarly for the second term. Combining and factoring:</p>
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 0.5rem 0;">
                            <p>Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ ‚â§ [(Œ£·µ¢ |x·µ¢|·µñ)^(1/p) + (Œ£·µ¢ |y·µ¢|·µñ)^(1/p)] (Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ)^(1/q)</p>
                        </div>
                        
                        <p>Divide both sides by (Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ)^(1/q) to get:</p>
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 0.5rem 0;">
                            <p>(Œ£·µ¢ |x·µ¢ + y·µ¢|·µñ)^(1-1/q) ‚â§ (Œ£·µ¢ |x·µ¢|·µñ)^(1/p) + (Œ£·µ¢ |y·µ¢|·µñ)^(1/p)</p>
                        </div>
                        
                        <p>Since 1 - 1/q = 1/p, we have proven the triangle inequality. ‚àé</p>
                    </div>

                    <h3>‚öñÔ∏è Norm Equivalence and Relationships</h3>
                    <p>Understanding relationships between different Lp norms is crucial for comparing Minkowski distances and understanding their relative behavior.</p>

                    <div class="formula-box">
                        <h4>üîó Fundamental Norm Inequalities</h4>
                        <p>For any vector x ‚àà ‚Ñù‚Åø and 1 ‚â§ p ‚â§ q ‚â§ ‚àû:</p>
                        
                        <div style="text-align: center; font-size: 1.1rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>‚Äñx‚Äñ_q ‚â§ ‚Äñx‚Äñ_p ‚â§ n^(1/p - 1/q) ‚Äñx‚Äñ_q</strong>
                        </div>
                        
                        <h5>üìä Specific Important Cases:</h5>
                        <ul>
                            <li><strong>L‚àû to L‚ÇÇ:</strong> ‚Äñx‚Äñ‚ÇÇ ‚â§ ‚àön ‚Äñx‚Äñ_‚àû</li>
                            <li><strong>L‚ÇÇ to L‚ÇÅ:</strong> ‚Äñx‚Äñ‚ÇÅ ‚â§ ‚àön ‚Äñx‚Äñ‚ÇÇ</li>
                            <li><strong>L‚àû to L‚ÇÅ:</strong> ‚Äñx‚Äñ‚ÇÅ ‚â§ n ‚Äñx‚Äñ_‚àû</li>
                            <li><strong>General bound:</strong> ‚Äñx‚Äñ_‚àû ‚â§ ‚Äñx‚Äñ_p ‚â§ n^(1/p) ‚Äñx‚Äñ_‚àû</li>
                        </ul>
                        
                        <h5>üéØ Tightness of Bounds:</h5>
                        <p>These bounds are tight and achieved by specific vectors:</p>
                        <ul>
                            <li><strong>Lower bounds:</strong> Achieved by vectors with one large component</li>
                            <li><strong>Upper bounds:</strong> Achieved by vectors with all components equal</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Norm Relationships in Vector Space</h4>
                        <p><strong>Image Description:</strong> A 3D plot showing the relationship between different Lp norms for 2D vectors. The plot shows level curves (contours) of constant norm value for p = 1, 1.5, 2, 3, ‚àû in different colors. The x and y axes represent vector components, and the z-axis shows norm magnitude. The evolution from diamond (L‚ÇÅ) through circle (L‚ÇÇ) to square (L‚àû) is clearly visible, with intermediate p values showing smooth transitions.</p>
                        <p><em>This demonstrates how norm relationships create nested geometric structures</em></p>
                    </div>

                    <h3>üìè Continuity and Differentiability Properties</h3>
                    <p>The smoothness properties of Minkowski distances have important implications for optimization algorithms and gradient-based methods.</p>

                    <div class="theorem-box">
                        <h4>üåä Smoothness Analysis</h4>
                        
                        <h5>üìä Continuity Properties:</h5>
                        <ul>
                            <li><strong>Continuity in space:</strong> d_p(x, y) is continuous in both x and y for fixed p</li>
                            <li><strong>Continuity in parameter:</strong> d_p(x, y) is continuous in p for fixed x ‚â† y</li>
                            <li><strong>Uniform continuity:</strong> On bounded sets, continuity is uniform</li>
                        </ul>
                        
                        <h5>üìê Differentiability Analysis:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>For p > 1:</strong> ‚Äñx‚Äñ_p is differentiable everywhere except at x = 0</p>
                            <p><strong>Gradient formula:</strong> ‚àá‚Äñx‚Äñ_p = (‚Äñx‚Äñ_p)^(1-p) ¬∑ (|x‚ÇÅ|^(p-1)sign(x‚ÇÅ), ..., |x‚Çô|^(p-1)sign(x‚Çô))</p>
                            <p><strong>For p = 1:</strong> ‚Äñx‚Äñ‚ÇÅ is not differentiable when any x·µ¢ = 0</p>
                            <p><strong>Subgradient:</strong> When p = 1, subgradients exist everywhere</p>
                        </div>
                        
                        <h5>‚ö° Implications for Optimization:</h5>
                        <ul>
                            <li><strong>Gradient descent:</strong> Works well for p > 1</li>
                            <li><strong>Subgradient methods:</strong> Required for p = 1</li>
                            <li><strong>Second-order methods:</strong> Hessian exists for p > 1 away from zero</li>
                        </ul>
                    </div>

                    <h3>üîÑ Dual Norms and Conjugate Functions</h3>
                    <p>The theory of dual norms provides deep insights into the structure of Lp spaces and their optimization properties.</p>

                    <div class="formula-box">
                        <h4>üéØ Dual Norm Theory</h4>
                        <p>For p ‚â• 1, the dual norm of ‚Äñ¬∑‚Äñ_p is ‚Äñ¬∑‚Äñ_q where q satisfies:</p>
                        
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>1/p + 1/q = 1</strong>
                        </div>
                        
                        <p>The dual norm is defined by:</p>
                        <div style="text-align: center; margin: 0.5rem 0;">
                            <strong>‚Äñy‚Äñ* = max{‚ü®x, y‚ü© : ‚Äñx‚Äñ_p ‚â§ 1}</strong>
                        </div>
                        
                        <h5>üìä Important Dual Relationships:</h5>
                        <table class="comparison-table" style="margin: 1rem 0;">
                            <thead>
                                <tr>
                                    <th>Primal Norm</th>
                                    <th>p Value</th>
                                    <th>Dual Norm</th>
                                    <th>q Value</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>L‚ÇÅ</td>
                                    <td>1</td>
                                    <td>L‚àû</td>
                                    <td>‚àû</td>
                                </tr>
                                <tr>
                                    <td>L‚ÇÇ</td>
                                    <td>2</td>
                                    <td>L‚ÇÇ</td>
                                    <td>2</td>
                                </tr>
                                <tr>
                                    <td>L‚ÇÑ</td>
                                    <td>4</td>
                                    <td>L‚ÇÑ/‚ÇÉ</td>
                                    <td>4/3</td>
                                </tr>
                                <tr>
                                    <td>L‚àû</td>
                                    <td>‚àû</td>
                                    <td>L‚ÇÅ</td>
                                    <td>1</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <h5>üéØ Optimization Significance:</h5>
                        <p>Dual norms appear naturally in:</p>
                        <ul>
                            <li><strong>Lagrange multiplier methods</strong></li>
                            <li><strong>Convex optimization theory</strong></li>
                            <li><strong>Regularization in machine learning</strong></li>
                            <li><strong>Sensitivity analysis</strong></li>
                        </ul>
                    </div>

                    <h3>üßÆ Advanced Mathematical Properties</h3>
                    <p>Several advanced properties of Minkowski distances have important theoretical and practical implications.</p>

                    <div class="property-box">
                        <h4>üî¨ Advanced Theorems</h4>
                        
                        <h5>üìê Clarkson's Inequalities:</h5>
                        <p>For p ‚â• 2 and vectors x, y ‚àà ‚Ñù‚Åø:</p>
                        <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                            ‚Äñx + y‚Äñ_p^p + ‚Äñx - y‚Äñ_p^p ‚â§ 2^(p-1)(‚Äñx‚Äñ_p^p + ‚Äñy‚Äñ_p^p)
                        </div>
                        
                        <h5>üéØ Uniform Convexity:</h5>
                        <p>Lp spaces are uniformly convex for 1 < p < ‚àû, which implies:</p>
                        <ul>
                            <li>Strong convergence of minimizing sequences</li>
                            <li>Uniqueness of best approximations</li>
                            <li>Stability of optimization algorithms</li>
                        </ul>
                        
                        <h5>üìä Banach Space Properties:</h5>
                        <p>The space (‚Ñù‚Åø, ‚Äñ¬∑‚Äñ_p) forms a Banach space for all p ‚â• 1:</p>
                        <ul>
                            <li><strong>Completeness:</strong> All Cauchy sequences converge</li>
                            <li><strong>Finite dimensionality:</strong> All norms are equivalent</li>
                            <li><strong>Reflexivity:</strong> For 1 < p < ‚àû, the space is reflexive</li>
                        </ul>
                        
                        <h5>üåê Interpolation Properties:</h5>
                        <p>Riesz-Thorin interpolation theorem connects different Lp norms:</p>
                        <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                            If 1/r = (1-Œ∏)/p + Œ∏/q for 0 ‚â§ Œ∏ ‚â§ 1, then ‚Äñx‚Äñ_r ‚â§ ‚Äñx‚Äñ_p^(1-Œ∏) ‚Äñx‚Äñ_q^Œ∏
                        </div>
                    </div>
                </div>

                <!-- Parameter Analysis Section -->
                <div id="parameter" class="content-section">
                    <h2>Parameter Analysis: The Behavior of p</h2>
                    
                    <p>The parameter p in Minkowski distance fundamentally controls the geometric and analytical behavior of the metric. Understanding how variations in p affect distance calculations, clustering results, and geometric interpretations is crucial for practical applications.</p>

                    <h3>üéõÔ∏è Parameter Space Exploration</h3>
                    <p>The parameter p lives in the interval [1, ‚àû), creating a one-parameter family of metrics with rich and varied behavior across this range.</p>

                    <div class="interactive-demo">
                        <h3>üîß Interactive Parameter Explorer</h3>
                        <p>Explore how parameter p affects distance calculations between two points in real-time.</p>
                        
                        <div class="p-value-demo">
                            <h4>üìä Parameter Control</h4>
                            <div style="margin: 1rem 0;">
                                <label for="p-value">Parameter p: </label>
                                <input type="range" id="p-value" class="parameter-slider" min="1" max="10" step="0.1" value="2">
                                <span id="p-display" style="font-weight: bold; margin-left: 1rem;">2.0</span>
                            </div>
                            
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 1rem 0;">
                                <div>
                                    <h5>Point A Coordinates:</h5>
                                    <input type="number" id="point-a-x" value="1" step="0.1" style="width: 80px; margin: 0.2rem;"> 
                                    <input type="number" id="point-a-y" value="2" step="0.1" style="width: 80px; margin: 0.2rem;">
                                    <input type="number" id="point-a-z" value="3" step="0.1" style="width: 80px; margin: 0.2rem;">
                                </div>
                                <div>
                                    <h5>Point B Coordinates:</h5>
                                    <input type="number" id="point-b-x" value="4" step="0.1" style="width: 80px; margin: 0.2rem;">
                                    <input type="number" id="point-b-y" value="1" step="0.1" style="width: 80px; margin: 0.2rem;">
                                    <input type="number" id="point-b-z" value="2" step="0.1" style="width: 80px; margin: 0.2rem;">
                                </div>
                            </div>
                            
                            <button onclick="updateParameterAnalysis()" class="azbn-btn">Calculate Distance</button>
                            
                            <div id="parameter-results" style="margin-top: 1rem;">
                                <h4>üìà Results</h4>
                                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                                    <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px;">
                                        <h5>Current Distance</h5>
                                        <div id="current-distance" style="font-size: 1.4rem; font-weight: bold; color: #2196f3;"></div>
                                    </div>
                                    <div style="background: #e8f5e8; padding: 1rem; border-radius: 6px;">
                                        <h5>Component Contributions</h5>
                                        <div id="component-breakdown" style="font-size: 0.9rem;"></div>
                                    </div>
                                    <div style="background: #fff8e1; padding: 1rem; border-radius: 6px;">
                                        <h5>Special Cases</h5>
                                        <div id="special-cases" style="font-size: 0.9rem;"></div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <h3>üìä Mathematical Analysis of Parameter Effects</h3>
                    <p>The parameter p influences several fundamental aspects of distance behavior. Let's analyze these effects systematically.</p>

                    <div class="formula-box">
                        <h4>‚ö° Sensitivity to Large Differences</h4>
                        <p>As p increases, the Minkowski distance becomes increasingly sensitive to the largest component differences. This can be quantified mathematically:</p>
                        
                        <h5>üìê Dominance Analysis:</h5>
                        <p>For vector differences v = x - y, let M = max_i |v·µ¢| be the largest component difference:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Lower bound:</strong> ‚Äñv‚Äñ_p ‚â• M</p>
                            <p><strong>Upper bound:</strong> ‚Äñv‚Äñ_p ‚â§ n^(1/p) M</p>
                            <p><strong>Limit behavior:</strong> lim_{p‚Üí‚àû} ‚Äñv‚Äñ_p = M</p>
                        </div>
                        
                        <h5>üéØ Practical Implications:</h5>
                        <ul>
                            <li><strong>p = 1:</strong> All differences contribute equally</li>
                            <li><strong>p = 2:</strong> Balanced contribution with some emphasis on larger differences</li>
                            <li><strong>p > 2:</strong> Increasingly dominated by largest differences</li>
                            <li><strong>p ‚Üí ‚àû:</strong> Only the largest difference matters</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üìà Visualization: Parameter p Effect on Distance Components</h4>
                        <p><strong>Image Description:</strong> A multi-panel plot showing how different component differences contribute to total distance as p varies. Three subplots: (1) Component differences as bar chart (|1-4|=3, |2-1|=1, |3-2|=1), (2) Weighted contributions (3^p, 1^p, 1^p) for p=1,2,4, shown as stacked bars, (3) Final distance calculation (3^p + 1^p + 1^p)^(1/p) as line graph vs p. Demonstrates how larger differences dominate as p increases.</p>
                        <p><em>This shows how parameter p controls the relative importance of different component differences</em></p>
                    </div>

                    <h3>üé® Geometric Evolution with Parameter p</h3>
                    <p>The geometric shape of "equidistant" regions evolves dramatically as p changes, affecting clustering boundaries and algorithm behavior.</p>

                    <div class="property-box">
                        <h4>üìê Unit Ball Evolution</h4>
                        <p>The unit ball B_p = {x: ‚Äñx‚Äñ_p ‚â§ 1} changes shape as p varies:</p>
                        
                        <h5>üîÑ Shape Progression:</h5>
                        <ul>
                            <li><strong>p = 1:</strong> Diamond/octahedron (sharp corners at axes)</li>
                            <li><strong>1 < p < 2:</strong> Rounded diamond (smooth curves, still "pointy")</li>
                            <li><strong>p = 2:</strong> Perfect circle/sphere (maximum "roundness")</li>
                            <li><strong>2 < p < ‚àû:</strong> Rounded square (increasingly flat sides)</li>
                            <li><strong>p ‚Üí ‚àû:</strong> Square/cube (flat sides, sharp corners)</li>
                        </ul>
                        
                        <h5>üìä Mathematical Characterization:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Curvature:</strong> Decreases as p moves away from 2</p>
                            <p><strong>Volume:</strong> V_p(n) has complex dependence on both p and dimension n</p>
                            <p><strong>Surface area:</strong> Concentrates at corners for large p</p>
                            <p><strong>Diameter:</strong> Increases monotonically with p</p>
                        </div>
                        
                        <h5>üéØ Clustering Implications:</h5>
                        <ul>
                            <li><strong>Low p:</strong> Creates angular, axis-aligned cluster boundaries</li>
                            <li><strong>p ‚âà 2:</strong> Produces circular/spherical clusters</li>
                            <li><strong>High p:</strong> Emphasizes outlier detection along dominant features</li>
                        </ul>
                    </div>

                    <h3>‚öñÔ∏è Statistical Properties Across Parameter Space</h3>
                    <p>The statistical behavior of Minkowski distances varies significantly with parameter p, affecting robustness, efficiency, and convergence properties.</p>

                    <div class="theorem-box">
                        <h4>üìä Statistical Analysis Framework</h4>
                        
                        <h5>üé≤ Random Vector Analysis:</h5>
                        <p>For random vectors X, Y with i.i.d. components from distribution F:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Expected distance:</strong> E[d_p(X, Y)] depends on p and F</p>
                            <p><strong>Variance:</strong> Var[d_p(X, Y)] generally decreases as p increases</p>
                            <p><strong>Concentration:</strong> Distance concentration varies with p</p>
                        </div>
                        
                        <h5>üìà High-Dimensional Behavior:</h5>
                        <p>In high dimensions (large n), parameter p significantly affects concentration of measure:</p>
                        
                        <table class="comparison-table" style="margin: 1rem 0;">
                            <thead>
                                <tr>
                                    <th>Parameter Range</th>
                                    <th>Concentration Rate</th>
                                    <th>Practical Impact</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>p = 1</td>
                                    <td>Slowest concentration</td>
                                    <td>Best discrimination in high dimensions</td>
                                </tr>
                                <tr>
                                    <td>1 < p < 2</td>
                                    <td>Moderate concentration</td>
                                    <td>Good balance of properties</td>
                                </tr>
                                <tr>
                                    <td>p = 2</td>
                                    <td>Standard concentration</td>
                                    <td>Classical behavior, well-studied</td>
                                </tr>
                                <tr>
                                    <td>p > 2</td>
                                    <td>Faster concentration</td>
                                    <td>May lose discrimination earlier</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <h5>üîç Robustness Properties:</h5>
                        <ul>
                            <li><strong>Outlier sensitivity:</strong> Increases with p (due to higher powers)</li>
                            <li><strong>Breakdown point:</strong> Decreases as p increases</li>
                            <li><strong>Influence function:</strong> Unbounded for p > 1</li>
                        </ul>
                    </div>

                    <h3>üéØ Parameter Selection Strategies</h3>
                    <p>Choosing the optimal parameter p requires balancing multiple considerations based on data characteristics and application requirements.</p>

                    <div class="formula-box">
                        <h4>üîß Selection Criteria Framework</h4>
                        
                        <h5>üìä Data-Driven Selection:</h5>
                        <ul>
                            <li><strong>Cross-validation:</strong> Test different p values using clustering quality metrics</li>
                            <li><strong>Silhouette analysis:</strong> Optimize silhouette score across p values</li>
                            <li><strong>Gap statistic:</strong> Use within/between cluster variance ratios</li>
                            <li><strong>Stability analysis:</strong> Choose p that gives most stable clusterings</li>
                        </ul>
                        
                        <h5>üéØ Application-Specific Guidelines:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>p = 1 (Manhattan):</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Count data and discrete features</li>
                                <li>High-dimensional sparse data</li>
                                <li>Outlier-robust applications</li>
                                <li>Independent feature assumptions</li>
                            </ul>
                            
                            <p><strong>p = 2 (Euclidean):</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Continuous numerical data</li>
                                <li>Geometric/spatial problems</li>
                                <li>Gaussian distributed features</li>
                                <li>When optimization simplicity matters</li>
                            </ul>
                            
                            <p><strong>1 < p < 2:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Balanced robustness and smoothness</li>
                                <li>Mixed data types</li>
                                <li>Moderate outlier presence</li>
                                <li>High-dimensional problems needing some robustness</li>
                            </ul>
                            
                            <p><strong>p > 2:</strong></p>
                            <ul style="font-size: 0.9rem;">
                                <li>Emphasis on dominant features</li>
                                <li>Outlier detection applications</li>
                                <li>When largest differences are most important</li>
                                <li>Specialized optimization landscapes</li>
                            </ul>
                        </div>
                        
                        <h5>‚ö° Computational Considerations:</h5>
                        <ul>
                            <li><strong>Integer p values:</strong> Often computationally simpler</li>
                            <li><strong>p = 1:</strong> No power operations needed</li>
                            <li><strong>p = 2:</strong> Benefits from optimized linear algebra libraries</li>
                            <li><strong>Large p:</strong> May require careful numerical handling</li>
                        </ul>
                    </div>

                    <div class="convergence-analysis">
                        <h4>üìà Parameter Sensitivity Analysis</h4>
                        <p>Understanding how sensitive your results are to parameter choice helps validate selection decisions.</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <h5>üîç Sensitivity Metrics:</h5>
                            <ul>
                                <li><strong>Distance correlation:</strong> How correlated are distances for different p values?</li>
                                <li><strong>Ranking stability:</strong> Do nearest neighbors change significantly?</li>
                                <li><strong>Clustering stability:</strong> How much do cluster assignments change?</li>
                                <li><strong>Performance sensitivity:</strong> How does clustering quality vary with p?</li>
                            </ul>
                            
                            <h5>üìä Analysis Protocol:</h5>
                            <ol>
                                <li>Compute distances for range of p values (e.g., 1.0, 1.5, 2.0, 2.5, 3.0)</li>
                                <li>Calculate pairwise correlations between distance matrices</li>
                                <li>Perform clustering with each p value</li>
                                <li>Measure clustering agreement (adjusted rand index)</li>
                                <li>Plot performance metrics vs p to identify stable regions</li>
                            </ol>
                        </div>
                    </div>
                </div>

                <!-- Special Cases Section -->
                <div id="special-cases" class="content-section">
                    <h2>Special Cases and Landmark Values</h2>
                    
                    <p>Certain values of parameter p create distance metrics with special mathematical properties and practical significance. These landmark cases serve as reference points for understanding the broader parameter space and often have unique computational or theoretical advantages.</p>

                    <h3>ü•á The Trinity: p = 1, 2, ‚àû</h3>
                    <p>Three values of p stand out as the most important and widely used in practice, each representing a fundamentally different approach to measuring distance.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #fff8e1; border: 2px solid #ff9800; padding: 1rem; border-radius: 8px;">
                            <h4>üî∑ L‚ÇÅ Norm (Manhattan Distance)</h4>
                            <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                                <strong>‚Äñx‚Äñ‚ÇÅ = Œ£·µ¢ |x·µ¢|</strong>
                            </div>
                            <h5>üéØ Special Properties:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Promotes sparsity in optimization</li>
                                <li>Non-differentiable at origin</li>
                                <li>Most robust to outliers</li>
                                <li>Creates diamond-shaped unit balls</li>
                                <li>Computationally simplest</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e3f2fd; border: 2px solid #2196f3; padding: 1rem; border-radius: 8px;">
                            <h4>üîµ L‚ÇÇ Norm (Euclidean Distance)</h4>
                            <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                                <strong>‚Äñx‚Äñ‚ÇÇ = ‚àö(Œ£·µ¢ x·µ¢¬≤)</strong>
                            </div>
                            <h5>üéØ Special Properties:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Self-dual (own dual norm)</li>
                                <li>Smooth and differentiable</li>
                                <li>Rotationally invariant</li>
                                <li>Creates spherical unit balls</li>
                                <li>Rich geometric theory</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e8f5e8; border: 2px solid #4caf50; padding: 1rem; border-radius: 8px;">
                            <h4>‚¨ú L‚àû Norm (Chebyshev Distance)</h4>
                            <div style="text-align: center; margin: 1rem 0; background: white; padding: 0.8rem; border-radius: 6px;">
                                <strong>‚Äñx‚Äñ‚àû = max_i |x·µ¢|</strong>
                            </div>
                            <h5>üéØ Special Properties:</h5>
                            <ul style="font-size: 0.9rem;">
                                <li>Focus on worst-case scenario</li>
                                <li>Simple to compute</li>
                                <li>Creates hypercube unit balls</li>
                                <li>Natural for minimax problems</li>
                                <li>Limit of Lp as p ‚Üí ‚àû</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üìê Detailed Analysis of p = 1 (Manhattan Distance)</h3>
                    <p>The L‚ÇÅ norm occupies a special place in optimization theory and robust statistics due to its unique mathematical properties.</p>

                    <div class="theorem-box">
                        <h4>üîç Deep Dive: L‚ÇÅ Properties</h4>
                        
                        <h5>üìä Mathematical Characteristics:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Convexity:</strong> Convex but not strictly convex</p>
                            <p><strong>Smoothness:</strong> Not differentiable when any component equals zero</p>
                            <p><strong>Subgradient:</strong> ‚àÇ‚Äñx‚Äñ‚ÇÅ = {g ‚àà ‚Ñù‚Åø : g·µ¢ ‚àà sign(x·µ¢) if x·µ¢ ‚â† 0, g·µ¢ ‚àà [-1,1] if x·µ¢ = 0}</p>
                            <p><strong>Dual norm:</strong> ‚Äñy‚Äñ‚àû = max_i |y·µ¢|</p>
                        </div>
                        
                        <h5>üéØ Sparsity-Inducing Properties:</h5>
                        <p>The L‚ÇÅ norm is famous for promoting sparsity in optimization problems:</p>
                        <ul>
                            <li><strong>LASSO regression:</strong> min_Œ≤ ‚Äñy - XŒ≤‚Äñ‚ÇÇ¬≤ + Œª‚ÄñŒ≤‚Äñ‚ÇÅ</li>
                            <li><strong>Compressed sensing:</strong> L‚ÇÅ minimization recovers sparse signals</li>
                            <li><strong>Feature selection:</strong> L‚ÇÅ penalty drives coefficients to exactly zero</li>
                        </ul>
                        
                        <h5>üèóÔ∏è Geometric Properties:</h5>
                        <ul>
                            <li><strong>Unit ball:</strong> Cross-polytope (diamond in 2D, octahedron in 3D)</li>
                            <li><strong>Vertices:</strong> Located at ¬±e·µ¢ (standard basis vectors)</li>
                            <li><strong>Facets:</strong> Defined by hyperplanes Œ£·µ¢ Œµ·µ¢x·µ¢ = 1 where Œµ·µ¢ ‚àà {¬±1}</li>
                            <li><strong>Volume:</strong> V‚ÇÅ(n) = 2‚Åø/n!</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üî∑ Visualization: L‚ÇÅ Unit Ball and Optimization Landscape</h4>
                        <p><strong>Image Description:</strong> Two side-by-side 3D plots. Left: L‚ÇÅ unit ball in 3D shown as a regular octahedron with vertices at (¬±1,0,0), (0,¬±1,0), (0,0,¬±1). Facets are triangular, creating sharp edges and corners. Right: Optimization landscape showing a quadratic function (smooth bowl) intersected with L‚ÇÅ constraint (octahedron). The intersection points show how L‚ÇÅ constraints force solutions to sparse vertices.</p>
                        <p><em>This illustrates why L‚ÇÅ constraints promote sparsity in optimization</em></p>
                    </div>

                    <h3>üåÄ Deep Analysis of p = 2 (Euclidean Distance)</h3>
                    <p>The L‚ÇÇ norm is distinguished by its rich geometric structure and self-duality, making it the most studied and applied norm in mathematics.</p>

                    <div class="formula-box">
                        <h4>‚≠ï Euclidean Norm: The Geometric Standard</h4>
                        
                        <h5>üìä Unique Mathematical Properties:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Self-dual:</strong> The dual of L‚ÇÇ is L‚ÇÇ itself</p>
                            <p><strong>Inner product norm:</strong> ‚Äñx‚Äñ‚ÇÇ = ‚àö‚ü®x, x‚ü©</p>
                            <p><strong>Parallelogram law:</strong> ‚Äñx + y‚Äñ‚ÇÇ¬≤ + ‚Äñx - y‚Äñ‚ÇÇ¬≤ = 2(‚Äñx‚Äñ‚ÇÇ¬≤ + ‚Äñy‚Äñ‚ÇÇ¬≤)</p>
                            <p><strong>Cauchy-Schwarz:</strong> |‚ü®x, y‚ü©| ‚â§ ‚Äñx‚Äñ‚ÇÇ‚Äñy‚Äñ‚ÇÇ</p>
                        </div>
                        
                        <h5>üéØ Hilbert Space Structure:</h5>
                        <p>The L‚ÇÇ norm endows ‚Ñù‚Åø with Hilbert space structure, enabling:</p>
                        <ul>
                            <li><strong>Orthogonality:</strong> Well-defined perpendicular vectors</li>
                            <li><strong>Projections:</strong> Unique closest point projections</li>
                            <li><strong>Fourier analysis:</strong> Orthogonal decompositions</li>
                            <li><strong>Spectral theory:</strong> Eigenvalue/eigenvector analysis</li>
                        </ul>
                        
                        <h5>üîÑ Rotation Invariance:</h5>
                        <p>The L‚ÇÇ norm is preserved under orthogonal transformations:</p>
                        <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                            <strong>‚ÄñQx‚Äñ‚ÇÇ = ‚Äñx‚Äñ‚ÇÇ for any orthogonal matrix Q</strong>
                        </div>
                        <p>This property makes L‚ÇÇ the natural choice for problems with rotational symmetry.</p>
                        
                        <h5>üìê Geometric Optimization:</h5>
                        <ul>
                            <li><strong>Gradient descent:</strong> Natural steepest descent direction</li>
                            <li><strong>Newton's method:</strong> Quadratic convergence properties</li>
                            <li><strong>Least squares:</strong> Closed-form solutions via linear algebra</li>
                            <li><strong>Principal components:</strong> L‚ÇÇ optimization leads to PCA</li>
                        </ul>
                    </div>

                    <h3>‚¨ú Analysis of p = ‚àû (Chebyshev Distance)</h3>
                    <p>The L‚àû norm represents the limit of Lp norms as p approaches infinity, focusing exclusively on the largest component difference.</p>

                    <div class="proof-box">
                        <h4>üîç L‚àû as Limit of Lp Norms</h4>
                        <p><strong>Theorem:</strong> For any vector x ‚àà ‚Ñù‚Åø, lim_{p‚Üí‚àû} ‚Äñx‚Äñ_p = ‚Äñx‚Äñ‚àû</p>
                        
                        <p><strong>Proof:</strong></p>
                        <p>Let M = ‚Äñx‚Äñ‚àû = max_i |x·µ¢| and assume M > 0. We can write:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <p>‚Äñx‚Äñ_p = (Œ£·µ¢ |x·µ¢|·µñ)^(1/p) = M(Œ£·µ¢ (|x·µ¢|/M)·µñ)^(1/p)</p>
                        </div>
                        
                        <p>Since |x·µ¢|/M ‚â§ 1 for all i, and there exists at least one index j with |x‚±º|/M = 1:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <p>1 ‚â§ (Œ£·µ¢ (|x·µ¢|/M)·µñ)^(1/p) ‚â§ (n ¬∑ 1·µñ)^(1/p) = n^(1/p)</p>
                        </div>
                        
                        <p>As p ‚Üí ‚àû, n^(1/p) ‚Üí 1. Also, the sum is dominated by the terms where |x·µ¢|/M = 1, so:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <p>lim_{p‚Üí‚àû} (Œ£·µ¢ (|x·µ¢|/M)·µñ)^(1/p) = 1</p>
                        </div>
                        
                        <p>Therefore: lim_{p‚Üí‚àû} ‚Äñx‚Äñ_p = M ¬∑ 1 = ‚Äñx‚Äñ‚àû ‚àé</p>
                    </div>

                    <div class="property-box">
                        <h4>üéØ L‚àû Norm Applications</h4>
                        
                        <h5>üìä Minimax Optimization:</h5>
                        <p>L‚àû naturally appears in minimax problems:</p>
                        <ul>
                            <li><strong>Chebyshev approximation:</strong> min_f max_x |f(x) - g(x)|</li>
                            <li><strong>Game theory:</strong> Mixed strategy equilibria</li>
                            <li><strong>Robust optimization:</strong> Worst-case scenario planning</li>
                            <li><strong>Linear programming:</strong> Dual formulations</li>
                        </ul>
                        
                        <h5>üéÆ Discrete Applications:</h5>
                        <ul>
                            <li><strong>Chess distance:</strong> King moves on a chessboard</li>
                            <li><strong>Image processing:</strong> Pixel-wise maximum differences</li>
                            <li><strong>Network analysis:</strong> Bottleneck identification</li>
                            <li><strong>Quality control:</strong> Worst-case defect measurement</li>
                        </ul>
                        
                        <h5>‚ö° Computational Advantages:</h5>
                        <ul>
                            <li><strong>Simple calculation:</strong> Just find maximum absolute difference</li>
                            <li><strong>No powers or roots:</strong> Avoids numerical precision issues</li>
                            <li><strong>Sparse-friendly:</strong> Only non-zero components matter for maximum</li>
                            <li><strong>Parallel computation:</strong> Easily parallelizable</li>
                        </ul>
                    </div>

                    <h3>üîç Intermediate Values: Fractional and Non-Integer p</h3>
                    <p>While integer values of p are most common, fractional and non-integer values can provide optimal solutions for specific applications.</p>

                    <div class="theorem-box">
                        <h4>üåä Smooth Interpolation Between Landmarks</h4>
                        
                        <h5>üìä p = 1.5 (Compromise Between L‚ÇÅ and L‚ÇÇ):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Formula:</strong> ‚Äñx‚Äñ‚ÇÅ.‚ÇÖ = (Œ£·µ¢ |x·µ¢|^1.5)^(2/3)</p>
                            <p><strong>Properties:</strong> Moderately robust, somewhat smooth</p>
                            <p><strong>Applications:</strong> Robust regression, signal processing</p>
                        </div>
                        
                        <h5>üìä p = 0.5 (Not a norm, but useful):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Formula:</strong> (Œ£·µ¢ |x·µ¢|^0.5)¬≤ (violates triangle inequality)</p>
                            <p><strong>Note:</strong> Not a valid metric, but appears in some applications</p>
                            <p><strong>Use:</strong> Sparse signal recovery, specialized optimization</p>
                        </div>
                        
                        <h5>üìê Optimization-Based Selection:</h5>
                        <p>The optimal p can be determined by solving:</p>
                        <div style="background: white; padding: 0.8rem; border-radius: 4px; margin: 0.5rem 0;">
                            <strong>p* = argmin_p J(p)</strong>
                        </div>
                        <p>Where J(p) is a problem-specific objective function (e.g., clustering quality, prediction error).</p>
                    </div>

                    <h3>üé® Computational Considerations for Special Cases</h3>
                    <p>Different values of p require different computational strategies for efficient and numerically stable implementation.</p>

                    <div class="formula-box">
                        <h4>üíª Implementation Strategies</h4>
                        
                        <h5>‚ö° p = 1 (Manhattan):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <strong>Pseudocode:</strong>
                            <pre style="margin: 0.5rem 0;">
distance = 0
for i in range(n):
    distance += abs(x[i] - y[i])
return distance
                            </pre>
                            <p><strong>Optimizations:</strong> Vectorize with SIMD, early termination for thresholds</p>
                        </div>
                        
                        <h5>‚ö° p = 2 (Euclidean):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <strong>Pseudocode:</strong>
                            <pre style="margin: 0.5rem 0;">
sum_squares = 0
for i in range(n):
    diff = x[i] - y[i]
    sum_squares += diff * diff
return sqrt(sum_squares)
                            </pre>
                            <p><strong>Optimizations:</strong> Use BLAS, avoid sqrt for comparisons, vectorize</p>
                        </div>
                        
                        <h5>‚ö° p = ‚àû (Chebyshev):</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <strong>Pseudocode:</strong>
                            <pre style="margin: 0.5rem 0;">
max_diff = 0
for i in range(n):
    diff = abs(x[i] - y[i])
    if diff > max_diff:
        max_diff = diff
return max_diff
                            </pre>
                            <p><strong>Optimizations:</strong> Parallel reduction, vectorized max operations</p>
                        </div>
                        
                        <h5>‚ö° General p:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <strong>Pseudocode:</strong>
                            <pre style="margin: 0.5rem 0;">
sum_powers = 0
for i in range(n):
    diff = abs(x[i] - y[i])
    sum_powers += pow(diff, p)
return pow(sum_powers, 1.0/p)
                            </pre>
                            <p><strong>Considerations:</strong> Numerical stability, overflow prevention, efficient power computation</p>
                        </div>
                    </div>
                </div>

                <!-- Continue with other sections... -->
                <!-- Due to length limits, I'll continue with the remaining sections in the next part -->

                <div class="navigation-buttons">
                    <a href="/tutorials/ml-fundamentals/clustering/chapter2" class="azbn-btn azbn-secondary" style="text-decoration: none;">‚Üê Chapter 2: Distance Metrics</a>
                    <a href="/tutorials/ml-fundamentals/clustering/chapter4" class="azbn-btn" style="text-decoration: none;">Chapter 4: Advanced Distance Metrics ‚Üí</a>
                </div>
            </div>
        </section>
    </main>

    <script>
        let quizAnswers = {};
        
        function showSection(sectionName) {
            // Hide all sections
            document.querySelectorAll('.content-section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            event.target.classList.add('active');
        }

        // Parameter Explorer Functions
        function updateParameterAnalysis() {
            const p = parseFloat(document.getElementById('p-value').value);
            const ax = parseFloat(document.getElementById('point-a-x').value) || 0;
            const ay = parseFloat(document.getElementById('point-a-y').value) || 0;
            const az = parseFloat(document.getElementById('point-a-z').value) || 0;
            const bx = parseFloat(document.getElementById('point-b-x').value) || 0;
            const by = parseFloat(document.getElementById('point-b-y').value) || 0;
            const bz = parseFloat(document.getElementById('point-b-z').value) || 0;
            
            // Calculate component differences
            const dx = Math.abs(ax - bx);
            const dy = Math.abs(ay - by);
            const dz = Math.abs(az - bz);
            
            // Calculate Minkowski distance
            let distance;
            if (p === Infinity || p > 20) {
                distance = Math.max(dx, dy, dz);
            } else {
                distance = Math.pow(Math.pow(dx, p) + Math.pow(dy, p) + Math.pow(dz, p), 1/p);
            }
            
            // Update display
            document.getElementById('current-distance').textContent = distance.toFixed(4);
            
            // Component breakdown
            const contributions = [
                `|${ax}-${bx}|^${p} = ${Math.pow(dx, p).toFixed(3)}`,
                `|${ay}-${by}|^${p} = ${Math.pow(dy, p).toFixed(3)}`,
                `|${az}-${bz}|^${p} = ${Math.pow(dz, p).toFixed(3)}`
            ];
            document.getElementById('component-breakdown').innerHTML = contributions.join('<br>');
            
            // Special cases
            const l1 = dx + dy + dz;
            const l2 = Math.sqrt(dx*dx + dy*dy + dz*dz);
            const linf = Math.max(dx, dy, dz);
            
            document.getElementById('special-cases').innerHTML = `
                L‚ÇÅ: ${l1.toFixed(3)}<br>
                L‚ÇÇ: ${l2.toFixed(3)}<br>
                L‚àû: ${linf.toFixed(3)}
            `;
        }
        
        // Initialize parameter slider
        document.getElementById('p-value').addEventListener('input', function() {
            const value = parseFloat(this.value);
            document.getElementById('p-display').textContent = value.toFixed(1);
            updateParameterAnalysis();
        });
        
        // Initialize coordinate inputs
        ['point-a-x', 'point-a-y', 'point-a-z', 'point-b-x', 'point-b-y', 'point-b-z'].forEach(id => {
            document.getElementById(id).addEventListener('input', updateParameterAnalysis);
        });
        
        // Initialize with default calculation
        window.addEventListener('load', function() {
            updateParameterAnalysis();
        });
    </script>
</body>
</html>