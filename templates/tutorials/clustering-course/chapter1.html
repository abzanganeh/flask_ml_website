<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Introduction to Clustering - Comprehensive Clustering Analysis Course</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering-course/clustering-course.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering-course/chapter1.css') }}">
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav">
            <div class="azbn-container">
                <a href="/tutorials/clustering-course" class="course-link">
                    <span>Comprehensive Clustering Analysis Course</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main>
        <!-- Comprehensive Chapter Header -->
        <div class="chapter-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 1: Introduction to Clustering and Unsupervised Learning</h1>
                <p class="chapter-subtitle">Discover the fundamental concepts of unsupervised learning and clustering analysis, from basic theory to mathematical foundations.</p>
                
                <!-- Chapter Progress Bar (1/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering-course/chapter1" class="chapter-nav-btn active">Chapter 1</a>
                    <a href="/tutorials/clustering-course/chapter2" class="chapter-nav-btn">Chapter 2</a>
                    <a href="/tutorials/clustering-course/chapter3" class="chapter-nav-btn">Chapter 3</a>
                    <a href="/tutorials/clustering-course/chapter4" class="chapter-nav-btn">Chapter 4</a>
                    <a href="/tutorials/clustering-course/chapter5" class="chapter-nav-btn">Chapter 5</a>
                    <a href="/tutorials/clustering-course/chapter6" class="chapter-nav-btn">Chapter 6</a>
                    <a href="/tutorials/clustering-course/chapter7" class="chapter-nav-btn">Chapter 7</a>
                    <a href="/tutorials/clustering-course/chapter8" class="chapter-nav-btn">Chapter 8</a>
                    <a href="/tutorials/clustering-course/chapter9" class="chapter-nav-btn">Chapter 9</a>
                    <a href="/tutorials/clustering-course/chapter10" class="chapter-nav-btn">Chapter 10</a>
                    <a href="/tutorials/clustering-course/chapter11" class="chapter-nav-btn">Chapter 11</a>
                    <a href="/tutorials/clustering-course/chapter12" class="chapter-nav-btn">Chapter 12</a>
                    <a href="/tutorials/clustering-course/chapter13" class="chapter-nav-btn">Chapter 13</a>
                    <a href="/tutorials/clustering-course/chapter14" class="chapter-nav-btn">Chapter 14</a>
                    <a href="/tutorials/clustering-course/chapter15" class="chapter-nav-btn">Chapter 15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="active" onclick="showSection('clustering', this)">What is Clustering?</button>
                    <button onclick="showSection('unsupervised', this)">Unsupervised Learning</button>
                    <button onclick="showSection('types', this)">Types of Clustering</button>
                    <button onclick="showSection('applications', this)">Real-World Applications</button>
                    <button onclick="showSection('challenges', this)">Challenges & Assumptions</button>
                    <button onclick="showSection('mathematical', this)">Mathematical Foundations</button>
                    <button onclick="showSection('demo', this)">Interactive Demo</button>
                    <button onclick="showSection('quiz', this)">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Learning Objectives at the top -->
                <div class="learning-objectives-card">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the fundamental differences between supervised and unsupervised learning</li>
                        <li>Master the core concepts and terminology of clustering analysis</li>
                        <li>Learn the mathematical foundations of similarity and dissimilarity measures</li>
                        <li>Explore various types of clustering problems and their applications</li>
                        <li>Recognize when to apply clustering in real-world scenarios</li>
                        <li>Understand the challenges and assumptions of clustering algorithms</li>
                    </ul>
                </div>

                <!-- Main Content Area -->
                <main class="chapter-main-content">

                <!-- What is Clustering Section -->
                <div id="clustering" class="section-content active">
                    <h2>What is Clustering?</h2>
                    
                    <p>Clustering is one of the most fundamental and widely used techniques in machine learning and data analysis. At its core, clustering is the task of organizing data points into groups (clusters) such that points within the same group are more similar to each other than to points in other groups.</p>

                    <div class="formula-box">
                        <h3>Mathematical Definition of Clustering</h3>
                        <p>Given a dataset X = {x‚ÇÅ, x‚ÇÇ, ..., x‚Çô} where each x·µ¢ represents a data point in d-dimensional space, clustering aims to partition X into k clusters C = {C‚ÇÅ, C‚ÇÇ, ..., C‚Çñ} such that:</p>
                        <div class="formula-center">
                            <strong>C‚ÇÅ ‚à™ C‚ÇÇ ‚à™ ... ‚à™ C‚Çñ = X</strong><br>
                            <strong>C·µ¢ ‚à© C‚±º = ‚àÖ for i ‚â† j</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>C‚ÇÅ ‚à™ C‚ÇÇ ‚à™ ... ‚à™ C‚Çñ = X</strong>: The union of all clusters must equal the entire dataset. This ensures every data point is assigned to exactly one cluster (completeness).</li>
                                <li><strong>C·µ¢ ‚à© C‚±º = ‚àÖ for i ‚â† j</strong>: The intersection of any two different clusters is empty. This ensures no data point belongs to multiple clusters simultaneously (exclusivity).</li>
                                <li><strong>C·µ¢</strong>: Represents the i-th cluster, a subset of the original dataset X</li>
                                <li><strong>X</strong>: The complete dataset containing all n data points</li>
                                <li><strong>k</strong>: The number of clusters we want to create</li>
                            </ul>
                        </div>
                        
                        <p>Where each cluster C·µ¢ contains data points that are similar according to some distance or similarity metric.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Basic Clustering Example</h4>
                        <img src="/static/images/tutorials/clustering-course/chapter1/basic-clustering-example.png" alt="A 2D scatter plot showing 30 data points in three natural groups. The first group (red circles) is located in the upper-left quadrant, the second group (blue triangles) in the upper-right, and the third group (green squares) in the lower-center. Each group contains roughly 10 points clustered closely together with clear separation between groups. Dashed circles outline each cluster boundary.">
                        
                    </div>

                    <h3>Key Concepts and Terminology</h3>
                    
                    <div class="concepts-grid">
                        <div class="concept-card-blue">
                            <h4>Cluster</h4>
                            <p>A group of data points that are similar to each other and dissimilar to points in other clusters. Mathematically, a cluster is a subset of the dataset.</p>
                        </div>
                        
                        <div class="concept-card-green">
                            <h4>Centroid</h4>
                            <p>The center point of a cluster, typically calculated as the mean of all points in the cluster: <strong>Œº·µ¢ = (1/|C·µ¢|) Œ£_{x‚ààC·µ¢} x</strong></p>
                        </div>
                        
                        <div class="concept-card-yellow">
                            <h4>Intra-cluster Distance</h4>
                            <p>The distance between points within the same cluster. Lower intra-cluster distances indicate more cohesive clusters.</p>
                        </div>
                        
                        <div class="concept-card-purple">
                            <h4>Inter-cluster Distance</h4>
                            <p>The distance between different clusters. Higher inter-cluster distances indicate better cluster separation.</p>
                        </div>
                    </div>

                    <h3> The Clustering Objective</h3>
                    <p>The fundamental goal of clustering is to maximize intra-cluster similarity while maximizing inter-cluster dissimilarity. This can be formalized mathematically as an optimization problem:</p>
                    <div class="explanation-box">
                        <p><strong>Intra-cluster similarity:</strong> You want data points within the same cluster to be as similar as possible. This means minimizing the distance between them.</p>


                        <p> <strong>Inter-cluster dissimilarity:</strong> You want clusters to be as distinct from each other as possible. This means maximizing the distance between clusters.</p>
                    </div>
                        <div class="formula-box">                        <h4> Optimization Objective</h4>
                        <p>Minimize the within-cluster sum of squares (WCSS):</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>WCSS = Œ£·µ¢‚Çå‚ÇÅ·µè Œ£_{x‚ààC·µ¢} ||x - Œº·µ¢||¬≤</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>WCSS</strong>: Within-Cluster Sum of Squares - measures the total squared distance of all points from their cluster centroids</li>
                                <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µè</strong>: Sum over all k clusters (outer summation)</li>
                                <li><strong>Œ£_{x‚ààC·µ¢}</strong>: Sum over all data points x that belong to cluster C·µ¢ (inner summation)</li>
                                <li><strong>||x - Œº·µ¢||¬≤</strong>: Squared Euclidean distance between data point x and the centroid Œº·µ¢ of cluster i</li>
                                <li><strong>Œº·µ¢</strong>: Centroid (mean) of cluster i, calculated as Œº·µ¢ = (1/|C·µ¢|) Œ£_{x‚ààC·µ¢} x</li>
                            </ul>
                            
                            <p><strong>Interpretation:</strong> Lower WCSS values indicate tighter, more cohesive clusters. The goal is to minimize this value to achieve optimal clustering.</p>
                        </div>
                        
                        <p>Where:</p>
                        <ul>
                            <li><strong>k</strong> = number of clusters</li>
                            <li><strong>C·µ¢</strong> = the i-th cluster</li>
                            <li><strong>Œº·µ¢</strong> = centroid of cluster i</li>
                            <li><strong>||x - Œº·µ¢||¬≤</strong> = squared Euclidean distance</li>
                        </ul>
                    </div>
                    <div class="explanation-box">
                        <h3> Why is Clustering Challenging?</h3>
                        <p>Clustering is considered one of the most challenging problems in machine learning for several mathematical and practical reasons:</p>
                        
                        <ol>
                            <li><strong>No Ground Truth:</strong> Unlike supervised learning, there's no "correct" answer to guide the algorithm</li>
                            <li><strong>Subjectivity:</strong> The definition of "similarity" depends on the application and domain</li>
                            <li><strong>Curse of Dimensionality:</strong> Distance metrics become less meaningful in high-dimensional spaces</li>
                            <li><strong>Scalability:</strong> Many algorithms have high computational complexity O(n¬≤) or worse</li>
                            <li><strong>Parameter Selection:</strong> Choosing the number of clusters k is often non-trivial</li>
                        </ol>
                    </div>
                </div>

                <!-- Unsupervised Learning Section -->
                <div id="unsupervised" class="section-content">
                    <h2>Understanding Unsupervised Learning</h2>
                    
                    <p>To fully appreciate clustering, we must understand its place within the broader context of machine learning. Clustering belongs to the category of <strong>unsupervised learning</strong>, which fundamentally differs from supervised learning in both methodology and objectives.</p>

                    <h3>Supervised vs Unsupervised Learning</h3>
                    
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Supervised Learning</th>
                                <th>Unsupervised Learning</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Data Structure</strong></td>
                                <td>Labeled data: (X, y) pairs</td>
                                <td>Unlabeled data: X only</td>
                            </tr>
                            <tr>
                                <td><strong>Objective</strong></td>
                                <td>Learn mapping f: X ‚Üí y</td>
                                <td>Discover hidden patterns in X</td>
                            </tr>
                            <tr>
                                <td><strong>Evaluation</strong></td>
                                <td>Compare predictions with true labels</td>
                                <td>Internal validation metrics</td>
                            </tr>
                            <tr>
                                <td><strong>Examples</strong></td>
                                <td>Classification, Regression</td>
                                <td>Clustering, Dimensionality Reduction</td>
                            </tr>
                            <tr>
                                <td><strong>Mathematical Goal</strong></td>
                                <td>Minimize prediction error</td>
                                <td>Maximize data structure discovery</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Learning Paradigms Comparison</h4>
                        <img src="/static/images/tutorials/clustering-course/chapter1/supervised_vs_unsupervised.png" alt=" Two side-by-side 2D plots. Left plot (Supervised): scattered points in red and blue with a clear decision boundary line separating them, labeled 'Known Classes'. Right plot (Unsupervised): same points but all in gray color with question marks, and dashed circles showing potential cluster groupings, labeled 'Hidden Patterns to Discover'">
                    </div>

                    <h3>The Mathematical Framework of Unsupervised Learning</h3>
                    
                    <p>In unsupervised learning, we work with a dataset D = {x‚ÇÅ, x‚ÇÇ, ..., x‚Çô} where each x·µ¢ ‚àà ‚Ñù·µà is a d-dimensional feature vector. Our goal is to discover hidden structures, patterns, or relationships within this data without any external guidance.</p>

                    <div class="formula-box">
                        <h4>Unsupervised Learning Objectives</h4>
                        <p>Common mathematical objectives in unsupervised learning include:</p>
                        <ol>
                            <li><strong>Density Estimation:</strong> Estimate p(x) - the probability density function of the data</li>
                            <li><strong>Dimensionality Reduction:</strong> Find a lower-dimensional representation: f: ‚Ñù·µà ‚Üí ‚Ñù·µè where k < d</li>
                            <li><strong>Clustering:</strong> Partition data into meaningful groups</li>
                            <li><strong>Association Rule Learning:</strong> Discover relationships between variables</li>
                        </ol>
                    </div>

                    <h3>Types of Unsupervised Learning</h3>
                    
                    <div class="unsupervised-types-grid">
                        <div class="type-card clustering">
                            <h4>Clustering</h4>
                            <p><strong>Goal:</strong> Group similar data points</p>
                            <p><strong>Algorithms:</strong> K-means, Hierarchical, DBSCAN</p>
                            <p><strong>Output:</strong> Cluster assignments</p>
                        </div>
                        
                        <div class="type-card dimensionality">
                            <h4>Dimensionality Reduction</h4>
                            <p><strong>Goal:</strong> Reduce feature space</p>
                            <p><strong>Algorithms:</strong> PCA, t-SNE, UMAP</p>
                            <p><strong>Output:</strong> Lower-dimensional representation</p>
                        </div>
                        
                        <div class="type-card anomaly">
                            <h4>Anomaly Detection</h4>
                            <p><strong>Goal:</strong> Identify outliers</p>
                            <p><strong>Algorithms:</strong> Isolation Forest, LOF</p>
                            <p><strong>Output:</strong> Anomaly scores</p>
                        </div>
                        
                        <div class="type-card association">
                            <h4>Association Rules</h4>
                            <p><strong>Goal:</strong> Find frequent patterns</p>
                            <p><strong>Algorithms:</strong> Apriori, FP-Growth</p>
                            <p><strong>Output:</strong> Rule sets</p>
                        </div>
                        
                        <div class="type-card neural">
                            <h4>Neural Networks</h4>
                            <p><strong>Goal:</strong> Learn representations & generate data</p>
                            <p><strong>Algorithms:</strong> Autoencoders, VAEs, GANs, SOMs</p>
                            <p><strong>Output:</strong> Learned features & generated samples</p>
                        </div>
                        
                        <div class="type-card density">
                            <h4>Density Estimation</h4>
                            <p><strong>Goal:</strong> Model data distribution</p>
                            <p><strong>Algorithms:</strong> KDE, Gaussian Mixtures, Parzen Windows</p>
                            <p><strong>Output:</strong> Probability density functions</p>
                        </div>
                    </div>

                    <h3>‚öñÔ∏è The Challenge of Validation</h3>
                    <p>One of the most significant challenges in unsupervised learning is validation. Without ground truth labels, how do we know if our clustering is "good"? This leads us to several important concepts:</p>

                    <div class="formula-box">
                        <h4> Internal Validation Measures</h4>
                        <p>Since we lack external labels, we must rely on internal measures of cluster quality:</p>
                        <ul>
                            <li><strong>Silhouette Coefficient:</strong> Measures how similar an object is to its own cluster compared to other clusters</li>
                            <li><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance</li>
                            <li><strong>Davies-Bouldin Index:</strong> Average similarity ratio of each cluster with its most similar cluster</li>
                        </ul>
                    </div>

                    <h3>Detailed Clustering Evaluation Metrics</h3>
                    <p>Understanding these metrics is crucial for assessing clustering quality and comparing different algorithms.</p>

                    <h4>Silhouette Coefficient</h4>
                    <p>The silhouette coefficient is one of the most intuitive and widely-used clustering evaluation metrics. It measures how similar an object is to its own cluster compared to other clusters, providing a clear indication of clustering quality.</p>

                    <div class="formula-box">
                        <h5>Silhouette Coefficient Formula</h5>
                        <div class="formula-display">
                            <strong>s(i) = (b(i) - a(i)) / max(a(i), b(i))</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>a(i)</strong>: Average distance from point i to all other points in the same cluster (intra-cluster distance)</li>
                                <li><strong>b(i)</strong>: Average distance from point i to all points in the nearest neighboring cluster (inter-cluster distance)</li>
                                <li><strong>s(i)</strong>: Silhouette coefficient for point i</li>
                            </ul>
                            
                            <p><strong>Range:</strong> [-1, 1] where 1 = well-clustered, 0 = on cluster boundary, -1 = misclassified</p>
                        </div>
                    </div>

                    <div class="algorithm-step">
                        <h5>How to Use Silhouette Coefficient:</h5>
                        <ol>
                            <li><strong>Calculate for each point:</strong> Compute s(i) for every data point in your dataset</li>
                            <li><strong>Average across all points:</strong> Take the mean of all silhouette coefficients to get the overall score</li>
                            <li><strong>Interpret the results:</strong>
                                <ul>
                                    <li><strong>0.7 - 1.0:</strong> Strong clustering structure</li>
                                    <li><strong>0.5 - 0.7:</strong> Reasonable clustering structure</li>
                                    <li><strong>0.25 - 0.5:</strong> Weak clustering structure</li>
                                    <li><strong>&lt; 0.25:</strong> No substantial clustering structure</li>
                                </ul>
                            </li>
                        </ol>
                    </div>

                    <div class="algorithm-step">
                        <h5>When to Use Silhouette Coefficient:</h5>
                        <ul>
                            <li><strong>Cluster validation:</strong> When you need to validate the quality of your clustering results</li>
                            <li><strong>Optimal k selection:</strong> To find the best number of clusters by comparing silhouette scores for different k values</li>
                            <li><strong>Algorithm comparison:</strong> To compare different clustering algorithms on the same dataset</li>
                            <li><strong>Outlier detection:</strong> Points with negative silhouette coefficients might be outliers or misclassified</li>
                        </ul>
                    </div>

                    <h4>Calinski-Harabasz Index (Variance Ratio Criterion)</h4>
                    <p>The Calinski-Harabasz Index, also known as the Variance Ratio Criterion, is a statistical measure that evaluates clustering quality by comparing the ratio of between-cluster variance to within-cluster variance. It's particularly useful for partitional clustering algorithms like K-means.</p>

                    <div class="formula-box">
                        <h5>Calinski-Harabasz Index Formula</h5>
                        <div class="formula-display">
                            <strong>CH = (SSB / (k-1)) / (SSW / (n-k))</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>SSB (Sum of Squares Between):</strong> Total squared distance between cluster centroids and the overall centroid</li>
                                <li><strong>SSW (Sum of Squares Within):</strong> Total squared distance between data points and their cluster centroids</li>
                                <li><strong>k</strong>: Number of clusters</li>
                                <li><strong>n</strong>: Number of data points</li>
                                <li><strong>(k-1)</strong>: Degrees of freedom for between-cluster variance</li>
                                <li><strong>(n-k)</strong>: Degrees of freedom for within-cluster variance</li>
                            </ul>
                            
                            <p><strong>Higher values indicate better clustering</strong> - more separation between clusters and tighter clusters</p>
                        </div>
                    </div>

                    <div class="algorithm-step">
                        <h5>How to Use Calinski-Harabasz Index:</h5>
                        <ol>
                            <li><strong>Calculate for different k values:</strong> Compute CH index for various numbers of clusters</li>
                            <li><strong>Find the maximum:</strong> The k value with the highest CH index is typically optimal</li>
                            <li><strong>Compare algorithms:</strong> Use CH index to compare different clustering algorithms</li>
                            <li><strong>Statistical significance:</strong> Higher CH values indicate statistically significant cluster separation</li>
                        </ol>
                    </div>

                    <div class="algorithm-step">
                        <h5>When to Use Calinski-Harabasz Index:</h5>
                        <ul>
                            <li><strong>K-means optimization:</strong> Particularly effective for finding optimal k in K-means clustering</li>
                            <li><strong>Spherical clusters:</strong> Works best when clusters are roughly spherical and similar in size</li>
                            <li><strong>Large datasets:</strong> Computationally efficient for large datasets</li>
                            <li><strong>Algorithm comparison:</strong> Good for comparing partitional clustering algorithms</li>
                        </ul>
                    </div>

                    <h4>Davies-Bouldin Index</h4>
                    <p>The Davies-Bouldin Index is a clustering evaluation metric that measures the average similarity ratio of each cluster with its most similar cluster. Unlike other metrics, lower values indicate better clustering quality, making it intuitive to interpret.</p>

                    <div class="formula-box">
                        <h5>Davies-Bouldin Index Formula</h5>
                        <div class="formula-display">
                            <strong>DB = (1/k) Œ£·µ¢‚Çå‚ÇÅ·µè max_{j‚â†i} (S·µ¢ + S‚±º) / M·µ¢‚±º</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>S·µ¢</strong>: Average distance from points in cluster i to cluster i's centroid (intra-cluster scatter)</li>
                                <li><strong>S‚±º</strong>: Average distance from points in cluster j to cluster j's centroid (intra-cluster scatter)</li>
                                <li><strong>M·µ¢‚±º</strong>: Distance between centroids of clusters i and j (inter-cluster separation)</li>
                                <li><strong>max_{j‚â†i}</strong>: Maximum value over all clusters j different from i</li>
                                <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µè</strong>: Sum over all k clusters</li>
                                <li><strong>(1/k)</strong>: Average over all clusters</li>
                            </ul>
                            
                            <p><strong>Lower values indicate better clustering</strong> - tighter clusters with better separation</p>
                        </div>
                    </div>

                    <div class="algorithm-step">
                        <h5>How to Use Davies-Bouldin Index:</h5>
                        <ol>
                            <li><strong>Calculate for each cluster:</strong> Compute the ratio (S·µ¢ + S‚±º) / M·µ¢‚±º for each cluster i with its most similar cluster j</li>
                            <li><strong>Find the maximum ratio:</strong> For each cluster, find the maximum ratio across all other clusters</li>
                            <li><strong>Average the results:</strong> Take the average of all maximum ratios to get the final DB index</li>
                            <li><strong>Interpret the score:</strong> Lower values (closer to 0) indicate better clustering</li>
                        </ol>
                    </div>

                    <div class="algorithm-step">
                        <h5>When to Use Davies-Bouldin Index:</h5>
                        <ul>
                            <li><strong>Cluster validation:</strong> When you need a simple, interpretable measure of clustering quality</li>
                            <li><strong>Optimal k selection:</strong> To find the best number of clusters by minimizing the DB index</li>
                            <li><strong>Algorithm comparison:</strong> To compare different clustering algorithms on the same dataset</li>
                            <li><strong>Compact clusters:</strong> Particularly effective when you expect compact, well-separated clusters</li>
                        </ul>
                    </div>
                </div>

                <!-- Types of Clustering Section -->
                <div id="types" class="section-content">
                    <h2>Types of Clustering Methods</h2>
                    
                    <p>Clustering algorithms can be categorized into several distinct types, each with its own mathematical foundations, assumptions, and optimal use cases. Understanding these categories is crucial for selecting the appropriate algorithm for your specific problem.</p>

                    <h3> Partitional Clustering</h3>
                    <p>Partitional clustering methods divide the dataset into k non-overlapping clusters. The most famous example is K-means clustering.</p>

                    <div class="algorithm-step">
                        <h4> Characteristics of Partitional Clustering:</h4>
                        <ul>
                            <li><strong>Fixed Number of Clusters:</strong> The number of clusters k must be specified in advance</li>
                            <li><strong>Non-overlapping:</strong> Each data point belongs to exactly one cluster</li>
                            <li><strong>Optimization-based:</strong> Typically minimize an objective function</li>
                            <li><strong>Computational Complexity:</strong> Generally O(nkt) where t is the number of iterations</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Partitional Clustering Example</h4>
                        <img src="/static/images/tutorials/clustering-course/chapter1/partial_clustering_example.png" alt="A 2D plot showing K-means clustering with k=3. Data points are colored according to their cluster assignment (red, blue, green). Three large X marks indicate cluster centroids. Voronoi diagram boundaries show the decision regions for each cluster. Points clearly belong to one cluster each with no overlap.">
                    </div>

                    <h3> Hierarchical Clustering</h3>
                    <p>Hierarchical clustering creates a tree-like structure of clusters, representing nested groupings at different levels of granularity.</p>

                    <div class="hierarchical-methods-grid">
                        <div class="method-card agglomerative">
                            <h4>‚¨ÜÔ∏è Agglomerative (Bottom-up)</h4>
                            <p><strong>Process:</strong> Start with each point as its own cluster, then merge closest clusters iteratively</p>
                            <p><strong>Mathematical Foundation:</strong></p>
                            <div class="code-block">
                                <code>
                                Initialize: C = {{ "{{' }}x‚ÇÅ}, {x‚ÇÇ}, ..., {x‚Çô}{{ '}}" }}<br>
                                While |C| > 1:<br>
                                &nbsp;&nbsp;Find closest clusters C·µ¢, C‚±º<br>
                                &nbsp;&nbsp;Merge: C = C ‚à™ {C·µ¢ ‚à™ C‚±º} \ {C·µ¢, C‚±º}
                                </code>
                            </div>
                        </div>
                        
                        <div class="method-card divisive">
                            <h4>‚¨áÔ∏è Divisive (Top-down)</h4>
                            <p><strong>Process:</strong> Start with all points in one cluster, then recursively split clusters</p>
                            <p><strong>Mathematical Foundation:</strong></p>
                            <div class="code-block">
                                <code>
                                Initialize: C = {X}<br>
                                While stopping criterion not met:<br>
                                &nbsp;&nbsp;Select cluster C·µ¢ to split<br>
                                &nbsp;&nbsp;Find optimal split of C·µ¢ into C‚ÇÅ, C‚ÇÇ
                                </code>
                            </div>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üå≥ Visualization: Dendrogram Example</h4>
                        <p><strong>Image Description:</strong> A hierarchical tree diagram (dendrogram) showing the merging process of 8 data points labeled A through H. The y-axis shows distance/height from 0 to 4. Branches merge at different heights indicating when clusters were combined. Three main clusters are visible when cut at height 2.5, shown with a red dashed horizontal line.</p>
                        <p><em>This will illustrate how hierarchical clustering creates a nested structure of clusters</em></p>
                    </div>

                    <h3>üè† Density-Based Clustering</h3>
                    <p>Density-based methods identify clusters as areas of high density separated by areas of low density. They can discover clusters of arbitrary shape and automatically handle noise.</p>

                    <div class="formula-box">
                        <h4> Mathematical Foundation of Density-Based Clustering</h4>
                        <p>Key concepts in density-based clustering:</p>
                        <ul>
                            <li><strong>Œµ-neighborhood:</strong> N‚Çë(x) = {y ‚àà D | dist(x,y) ‚â§ Œµ}</li>
                            <li><strong>Core point:</strong> A point x where |N‚Çë(x)| ‚â• minPts</li>
                            <li><strong>Density-reachable:</strong> Point y is density-reachable from x if there exists a chain of core points connecting them</li>
                            <li><strong>Density-connected:</strong> Points x and y are density-connected if both are density-reachable from some point z</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4> Visualization: DBSCAN Clustering Example</h4>
                        <p><strong>Image Description:</strong> A 2D scatter plot showing DBSCAN results with irregularly shaped clusters. Three distinct regions: (1) dense circular cluster in red, (2) crescent-shaped cluster in blue, (3) elongated cluster in green. Black dots represent noise points. Larger dots indicate core points, medium dots are border points, small dots are noise.</p>
                        <p><em>This demonstrates DBSCAN's ability to find arbitrary-shaped clusters and identify outliers</em></p>
                    </div>

                    <h3> Model-Based Clustering</h3>
                    <p>Model-based clustering assumes that data is generated from a mixture of probability distributions. The most common approach is Gaussian Mixture Models (GMM).</p>

                    <div class="formula-box">
                        <h4>üé≤ Gaussian Mixture Model Mathematics</h4>
                        <p>A GMM represents the data as a mixture of k Gaussian distributions:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>p(x) = Œ£·µ¢‚Çå‚ÇÅ·µè œÄ·µ¢ ùí©(x | Œº·µ¢, Œ£·µ¢)</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>p(x)</strong>: Probability density function of observing data point x</li>
                                <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µè</strong>: Sum over all k Gaussian components (clusters)</li>
                                <li><strong>œÄ·µ¢</strong>: Mixing coefficient (weight) for the i-th Gaussian component, representing the prior probability that a data point belongs to cluster i</li>
                                <li><strong>ùí©(x | Œº·µ¢, Œ£·µ¢)</strong>: Multivariate Gaussian distribution with mean vector Œº·µ¢ and covariance matrix Œ£·µ¢</li>
                                <li><strong>Œº·µ¢</strong>: Mean vector (center) of the i-th Gaussian component</li>
                                <li><strong>Œ£·µ¢</strong>: Covariance matrix of the i-th Gaussian component, controlling the shape and orientation</li>
                            </ul>
                            
                            <p><strong>Interpretation:</strong> This formula represents the probability of observing any data point x as a weighted sum of k Gaussian distributions. Each Gaussian represents one cluster, and the mixing coefficients determine how likely each cluster is.</p>
                        </div>
                        
                        <p>Where:</p>
                        <ul>
                            <li><strong>œÄ·µ¢</strong> = mixing coefficient (prior probability of cluster i)</li>
                            <li><strong>ùí©(x | Œº·µ¢, Œ£·µ¢)</strong> = multivariate Gaussian with mean Œº·µ¢ and covariance Œ£·µ¢</li>
                            <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µè œÄ·µ¢ = 1</strong> (mixing coefficients sum to 1)</li>
                        </ul>
                    </div>

                    <h3>üìã Comparison of Clustering Types</h3>
                    
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Cluster Shape</th>
                                <th>Number of Clusters</th>
                                <th>Handles Noise</th>
                                <th>Computational Complexity</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>K-means</strong></td>
                                <td>Spherical</td>
                                <td>Pre-specified</td>
                                <td>No</td>
                                <td>O(nkt)</td>
                            </tr>
                            <tr>
                                <td><strong>Hierarchical</strong></td>
                                <td>Any</td>
                                <td>Determined by cut</td>
                                <td>Limited</td>
                                <td>O(n¬≥)</td>
                            </tr>
                            <tr>
                                <td><strong>DBSCAN</strong></td>
                                <td>Arbitrary</td>
                                <td>Automatic</td>
                                <td>Yes</td>
                                <td>O(n log n)</td>
                            </tr>
                            <tr>
                                <td><strong>GMM</strong></td>
                                <td>Elliptical</td>
                                <td>Pre-specified</td>
                                <td>Limited</td>
                                <td>O(nkt)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Applications Section -->
                <div id="applications" class="section-content">
                    <h2>Real-World Applications of Clustering</h2>
                    
                    <p>Clustering algorithms have found applications across virtually every domain where data analysis is performed. Understanding these applications helps contextualize the theoretical concepts and demonstrates the practical importance of clustering techniques.</p>

                    <h3>üõí Customer Segmentation and Marketing</h3>
                    <p>One of the most commercially successful applications of clustering is in customer segmentation, where businesses group customers based on purchasing behavior, demographics, and preferences.</p>

                    <div class="algorithm-step">
                        <h4> Mathematical Approach to Customer Segmentation:</h4>
                        <p>Given customer data matrix X ‚àà ‚Ñù‚ÅøÀ£·µà where:</p>
                        <ul>
                            <li>n = number of customers</li>
                            <li>d = number of features (age, income, purchase frequency, etc.)</li>
                            <li>x·µ¢‚±º = j-th feature value for customer i</li>
                        </ul>
                        <p>The goal is to partition customers into k segments such that customers within each segment have similar purchasing patterns and can be targeted with tailored marketing strategies.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Customer Segmentation Example</h4>
                        <p><strong>Image Description:</strong> A 3D scatter plot with axes labeled "Annual Income ($)", "Age (years)", and "Spending Score (1-100)". Four distinct clusters are visible: (1) Young, low income, low spending (blue), (2) Young, low income, high spending (red), (3) Middle-aged, high income, high spending (green), (4) Older, high income, low spending (purple). Each cluster has a 3D ellipsoid boundary showing the cluster region.</p>
                        <p><em>This demonstrates how customers naturally group into distinct behavioral segments</em></p>
                    </div>

                    <h3>üß¨ Bioinformatics and Genomics</h3>
                    <p>In bioinformatics, clustering is used to analyze gene expression data, protein sequences, and other biological data to understand relationships and functions.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üß¨ Gene Expression Analysis</h4>
                            <p><strong>Problem:</strong> Group genes with similar expression patterns</p>
                            <p><strong>Data:</strong> Expression levels across different conditions/tissues</p>
                            <p><strong>Mathematics:</strong> Correlation-based distance metrics</p>
                            <p><strong>Insight:</strong> Co-expressed genes often have related functions</p>
                        </div>
                        
                        <div style="background: #f3e5f5; padding: 1rem; border-radius: 8px;">
                            <h4> Protein Structure Analysis</h4>
                            <p><strong>Problem:</strong> Classify proteins by structural similarity</p>
                            <p><strong>Data:</strong> 3D coordinates, amino acid sequences</p>
                            <p><strong>Mathematics:</strong> RMSD (Root Mean Square Deviation)</p>
                            <p><strong>Insight:</strong> Similar structures suggest similar functions</p>
                        </div>
                    </div>

                    <h3>üñºÔ∏è Image Segmentation and Computer Vision</h3>
                    <p>Clustering plays a crucial role in computer vision for tasks like image segmentation, object recognition, and feature extraction.</p>

                    <div class="formula-box">
                        <h4> Mathematical Framework for Image Segmentation</h4>
                        <p>In image segmentation, we treat each pixel as a data point in a feature space:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>x = [R, G, B, X, Y]·µÄ</strong>
                        </div>
                        <p>Where R, G, B are color values and X, Y are spatial coordinates. The goal is to partition pixels into regions corresponding to different objects or textures.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üñºÔ∏è Visualization: Image Segmentation Process</h4>
                        <p><strong>Image Description:</strong> Three side-by-side images: (1) Original photo of a landscape with sky, mountains, and lake, (2) Same image with each pixel colored by its cluster assignment - sky in blue, mountains in brown, lake in dark blue, (3) Segmentation boundaries overlaid as white lines on the original image, clearly delineating the three regions.</p>
                        <p><em>This shows how clustering can automatically identify distinct regions in images</em></p>
                    </div>

                    <h3>üì± Social Network Analysis</h3>
                    <p>In social networks, clustering helps identify communities, influence groups, and patterns of interaction.</p>

                    <div class="algorithm-step">
                        <h4>üåê Graph-Based Clustering Mathematics:</h4>
                        <p>Given a graph G = (V, E) representing a social network:</p>
                        <ul>
                            <li><strong>V</strong> = set of users/nodes</li>
                            <li><strong>E</strong> = set of connections/edges</li>
                            <li><strong>Adjacency Matrix A:</strong> A·µ¢‚±º = 1 if edge exists between users i and j</li>
                        </ul>
                        <p>Community detection algorithms like modularity maximization aim to partition V into communities with high internal connectivity and low external connectivity.</p>
                    </div>

                    <h3>üí∞ Financial Services and Fraud Detection</h3>
                    
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4>üí≥ Credit Card Fraud Detection</h4>
                            <p><strong>Approach:</strong> Cluster normal transactions and identify outliers</p>
                            <p><strong>Features:</strong> Amount, time, location, merchant type</p>
                            <p><strong>Algorithm:</strong> Isolation Forest, DBSCAN for anomaly detection</p>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4> Portfolio Management</h4>
                            <p><strong>Approach:</strong> Group stocks with similar risk/return profiles</p>
                            <p><strong>Features:</strong> Returns, volatility, correlations</p>
                            <p><strong>Algorithm:</strong> K-means, hierarchical clustering</p>
                        </div>
                    </div>

                    <h3>üè• Healthcare and Medical Diagnosis</h3>
                    <p>Medical applications of clustering range from patient stratification to drug discovery and epidemiological studies.</p>

                    <div class="visualization-placeholder">
                        <h4>üè• Visualization: Patient Clustering for Personalized Medicine</h4>
                        <p><strong>Image Description:</strong> A 2D projection of high-dimensional patient data showing three distinct clusters: (1) Young patients with mild symptoms (green cluster, lower-left), (2) Middle-aged patients with moderate symptoms (blue cluster, center), (3) Elderly patients with severe symptoms (red cluster, upper-right). Each point represents a patient, with medical icons indicating different treatment protocols for each cluster.</p>
                        <p><em>This illustrates how patient clustering enables personalized treatment strategies</em></p>
                    </div>

                    <h3> Document Classification and Text Mining</h3>
                    <p>In natural language processing, clustering helps organize documents, identify topics, and analyze sentiment patterns.</p>

                    <div class="formula-box">
                        <h4>üìù Text Clustering Mathematics</h4>
                        <p>Documents are typically represented using the Vector Space Model:</p>
                        <ul>
                            <li><strong>TF-IDF Vector:</strong> x·µ¢‚±º = tf·µ¢‚±º √ó log(N/df‚±º)</li>
                            <li><strong>tf·µ¢‚±º:</strong> Term frequency of word j in document i</li>
                            <li><strong>df‚±º:</strong> Document frequency of word j</li>
                            <li><strong>N:</strong> Total number of documents</li>
                        </ul>
                        <p>Cosine similarity is often used as the distance metric for text clustering due to the high dimensionality and sparsity of text data.</p>
                    </div>
                </div>

                <!-- Challenges Section -->
                <div id="challenges" class="section-content">
                    <h2>Challenges and Assumptions in Clustering</h2>
                    
                    <p>While clustering is a powerful tool for data analysis, it comes with significant challenges and limitations that must be understood to apply these techniques effectively. These challenges stem from both theoretical limitations and practical implementation issues.</p>

                    <h3> The Fundamental Challenge: Defining Similarity</h3>
                    <p>The core challenge in clustering is that the notion of "similarity" is inherently subjective and context-dependent. What constitutes a meaningful cluster varies dramatically across domains and applications.</p>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Subjective Nature of Clustering</h4>
                        <p><strong>Image Description:</strong> Four panels showing the same 2D data points clustered in different ways: (1) Two vertical clusters, (2) Two horizontal clusters, (3) Three circular clusters, (4) One large cluster with outliers. Each clustering is mathematically valid but represents different interpretations of the data structure. A question mark in the center asks "Which clustering is correct?"</p>
                        <p><em>This demonstrates that multiple valid clusterings can exist for the same dataset</em></p>
                    </div>

                    <h3>The Curse of Dimensionality</h3>
                    <p>As the number of features increases, distance-based clustering algorithms face fundamental mathematical challenges that can render them ineffective.</p>

                    <div class="formula-box">
                        <h4> Mathematical Analysis of High-Dimensional Distance</h4>
                        <p>In high-dimensional spaces, the ratio of the maximum to minimum distance approaches 1:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>lim_{d‚Üí‚àû} (d_max - d_min) / d_min ‚Üí 0</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>lim_{d‚Üí‚àû}</strong>: Limit as the number of dimensions d approaches infinity</li>
                                <li><strong>d_max</strong>: Maximum distance between any two points in the dataset</li>
                                <li><strong>d_min</strong>: Minimum distance between any two points in the dataset</li>
                                <li><strong>(d_max - d_min) / d_min</strong>: Relative difference between maximum and minimum distances</li>
                                <li><strong>‚Üí 0</strong>: Approaches zero as dimensions increase</li>
                            </ul>
                            
                            <p><strong>Interpretation:</strong> As the number of dimensions increases, the relative difference between the maximum and minimum distances approaches zero. This means that in high-dimensional spaces, all points become approximately equidistant from each other, making distance-based clustering algorithms ineffective.</p>
                        </div>
                        
                        <p>This phenomenon, known as the "concentration of distances," means that all points appear equidistant in high-dimensional spaces, making clustering based on distance metrics ineffective.</p>
                        
                        <h5>Consequences for Clustering:</h5>
                        <ul>
                            <li>Distance metrics lose discriminative power</li>
                            <li>Nearest neighbor relationships become less meaningful</li>
                            <li>Cluster boundaries become less distinct</li>
                            <li>Traditional algorithms may fail to find meaningful clusters</li>
                        </ul>
                    </div>

                    <h3>üî¢ The Parameter Selection Problem</h3>
                    <p>Most clustering algorithms require the specification of parameters that significantly affect the results, yet there's often no principled way to choose these parameters.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #ffebee; padding: 1rem; border-radius: 8px;">
                            <h4> K-means: Number of Clusters (k)</h4>
                            <p><strong>Challenge:</strong> How to choose k?</p>
                            <p><strong>Methods:</strong> Elbow method, silhouette analysis, gap statistic</p>
                            <p><strong>Limitation:</strong> No universally optimal approach</p>
                        </div>
                        
                        <div style="background: #fff3e0; padding: 1rem; border-radius: 8px;">
                            <h4> DBSCAN: Œµ and minPts</h4>
                            <p><strong>Challenge:</strong> Sensitive to parameter choices</p>
                            <p><strong>Methods:</strong> k-distance plots, domain knowledge</p>
                            <p><strong>Limitation:</strong> Different densities require different parameters</p>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üå≥ Hierarchical: Linkage and Cut Height</h4>
                            <p><strong>Challenge:</strong> Which level to cut the dendrogram?</p>
                            <p><strong>Methods:</strong> Gap statistics, stability analysis</p>
                            <p><strong>Limitation:</strong> Single linkage vs complete linkage trade-offs</p>
                        </div>
                    </div>

                    <h3>‚è±Ô∏è Computational Complexity Challenges</h3>
                    <p>Many clustering algorithms have high computational complexity, making them impractical for large datasets.</p>

                    <div class="formula-box">
                        <h4> Complexity Analysis of Common Algorithms</h4>
                        <table class="comparison-table" style="width: 100%; margin: 1rem 0;">
                            <thead>
                                <tr>
                                    <th>Algorithm</th>
                                    <th>Time Complexity</th>
                                    <th>Space Complexity</th>
                                    <th>Scalability</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>K-means</strong></td>
                                    <td>O(nkt)</td>
                                    <td>O(n + k)</td>
                                    <td>Good</td>
                                </tr>
                                <tr>
                                    <td><strong>Agglomerative HC</strong></td>
                                    <td>O(n¬≥)</td>
                                    <td>O(n¬≤)</td>
                                    <td>Poor</td>
                                </tr>
                                <tr>
                                    <td><strong>DBSCAN</strong></td>
                                    <td>O(n log n)</td>
                                    <td>O(n)</td>
                                    <td>Moderate</td>
                                </tr>
                                <tr>
                                    <td><strong>Spectral Clustering</strong></td>
                                    <td>O(n¬≥)</td>
                                    <td>O(n¬≤)</td>
                                    <td>Poor</td>
                                </tr>
                            </tbody>
                        </table>
                        <p>Where: n = number of data points, k = number of clusters, t = number of iterations</p>
                    </div>

                    <h3>The Evaluation Problem</h3>
                    <p>Unlike supervised learning, clustering lacks ground truth labels, making evaluation subjective and challenging.</p>

                    <div class="algorithm-step">
                        <h4> Types of Clustering Validation:</h4>
                        <ol>
                            <li><strong>Internal Validation:</strong> Based only on the data and clustering results
                                <ul>
                                    <li>Silhouette coefficient</li>
                                    <li>Calinski-Harabasz index</li>
                                    <li>Davies-Bouldin index</li>
                                </ul>
                            </li>
                            <li><strong>External Validation:</strong> Compares results to known ground truth
                                <ul>
                                    <li>Adjusted Rand Index</li>
                                    <li>Normalized Mutual Information</li>
                                    <li>Fowlkes-Mallows Index</li>
                                </ul>
                            </li>
                            <li><strong>Relative Validation:</strong> Compares different clustering algorithms
                                <ul>
                                    <li>Stability analysis</li>
                                    <li>Cross-validation approaches</li>
                                    <li>Consensus clustering</li>
                                </ul>
                            </li>
                        </ol>
                    </div>

                    <h3>Initialization and Local Optima</h3>
                    <p>Many clustering algorithms are sensitive to initialization and can get trapped in local optima, leading to inconsistent results.</p>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Local Optima Problem in K-means</h4>
                        <p><strong>Image Description:</strong> Two side-by-side 2D plots showing the same dataset clustered with different initializations. Left plot shows poor initialization with centroids starting in suboptimal positions, resulting in unbalanced clusters. Right plot shows good initialization (K-means++) with well-balanced, natural clusters. Both plots show iteration paths of centroid movement as arrows.</p>
                        <p><em>This illustrates how initialization affects final clustering results</em></p>
                    </div>

                    <h3> Assumptions of Common Clustering Algorithms</h3>
                    <p>Each clustering algorithm makes implicit assumptions about the data structure. Violating these assumptions can lead to poor results.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                            <h4> K-means Assumptions</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Clusters are spherical (isotropic)</li>
                                <li>Clusters have similar sizes</li>
                                <li>Clusters have similar densities</li>
                                <li>Features are continuous and scaled</li>
                                <li>No outliers in the data</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>üå≥ Hierarchical Clustering Assumptions</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Nested cluster structure exists</li>
                                <li>Distance metric is meaningful</li>
                                <li>Linkage criterion matches data structure</li>
                                <li>No noise points</li>
                                <li>Computational resources are sufficient</li>
                            </ul>
                        </div>
                        
                        <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                            <h4> DBSCAN Assumptions</h4>
                            <ul style="font-size: 0.9rem;">
                                <li>Clusters have uniform density</li>
                                <li>Clusters are separated by low-density regions</li>
                                <li>Parameter Œµ is appropriate for all clusters</li>
                                <li>Distance metric captures similarity well</li>
                                <li>Noise points can be identified</li>
                            </ul>
                        </div>
                    </div>

                    <h3>üõ†Ô∏è Strategies for Addressing Clustering Challenges</h3>
                    
                    <div class="algorithm-step">
                        <h4> Best Practices for Robust Clustering:</h4>
                        <ol>
                            <li><strong>Data Preprocessing:</strong>
                                <ul>
                                    <li>Feature scaling and normalization</li>
                                    <li>Dimensionality reduction for high-dimensional data</li>
                                    <li>Outlier detection and treatment</li>
                                </ul>
                            </li>
                            <li><strong>Algorithm Selection:</strong>
                                <ul>
                                    <li>Understand data characteristics and assumptions</li>
                                    <li>Try multiple algorithms and compare results</li>
                                    <li>Use ensemble clustering methods</li>
                                </ul>
                            </li>
                            <li><strong>Parameter Tuning:</strong>
                                <ul>
                                    <li>Use systematic parameter selection methods</li>
                                    <li>Apply cross-validation when possible</li>
                                    <li>Incorporate domain knowledge</li>
                                </ul>
                            </li>
                            <li><strong>Validation:</strong>
                                <ul>
                                    <li>Use multiple evaluation metrics</li>
                                    <li>Perform stability analysis</li>
                                    <li>Validate results with domain experts</li>
                                </ul>
                            </li>
                        </ol>
                    </div>
                </div>

                <!-- Mathematical Foundations Section -->
                <div id="mathematical" class="section-content">
                    <h2>Mathematical Foundations of Clustering</h2>
                    
                    <p>To truly understand clustering algorithms, we must delve into their mathematical foundations. This section covers the essential mathematical concepts that underpin all clustering methods, providing the theoretical framework necessary for advanced understanding and implementation.</p>

                    <h3>Vector Spaces and Data Representation</h3>
                    <p>All clustering algorithms operate on data represented as vectors in a mathematical space. Understanding this representation is crucial for algorithmic success.</p>

                    <div class="formula-box">
                        <h4> Mathematical Data Representation</h4>
                        <p>Given a dataset D with n observations and d features:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>X = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]·µÄ ‚àà ‚Ñù‚ÅøÀ£·µà</strong>
                        </div>
                        <p>Where each observation x·µ¢ ‚àà ‚Ñù·µà is a d-dimensional vector:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>x·µ¢ = [x·µ¢‚ÇÅ, x·µ¢‚ÇÇ, ..., x·µ¢·µà]·µÄ</strong>
                        </div>
                        <p>The choice of feature space ‚Ñù·µà and the scaling of features significantly impacts clustering results.</p>
                    </div>

                    <h3> Distance and Similarity Measures</h3>
                    <p>The foundation of most clustering algorithms is the concept of distance or similarity between data points. Different measures capture different notions of similarity.</p>

                    <h4> Euclidean Distance</h4>
                    <p>The most commonly used distance metric, representing straight-line distance in multidimensional space.</p>

                    <div class="formula-box">
                        <h5> Euclidean Distance Formula</h5>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>d_E(x, y) = ‚àö(Œ£·µ¢‚Çå‚ÇÅ·µà (x·µ¢ - y·µ¢)¬≤)</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>d_E(x, y)</strong>: Euclidean distance between points x and y</li>
                                <li><strong>‚àö</strong>: Square root function</li>
                                <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µà</strong>: Sum over all d dimensions (features)</li>
                                <li><strong>(x·µ¢ - y·µ¢)¬≤</strong>: Squared difference between the i-th components of points x and y</li>
                                <li><strong>x·µ¢, y·µ¢</strong>: The i-th coordinate (feature value) of points x and y respectively</li>
                            </ul>
                            
                            <p><strong>Interpretation:</strong> This measures the straight-line distance between two points in d-dimensional space. It's the most intuitive distance metric and works well for continuous, normally distributed data.</p>
                        </div>
                        
                        <p>Or in vector notation:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>d_E(x, y) = ||x - y||‚ÇÇ</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Vector Notation Breakdown:</h5>
                            <ul>
                                <li><strong>||x - y||‚ÇÇ</strong>: L2 norm (Euclidean norm) of the vector (x - y)</li>
                                <li><strong>x - y</strong>: Vector difference between points x and y</li>
                                <li><strong>||¬∑||‚ÇÇ</strong>: L2 norm operator, equivalent to the square root of the sum of squared components</li>
                            </ul>
                        </div>
                        <p><strong>Properties:</strong></p>
                        <ul>
                            <li>Non-negative: d(x, y) ‚â• 0</li>
                            <li>Symmetric: d(x, y) = d(y, x)</li>
                            <li>Triangle inequality: d(x, z) ‚â§ d(x, y) + d(y, z)</li>
                            <li>Identity: d(x, y) = 0 ‚ü∫ x = y</li>
                        </ul>
                    </div>

                    <h4>üèôÔ∏è Manhattan Distance (L1 Norm)</h4>
                    <p>Also known as city block distance, it measures distance along axes at right angles.</p>

                    <div class="formula-box">
                        <h5>üè¢ Manhattan Distance Formula</h5>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>d_M(x, y) = Œ£·µ¢‚Çå‚ÇÅ·µà |x·µ¢ - y·µ¢|</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>d_M(x, y)</strong>: Manhattan distance (L1 distance) between points x and y</li>
                                <li><strong>Œ£·µ¢‚Çå‚ÇÅ·µà</strong>: Sum over all d dimensions (features)</li>
                                <li><strong>|x·µ¢ - y·µ¢|</strong>: Absolute difference between the i-th components of points x and y</li>
                                <li><strong>x·µ¢, y·µ¢</strong>: The i-th coordinate (feature value) of points x and y respectively</li>
                            </ul>
                            
                            <p><strong>Interpretation:</strong> This measures the sum of absolute differences along each dimension. It's called "Manhattan distance" because it's like walking along city blocks - you can only move horizontally or vertically, not diagonally. It's more robust to outliers than Euclidean distance.</p>
                        </div>
                        <p>Or in vector notation:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>d_M(x, y) = ||x - y||‚ÇÅ</strong>
                        </div>
                        <p><strong>When to use:</strong> When movement is restricted to orthogonal directions (like city streets), or when you want to reduce the influence of outliers.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Euclidean vs Manhattan Distance</h4>
                        <p><strong>Image Description:</strong> A 2D grid showing two points A and B. A red straight line connects them directly (Euclidean distance = 5.0). Multiple blue step-like paths show different Manhattan distance routes, all with the same length (Manhattan distance = 7.0). Grid lines help visualize the orthogonal movement restriction in Manhattan distance.</p>
                        <p><em>This illustrates the geometric difference between distance metrics</em></p>
                    </div>

                    <h4>Cosine Similarity</h4>
                    <p>Measures the cosine of the angle between two vectors, focusing on orientation rather than magnitude.</p>

                    <div class="formula-box">
                        <h5>Cosine Similarity Formula</h5>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>cos_sim(x, y) = (x ¬∑ y) / (||x||‚ÇÇ ||y||‚ÇÇ)</strong>
                        </div>
                        <p>Expanded form:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>cos_sim(x, y) = (Œ£·µ¢‚Çå‚ÇÅ·µà x·µ¢y·µ¢) / (‚àö(Œ£·µ¢‚Çå‚ÇÅ·µà x·µ¢¬≤) ‚àö(Œ£·µ¢‚Çå‚ÇÅ·µà y·µ¢¬≤))</strong>
                        </div>
                        <p><strong>Range:</strong> [-1, 1] where 1 = identical direction, 0 = orthogonal, -1 = opposite direction</p>
                        <p><strong>Cosine Distance:</strong> d_cos(x, y) = 1 - cos_sim(x, y)</p>
                    </div>

                    <h4>üìè Minkowski Distance (Lp Norm)</h4>
                    <p>A generalized distance metric that includes Euclidean and Manhattan distances as special cases.</p>

                    <div class="formula-box">
                        <h5>üî¢ Minkowski Distance Formula</h5>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>d_p(x, y) = (Œ£·µ¢‚Çå‚ÇÅ·µà |x·µ¢ - y·µ¢|·µñ)^(1/p)</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>d_p(x, y)</strong>: Minkowski distance of order p between points x and y</li>
                                <li><strong>p</strong>: Order parameter (p ‚â• 1)</li>
                                <li><strong>|x·µ¢ - y·µ¢|·µñ</strong>: p-th power of absolute difference in dimension i</li>
                                <li><strong>^(1/p)</strong>: p-th root of the sum</li>
                            </ul>
                            
                            <p><strong>Special Cases:</strong></p>
                            <ul>
                                <li><strong>p = 1:</strong> Manhattan distance (L1 norm)</li>
                                <li><strong>p = 2:</strong> Euclidean distance (L2 norm)</li>
                                <li><strong>p = ‚àû:</strong> Chebyshev distance (L‚àû norm)</li>
                            </ul>
                        </div>
                    </div>

                    <h4>‚ôüÔ∏è Chebyshev Distance (L‚àû Norm)</h4>
                    <p>Measures the maximum difference across all dimensions, representing the "king's move" distance in chess.</p>

                    <div class="formula-box">
                        <h5>üëë Chebyshev Distance Formula</h5>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>d_‚àû(x, y) = max·µ¢ |x·µ¢ - y·µ¢|</strong>
                        </div>
                        
                        <div class="formula-explanation">
                            <h5>Formula Breakdown:</h5>
                            <ul>
                                <li><strong>d_‚àû(x, y)</strong>: Chebyshev distance between points x and y</li>
                                <li><strong>max·µ¢</strong>: Maximum value across all dimensions i</li>
                                <li><strong>|x·µ¢ - y·µ¢|</strong>: Absolute difference in dimension i</li>
                            </ul>
                            
                            <p><strong>When to use:</strong> When you want to measure the maximum deviation across any single dimension, useful in quality control and outlier detection.</p>
                        </div>
                    </div>

                    <h3> Optimization Theory in Clustering</h3>
                    <p>Most clustering algorithms can be formulated as optimization problems. Understanding these formulations helps in algorithm design and analysis.</p>

                    <h4> K-means as an Optimization Problem</h4>
                    
                    <div class="formula-box">
                        <h5>üé≤ K-means Objective Function</h5>
                        <p>K-means minimizes the within-cluster sum of squared errors (WCSS):</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>J = Œ£·µ¢‚Çå‚ÇÅ·µè Œ£‚Çì‚ààC·µ¢ ||x - Œº·µ¢||¬≤</strong>
                        </div>
                        <p>Where:</p>
                        <ul>
                            <li><strong>k</strong> = number of clusters</li>
                            <li><strong>C·µ¢</strong> = set of points in cluster i</li>
                            <li><strong>Œº·µ¢</strong> = centroid of cluster i</li>
                        </ul>
                        
                        <h6>Lagrangian Formulation:</h6>
                        <p>The optimal centroid for each cluster is the mean of points in that cluster:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>Œº·µ¢* = (1/|C·µ¢|) Œ£‚Çì‚ààC·µ¢ x</strong>
                        </div>
                        <p>This can be proven by taking the derivative of J with respect to Œº·µ¢ and setting it to zero.</p>
                    </div>

                    <h4>üåä Gradient Descent in Clustering</h4>
                    <p>Some clustering algorithms use gradient-based optimization to find optimal solutions.</p>

                    <div class="algorithm-step">
                        <h5> Gradient Descent for Clustering</h5>
                        <p>For a general clustering objective function J(Œ∏), gradient descent updates parameters as:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>Œ∏_{t+1} = Œ∏‚Çú - Œ± ‚àáJ(Œ∏‚Çú)</strong>
                        </div>
                        <p>Where:</p>
                        <ul>
                            <li><strong>Œ±</strong> = learning rate</li>
                            <li><strong>‚àáJ(Œ∏‚Çú)</strong> = gradient of objective function at iteration t</li>
                        </ul>
                        <p>This approach is used in algorithms like Gaussian Mixture Models with EM algorithm.</p>
                    </div>

                    <h3> Information Theory in Clustering</h3>
                    <p>Information theory provides tools for measuring cluster quality and comparing clustering results.</p>

                    <div class="formula-box">
                        <h4> Mutual Information</h4>
                        <p>Measures the amount of information shared between two clustering assignments:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>MI(C, C') = Œ£·µ¢‚±º P(i,j) log(P(i,j)/(P(i)P'(j)))</strong>
                        </div>
                        <p>Where:</p>
                        <ul>
                            <li><strong>P(i)</strong> = probability that a point belongs to cluster i in clustering C</li>
                            <li><strong>P'(j)</strong> = probability that a point belongs to cluster j in clustering C'</li>
                            <li><strong>P(i,j)</strong> = joint probability</li>
                        </ul>
                        
                        <h5>Normalized Mutual Information (NMI):</h5>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>NMI(C, C') = MI(C, C') / ‚àö(H(C)H(C'))</strong>
                        </div>
                        <p>Where H(C) is the entropy of clustering C.</p>
                    </div>

                    <h3>üé≤ Probability Theory and Model-Based Clustering</h3>
                    <p>Model-based clustering approaches use probability theory to model the data generation process.</p>

                    <div class="formula-box">
                        <h4> Gaussian Mixture Model Mathematics</h4>
                        <p>A GMM assumes data is generated from a mixture of Gaussian distributions:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>p(x) = Œ£·µ¢‚Çå‚ÇÅ·µè œÄ·µ¢ ùí©(x | Œº·µ¢, Œ£·µ¢)</strong>
                        </div>
                        <p>The multivariate Gaussian density is:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.0rem;">
                            <strong>ùí©(x | Œº, Œ£) = (2œÄ)^(-d/2) |Œ£|^(-1/2) exp(-¬Ω(x-Œº)·µÄŒ£‚Åª¬π(x-Œº))</strong>
                        </div>
                        
                        <h5>Maximum Likelihood Estimation:</h5>
                        <p>Parameters are estimated by maximizing the log-likelihood:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>‚Ñì(Œ∏) = Œ£‚Çô‚Çå‚ÇÅ·¥∫ log(Œ£·µ¢‚Çå‚ÇÅ·µè œÄ·µ¢ ùí©(x‚Çô | Œº·µ¢, Œ£·µ¢))</strong>
                        </div>
                    </div>

                    <h3>üîó Graph Theory in Clustering</h3>
                    <p>Graph-based clustering algorithms model data as a graph and use graph properties to identify clusters.</p>

                    <div class="algorithm-step">
                        <h4>üåê Graph Construction for Clustering</h4>
                        <ol>
                            <li><strong>k-Nearest Neighbors Graph:</strong> Connect each point to its k nearest neighbors</li>
                            <li><strong>Œµ-Neighborhood Graph:</strong> Connect points within distance Œµ</li>
                            <li><strong>Fully Connected Graph:</strong> Connect all pairs with weighted edges</li>
                        </ol>
                        
                        <h5>Graph Laplacian:</h5>
                        <p>The normalized graph Laplacian is defined as:</p>
                        <div style="text-align: center; margin: 1rem 0;">
                            <strong>L = D^(-1/2)(D - W)D^(-1/2)</strong>
                        </div>
                        <p>Where D is the degree matrix and W is the adjacency matrix.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>üåê Visualization: Graph-Based Clustering</h4>
                        <p><strong>Image Description:</strong> A network graph showing 20 nodes connected by edges. Three distinct communities are visible: a densely connected triangular group (red), a linear chain-like group (blue), and a star-shaped group with one central hub (green). Edge thickness represents connection strength. The graph structure clearly reveals the cluster boundaries better than distance-based methods would.</p>
                        <p><em>This shows how graph structure can reveal clusters not apparent in Euclidean space</em></p>
                    </div>

                    <h3> Linear Algebra Applications</h3>
                    <p>Linear algebra plays a crucial role in dimensionality reduction and spectral clustering methods.</p>

                    <div class="formula-box">
                        <h4> Eigenvalue Decomposition in Clustering</h4>
                        <p>Many clustering algorithms rely on eigenvalue decomposition:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>Av = Œªv</strong>
                        </div>
                        <p>Where A is a matrix (e.g., covariance matrix, graph Laplacian), Œª are eigenvalues, and v are eigenvectors.</p>
                        
                        <h5>Applications:</h5>
                        <ul>
                            <li><strong>PCA:</strong> Eigendecomposition of covariance matrix</li>
                            <li><strong>Spectral Clustering:</strong> Eigendecomposition of graph Laplacian</li>
                            <li><strong>Kernel PCA:</strong> Eigendecomposition of kernel matrix</li>
                        </ul>
                    </div>
                </div>

                <!-- Interactive Demo Section -->
                <div id="demo" class="section-content">
                    <h2>Interactive Clustering Demonstration</h2>
                    
                    <p>Experience clustering algorithms in action with this interactive demonstration. You can generate different datasets, adjust parameters, and see how various algorithms perform in real-time.</p>

                    <div class="interactive-demo">
                        <h3>üéÆ Interactive K-means Clustering Demo</h3>
                        <p>Generate random data and watch K-means algorithm find clusters step by step.</p>
                        
                        <div class="demo-controls">
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin: 1rem 0;">
                                <div>
                                    <label for="num-points">Number of Points:</label>
                                    <input type="range" id="num-points" min="20" max="100" value="50">
                                    <span id="points-value">50</span>
                                </div>
                                <div>
                                    <label for="num-clusters">Number of Clusters (k):</label>
                                    <input type="range" id="num-clusters" min="2" max="8" value="3">
                                    <span id="clusters-value">3</span>
                                </div>
                                <div>
                                    <label for="data-type">Dataset Type:</label>
                                    <select id="data-type">
                                        <option value="random">Random</option>
                                        <option value="blobs">Gaussian Blobs</option>
                                        <option value="circles">Concentric Circles</option>
                                        <option value="moons">Half Moons</option>
                                    </select>
                                </div>
                            </div>
                            
                            <div style="text-align: center; margin: 1rem 0;">
                                <button onclick="generateData()" class="azbn-btn">Generate New Data</button>
                                <button onclick="runKmeans()" class="azbn-btn">Run K-means</button>
                                <button onclick="stepKmeans()" class="azbn-btn azbn-secondary">Step Through Algorithm</button>
                                <button onclick="resetKmeans()" class="azbn-btn azbn-secondary">Reset</button>
                            </div>
                        </div>

                        <div class="cluster-visualization" id="kmeans-canvas">
                            <p style="text-align: center; margin-top: 100px; color: #666;">
                                Click "Generate New Data" to start the demonstration
                            </p>
                        </div>

                        <div id="algorithm-status" style="background: #f8f9fa; padding: 1rem; border-radius: 6px; margin: 1rem 0; display: none;">
                            <h4>Algorithm Status:</h4>
                            <p id="status-text">Ready to start</p>
                            <p id="iteration-count">Iteration: 0</p>
                            <p id="convergence-info">WCSS: Not calculated</p>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3> Distance Metrics Comparison</h3>
                        <p>Compare how different distance metrics affect clustering results on the same dataset.</p>
                        
                        <div class="demo-controls">
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 1rem; margin: 1rem 0;">
                                <div>
                                    <input type="checkbox" id="euclidean" checked>
                                    <label for="euclidean">Euclidean</label>
                                </div>
                                <div>
                                    <input type="checkbox" id="manhattan" checked>
                                    <label for="manhattan">Manhattan</label>
                                </div>
                                <div>
                                    <input type="checkbox" id="cosine" checked>
                                    <label for="cosine">Cosine</label>
                                </div>
                            </div>
                            
                            <button onclick="compareDistances()" class="azbn-btn">Compare Distance Metrics</button>
                        </div>

                        <div id="distance-comparison" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 1rem 0;">
                            <!-- Distance comparison visualizations will be inserted here -->
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3> Cluster Evaluation Metrics</h3>
                        <p>Understand how different evaluation metrics assess clustering quality.</p>
                        
                        <div class="demo-controls">
                            <button onclick="demonstrateMetrics()" class="azbn-btn">Calculate Metrics</button>
                        </div>

                        <div id="metrics-display" style="background: white; padding: 1rem; border: 1px solid #ddd; border-radius: 6px; margin: 1rem 0; display: none;">
                            <h4>Clustering Evaluation Results:</h4>
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                                <div style="background: #e3f2fd; padding: 1rem; border-radius: 6px;">
                                    <h5>Silhouette Score</h5>
                                    <p id="silhouette-score">-</p>
                                    <p style="font-size: 0.9rem;">Range: [-1, 1]<br>Higher is better</p>
                                </div>
                                <div style="background: #e8f5e8; padding: 1rem; border-radius: 6px;">
                                    <h5>Calinski-Harabasz</h5>
                                    <p id="ch-score">-</p>
                                    <p style="font-size: 0.9rem;">Range: [0, ‚àû)<br>Higher is better</p>
                                </div>
                                <div style="background: #fff8e1; padding: 1rem; border-radius: 6px;">
                                    <h5>Davies-Bouldin</h5>
                                    <p id="db-score">-</p>
                                    <p style="font-size: 0.9rem;">Range: [0, ‚àû)<br>Lower is better</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="visualization-placeholder">
                        <h4> Visualization: Algorithm Performance Comparison</h4>
                        <p><strong>Image Description:</strong> A 2x3 grid showing the same dataset clustered by different algorithms: K-means (spherical clusters), Hierarchical (dendrogram-based), DBSCAN (irregular shapes), GMM (elliptical), Spectral (complex boundaries), and Mean Shift (density-based). Each subplot shows the algorithm name, clustering result with different colors, and evaluation metrics below.</p>
                        <p><em>This will demonstrate how different algorithms perform on the same data</em></p>
                    </div>
                </div>

                <!-- Quiz Section -->
                <div id="quiz" class="section-content">
                    <h2>Chapter 1 Quiz: Clustering Fundamentals</h2>
                    <p>Test your understanding of clustering concepts, mathematical foundations, and practical applications.</p>

                    <div class="quiz-question">
                        <h4>Question 1: What is the fundamental difference between supervised and unsupervised learning?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q1" value="a" id="q1a">
                            <label for="q1a">Supervised learning uses more data points</label><br>
                            <input type="radio" name="q1" value="b" id="q1b">
                            <label for="q1b">Unsupervised learning lacks ground truth labels</label><br>
                            <input type="radio" name="q1" value="c" id="q1c">
                            <label for="q1c">Supervised learning is always more accurate</label><br>
                            <input type="radio" name="q1" value="d" id="q1d">
                            <label for="q1d">There is no significant difference</label>
                        </div>
                        <button onclick="checkAnswer(1, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q1-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 2: What does the K-means algorithm minimize?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q2" value="a" id="q2a">
                            <label for="q2a">The number of clusters</label><br>
                            <input type="radio" name="q2" value="b" id="q2b">
                            <label for="q2b">The within-cluster sum of squared errors (WCSS)</label><br>
                            <input type="radio" name="q2" value="c" id="q2c">
                            <label for="q2c">The between-cluster distance</label><br>
                            <input type="radio" name="q2" value="d" id="q2d">
                            <label for="q2c">The computational complexity</label>
                        </div>
                        <button onclick="checkAnswer(2, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q2-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 3: Which distance metric is most appropriate for high-dimensional text data?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q3" value="a" id="q3a">
                            <label for="q3a">Euclidean distance</label><br>
                            <input type="radio" name="q3" value="b" id="q3b">
                            <label for="q3b">Manhattan distance</label><br>
                            <input type="radio" name="q3" value="c" id="q3c">
                            <label for="q3c">Cosine similarity</label><br>
                            <input type="radio" name="q3" value="d" id="q3d">
                            <label for="q3d">Hamming distance</label>
                        </div>
                        <button onclick="checkAnswer(3, 'c')" class="azbn-btn">Check Answer</button>
                        <div id="q3-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 4: What is the main challenge of the "curse of dimensionality" in clustering?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q4" value="a" id="q4a">
                            <label for="q4a">Increased computational cost</label><br>
                            <input type="radio" name="q4" value="b" id="q4b">
                            <label for="q4b">All distances become approximately equal</label><br>
                            <input type="radio" name="q4" value="c" id="q4c">
                            <label for="q4c">More clusters are needed</label><br>
                            <input type="radio" name="q4" value="d" id="q4d">
                            <label for="q4d">Data becomes more sparse</label>
                        </div>
                        <button onclick="checkAnswer(4, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q4-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 5: Which clustering type can automatically determine the number of clusters?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q5" value="a" id="q5a">
                            <label for="q5a">K-means</label><br>
                            <input type="radio" name="q5" value="b" id="q5b">
                            <label for="q5b">DBSCAN</label><br>
                            <input type="radio" name="q5" value="c" id="q5c">
                            <label for="q5c">Gaussian Mixture Models</label><br>
                            <input type="radio" name="q5" value="d" id="q5d">
                            <label for="q5d">All of the above</label>
                        </div>
                        <button onclick="checkAnswer(5, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q5-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div style="text-align: center; margin: 2rem 0;">
                        <button onclick="submitQuiz()" class="azbn-btn">Submit Quiz</button>
                        <div id="quiz-score" style="margin-top: 1rem; font-size: 1.2rem; font-weight: bold;"></div>
                    </div>
                </div>

                <!-- Sub-section Navigation -->
                <div class="sub-section-nav-footer">
                    <div class="sub-nav-buttons">
                        <button id="prev-subsection" class="sub-nav-btn prev-btn" onclick="navigateSubSection('prev')" style="display: none;">
                            <span>‚Üê Previous</span>
                            <span class="sub-nav-label" id="prev-label"></span>
                        </button>
                        <button id="next-subsection" class="sub-nav-btn next-btn" onclick="navigateSubSection('next')">
                            <span class="sub-nav-label" id="next-label"></span>
                            <span>Next ‚Üí</span>
                        </button>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <a href="/tutorials/clustering-course" class="azbn-btn azbn-secondary" style="text-decoration: none;">‚Üê Back to Course</a>
                    <a href="/tutorials/clustering-course/chapter2" class="azbn-btn" style="text-decoration: none;">Chapter 2: Distance Metrics ‚Üí</a>
                </div>
                    </main>
                </div>
            </div>
        </section>

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Alireza Barzin Zanganeh</h3>
                    <p>ML Engineer & Data Scientist</p>
                    <p>Passionate about creating intelligent solutions through machine learning and data science.</p>
                </div>
                
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="/#projects">Projects</a></li>
                        <li><a href="/tutorials/">Tutorials</a></li>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/contact">Contact</a></li>
                    </ul>
                </div>
                
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="https://linkedin.com/in/alireza-barzin-zanganeh" target="_blank" rel="noopener" aria-label="LinkedIn">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://github.com/alireza-barzin" target="_blank" rel="noopener" aria-label="GitHub">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="mailto:alireza.barzin.zanganeh@gmail.com" aria-label="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2023 Alireza Barzin Zanganeh. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="{{ url_for('static', filename='js/tutorials/clustering-course/chapter1.js') }}"></script>
</body>
</html>
            document.querySelectorAll('.section-content').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            
            // Add active class to clicked button
            if (clickedElement) {
                clickedElement.classList.add('active');
            }
            
            // Update section progress bar
            updateSectionProgress(sectionName);
        }

        function updateSectionProgress(sectionName) {
            const sections = ['clustering', 'unsupervised', 'types', 'applications', 'challenges', 'mathematical', 'demo', 'quiz'];
            const currentIndex = sections.indexOf(sectionName);
            const progress = ((currentIndex + 1) / sections.length) * 100;
            document.querySelector('.section-progress-fill').style.width = progress + '%';
        }

        // K-means Demo Functions
        function generateData() {
            const numPoints = parseInt(document.getElementById('num-points').value);
            const dataType = document.getElementById('data-type').value;
            
            currentData = [];
            
            if (dataType === 'random') {
                for (let i = 0; i < numPoints; i++) {
                    currentData.push({
                        x: Math.random() * 400 + 50,
                        y: Math.random() * 200 + 50,
                        cluster: -1
                    });
                }
            } else if (dataType === 'blobs') {
                const centers = [
                    {x: 150, y: 100}, {x: 350, y: 100}, {x: 250, y: 200}
                ];
                
                for (let i = 0; i < numPoints; i++) {
                    const center = centers[i % centers.length];
                    currentData.push({
                        x: center.x + (Math.random() - 0.5) * 80,
                        y: center.y + (Math.random() - 0.5) * 60,
                        cluster: -1
                    });
                }
            }
            // Add more data types as needed
            
            resetKmeans();
            drawVisualization();
        }

        function runKmeans() {
            if (currentData.length === 0) {
                alert('Please generate data first!');
                return;
            }
            
            const k = parseInt(document.getElementById('num-clusters').value);
            initializeCentroids(k);
            
            isRunning = true;
            kmeans_iteration = 0;
            
            const runStep = () => {
                if (isRunning) {
                    const converged = kmeansStep();
                    updateStatus();
                    drawVisualization();
                    
                    if (!converged && kmeans_iteration < 100) {
                        setTimeout(runStep, 1000); // 1 second delay between steps
                    } else {
                        isRunning = false;
                        updateStatus();
                    }
                }
            };
            
            runStep();
        }

        function stepKmeans() {
            if (currentData.length === 0) {
                alert('Please generate data first!');
                return;
            }
            
            if (currentCentroids.length === 0) {
                const k = parseInt(document.getElementById('num-clusters').value);
                initializeCentroids(k);
            }
            
            kmeansStep();
            updateStatus();
            drawVisualization();
        }

        function resetKmeans() {
            currentCentroids = [];
            currentClusters = [];
            kmeans_iteration = 0;
            isRunning = false;
            
            // Reset cluster assignments
            currentData.forEach(point => point.cluster = -1);
            
            updateStatus();
            document.getElementById('algorithm-status').style.display = 'none';
        }

        function initializeCentroids(k) {
            currentCentroids = [];
            for (let i = 0; i < k; i++) {
                currentCentroids.push({
                    x: Math.random() * 400 + 50,
                    y: Math.random() * 200 + 50
                });
            }
        }

        function kmeansStep() {
            if (currentCentroids.length === 0) return true;
            
            const oldCentroids = JSON.parse(JSON.stringify(currentCentroids));
            
            // Assign points to clusters
            currentData.forEach(point => {
                let minDistance = Infinity;
                let bestCluster = 0;
                
                currentCentroids.forEach((centroid, index) => {
                    const distance = euclideanDistance(point, centroid);
                    if (distance < minDistance) {
                        minDistance = distance;
                        bestCluster = index;
                    }
                });
                
                point.cluster = bestCluster;
            });
            
            // Update centroids
            currentCentroids.forEach((centroid, index) => {
                const clusterPoints = currentData.filter(point => point.cluster === index);
                
                if (clusterPoints.length > 0) {
                    centroid.x = clusterPoints.reduce((sum, point) => sum + point.x, 0) / clusterPoints.length;
                    centroid.y = clusterPoints.reduce((sum, point) => sum + point.y, 0) / clusterPoints.length;
                }
            });
            
            kmeans_iteration++;
            
            // Check convergence
            const converged = currentCentroids.every((centroid, index) => {
                const oldCentroid = oldCentroids[index];
                return euclideanDistance(centroid, oldCentroid) < 1;
            });
            
            return converged;
        }

        function euclideanDistance(p1, p2) {
            return Math.sqrt(Math.pow(p1.x - p2.x, 2) + Math.pow(p1.y - p2.y, 2));
        }

        function calculateWCSS() {
            let wcss = 0;
            currentData.forEach(point => {
                if (point.cluster >= 0 && point.cluster < currentCentroids.length) {
                    const centroid = currentCentroids[point.cluster];
                    wcss += Math.pow(euclideanDistance(point, centroid), 2);
                }
            });
            return wcss;
        }

        function updateStatus() {
            const statusDiv = document.getElementById('algorithm-status');
            statusDiv.style.display = 'block';
            
            document.getElementById('status-text').textContent = 
                isRunning ? 'Running...' : (kmeans_iteration > 0 ? 'Converged' : 'Ready to start');
            document.getElementById('iteration-count').textContent = `Iteration: ${kmeans_iteration}`;
            
            if (kmeans_iteration > 0) {
                const wcss = calculateWCSS();
                document.getElementById('convergence-info').textContent = `WCSS: ${wcss.toFixed(2)}`;
            }
        }

        function drawVisualization() {
            const canvas = document.getElementById('kmeans-canvas');
            canvas.innerHTML = '';
            
            const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
            svg.setAttribute('width', '100%');
            svg.setAttribute('height', '300');
            svg.setAttribute('viewBox', '0 0 500 300');
            
            const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#f9ca24', '#f0932b', '#eb4d4b', '#6c5ce7', '#a29bfe'];
            
            // Draw data points
            currentData.forEach(point => {
                const circle = document.createElementNS('http://www.w3.org/2000/svg', 'circle');
                circle.setAttribute('cx', point.x);
                circle.setAttribute('cy', point.y);
                circle.setAttribute('r', 4);
                circle.setAttribute('fill', point.cluster >= 0 ? colors[point.cluster % colors.length] : '#666');
                circle.setAttribute('stroke', '#333');
                circle.setAttribute('stroke-width', 1);
                svg.appendChild(circle);
            });
            
            // Draw centroids
            currentCentroids.forEach((centroid, index) => {
                const rect = document.createElementNS('http://www.w3.org/2000/svg', 'rect');
                rect.setAttribute('x', centroid.x - 6);
                rect.setAttribute('y', centroid.y - 6);
                rect.setAttribute('width', 12);
                rect.setAttribute('height', 12);
                rect.setAttribute('fill', colors[index % colors.length]);
                rect.setAttribute('stroke', '#000');
                rect.setAttribute('stroke-width', 2);
                svg.appendChild(rect);
            });
            
            canvas.appendChild(svg);
        }

        function compareDistances() {
            // This would implement distance metric comparison
            alert('Distance comparison feature - implementation would show different clustering results using various distance metrics');
        }

        function demonstrateMetrics() {
            if (currentData.length === 0 || currentCentroids.length === 0) {
                alert('Please run K-means clustering first!');
                return;
            }
            
            // Calculate mock metrics (in real implementation, these would be properly calculated)
            const silhouette = (Math.random() * 0.6 + 0.2).toFixed(3);
            const ch_score = (Math.random() * 200 + 50).toFixed(1);
            const db_score = (Math.random() * 2 + 0.5).toFixed(3);
            
            document.getElementById('silhouette-score').textContent = silhouette;
            document.getElementById('ch-score').textContent = ch_score;
            document.getElementById('db-score').textContent = db_score;
            
            document.getElementById('metrics-display').style.display = 'block';
        }

    </script>
    
    <!-- Load external JavaScript -->
    <script src="{{ url_for('static', filename='js/tutorials/clustering-course/chapter1.js') }}"></script>
</body>
</html>