<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 4: Advanced Distance Metrics - Comprehensive Clustering Analysis Course</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering-course/clustering-course.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <style>
        .visualization-placeholder {
            background: linear-gradient(45deg, #f0f0f0, #e0e0e0);
            border: 2px dashed #999;
            padding: 2rem;
            text-align: center;
            border-radius: 8px;
            margin: 1rem 0;
            font-style: italic;
            color: #666;
        }
        .interactive-demo {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }
        .formula-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .theorem-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .proof-box {
            background: #fff8e1;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .property-box {
            background: #e8f5e8;
            border-left: 4px solid #4caf50;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .application-box {
            background: #fce4ec;
            border-left: 4px solid #e91e63;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f5f5f5;
            font-weight: bold;
        }
        .demo-controls {
            background: #e8f5e8;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .metric-demo {
            background: white;
            border: 1px solid #ddd;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .distance-result {
            background: #f0f8ff;
            border: 1px solid #4169e1;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .selection-guide {
            background: #fff8e1;
            border: 1px solid #ffa726;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .quiz-question {
            background: #f5f5f5;
            border: 1px solid #ddd;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav" style="top: 50px;">
            <div class="azbn-container" style="display: flex; justify-content: space-between; align-items: center;">
                <a href="/tutorials/clustering-course" style="text-decoration: none; color: #4f46e5; display: flex; align-items: center; gap: 0.5rem;">
                    <img src="/static/images/logo.png" alt="Logo" style="height: 40px;">
                    <span>Comprehensive Clustering Analysis Course - Clustering Course</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main style="padding-top: 100px;">
        <section class="azbn-section">
            <div class="azbn-container">
                <!-- Comprehensive Chapter Header -->
                <div class="chapter-header">
                    <div class="azbn-container">
                        <h1 class="chapter-title">Chapter 4: Advanced Distance Metrics</h1>
                        <p class="chapter-subtitle">Explore specialized distance metrics beyond the Lp family: Cosine similarity, Hamming distance, Mahalanobis distance, and domain-specific measures for text, categorical data, and high-dimensional problems.</p>
                        
                        <!-- Chapter Progress Bar (4/15) -->
                        <div class="chapter-progress">
                            <div class="chapter-progress-fill" style="width: 26.67%;"></div>
                        </div>
                        
                        <!-- Chapter Navigation (All 15 chapters) -->
                        <div class="chapter-navigation">
                            <a href="/tutorials/clustering-course/chapter1" class="chapter-nav-btn">1</a>
                            <a href="/tutorials/clustering-course/chapter2" class="chapter-nav-btn">2</a>
                            <a href="/tutorials/clustering-course/chapter3" class="chapter-nav-btn">3</a>
                            <a href="/tutorials/clustering-course/chapter4" class="chapter-nav-btn active">4</a>
                            <a href="/tutorials/clustering-course/chapter5" class="chapter-nav-btn">5</a>
                            <a href="/tutorials/clustering-course/chapter6" class="chapter-nav-btn">6</a>
                            <a href="/tutorials/clustering-course/chapter7" class="chapter-nav-btn">7</a>
                            <a href="/tutorials/clustering-course/chapter8" class="chapter-nav-btn">8</a>
                            <a href="/tutorials/clustering-course/chapter9" class="chapter-nav-btn">9</a>
                            <a href="/tutorials/clustering-course/chapter10" class="chapter-nav-btn">10</a>
                            <a href="/tutorials/clustering-course/chapter11" class="chapter-nav-btn">11</a>
                            <a href="/tutorials/clustering-course/chapter12" class="chapter-nav-btn">12</a>
                            <a href="/tutorials/clustering-course/chapter13" class="chapter-nav-btn">13</a>
                            <a href="/tutorials/clustering-course/chapter14" class="chapter-nav-btn">14</a>
                            <a href="/tutorials/clustering-course/chapter15" class="chapter-nav-btn">15</a>
                        </div>
                        
                        <!-- Section Progress Bar -->
                        <div class="section-progress">
                            <div class="section-progress-fill" style="width: 12.5%;"></div>
                        </div>
                        
                        <!-- Section Navigation -->
                        <div class="section-nav">
                            <button class="active" onclick="showSection('cosine', this)">Cosine Similarity</button>
                            <button onclick="showSection('hamming', this)">Hamming Distance</button>
                            <button onclick="showSection('mahalanobis', this)">Mahalanobis Distance</button>
                            <button onclick="showSection('edit', this)">Edit Distances</button>
                            <button onclick="showSection('specialized', this)">Specialized Metrics</button>
                            <button onclick="showSection('selection', this)">Metric Selection</button>
                            <button onclick="showSection('demo', this)">Interactive Demo</button>
                            <button onclick="showSection('quiz', this)">Quiz</button>
                        </div>
                    </div>
                </div>

                <div class="learning-objectives-card">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Master cosine similarity and angular distance for high-dimensional data</li>
                        <li>Understand Hamming distance and edit distances for categorical data</li>
                        <li>Learn Mahalanobis distance for correlated features and statistical applications</li>
                        <li>Explore correlation-based distances for time series and gene expression data</li>
                        <li>Understand when and how to use each specialized metric</li>
                        <li>Implement efficient algorithms for computing advanced distance measures</li>
                        <li>Design custom distance metrics for domain-specific problems</li>
                    </ul>

                </div>


                <!-- Cosine Similarity Section -->
                <div id="cosine" class="section-content active">
                    <h2>Beyond Minkowski: The World of Specialized Distance Metrics</h2>
                    
                    <p>While Minkowski distances provide a powerful foundation for measuring similarity, many real-world applications require specialized metrics that capture domain-specific notions of distance and similarity. This chapter explores advanced distance measures that excel in particular contexts, from high-dimensional sparse data to categorical variables and correlated features.</p>

                    <h3>Why Specialized Metrics Matter</h3>
                    <p>Different types of data and applications expose the limitations of general-purpose metrics like Euclidean and Manhattan distance:</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: #ffebee; padding: 1rem; border-radius: 8px;">
                            <h4>Limitations of Standard Metrics</h4>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>High dimensions:</strong> Euclidean distance loses discriminative power</li>
                                <li><strong>Sparse data:</strong> Most components are zero, distances become meaningless</li>
                                <li><strong>Categorical data:</strong> Ordering assumptions don't apply</li>
                                <li><strong>Correlated features:</strong> Some directions more important than others</li>
                                <li><strong>Scale differences:</strong> Features with different units dominate</li>
                            </ul>
                        </div>
                        
                        <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                            <h4>Specialized Solutions</h4>
                            <ul style="font-size: 0.9rem;">
                                <li><strong>Cosine similarity:</strong> Direction matters more than magnitude</li>
                                <li><strong>Hamming distance:</strong> Direct comparison of categorical values</li>
                                <li><strong>Mahalanobis distance:</strong> Accounts for feature correlations</li>
                                <li><strong>Edit distances:</strong> Sequence and string comparisons</li>
                                <li><strong>Custom metrics:</strong> Domain-specific similarity measures</li>
                            </ul>
                        </div>
                    </div>

                    <h3>Taxonomy of Advanced Distance Metrics</h3>
                    <p>Advanced distance metrics can be categorized by the type of data they're designed for and the similarity concept they capture:</p>

                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Category</th>
                                <th>Metrics</th>
                                <th>Data Type</th>
                                <th>Key Concept</th>
                                <th>Primary Applications</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Angular Metrics</strong></td>
                                <td>Cosine, Angular distance</td>
                                <td>Continuous vectors</td>
                                <td>Direction vs magnitude</td>
                                <td>Text analysis, recommender systems</td>
                            </tr>
                            <tr>
                                <td><strong>Edit Distances</strong></td>
                                <td>Hamming, Levenshtein, Jaccard</td>
                                <td>Discrete sequences</td>
                                <td>Transformation cost</td>
                                <td>DNA sequences, spell checking</td>
                            </tr>
                            <tr>
                                <td><strong>Statistical Distances</strong></td>
                                <td>Mahalanobis, Bhattacharyya</td>
                                <td>Continuous features</td>
                                <td>Statistical relationships</td>
                                <td>Multivariate analysis, anomaly detection</td>
                            </tr>
                            <tr>
                                <td><strong>Correlation Distances</strong></td>
                                <td>Pearson, Spearman, Kendall</td>
                                <td>Time series, rankings</td>
                                <td>Co-variation patterns</td>
                                <td>Gene expression, financial data</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Cosine Similarity Section -->
                <div id="hamming" class="section-content">
                    <h2>Cosine Similarity: Angular Distance in High Dimensions</h2>
                    
                    <p>Cosine similarity is one of the most important and widely used similarity measures in machine learning, particularly for high-dimensional sparse data like text documents, user preferences, and recommendation systems. Unlike Euclidean distance, cosine similarity focuses on the angle between vectors rather than their magnitude.</p>

                    <h3>Mathematical Foundation</h3>
                    <p>Cosine similarity measures the cosine of the angle between two non-zero vectors, providing a measure of orientation rather than magnitude.</p>

                    <div class="formula-box">
                        <h3>Cosine Similarity Formula</h3>
                        <p>For two vectors x, y ∈ ℝⁿ, cosine similarity is defined as:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1.5rem 0; background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <strong>cos_sim(x, y) = (x · y) / (‖x‖₂ ‖y‖₂)</strong>
                        </div>
                        
                        <p>Expanded form:</p>
                        <div style="text-align: center; font-size: 1.1rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>cos_sim(x, y) = (Σᵢ₌₁ⁿ xᵢyᵢ) / (√(Σᵢ₌₁ⁿ xᵢ²) √(Σᵢ₌₁ⁿ yᵢ²))</strong>
                        </div>
                        
                        <h4>Related Measures:</h4>
                        <ul>
                            <li><strong>Cosine Distance:</strong> cos_dist(x, y) = 1 - cos_sim(x, y)</li>
                            <li><strong>Angular Distance:</strong> θ = arccos(cos_sim(x, y))</li>
                            <li><strong>Angular Similarity:</strong> 1 - θ/π (normalized to [0, 1])</li>
                        </ul>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Cosine Similarity Geometric Interpretation</h4>
                        <p><strong>Image Description:</strong> A 2D coordinate system showing three vectors from the origin: A (pointing right), B (pointing up-right at 45°), and C (pointing up). Angle measurements show: angle(A,B) = 45°, cos_sim(A,B) = 0.707; angle(A,C) = 90°, cos_sim(A,C) = 0; angle(B,C) = 45°, cos_sim(B,C) = 0.707. Vector magnitudes are different but angles are preserved.</p>
                        <p><em>This shows how cosine similarity measures angle between vectors, independent of their lengths</em></p>
                    </div>

                    <h3>Properties and Applications</h3>
                    <div class="property-box">
                        <h4>Key Properties</h4>
                        <ul>
                            <li><strong>Range:</strong> [-1, 1] for any real vectors</li>
                            <li><strong>Scale invariant:</strong> cos_sim(αx, βy) = cos_sim(x, y) for α, β > 0</li>
                            <li><strong>Directional:</strong> Captures vector orientation, not magnitude</li>
                            <li><strong>High-dimensional friendly:</strong> Less affected by curse of dimensionality</li>
                        </ul>
                        
                        <h4>Primary Applications</h4>
                        <ul>
                            <li><strong>Text Analysis:</strong> Document similarity using TF-IDF vectors</li>
                            <li><strong>Recommendation Systems:</strong> User-item and item-item similarity</li>
                            <li><strong>Information Retrieval:</strong> Query-document matching</li>
                            <li><strong>Computer Vision:</strong> Feature vector comparison</li>
                        </ul>
                    </div>
                </div>

                <!-- Hamming Distance Section -->
                <div id="mahalanobis" class="section-content">
                    <h2>Hamming Distance and Edit Distances</h2>
                    
                    <p>For categorical data, sequences, and discrete structures, traditional Lp distances often fail to capture meaningful similarity. Edit distances, led by Hamming distance, provide natural measures for comparing discrete sequences by counting the operations needed to transform one sequence into another.</p>

                    <h3>Hamming Distance: The Foundation</h3>
                    <p>Named after Richard Hamming, this distance measures the number of positions at which corresponding symbols differ between two strings of equal length.</p>

                    <div class="formula-box">
                        <h3>Hamming Distance Definition</h3>
                        <p>For two strings x, y of equal length n over alphabet Σ:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1.5rem 0; background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <strong>d_H(x, y) = Σᵢ₌₁ⁿ [xᵢ ≠ yᵢ]</strong>
                        </div>
                        
                        <p>Where [xᵢ ≠ yᵢ] is the Iverson bracket: 1 if xᵢ ≠ yᵢ, 0 otherwise.</p>
                        
                        <h4>Properties:</h4>
                        <ul>
                            <li><strong>Range:</strong> [0, n] where n is string length</li>
                            <li><strong>Valid metric:</strong> Satisfies all four metric axioms</li>
                            <li><strong>Discrete:</strong> Only integer values possible</li>
                            <li><strong>Position-dependent:</strong> Order matters in comparison</li>
                        </ul>
                    </div>

                    <h3>Levenshtein Distance: Edit Distance for Different Lengths</h3>
                    <p>While Hamming distance requires equal-length strings, Levenshtein distance allows insertions, deletions, and substitutions to compare strings of different lengths.</p>

                    <div class="theorem-box">
                        <h4>Levenshtein Distance Definition</h4>
                        <p>The minimum number of single-character edits (insertions, deletions, substitutions) needed to change one string into another.</p>
                        
                        <h5>Dynamic Programming Formulation:</h5>
                        <p>Let L(i, j) be the distance between the first i characters of string x and first j characters of string y:</p>
                        
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Base cases:</strong></p>
                            <ul style="margin: 0.5rem 0;">
                                <li>L(0, j) = j (insert j characters)</li>
                                <li>L(i, 0) = i (delete i characters)</li>
                            </ul>
                            
                            <p><strong>Recursive relation:</strong></p>
                            <div style="text-align: center; margin: 1rem 0;">
                                L(i, j) = min{<br>
                                L(i-1, j) + 1,      (deletion)<br>
                                L(i, j-1) + 1,      (insertion)<br>
                                L(i-1, j-1) + cost  (substitution)
                            </div>
                            <p>Where cost = 0 if xᵢ = yⱼ, 1 otherwise</p>
                        </div>
                    </div>

                    <h3>Applications</h3>
                    <div class="application-box">
                        <h4>Primary Use Cases</h4>
                        
                        <h5>Computer Science:</h5>
                        <ul>
                            <li><strong>Spell checking:</strong> Find closest dictionary words</li>
                            <li><strong>DNA sequence analysis:</strong> Genetic similarity and evolution</li>
                            <li><strong>Version control:</strong> File difference computation</li>
                            <li><strong>Plagiarism detection:</strong> Document similarity</li>
                        </ul>
                        
                        <h5>Bioinformatics:</h5>
                        <ul>
                            <li><strong>Sequence alignment:</strong> DNA, RNA, protein comparison</li>
                            <li><strong>Phylogenetic analysis:</strong> Evolutionary relationships</li>
                            <li><strong>Mutation detection:</strong> Genetic variations</li>
                            <li><strong>Drug discovery:</strong> Molecular similarity</li>
                        </ul>
                    </div>
                </div>

                <!-- Mahalanobis Distance Section -->
                <div id="edit" class="section-content">
                    <h2>Mahalanobis Distance: Statistical Distance for Correlated Data</h2>
                    
                    <p>Named after P.C. Mahalanobis, this distance metric accounts for correlations between variables and differences in variance. It's particularly valuable when features have different scales or when correlations between features contain important information.</p>

                    <h3>Mathematical Foundation</h3>
                    <p>The Mahalanobis distance generalizes the Euclidean distance by incorporating the covariance structure of the data.</p>

                    <div class="formula-box">
                        <h3>Mahalanobis Distance Definition</h3>
                        <p>For two points x, y ∈ ℝᵈ and covariance matrix Σ:</p>
                        
                        <div style="text-align: center; font-size: 1.3rem; margin: 1.5rem 0; background: white; padding: 1.5rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <strong>d_M(x, y) = √((x - y)ᵀ Σ⁻¹ (x - y))</strong>
                        </div>
                        
                        <h4>Key Components:</h4>
                        <ul>
                            <li><strong>Σ:</strong> Covariance matrix of the data distribution</li>
                            <li><strong>Σ⁻¹:</strong> Inverse covariance (precision matrix)</li>
                            <li><strong>(x - y):</strong> Difference vector between points</li>
                        </ul>
                        
                        <h4>Special Cases:</h4>
                        <ul>
                            <li><strong>Σ = I:</strong> Reduces to Euclidean distance</li>
                            <li><strong>Σ = σ²I:</strong> Scaled Euclidean distance</li>
                            <li><strong>Diagonal Σ:</strong> Standardized Euclidean distance</li>
                        </ul>
                    </div>

                    <h3>Geometric Interpretation</h3>
                    <p>The Mahalanobis distance transforms the space to account for the "shape" of the data distribution.</p>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Mahalanobis vs Euclidean Distance</h4>
                        <p><strong>Image Description:</strong> Two side-by-side 2D plots showing the same elliptical data distribution. Left plot shows Euclidean distance with circular contours that don't align with the data's natural orientation. Right plot shows Mahalanobis distance with elliptical contours that follow the data's covariance structure. Both plots include a reference point with distance contours radiating outward, demonstrating how Mahalanobis distance accounts for correlation and variance differences.</p>
                        <p><em>This shows how Mahalanobis distance adapts to the data's covariance structure</em></p>
                    </div>

                    <div class="theorem-box">
                        <h4>Properties of Mahalanobis Distance</h4>
                        
                        <h5>Statistical Properties:</h5>
                        <ul>
                            <li><strong>Scale invariant:</strong> Unaffected by linear transformations</li>
                            <li><strong>Rotation invariant:</strong> Accounts for principal component directions</li>
                            <li><strong>Unit-free:</strong> Automatically handles different variable scales</li>
                            <li><strong>Outlier sensitive:</strong> Identifies statistical outliers effectively</li>
                        </ul>
                        
                        <h5>Metric Properties:</h5>
                        <ul>
                            <li><strong>Valid metric:</strong> Satisfies all four metric axioms when Σ is positive definite</li>
                            <li><strong>Non-negativity:</strong> d_M(x, y) ≥ 0</li>
                            <li><strong>Symmetry:</strong> d_M(x, y) = d_M(y, x)</li>
                            <li><strong>Triangle inequality:</strong> Holds when Σ is positive definite</li>
                        </ul>
                        
                        <h5>Relationship to Multivariate Normal Distribution:</h5>
                        <p>For multivariate normal distribution N(μ, Σ), the squared Mahalanobis distance from point x to mean μ follows a χ² distribution with d degrees of freedom.</p>
                    </div>

                    <h3>Computational Considerations</h3>
                    <p>Computing Mahalanobis distance requires careful handling of the covariance matrix and its inverse.</p>

                    <div class="property-box">
                        <h4>Implementation Strategies</h4>
                        
                        <h5>Covariance Matrix Estimation:</h5>
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">compute_covariance</span>(<span style="color: #f8f8f2;">X</span>):
    <span style="color: #75715e;"># Sample covariance matrix</span>
    <span style="color: #f8f8f2;">n, d</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X.shape</span>
    <span style="color: #f8f8f2;">X_centered</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">np.mean(X, axis</span><span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span><span style="color: #f8f8f2;">)</span>
    <span style="color: #f8f8f2;">cov_matrix</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.dot(X_centered.T, X_centered)</span> <span style="color: #f92672;">/</span> <span style="color: #f92672;">(</span><span style="color: #f8f8f2;">n</span> <span style="color: #f92672;">-</span> <span style="color: #ae81ff;">1</span><span style="color: #f92672;">)</span>
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">cov_matrix</span>
                        </div>
                        
                        <h5>Efficient Distance Computation:</h5>
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">mahalanobis_distance</span>(<span style="color: #f8f8f2;">x, y, cov_inv</span>):
    <span style="color: #75715e;"># Using precomputed inverse</span>
    <span style="color: #f8f8f2;">diff</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">x</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">y</span>
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">np.sqrt(np.dot(diff, np.dot(cov_inv, diff)))</span>

<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">mahalanobis_distance_cholesky</span>(<span style="color: #f8f8f2;">x, y, L_inv</span>):
    <span style="color: #75715e;"># Using Cholesky decomposition for numerical stability</span>
    <span style="color: #f8f8f2;">diff</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">x</span> <span style="color: #f92672;">-</span> <span style="color: #f8f8f2;">y</span>
    <span style="color: #f8f8f2;">z</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.dot(L_inv, diff)</span>
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">np.linalg.norm(z)</span>
                        </div>
                        
                        <h5>Numerical Considerations:</h5>
                        <ul>
                            <li><strong>Singular covariance:</strong> Use regularization or pseudoinverse</li>
                            <li><strong>Ill-conditioned matrix:</strong> Cholesky decomposition for stability</li>
                            <li><strong>High dimensions:</strong> Regularized or shrinkage estimators</li>
                            <li><strong>Limited samples:</strong> Robust covariance estimation methods</li>
                        </ul>
                    </div>

                    <h3>Applications</h3>
                    <div class="application-box">
                        <h4>Primary Use Cases</h4>
                        
                        <h5>Statistical Analysis:</h5>
                        <ul>
                            <li><strong>Outlier detection:</strong> Identify multivariate outliers</li>
                            <li><strong>Quality control:</strong> Manufacturing process monitoring</li>
                            <li><strong>Anomaly detection:</strong> Network security and fraud detection</li>
                            <li><strong>Classification:</strong> Nearest neighbor with statistical distance</li>
                        </ul>
                        
                        <h5>Machine Learning:</h5>
                        <ul>
                            <li><strong>Feature space transformation:</strong> Whitening transformations</li>
                            <li><strong>Clustering:</strong> When features have different scales/correlations</li>
                            <li><strong>Similarity learning:</strong> Learning appropriate distance metrics</li>
                            <li><strong>Dimensionality reduction:</strong> PCA preprocessing alternative</li>
                        </ul>
                    </div>
                </div>

                <!-- Correlation-Based Metrics Section -->
                <div id="specialized" class="section-content">
                    <h2>Correlation-Based Distance Metrics</h2>
                    
                    <p>Correlation-based distances measure similarity based on the linear relationship between variables, making them particularly useful for time series analysis, gene expression data, and any application where the pattern of co-variation is more important than absolute values.</p>

                    <h3>Pearson Correlation Distance</h3>
                    <p>Based on the Pearson correlation coefficient, this metric captures linear relationships between variables.</p>

                    <div class="formula-box">
                        <h3>Correlation-Based Distance Formulations</h3>
                        
                        <h4>Pearson Correlation Distance:</h4>
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>d_pearson(x, y) = 1 - |r(x, y)|</strong>
                        </div>
                        
                        <p>Where the Pearson correlation coefficient is:</p>
                        <div style="text-align: center; font-size: 1.1rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>r(x, y) = Σᵢ(xᵢ - x̄)(yᵢ - ȳ) / √(Σᵢ(xᵢ - x̄)² Σᵢ(yᵢ - ȳ)²)</strong>
                        </div>
                        
                        <h4>Alternative Formulations:</h4>
                        <ul>
                            <li><strong>Signed distance:</strong> d = 1 - r(x, y) (preserves direction)</li>
                            <li><strong>Absolute distance:</strong> d = 1 - |r(x, y)| (ignores direction)</li>
                            <li><strong>Squared distance:</strong> d = 1 - r²(x, y) (related to R-squared)</li>
                        </ul>
                    </div>

                    <h3>Spearman and Kendall Distances</h3>
                    <p>Non-parametric correlation measures that capture monotonic relationships rather than just linear ones.</p>

                    <div class="theorem-box">
                        <h4>Rank-Based Correlation Distances</h4>
                        
                        <h5>Spearman Rank Correlation Distance:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p>Based on ranks rather than raw values:</p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>d_spearman(x, y) = 1 - ρ(rank(x), rank(y))</strong>
                            </div>
                            <p>Where ρ is the Pearson correlation of the ranks.</p>
                        </div>
                        
                        <h5>Kendall Tau Distance:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p>Based on concordant and discordant pairs:</p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>d_kendall(x, y) = 1 - τ(x, y)</strong>
                            </div>
                            <p>Where τ = (P - Q) / (P + Q), P = concordant pairs, Q = discordant pairs.</p>
                        </div>
                        
                        <h5>Properties Comparison:</h5>
                        <table class="comparison-table" style="margin-top: 1rem;">
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Captures</th>
                                    <th>Robust to Outliers</th>
                                    <th>Computational Complexity</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Pearson</strong></td>
                                    <td>Linear relationships</td>
                                    <td>No</td>
                                    <td>O(n)</td>
                                </tr>
                                <tr>
                                    <td><strong>Spearman</strong></td>
                                    <td>Monotonic relationships</td>
                                    <td>Yes</td>
                                    <td>O(n log n)</td>
                                </tr>
                                <tr>
                                    <td><strong>Kendall</strong></td>
                                    <td>Concordance patterns</td>
                                    <td>Yes</td>
                                    <td>O(n²) or O(n log n)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Correlation Distance Behavior</h4>
                        <p><strong>Image Description:</strong> A 3x3 grid of scatter plots showing different relationships between variables x and y. Each plot shows the same data with different correlation patterns: perfect positive linear (r=1), perfect negative linear (r=-1), no correlation (r=0), strong positive non-linear (monotonic), weak positive linear, strong negative non-linear, U-shaped relationship, random noise, and step function. Below each plot are the Pearson, Spearman, and Kendall correlation values, demonstrating how each metric responds differently to various relationship types.</p>
                        <p><em>This demonstrates how different correlation measures capture different types of relationships</em></p>
                    </div>

                    <h3>Applications in Time Series and Bioinformatics</h3>
                    <div class="application-box">
                        <h4>Domain-Specific Applications</h4>
                        
                        <h5>Time Series Analysis:</h5>
                        <ul>
                            <li><strong>Stock correlation:</strong> Portfolio diversification analysis</li>
                            <li><strong>Economic indicators:</strong> Leading/lagging relationships</li>
                            <li><strong>Climate data:</strong> Regional temperature correlations</li>
                            <li><strong>Sensor networks:</strong> Spatial correlation patterns</li>
                        </ul>
                        
                        <h5>Gene Expression Analysis:</h5>
                        <ul>
                            <li><strong>Co-expression networks:</strong> Gene regulatory relationships</li>
                            <li><strong>Pathway analysis:</strong> Functional gene groupings</li>
                            <li><strong>Disease classification:</strong> Expression pattern similarity</li>
                            <li><strong>Drug discovery:</strong> Compound effect patterns</li>
                        </ul>
                        
                        <h5>Implementation Example:</h5>
                        <div style="background: #1e1e1e; color: #f8f8f2; padding: 1rem; border-radius: 6px; font-family: 'Courier New', monospace; font-size: 0.9rem;">
<span style="color: #f92672;">def</span> <span style="color: #a6e22e;">correlation_distance_matrix</span>(<span style="color: #f8f8f2;">X, method</span><span style="color: #f92672;">=</span><span style="color: #e6db74;">'pearson'</span><span style="color: #f8f8f2;">)</span>:
    <span style="color: #75715e;"># Compute pairwise correlation distances</span>
    <span style="color: #f8f8f2;">n_samples</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">X.shape[</span><span style="color: #ae81ff;">0</span><span style="color: #f8f8f2;">]</span>
    <span style="color: #f8f8f2;">dist_matrix</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.zeros((n_samples, n_samples))</span>
    
    <span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">i</span> <span style="color: #f92672;">in</span> <span style="color: #66d9ef;">range</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">n_samples</span><span style="color: #f92672;">)</span>:
        <span style="color: #f92672;">for</span> <span style="color: #f8f8f2;">j</span> <span style="color: #f92672;">in</span> <span style="color: #66d9ef;">range</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">i</span><span style="color: #f92672;">+</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">,</span> <span style="color: #f8f8f2;">n_samples</span><span style="color: #f92672;">)</span>:
            <span style="color: #f92672;">if</span> <span style="color: #f8f8f2;">method</span> <span style="color: #f92672;">==</span> <span style="color: #e6db74;">'pearson'</span>:
                <span style="color: #f8f8f2;">corr</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">np.corrcoef(X[i], X[j])[</span><span style="color: #ae81ff;">0</span><span style="color: #f92672;">,</span> <span style="color: #ae81ff;">1</span><span style="color: #f8f8f2;">]</span>
            <span style="color: #f92672;">elif</span> <span style="color: #f8f8f2;">method</span> <span style="color: #f92672;">==</span> <span style="color: #e6db74;">'spearman'</span>:
                <span style="color: #f8f8f2;">corr</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">scipy.stats.spearmanr(X[i], X[j])[</span><span style="color: #ae81ff;">0</span><span style="color: #f8f8f2;">]</span>
            
            <span style="color: #f8f8f2;">dist_matrix[i, j]</span> <span style="color: #f92672;">=</span> <span style="color: #ae81ff;">1</span> <span style="color: #f92672;">-</span> <span style="color: #66d9ef;">abs</span><span style="color: #f92672;">(</span><span style="color: #f8f8f2;">corr</span><span style="color: #f92672;">)</span>
            <span style="color: #f8f8f2;">dist_matrix[j, i]</span> <span style="color: #f92672;">=</span> <span style="color: #f8f8f2;">dist_matrix[i, j]</span>
    
    <span style="color: #66d9ef;">return</span> <span style="color: #f8f8f2;">dist_matrix</span>
                        </div>
                    </div>
                </div>

                <!-- Specialized Metrics Section -->
                <div id="selection" class="section-content">
                    <h2>Specialized Distance Metrics</h2>
                    
                    <p>Beyond the standard metrics, many domain-specific distance measures have been developed to capture particular notions of similarity relevant to specific applications.</p>

                    <h3>Earth Mover's Distance (Wasserstein Distance)</h3>
                    <p>Measures the minimum cost to transform one distribution into another, interpreting distributions as piles of earth.</p>

                    <div class="formula-box">
                        <h3>Earth Mover's Distance</h3>
                        <p>For two probability distributions P and Q over metric space (M, d):</p>
                        
                        <div style="text-align: center; font-size: 1.2rem; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>EMD(P, Q) = min_{γ∈Γ(P,Q)} ∫∫ d(x, y) dγ(x, y)</strong>
                        </div>
                        
                        <p>Where Γ(P, Q) is the set of all couplings between P and Q.</p>
                        
                        <h4>Discrete Version:</h4>
                        <p>For discrete distributions with supports {x₁, ..., xₙ} and {y₁, ..., yₘ}:</p>
                        <div style="text-align: center; margin: 1rem 0; background: white; padding: 1rem; border-radius: 6px;">
                            <strong>EMD = min Σᵢⱼ dᵢⱼ fᵢⱼ</strong><br>
                            subject to: Σⱼ fᵢⱼ = aᵢ, Σᵢ fᵢⱼ = bⱼ, fᵢⱼ ≥ 0
                        </div>
                    </div>

                    <h3>Information-Theoretic Distances</h3>
                    <p>Based on information theory, these metrics measure difference in information content between distributions.</p>

                    <div class="theorem-box">
                        <h4>Information-Theoretic Measures</h4>
                        
                        <h5>Kullback-Leibler Divergence:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p>Measures information gained when updating from prior Q to posterior P:</p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>KL(P||Q) = Σᵢ P(i) log(P(i)/Q(i))</strong>
                            </div>
                            <p><strong>Note:</strong> Not symmetric, not a metric</p>
                        </div>
                        
                        <h5>Jensen-Shannon Divergence:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p>Symmetric version based on KL divergence:</p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>JS(P, Q) = ½KL(P||M) + ½KL(Q||M)</strong>
                            </div>
                            <p>Where M = ½(P + Q). √JS(P, Q) is a metric.</p>
                        </div>
                        
                        <h5>Mutual Information Distance:</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p>Measures shared information between variables:</p>
                            <div style="text-align: center; margin: 0.5rem 0;">
                                <strong>d_MI(X, Y) = H(X) + H(Y) - 2I(X, Y)</strong>
                            </div>
                            <p>Where H is entropy and I is mutual information.</p>
                        </div>
                    </div>

                    <h3>Graph-Based Distances</h3>
                    <p>For data represented as graphs or networks, specialized distance measures capture structural similarity.</p>

                    <div class="property-box">
                        <h4>Network Distance Measures</h4>
                        
                        <h5>Shortest Path Distance:</h5>
                        <ul>
                            <li><strong>Definition:</strong> Length of shortest path between nodes</li>
                            <li><strong>Algorithm:</strong> Dijkstra's algorithm for weighted graphs</li>
                            <li><strong>Properties:</strong> Valid metric on connected graphs</li>
                            <li><strong>Applications:</strong> Social networks, transportation networks</li>
                        </ul>
                        
                        <h5>Random Walk Distance:</h5>
                        <ul>
                            <li><strong>Definition:</strong> Expected hitting time between nodes</li>
                            <li><strong>Computation:</strong> Based on transition probabilities</li>
                            <li><strong>Properties:</strong> Captures global graph structure</li>
                            <li><strong>Applications:</strong> Web page ranking, protein interactions</li>
                        </ul>
                        
                        <h5>Spectral Distances:</h5>
                        <ul>
                            <li><strong>Definition:</strong> Based on graph Laplacian eigenvalues</li>
                            <li><strong>Computation:</strong> Eigendecomposition of Laplacian matrix</li>
                            <li><strong>Properties:</strong> Captures global connectivity patterns</li>
                            <li><strong>Applications:</strong> Community detection, graph clustering</li>
                        </ul>
                    </div>

                    <div class="application-box">
                        <h4>Specialized Applications</h4>
                        
                        <h5>Computer Vision:</h5>
                        <ul>
                            <li><strong>Histogram intersection:</strong> Color histogram comparison</li>
                            <li><strong>Structural similarity (SSIM):</strong> Image quality assessment</li>
                            <li><strong>Hausdorff distance:</strong> Shape comparison</li>
                            <li><strong>Chamfer distance:</strong> Point cloud alignment</li>
                        </ul>
                        
                        <h5>Natural Language Processing:</h5>
                        <ul>
                            <li><strong>Word Mover's Distance:</strong> Document similarity using word embeddings</li>
                            <li><strong>BLEU score distance:</strong> Machine translation evaluation</li>
                            <li><strong>Semantic similarity:</strong> WordNet-based measures</li>
                            <li><strong>Edit distance variants:</strong> Optimal string alignment</li>
                        </ul>
                        
                        <h5>Bioinformatics:</h5>
                        <ul>
                            <li><strong>Phylogenetic distances:</strong> Evolutionary relationships</li>
                            <li><strong>Protein structure distance:</strong> RMSD, GDT-TS</li>
                            <li><strong>Sequence alignment distances:</strong> BLAST E-values</li>
                            <li><strong>Metabolic pathway distance:</strong> Network-based measures</li>
                        </ul>
                    </div>
                </div>

                <!-- Metric Selection Guide Section -->
                <div id="demo" class="section-content">
                    <h2>Distance Metric Selection Guide</h2>
                    
                    <p>Choosing the right distance metric is crucial for clustering success. This guide provides decision frameworks and practical considerations for metric selection based on data characteristics and application requirements.</p>

                    <div class="selection-guide">
                        <h3>Decision Framework</h3>
                        
                        <h4>Step 1: Analyze Your Data</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Data Characteristic</th>
                                    <th>Recommended Metrics</th>
                                    <th>Avoid</th>
                                    <th>Reasoning</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>High-dimensional sparse</strong></td>
                                    <td>Cosine, Jaccard</td>
                                    <td>Euclidean, Manhattan</td>
                                    <td>Curse of dimensionality, many zeros</td>
                                </tr>
                                <tr>
                                    <td><strong>Different scales/units</strong></td>
                                    <td>Mahalanobis, Cosine</td>
                                    <td>Euclidean without normalization</td>
                                    <td>Scale invariance needed</td>
                                </tr>
                                <tr>
                                    <td><strong>Categorical/discrete</strong></td>
                                    <td>Hamming, Jaccard</td>
                                    <td>Euclidean, Cosine</td>
                                    <td>Continuous metrics inappropriate</td>
                                </tr>
                                <tr>
                                    <td><strong>Correlated features</strong></td>
                                    <td>Mahalanobis</td>
                                    <td>Euclidean</td>
                                    <td>Account for correlation structure</td>
                                </tr>
                                <tr>
                                    <td><strong>Time series</strong></td>
                                    <td>Correlation-based, DTW</td>
                                    <td>Point-wise Euclidean</td>
                                    <td>Temporal relationships important</td>
                                </tr>
                                <tr>
                                    <td><strong>Presence/absence</strong></td>
                                    <td>Jaccard, Dice</td>
                                    <td>Euclidean</td>
                                    <td>Binary nature of data</td>
                                </tr>
                            </tbody>
                        </table>
                        
                        <h4>Step 2: Consider Application Domain</h4>
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                            <div style="background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                                <h5>Text/NLP</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li><strong>First choice:</strong> Cosine similarity</li>
                                    <li><strong>Alternative:</strong> Jaccard for n-grams</li>
                                    <li><strong>Consider:</strong> Word Mover's Distance</li>
                                </ul>
                            </div>
                            
                            <div style="background: #e8f5e8; padding: 1rem; border-radius: 8px;">
                                <h5>Image/Computer Vision</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li><strong>Histograms:</strong> Chi-square, intersection</li>
                                    <li><strong>Features:</strong> Euclidean after normalization</li>
                                    <li><strong>Shapes:</strong> Hausdorff distance</li>
                                </ul>
                            </div>
                            
                            <div style="background: #fff8e1; padding: 1rem; border-radius: 8px;">
                                <h5>Bioinformatics</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li><strong>Sequences:</strong> Edit distances</li>
                                    <li><strong>Expression:</strong> Correlation-based</li>
                                    <li><strong>Networks:</strong> Graph distances</li>
                                </ul>
                            </div>
                            
                            <div style="background: #fce4ec; padding: 1rem; border-radius: 8px;">
                                <h5>Finance</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li><strong>Returns:</strong> Correlation distance</li>
                                    <li><strong>Risk:</strong> Mahalanobis distance</li>
                                    <li><strong>Ratios:</strong> Euclidean after scaling</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <h3>Practical Evaluation</h3>
                    <div class="property-box">
                        <h4>Metric Validation Strategies</h4>
                        
                        <h5>1. Visualization-Based Evaluation:</h5>
                        <ul>
                            <li><strong>Distance matrix heatmaps:</strong> Visual cluster structure</li>
                            <li><strong>MDS/t-SNE plots:</strong> 2D representation of distances</li>
                            <li><strong>Dendrogram analysis:</strong> Hierarchical clustering results</li>
                            <li><strong>Nearest neighbor inspection:</strong> Manual verification of similar points</li>
                        </ul>
                        
                        <h5>2. Clustering Quality Metrics:</h5>
                        <ul>
                            <li><strong>Silhouette score:</strong> Measure separation quality</li>
                            <li><strong>Calinski-Harabasz index:</strong> Cluster separation vs compactness</li>
                            <li><strong>Davies-Bouldin index:</strong> Average similarity between clusters</li>
                            <li><strong>Adjusted Rand Index:</strong> Compare with ground truth if available</li>
                        </ul>
                        
                        <h5>3. Computational Considerations:</h5>
                        <ul>
                            <li><strong>Time complexity:</strong> Algorithm scalability requirements</li>
                            <li><strong>Memory usage:</strong> Distance matrix storage needs</li>
                            <li><strong>Numerical stability:</strong> Precision requirements</li>
                            <li><strong>Implementation availability:</strong> Library support</li>
                        </ul>
                    </div>

                    <div class="theorem-box">
                        <h4>Common Pitfalls and Solutions</h4>
                        
                        <h5>Pitfall 1: Scale Domination</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Problem:</strong> Features with large scales dominate distance calculations</p>
                            <p><strong>Solution:</strong> Normalize features or use scale-invariant metrics (Cosine, Mahalanobis)</p>
                            <p><strong>Example:</strong> Age (0-100) vs Income ($0-$1M) in customer clustering</p>
                        </div>
                        
                        <h5>Pitfall 2: Curse of Dimensionality</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Problem:</strong> All distances become similar in high dimensions</p>
                            <p><strong>Solution:</strong> Use angular distances (Cosine) or reduce dimensionality first</p>
                            <p><strong>Example:</strong> Document clustering with TF-IDF vectors</p>
                        </div>
                        
                        <h5>Pitfall 3: Ignoring Data Structure</h5>
                        <div style="background: white; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
                            <p><strong>Problem:</strong> Using inappropriate metric for data type</p>
                            <p><strong>Solution:</strong> Match metric to data characteristics and domain knowledge</p>
                            <p><strong>Example:</strong> Using Euclidean distance for categorical variables</p>
                        </div>
                    </div>
                </div>

                <!-- Interactive Demos Section -->
                <div id="quiz" class="section-content">
                    <h2>Interactive Distance Metric Demonstrations</h2>
                    
                    <p>Explore how different distance metrics behave with various data types and distributions through interactive demonstrations.</p>

                    <div class="interactive-demo">
                        <h3>Demo 1: Distance Metric Comparison</h3>
                        <p>Compare how different metrics measure distances between points in 2D space.</p>
                        
                        <div class="demo-controls">
                            <label for="point1-x">Point 1 X:</label>
                            <input type="number" id="point1-x" value="0" min="-10" max="10" step="0.1">
                            <label for="point1-y">Point 1 Y:</label>
                            <input type="number" id="point1-y" value="0" min="-10" max="10" step="0.1">
                            
                            <label for="point2-x">Point 2 X:</label>
                            <input type="number" id="point2-x" value="3" min="-10" max="10" step="0.1">
                            <label for="point2-y">Point 2 Y:</label>
                            <input type="number" id="point2-y" value="4" min="-10" max="10" step="0.1">
                            
                            <button onclick="calculateDistances()" class="azbn-btn">Calculate Distances</button>
                        </div>
                        
                        <div id="distance-results" class="distance-result">
                            <h4>Distance Results:</h4>
                            <div id="distance-output">Click "Calculate Distances" to see results</div>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3>Demo 2: High-Dimensional Distance Behavior</h3>
                        <p>Observe how distance metrics behave as dimensionality increases.</p>
                        
                        <div class="demo-controls">
                            <label for="dimensions">Number of Dimensions:</label>
                            <input type="range" id="dimensions" min="2" max="100" value="10">
                            <span id="dim-value">10</span>
                            
                            <label for="num-points">Number of Random Points:</label>
                            <input type="range" id="num-points" min="10" max="1000" value="100">
                            <span id="points-value">100</span>
                            
                            <button onclick="generateHighDimDemo()" class="azbn-btn">Generate Demo</button>
                        </div>
                        
                        <div id="highdim-results" class="distance-result">
                            <h4>Distance Statistics:</h4>
                            <div id="highdim-output">Adjust parameters and click "Generate Demo"</div>
                        </div>
                    </div>

                    <div class="interactive-demo">
                        <h3>Demo 3: Correlation Distance Visualization</h3>
                        <p>See how correlation-based distances work with time series data.</p>
                        
                        <div class="demo-controls">
                            <label for="series-length">Series Length:</label>
                            <input type="range" id="series-length" min="10" max="100" value="50">
                            <span id="length-value">50</span>
                            
                            <label for="correlation">Target Correlation:</label>
                            <input type="range" id="correlation" min="-100" max="100" value="80">
                            <span id="corr-value">0.8</span>
                            
                            <button onclick="generateCorrelationDemo()" class="azbn-btn">Generate Series</button>
                        </div>
                        
                        <div id="correlation-results" class="distance-result">
                            <h4>Correlation Analysis:</h4>
                            <div id="correlation-output">Adjust parameters and click "Generate Series"</div>
                        </div>
                    </div>
                </div>

                <!-- Quiz Section -->
                <div id="quiz" class="section-content">
                    <h2>Knowledge Assessment Quiz</h2>
                    
                    <div class="quiz-question">
                        <h4>Question 1: When is cosine similarity most appropriate?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q1" value="a" id="q1a">
                            <label for="q1a">When features have the same scale</label><br>
                            <input type="radio" name="q1" value="b" id="q1b">
                            <label for="q1b">When direction matters more than magnitude</label><br>
                            <input type="radio" name="q1" value="c" id="q1c">
                            <label for="q1c">When data is categorical</label><br>
                            <input type="radio" name="q1" value="d" id="q1d">
                            <label for="q1d">When features are normally distributed</label>
                        </div>
                        <button onclick="checkAnswer(1, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q1-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 2: What does the Mahalanobis distance account for that Euclidean distance doesn't?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q2" value="a" id="q2a">
                            <label for="q2a">Non-linear relationships</label><br>
                            <input type="radio" name="q2" value="b" id="q2b">
                            <label for="q2b">Feature correlations and different variances</label><br>
                            <input type="radio" name="q2" value="c" id="q2c">
                            <label for="q2c">Categorical variables</label><br>
                            <input type="radio" name="q2" value="d" id="q2d">
                            <label for="q2d">Missing values</label>
                        </div>
                        <button onclick="checkAnswer(2, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q2-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 3: Which distance metric is most appropriate for binary/categorical data?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q3" value="a" id="q3a">
                            <label for="q3a">Euclidean distance</label><br>
                            <input type="radio" name="q3" value="b" id="q3b">
                            <label for="q3b">Cosine similarity</label><br>
                            <input type="radio" name="q3" value="c" id="q3c">
                            <label for="q3c">Hamming distance</label><br>
                            <input type="radio" name="q3" value="d" id="q3d">
                            <label for="q3d">Pearson correlation</label>
                        </div>
                        <button onclick="checkAnswer(3, 'c')" class="azbn-btn">Check Answer</button>
                        <div id="q3-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 4: What is a key limitation of cosine distance (1 - cosine similarity)?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q4" value="a" id="q4a">
                            <label for="q4a">It's computationally expensive</label><br>
                            <input type="radio" name="q4" value="b" id="q4b">
                            <label for="q4b">It violates the triangle inequality</label><br>
                            <input type="radio" name="q4" value="c" id="q4c">
                            <label for="q4c">It can't handle negative values</label><br>
                            <input type="radio" name="q4" value="d" id="q4d">
                            <label for="q4d">It requires normalized data</label>
                        </div>
                        <button onclick="checkAnswer(4, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q4-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div class="quiz-question">
                        <h4>Question 5: When would you choose Spearman correlation over Pearson correlation?</h4>
                        <div style="margin: 1rem 0;">
                            <input type="radio" name="q5" value="a" id="q5a">
                            <label for="q5a">When relationships are strictly linear</label><br>
                            <input type="radio" name="q5" value="b" id="q5b">
                            <label for="q5b">When data contains outliers or non-linear monotonic relationships</label><br>
                            <input type="radio" name="q5" value="c" id="q5c">
                            <label for="q5c">When computational speed is critical</label><br>
                            <input type="radio" name="q5" value="d" id="q5d">
                            <label for="q5d">When data is normally distributed</label>
                        </div>
                        <button onclick="checkAnswer(5, 'b')" class="azbn-btn">Check Answer</button>
                        <div id="q5-result" style="margin-top: 1rem;"></div>
                    </div>

                    <div style="margin: 2rem 0; padding: 1rem; background: #f0f8ff; border-radius: 6px;">
                        <h4>Quiz Summary</h4>
                        <div id="quiz-summary">Complete all questions to see your results</div>
                        <button onclick="calculateQuizScore()" class="azbn-btn" style="margin-top: 1rem;">Calculate Final Score</button>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <a href="/tutorials/clustering-course/chapter3" class="azbn-btn azbn-secondary" style="text-decoration: none;">← Chapter 3: Minkowski Distance</a>
                    <a href="/tutorials/clustering-course/chapter5" class="azbn-btn" style="text-decoration: none;">Chapter 5: K-Means Theory →</a>
                </div>
            </div>
        </section>
    </main>

    </main>

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Alireza Barzin Zanganeh</h3>
                    <p>ML Engineer & Data Scientist</p>
                    <p>Passionate about creating intelligent solutions through machine learning and data science.</p>
                </div>
                
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="/#projects">Projects</a></li>
                        <li><a href="/tutorials/">Tutorials</a></li>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/contact">Contact</a></li>
                    </ul>
                </div>
                
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="https://linkedin.com/in/alireza-barzin-zanganeh" target="_blank" rel="noopener" aria-label="LinkedIn">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://github.com/alireza-barzin" target="_blank" rel="noopener" aria-label="GitHub">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="mailto:alireza.barzin.zanganeh@gmail.com" aria-label="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2023 Alireza Barzin Zanganeh. All rights reserved.</p>
            </div>
        </div>
    </footer>
    <script>
        let quizAnswers = {};
        
        function showSection(sectionName, clickedElement) {
            // Hide all sections
            document.querySelectorAll('.content-section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            
            // Add active class to clicked button
            if (clickedElement) {
                clickedElement.classList.add('active');
            }
        }

        function calculateDistances() {
            const x1 = parseFloat(document.getElementById('point1-x').value);
            const y1 = parseFloat(document.getElementById('point1-y').value);
            const x2 = parseFloat(document.getElementById('point2-x').value);
            const y2 = parseFloat(document.getElementById('point2-y').value);
            
            const euclidean = Math.sqrt((x2-x1)**2 + (y2-y1)**2);
            const manhattan = Math.abs(x2-x1) + Math.abs(y2-y1);
            const chebyshev = Math.max(Math.abs(x2-x1), Math.abs(y2-y1));
            
            // Cosine similarity
            const dot = x1*x2 + y1*y2;
            const norm1 = Math.sqrt(x1*x1 + y1*y1);
            const norm2 = Math.sqrt(x2*x2 + y2*y2);
            const cosine = norm1 * norm2 > 0 ? dot / (norm1 * norm2) : 0;
            const cosineDistance = 1 - cosine;
            
            document.getElementById('distance-output').innerHTML = `
                <strong>Euclidean Distance:</strong> ${euclidean.toFixed(3)}<br>
                <strong>Manhattan Distance:</strong> ${manhattan.toFixed(3)}<br>
                <strong>Chebyshev Distance:</strong> ${chebyshev.toFixed(3)}<br>
                <strong>Cosine Similarity:</strong> ${cosine.toFixed(3)}<br>
                <strong>Cosine Distance:</strong> ${cosineDistance.toFixed(3)}
            `;
        }

        function generateHighDimDemo() {
            const dims = parseInt(document.getElementById('dimensions').value);
            const numPoints = parseInt(document.getElementById('num-points').value);
            
            // Generate random points
            const points = [];
            for (let i = 0; i < numPoints; i++) {
                const point = [];
                for (let j = 0; j < dims; j++) {
                    point.push(Math.random() * 2 - 1);
                }
                points.push(point);
            }
            
            // Calculate distances between first point and all others
            const euclideanDists = [];
            const cosineDists = [];
            
            for (let i = 1; i < numPoints; i++) {
                let eucDist = 0;
                let dot = 0;
                let norm1 = 0;
                let norm2 = 0;
                
                for (let j = 0; j < dims; j++) {
                    eucDist += (points[0][j] - points[i][j]) ** 2;
                    dot += points[0][j] * points[i][j];
                    norm1 += points[0][j] ** 2;
                    norm2 += points[i][j] ** 2;
                }
                
                euclideanDists.push(Math.sqrt(eucDist));
                const cosine = dot / (Math.sqrt(norm1) * Math.sqrt(norm2));
                cosineDists.push(1 - cosine);
            }
            
            const eucMean = euclideanDists.reduce((a, b) => a + b) / euclideanDists.length;
            const eucStd = Math.sqrt(euclideanDists.reduce((a, b) => a + (b - eucMean) ** 2, 0) / euclideanDists.length);
            const cosMean = cosineDists.reduce((a, b) => a + b) / cosineDists.length;
            const cosStd = Math.sqrt(cosineDists.reduce((a, b) => a + (b - cosMean) ** 2, 0) / cosineDists.length);
            
            document.getElementById('highdim-output').innerHTML = `
                <strong>Euclidean Distance:</strong><br>
                Mean: ${eucMean.toFixed(3)}, Std: ${eucStd.toFixed(3)}<br>
                <strong>Cosine Distance:</strong><br>
                Mean: ${cosMean.toFixed(3)}, Std: ${cosStd.toFixed(3)}<br>
                <em>As dimensions increase, Euclidean distances become more similar (higher mean, lower relative std)</em>
            `;
        }

        function generateCorrelationDemo() {
            const length = parseInt(document.getElementById('series-length').value);
            const targetCorr = parseFloat(document.getElementById('correlation').value) / 100;
            
            // Generate correlated time series
            const series1 = [];
            const series2 = [];
            
            for (let i = 0; i < length; i++) {
                const x = Math.random() * 2 - 1;
                const y = targetCorr * x + Math.sqrt(1 - targetCorr * targetCorr) * (Math.random() * 2 - 1);
                series1.push(x);
                series2.push(y);
            }
            
            // Calculate correlation
            const mean1 = series1.reduce((a, b) => a + b) / length;
            const mean2 = series2.reduce((a, b) => a + b) / length;
            
            let numerator = 0;
            let denom1 = 0;
            let denom2 = 0;
            
            for (let i = 0; i < length; i++) {
                numerator += (series1[i] - mean1) * (series2[i] - mean2);
                denom1 += (series1[i] - mean1) ** 2;
                denom2 += (series2[i] - mean2) ** 2;
            }
            
            const actualCorr = numerator / Math.sqrt(denom1 * denom2);
            const corrDistance = 1 - Math.abs(actualCorr);
            
            document.getElementById('correlation-output').innerHTML = `
                <strong>Target Correlation:</strong> ${targetCorr.toFixed(3)}<br>
                <strong>Actual Correlation:</strong> ${actualCorr.toFixed(3)}<br>
                <strong>Correlation Distance:</strong> ${corrDistance.toFixed(3)}<br>
                <em>Lower correlation distance indicates stronger linear relationship</em>
            `;
        }

        function checkAnswer(questionNum, correctAnswer) {
            const selectedAnswer = document.querySelector(`input[name="q${questionNum}"]:checked`);
            const resultDiv = document.getElementById(`q${questionNum}-result`);
            
            if (!selectedAnswer) {
                resultDiv.innerHTML = '<p style="color: orange;">Please select an answer first.</p>';
                return;
            }
            
            const isCorrect = selectedAnswer.value === correctAnswer;
            quizAnswers[questionNum] = isCorrect;
            
            if (isCorrect) {
                resultDiv.innerHTML = '<p style="color: green;">✅ Correct!</p>';
            } else {
                resultDiv.innerHTML = '<p style="color: red;">❌ Incorrect. Try again!</p>';
            }
        }

        function calculateQuizScore() {
            const totalQuestions = 5;
            const correctAnswers = Object.values(quizAnswers).filter(answer => answer === true).length;
            const percentage = Math.round((correctAnswers / totalQuestions) * 100);
            
            let message = `Score: ${correctAnswers}/${totalQuestions} (${percentage}%)`;
            
            if (percentage >= 90) {
                message += " - Excellent! 🏆";
            } else if (percentage >= 70) {
                message += " - Good job! 👍";
            } else {
                message += " - Review the material and try again.";
            }
            
            document.getElementById('quiz-summary').innerHTML = message;
        }

        // Update slider values
        document.getElementById('dimensions').addEventListener('input', function() {
            document.getElementById('dim-value').textContent = this.value;
        });
        
        document.getElementById('num-points').addEventListener('input', function() {
            document.getElementById('points-value').textContent = this.value;
        });
        
        document.getElementById('series-length').addEventListener('input', function() {
            document.getElementById('length-value').textContent = this.value;
        });
        
        document.getElementById('correlation').addEventListener('input', function() {
            document.getElementById('corr-value').textContent = (this.value / 100).toFixed(1);
        });

        function showSection(sectionName, clickedElement) {
            // Hide all section content
            document.querySelectorAll(".section-content").forEach(section => {
                section.classList.remove("active");
            });
            
            // Show the selected section
            const selectedSection = document.getElementById(sectionName);
            if (selectedSection) {
                selectedSection.classList.add("active");
            }
            
            // Update navigation buttons
            document.querySelectorAll(".section-nav button").forEach(button => {
                button.classList.remove("active");
            });
            
            if (clickedElement) {
                clickedElement.classList.add("active");
            }
            
            // Update section progress bar
            updateSectionProgress(sectionName);
        }
        
        function updateSectionProgress(sectionName) {
            const sections = ['cosine', 'hamming', 'mahalanobis', 'edit', 'specialized', 'selection', 'demo', 'quiz'];
            const currentIndex = sections.indexOf(sectionName);
            const progress = ((currentIndex + 1) / sections.length) * 100;
            
            const progressFill = document.querySelector('.section-progress-fill');
            if (progressFill) {
                progressFill.style.width = progress + '%';
            }
        }

        // Initialize with default section
        window.addEventListener('load', function() {
            showSection('cosine');
        });
    </script>

    <div class="navigation-buttons" style="text-align: center; margin: 2rem 0;">
        <a href="/tutorials/clustering-course/chapter3" class="azbn-btn azbn-secondary" style="text-decoration: none;">← Chapter 3: Minkowski Distance</a>
        <a href="/tutorials/clustering-course/chapter5" class="azbn-btn" style="text-decoration: none;">Chapter 5: K-Means Theory →</a>
    </div>

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Alireza Barzin Zanganeh</h3>
                    <p>ML Engineer & Data Scientist</p>
                    <p>Passionate about creating intelligent solutions through machine learning and data science.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="/#projects">Projects</a></li>
                        <li><a href="/tutorials/">Tutorials</a></li>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/contact">Contact</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="https://linkedin.com/in/alireza-barzin-zanganeh" target="_blank" rel="noopener" aria-label="LinkedIn">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://github.com/alireza-barzin" target="_blank" rel="noopener" aria-label="GitHub">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="mailto:alireza.barzin.zanganeh@gmail.com" aria-label="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2023 Alireza Barzin Zanganeh. All rights reserved.</p>
            </div>
        </div>
    </footer>
</body>
</html>