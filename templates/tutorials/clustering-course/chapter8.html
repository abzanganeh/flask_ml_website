<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 8: Hierarchical Clustering Theory - Comprehensive Clustering Analysis Course</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tutorials/clustering-course/clustering-course.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/main.css') }}">
    <style>
        .visualization-placeholder {
            background: linear-gradient(45deg, #f0f0f0, #e0e0e0);
            border: 2px dashed #999;
            padding: 2rem;
            text-align: center;
            border-radius: 8px;
            margin: 1rem 0;
            font-style: italic;
            color: #666;
        }
        .interactive-demo {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }
        .formula-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .theorem-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .proof-box {
            background: #fff8e1;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .property-box {
            background: #e8f5e8;
            border-left: 4px solid #4caf50;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .algorithm-step {
            background: #fff3e0;
            border: 1px solid #ff9800;
            border-radius: 6px;
            padding: 1rem;
            margin: 0.5rem 0;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }
        .comparison-table th {
            background-color: #f5f5f5;
            font-weight: bold;
        }
        .demo-controls {
            background: #e8f5e8;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        .data-point {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            display: inline-block;
            margin: 2px;
        }
        .cluster-visualization {
            border: 1px solid #ccc;
            background: white;
            padding: 1rem;
            margin: 1rem 0;
            min-height: 300px;
            position: relative;
        }
    </style>
</head>
<body>
    <header class="azbn-header">
        <nav class="azbn-nav" style="top: 50px;">
            <div class="azbn-container" style="display: flex; justify-content: space-between; align-items: center;">
                <a href="/tutorials/clustering-course" style="text-decoration: none; color: #4f46e5; display: flex; align-items: center; gap: 0.5rem;">
                    <span>Comprehensive Clustering Analysis Course</span>
                </a>
                <div class="azbn-links">
                    <a href="/">Home</a>
                    <a href="/#about">About</a>
                    <a href="/tutorials/">Tutorials</a>
                    <a href="/#projects">Projects</a>
                    <a href="/contact">Contact</a>
                </div>
            </div>
        </nav>
    </header>

    <main style="padding-top: 100px;">
        <!-- Comprehensive Chapter Header -->
        <div class="chapter-header">
            <div class="azbn-container">
                <h1 class="chapter-title">Chapter 8: Hierarchical Clustering Theory</h1>
                <p class="chapter-subtitle">Explore the mathematical foundations of hierarchical clustering, from agglomerative and divisive methods to ultrametric spaces and linkage criteria.</p>
                
                <!-- Chapter Progress Bar (8/15) -->
                <div class="chapter-progress">
                    <div class="chapter-progress-fill" style="width: 53.33%;"></div>
                </div>
                
                <!-- Chapter Navigation (All 15 chapters) -->
                <div class="chapter-navigation">
                    <a href="/tutorials/clustering-course/chapter1" class="chapter-nav-btn">1</a>
                    <a href="/tutorials/clustering-course/chapter2" class="chapter-nav-btn">2</a>
                    <a href="/tutorials/clustering-course/chapter3" class="chapter-nav-btn">3</a>
                    <a href="/tutorials/clustering-course/chapter4" class="chapter-nav-btn">4</a>
                    <a href="/tutorials/clustering-course/chapter5" class="chapter-nav-btn">5</a>
                    <a href="/tutorials/clustering-course/chapter6" class="chapter-nav-btn">6</a>
                    <a href="/tutorials/clustering-course/chapter7" class="chapter-nav-btn">7</a>
                    <a href="/tutorials/clustering-course/chapter8" class="chapter-nav-btn active">8</a>
                    <a href="/tutorials/clustering-course/chapter9" class="chapter-nav-btn">9</a>
                    <a href="/tutorials/clustering-course/chapter10" class="chapter-nav-btn">10</a>
                    <a href="/tutorials/clustering-course/chapter11" class="chapter-nav-btn">11</a>
                    <a href="/tutorials/clustering-course/chapter12" class="chapter-nav-btn">12</a>
                    <a href="/tutorials/clustering-course/chapter13" class="chapter-nav-btn">13</a>
                    <a href="/tutorials/clustering-course/chapter14" class="chapter-nav-btn">14</a>
                    <a href="/tutorials/clustering-course/chapter15" class="chapter-nav-btn">15</a>
                </div>
                
                <!-- Section Progress Bar -->
                <div class="section-progress">
                    <div class="section-progress-fill" style="width: 12.5%;"></div>
                </div>
                
                <!-- Section Navigation -->
                <div class="section-nav">
                    <button class="active" onclick="showSection('introduction', this)">Introduction</button>
                    <button onclick="showSection('mathematical', this)">Mathematical Theory</button>
                    <button onclick="showSection('linkage', this)">Linkage Criteria</button>
                    <button onclick="showSection('ward', this)">Ward's Method</button>
                    <button onclick="showSection('graph', this)">Graph Theory</button>
                    <button onclick="showSection('stability', this)">Stability Analysis</button>
                    <button onclick="showSection('demo', this)">Interactive Demo</button>
                    <button onclick="showSection('quiz', this)">Quiz</button>
                </div>
            </div>
        </div>

        <section class="azbn-section">
            <div class="azbn-container">
                <div class="learning-objectives-card">
                    <h2>Learning Objectives</h2>
                    <ul>
                        <li>Understand the mathematical foundations of hierarchical clustering</li>
                        <li>Master agglomerative and divisive clustering approaches</li>
                        <li>Learn ultrametric spaces and hierarchical consistency</li>
                        <li>Explore linkage criteria and their mathematical properties</li>
                        <li>Understand Ward's method and its statistical foundation</li>
                        <li>Analyze stability and perturbation theory in hierarchical clustering</li>
                    </ul>
                </div>

                <!-- Introduction Section -->
                <div id="introduction" class="section-content active">
                    <h2>Hierarchical Clustering: Revealing Nested Structure in Data</h2>
                    
                    <p>Hierarchical clustering creates a tree-like structure of clusters, allowing us to explore data at multiple levels of granularity. Unlike K-means which produces a single flat clustering, hierarchical methods reveal the nested structure of data.</p>

                    <h3>Core Concepts and Motivation</h3>
                    
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
                        <div class="property-box">
                            <h4>Advantages of Hierarchical Approach</h4>
                            <ul>
                                <li><strong>No need to specify k:</strong> Produces clustering for all possible numbers of clusters</li>
                                <li><strong>Deterministic:</strong> Same result every time (unlike K-means)</li>
                                <li><strong>Interpretable:</strong> Dendrogram provides clear visualization</li>
                                <li><strong>Flexible:</strong> Can handle clusters of different shapes and sizes</li>
                            </ul>
                        </div>
                        
                        <div class="property-box">
                            <h4>Types of Hierarchical Structure</h4>
                            <ul>
                                <li><strong>Agglomerative:</strong> Bottom-up approach, start with individual points</li>
                                <li><strong>Divisive:</strong> Top-down approach, start with all points</li>
                                <li><strong>Binary tree:</strong> Each merge/split involves exactly two clusters</li>
                                <li><strong>Ultrametric:</strong> Distance structure satisfies ultrametric inequality</li>
                            </ul>
                        </div>
                        
                        <div class="property-box">
                            <h4>Key Applications</h4>
                            <ul>
                                <li><strong>Biology:</strong> Phylogenetic trees, gene expression analysis</li>
                                <li><strong>Social Networks:</strong> Community detection at multiple scales</li>
                                <li><strong>Document Analysis:</strong> Topic modeling and text clustering</li>
                                <li><strong>Image Segmentation:</strong> Multi-scale region analysis</li>
                            </ul>
                        </div>
                    </div>

                    <h3>Mathematical Framework</h3>
                    
                    <div class="formula-box">
                        <h4>Mathematical Foundations</h4>
                        <p>Given a dataset X = {x₁, x₂, ..., xₙ}, hierarchical clustering produces a sequence of partitions:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>P₀ = {{ "{{" }}x₁}, {x₂}, ..., {xₙ}{{ "}}" }}</strong><br>
                            <strong>P₁, P₂, ..., Pₙ₋₁</strong><br>
                            <strong>Pₙ₋₁ = {X}</strong>
                        </div>
                        <p>Where each partition Pᵢ is a refinement of Pᵢ₊₁, and the sequence forms a hierarchy.</p>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>Visualization: Hierarchical Clustering Overview</h4>
                        <p>This section will contain an interactive visualization showing how hierarchical clustering builds a tree structure from individual data points, with different levels of granularity.</p>
                    </div>

                    <h3>Comparison with Partitional Clustering</h3>
                    
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Hierarchical Clustering</th>
                                <th>Partitional Clustering (K-means)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Number of clusters</strong></td>
                                <td>Produces all possible k values</td>
                                <td>Requires k to be specified</td>
                            </tr>
                            <tr>
                                <td><strong>Deterministic</strong></td>
                                <td>Yes, same result every time</td>
                                <td>No, depends on initialization</td>
                            </tr>
                            <tr>
                                <td><strong>Computational complexity</strong></td>
                                <td>O(n³) for most methods</td>
                                <td>O(nkt) where t is iterations</td>
                            </tr>
                            <tr>
                                <td><strong>Memory requirements</strong></td>
                                <td>O(n²) for distance matrix</td>
                                <td>O(nk) for centroids</td>
                            </tr>
                            <tr>
                                <td><strong>Interpretability</strong></td>
                                <td>High (dendrogram)</td>
                                <td>Medium (centroids)</td>
                            </tr>
                            <tr>
                                <td><strong>Scalability</strong></td>
                                <td>Poor for large datasets</td>
                                <td>Good for large datasets</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>Types of Hierarchical Clustering</h3>
                    
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 2rem; margin: 2rem 0;">
                        <div class="algorithm-step">
                            <h4>Agglomerative (Bottom-up)</h4>
                            <p><strong>Algorithm:</strong></p>
                            <ol>
                                <li>Start with each point as its own cluster</li>
                                <li>Find the two closest clusters</li>
                                <li>Merge them into a new cluster</li>
                                <li>Update distance matrix</li>
                                <li>Repeat until all points are in one cluster</li>
                            </ol>
                            <p><strong>Complexity:</strong> O(n³) for most linkage criteria</p>
                        </div>
                        
                        <div class="algorithm-step">
                            <h4>Divisive (Top-down)</h4>
                            <p><strong>Algorithm:</strong></p>
                            <ol>
                                <li>Start with all points in one cluster</li>
                                <li>Find the best way to split the cluster</li>
                                <li>Create two new clusters</li>
                                <li>Recursively split each cluster</li>
                                <li>Stop when each point is its own cluster</li>
                            </ol>
                            <p><strong>Complexity:</strong> O(2ⁿ) in worst case</p>
                        </div>
                    </div>

                    <h3>Key Challenges and Considerations</h3>
                    
                    <div class="theorem-box">
                        <h4>Fundamental Challenges</h4>
                        <ol>
                            <li><strong>Computational Complexity:</strong> Most hierarchical methods are O(n³), making them impractical for large datasets</li>
                            <li><strong>Memory Requirements:</strong> Need to store O(n²) distance matrix</li>
                            <li><strong>Linkage Choice:</strong> Different linkage criteria produce very different results</li>
                            <li><strong>Chaining Effect:</strong> Single linkage can create elongated clusters</li>
                            <li><strong>Noise Sensitivity:</strong> Outliers can significantly affect the hierarchy</li>
                            <li><strong>Irreversibility:</strong> Once clusters are merged, they cannot be split</li>
                        </ol>
                    </div>
                </div>

                <!-- Mathematical Theory Section -->
                <div id="mathematical" class="section-content">
                    <h2>Mathematical Theory of Hierarchical Clustering</h2>
                    
                    <h3>Ultrametric Spaces and Hierarchical Consistency</h3>
                    
                    <div class="formula-box">
                        <h4>Ultrametric Spaces</h4>
                        <p>An ultrametric space (X, d) satisfies the ultrametric inequality:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.2rem;">
                            <strong>d(x, y) ≤ max{d(x, z), d(y, z)}</strong>
                        </div>
                        <p>for all x, y, z ∈ X. This is stronger than the triangle inequality and has important implications for hierarchical clustering.</p>
                    </div>

                    <div class="theorem-box">
                        <h4>Ultrametric Properties</h4>
                        <ol>
                            <li><strong>All triangles are isosceles:</strong> In any triangle, at least two sides are equal</li>
                            <li><strong>Every point is a center:</strong> For any point x, the ball B(x, r) is also B(y, r) for any y ∈ B(x, r)</li>
                            <li><strong>Hierarchical structure:</strong> The space naturally induces a hierarchy of clusters</li>
                            <li><strong>Distance preservation:</strong> Distances in the hierarchy correspond to ultrametric distances</li>
                        </ol>
                    </div>

                    <h3>Hierarchical Clustering Axioms</h3>
                    
                    <div class="theorem-box">
                        <h4>Kleinberg's Impossibility Theorem (2003)</h4>
                        <p>No clustering algorithm can simultaneously satisfy all three of the following axioms:</p>
                        <ol>
                            <li><strong>Scale Invariance:</strong> f(D) = f(αD) for any α > 0</li>
                            <li><strong>Richness:</strong> For any partition Γ of X, there exists a distance function d such that f(d) = Γ</li>
                            <li><strong>Consistency:</strong> If d' is obtained from d by shrinking distances within clusters and expanding distances between clusters, then f(d') = f(d)</li>
                        </ol>
                        <p>This theorem shows that hierarchical clustering must make trade-offs between these desirable properties.</p>
                    </div>

                    <h3>Linkage Criteria and Their Properties</h3>
                    
                    <div class="formula-box">
                        <h4>Mathematical Formulation of Linkage Criteria</h4>
                        <p>For clusters C₁ and C₂, the linkage distance is defined as:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>d(C₁, C₂) = f({d(x, y) : x ∈ C₁, y ∈ C₂})</strong>
                        </div>
                        <p>where f is a function that aggregates pairwise distances:</p>
                        <ul>
                            <li><strong>Single linkage:</strong> f = min (minimum distance)</li>
                            <li><strong>Complete linkage:</strong> f = max (maximum distance)</li>
                            <li><strong>Average linkage:</strong> f = mean (average distance)</li>
                            <li><strong>Ward linkage:</strong> f = increase in within-cluster variance</li>
                        </ul>
                    </div>
                </div>

                <!-- Linkage Criteria Section -->
                <div id="linkage" class="section-content">
                    <h2>Linkage Criteria: Mathematical Properties and Applications</h2>
                    
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
                        <div class="algorithm-step">
                            <h4>Single Linkage</h4>
                            <p><strong>Formula:</strong> d(C₁, C₂) = min{d(x, y) : x ∈ C₁, y ∈ C₂}</p>
                            <p><strong>Properties:</strong></p>
                            <ul>
                                <li>Creates elongated clusters (chaining effect)</li>
                                <li>Sensitive to noise and outliers</li>
                                <li>Good for detecting connected components</li>
                                <li>Computationally efficient</li>
                            </ul>
                        </div>
                        
                        <div class="algorithm-step">
                            <h4>Complete Linkage</h4>
                            <p><strong>Formula:</strong> d(C₁, C₂) = max{d(x, y) : x ∈ C₁, y ∈ C₂}</p>
                            <p><strong>Properties:</strong></p>
                            <ul>
                                <li>Creates compact, spherical clusters</li>
                                <li>Less sensitive to noise</li>
                                <li>Can break large clusters prematurely</li>
                                <li>Good for detecting well-separated clusters</li>
                            </ul>
                        </div>
                        
                        <div class="algorithm-step">
                            <h4>Average Linkage</h4>
                            <p><strong>Formula:</strong> d(C₁, C₂) = (1/|C₁||C₂|) Σ d(x, y)</p>
                            <p><strong>Properties:</strong></p>
                            <ul>
                                <li>Balanced approach between single and complete</li>
                                <li>Good for most types of data</li>
                                <li>Computationally more expensive</li>
                                <li>Less sensitive to outliers than single linkage</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Ward's Method Section -->
                <div id="ward" class="section-content">
                    <h2>Ward's Method: Statistical Foundation</h2>
                    
                    <div class="formula-box">
                        <h3>Ward's Linkage Mathematical Derivation</h3>
                        <p>Ward's method minimizes the increase in within-cluster sum of squares (WSS) when merging clusters.</p>
                        
                        <h4>Objective Function:</h4>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>WSS = Σᵢ Σⱼ ||xⱼ - μᵢ||²</strong>
                        </div>
                        <p>where μᵢ is the centroid of cluster i.</p>
                        
                        <h4>Merge Criterion:</h4>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>ΔWSS = (|C₁||C₂|/(|C₁|+|C₂|)) ||μ₁ - μ₂||²</strong>
                        </div>
                        <p>Merge the pair of clusters that minimizes ΔWSS.</p>
                    </div>

                    <div class="theorem-box">
                        <h4>Properties of Ward's Method:</h4>
                        <ol>
                            <li><strong>Variance minimization:</strong> Each merge minimizes the increase in total within-cluster variance</li>
                            <li><strong>Spherical clusters:</strong> Tends to create clusters of similar size and shape</li>
                            <li><strong>Statistical foundation:</strong> Based on analysis of variance (ANOVA) principles</li>
                            <li><strong>Monotonicity:</strong> The distance between merged clusters increases monotonically</li>
                            <li><strong>Optimal for Gaussian data:</strong> Works well when clusters are approximately Gaussian</li>
                        </ol>
                    </div>
                </div>

                <!-- Graph Theory Section -->
                <div id="graph" class="section-content">
                    <h2>Graph-Theoretic Perspective</h2>
                    
                    <h3>Minimum Spanning Tree Connection</h3>
                    
                    <div class="theorem-box">
                        <h4>Single Linkage and MST</h4>
                        <p>Single linkage hierarchical clustering is equivalent to finding the minimum spanning tree (MST) of the complete graph with edge weights equal to pairwise distances.</p>
                        <p><strong>Algorithm:</strong></p>
                        <ol>
                            <li>Construct complete graph G with vertices as data points</li>
                            <li>Edge weights = pairwise distances</li>
                            <li>Find MST of G</li>
                            <li>Remove edges in order of decreasing weight</li>
                            <li>Each edge removal creates a new clustering level</li>
                        </ol>
                    </div>

                    <div class="visualization-placeholder">
                        <h4>Visualization: MST and Single Linkage Connection</h4>
                        <p>This section will show how the minimum spanning tree corresponds to the single linkage dendrogram, with interactive visualization of edge removal and cluster formation.</p>
                    </div>
                </div>

                <!-- Stability Analysis Section -->
                <div id="stability" class="section-content">
                    <h2>Stability and Perturbation Analysis</h2>
                    
                    <h3>Stability Theory</h3>
                    
                    <div class="formula-box">
                        <h4>Perturbation Analysis</h4>
                        <p>For a dataset X and its perturbation X' = X + ε, where ε is noise, we can analyze how the clustering changes:</p>
                        <div style="text-align: center; margin: 1rem 0; font-size: 1.1rem;">
                            <strong>Stability = 1 - (|C(X) Δ C(X')| / |C(X) ∪ C(X')|)</strong>
                        </div>
                        <p>where C(X) is the clustering of X and Δ denotes symmetric difference.</p>
                    </div>

                    <div class="theorem-box">
                        <h4>Stability Properties by Linkage Method</h4>
                        <ol>
                            <li><strong>Single linkage:</strong> Most sensitive to noise, can create long chains</li>
                            <li><strong>Complete linkage:</strong> More stable, but can break large clusters</li>
                            <li><strong>Average linkage:</strong> Moderate stability, good balance</li>
                            <li><strong>Ward's method:</strong> Generally stable, good for well-separated clusters</li>
                        </ol>
                    </div>
                </div>

                <!-- Interactive Demo Section -->
                <div id="demo" class="section-content">
                    <h2>Interactive Hierarchical Clustering Demo</h2>
                    
                    <div class="interactive-demo">
                        <h3>Demo Controls</h3>
                        <div class="demo-controls">
                            <label for="demo-dataset">Dataset:</label>
                            <select id="demo-dataset">
                                <option value="gaussian">Gaussian Clusters</option>
                                <option value="moons">Two Moons</option>
                                <option value="circles">Concentric Circles</option>
                                <option value="random">Random Points</option>
                            </select>
                            
                            <label for="demo-linkage">Linkage Method:</label>
                            <select id="demo-linkage">
                                <option value="single">Single Linkage</option>
                                <option value="complete">Complete Linkage</option>
                                <option value="average">Average Linkage</option>
                                <option value="ward">Ward's Method</option>
                            </select>
                            
                            <button onclick="runHierarchicalDemo()">Run Clustering</button>
                        </div>
                        
                        <div class="cluster-visualization" id="hierarchical-demo">
                            <p>Click "Run Clustering" to see the hierarchical clustering in action.</p>
                        </div>
                    </div>
                </div>

                <!-- Quiz Section -->
                <div id="quiz" class="section-content">
                    <h2>Knowledge Assessment Quiz</h2>
                    
                    <div class="interactive-demo">
                        <h3>Question 1: What is the main advantage of hierarchical clustering over K-means?</h3>
                        <div class="demo-controls">
                            <input type="radio" name="q1" value="a" id="q1a">
                            <label for="q1a">It's faster computationally</label><br>
                            <input type="radio" name="q1" value="b" id="q1b">
                            <label for="q1b">It doesn't require specifying the number of clusters</label><br>
                            <input type="radio" name="q1" value="c" id="q1c">
                            <label for="q1c">It always finds the global optimum</label><br>
                            <input type="radio" name="q1" value="d" id="q1d">
                            <label for="q1d">It works better with high-dimensional data</label>
                        </div>
                        
                        <h3>Question 2: Which linkage method is most sensitive to noise and outliers?</h3>
                        <div class="demo-controls">
                            <input type="radio" name="q2" value="a" id="q2a">
                            <label for="q2a">Complete linkage</label><br>
                            <input type="radio" name="q2" value="b" id="q2b">
                            <label for="q2b">Average linkage</label><br>
                            <input type="radio" name="q2" value="c" id="q2c">
                            <label for="q2c">Single linkage</label><br>
                            <input type="radio" name="q2" value="d" id="q2d">
                            <label for="q2d">Ward's method</label>
                        </div>
                        
                        <button onclick="checkQuizAnswers()">Check Answers</button>
                        <div id="quiz-results"></div>
                    </div>
                </div>

                <div class="chapter-navigation">
                    <button id="prev-chapter" class="nav-btn secondary" onclick="navigateToPreviousChapter()">
                        ← Previous Chapter: Optimal K Selection
                    </button>
                    <button id="next-chapter" class="nav-btn primary" onclick="navigateToNextChapter()">
                        Next Chapter: Linkage Criteria →
                    </button>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="footer-container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Alireza Barzin Zanganeh</h3>
                    <p>ML Engineer & Data Scientist</p>
                    <p>Passionate about creating intelligent solutions through machine learning and data science.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul>
                        <li><a href="/#projects">Projects</a></li>
                        <li><a href="/tutorials/">Tutorials</a></li>
                        <li><a href="/blog/">Blog</a></li>
                        <li><a href="/contact">Contact</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Connect</h4>
                    <div class="social-links">
                        <a href="https://linkedin.com/in/alireza-barzin-zanganeh" target="_blank" rel="noopener" aria-label="LinkedIn">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://github.com/alireza-barzin" target="_blank" rel="noopener" aria-label="GitHub">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="mailto:alireza.barzin.zanganeh@gmail.com" aria-label="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2023 Alireza Barzin Zanganeh. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script>
        function showSection(sectionName, clickedElement) {
            // Hide all sections
            document.querySelectorAll('.section-content').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionName).classList.add('active');
            
            // Update navigation buttons
            document.querySelectorAll('.section-nav button').forEach(button => {
                button.classList.remove('active');
            });
            
            // Add active class to clicked button
            if (clickedElement) {
                clickedElement.classList.add('active');
            }
            
            // Update section progress bar
            updateSectionProgress(sectionName);
        }

        function updateSectionProgress(sectionName) {
            const sections = ['introduction', 'mathematical', 'linkage', 'ward', 'graph', 'stability', 'demo', 'quiz'];
            const currentIndex = sections.indexOf(sectionName);
            const progress = ((currentIndex + 1) / sections.length) * 100;
            document.querySelector('.section-progress-fill').style.width = progress + '%';
        }

        function navigateToPreviousChapter() {
            window.location.href = '/tutorials/clustering-course/chapter7';
        }

        function navigateToNextChapter() {
            window.location.href = '/tutorials/clustering-course/chapter9';
        }

        function runHierarchicalDemo() {
            const dataset = document.getElementById('demo-dataset').value;
            const linkage = document.getElementById('demo-linkage').value;
            
            // Placeholder for demo functionality
            document.getElementById('hierarchical-demo').innerHTML = `
                <h4>Demo Results for ${dataset} with ${linkage} linkage</h4>
                <p>This would show an interactive hierarchical clustering visualization.</p>
                <p>Features would include:</p>
                <ul>
                    <li>Interactive dendrogram</li>
                    <li>Step-by-step clustering process</li>
                    <li>Different levels of granularity</li>
                    <li>Comparison of linkage methods</li>
                </ul>
            `;
        }

        function checkQuizAnswers() {
            const answers = {
                q1: 'b',
                q2: 'c'
            };
            
            let score = 0;
            let total = 2;
            
            for (let question in answers) {
                const selected = document.querySelector(`input[name="${question}"]:checked`);
                if (selected && selected.value === answers[question]) {
                    score++;
                }
            }
            
            const results = document.getElementById('quiz-results');
            results.innerHTML = `
                <h4>Quiz Results</h4>
                <p>You scored ${score}/${total} (${Math.round(score/total*100)}%)</p>
                <p><strong>Correct Answers:</strong></p>
                <ul>
                    <li>Question 1: b) It doesn't require specifying the number of clusters</li>
                    <li>Question 2: c) Single linkage</li>
                </ul>
            `;
        }

        // Initialize with default section
        window.addEventListener('load', function() {
            showSection('introduction');
        });
    </script>
</body>
</html>